{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MaskDINO panoptic segmentation implementation with Google Colab"
      ],
      "metadata": {
        "id": "-MgDarDwGfX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of MaskDINO from https://github.com/IDEA-Research/MaskDINO with Google Colab environment"
      ],
      "metadata": {
        "id": "WqrDij7CGqw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "1MlxcYsLHBWe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imEfi2H_NMTC"
      },
      "source": [
        "## Clone MaskDINO repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgpKcxzXtcV5",
        "outputId": "5b8e457b-7125-4302-fafc-c921c641fb77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MaskDINO'...\n",
            "remote: Enumerating objects: 395, done.\u001b[K\n",
            "remote: Counting objects: 100% (118/118), done.\u001b[K\n",
            "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
            "remote: Total 395 (delta 77), reused 59 (delta 59), pack-reused 277\u001b[K\n",
            "Receiving objects: 100% (395/395), 2.29 MiB | 23.25 MiB/s, done.\n",
            "Resolving deltas: 100% (189/189), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/IDEA-Research/MaskDINO.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6VgtE_8NhTr"
      },
      "source": [
        "## Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLjCTuzTtq1Y",
        "outputId": "d700d318-a384-4349-9483-85ee3e2eb25f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-8c9y3786\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-8c9y3786\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit eb96ee1d4752ff5896f623f738641fba9c755237\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (3.7.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.0.7)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.4.0)\n",
            "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (4.66.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.15.2)\n",
            "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Collecting omegaconf<2.4,>=2.1 (from detectron2==0.6)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core>=1.1 (from detectron2==0.6)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting black (from detectron2==0.6)\n",
            "  Downloading black-24.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (24.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.25.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.1)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->detectron2==0.6)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.2.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.10.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.62.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->detectron2==0.6) (3.2.2)\n",
            "Building wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp310-cp310-linux_x86_64.whl size=6149975 sha256=5ff11c4eda00bf837fd1eb8a8ef130fc98a8d2c28bdc293352c1d782e75518f3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qrqbtlpk/wheels/47/e5/15/94c80df2ba85500c5d76599cc307c0a7079d0e221bb6fc4375\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=2510a3da8260c379334c15b79e978152a569b891bf81dce58c0510510fdcfb70\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=ec3608fd4a2a1808fdfb5335a127e16049e05332daf48d047f0054287f1fad19\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built detectron2 fvcore antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 black-24.3.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.12.1 portalocker-2.8.2 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFeArkpMUuxw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c41eb5ae-29d0-4ec4-ee91-b0329a89027f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MaskDINO\n"
          ]
        }
      ],
      "source": [
        "%cd MaskDINO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmINMkY7t_hq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9448cad-0af6-4cd1-be03-c0599b63112a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (3.0.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.11.4)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.0.3)\n",
            "Collecting timm (from -r requirements.txt (line 4))\n",
            "  Downloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.9.0)\n",
            "Collecting submitit (from -r requirements.txt (line 6))\n",
            "  Downloading submitit-1.5.1-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.19.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.8.0.76)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->-r requirements.txt (line 2)) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 4)) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 4)) (0.17.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 4)) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 4)) (0.20.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 4)) (0.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from submitit->-r requirements.txt (line 6)) (2.2.1)\n",
            "Requirement already satisfied: typing_extensions>=3.7.4.2 in /usr/local/lib/python3.10/dist-packages (from submitit->-r requirements.txt (line 6)) (4.10.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (3.2.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (2024.2.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 4)) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 4)) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 4)) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 4)) (4.66.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm->-r requirements.txt (line 4)) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm->-r requirements.txt (line 4)) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->timm->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->timm->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->timm->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->timm->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->timm->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->timm->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch->timm->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->timm->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->timm->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch->timm->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch->timm->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm->-r requirements.txt (line 4)) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->timm->-r requirements.txt (line 4))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm->-r requirements.txt (line 4)) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 4)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 4)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 4)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 4)) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm->-r requirements.txt (line 4)) (1.3.0)\n",
            "Installing collected packages: submitit, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 submitit-1.5.1 timm-0.9.16\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxTUNTSBuI9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dca6ec9-99f1-4e4e-c29b-0a12c251bee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MaskDINO/maskdino/modeling/pixel_decoder/ops\n",
            "running build\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib.linux-x86_64-cpython-310\n",
            "creating build/lib.linux-x86_64-cpython-310/modules\n",
            "copying modules/__init__.py -> build/lib.linux-x86_64-cpython-310/modules\n",
            "copying modules/ms_deform_attn.py -> build/lib.linux-x86_64-cpython-310/modules\n",
            "creating build/lib.linux-x86_64-cpython-310/functions\n",
            "copying functions/__init__.py -> build/lib.linux-x86_64-cpython-310/functions\n",
            "copying functions/ms_deform_attn_func.py -> build/lib.linux-x86_64-cpython-310/functions\n",
            "running build_ext\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:500: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:415: UserWarning: The detected CUDA version (12.2) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:425: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.2\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "building 'MultiScaleDeformableAttention' extension\n",
            "creating build/temp.linux-x86_64-cpython-310\n",
            "creating build/temp.linux-x86_64-cpython-310/content\n",
            "creating build/temp.linux-x86_64-cpython-310/content/MaskDINO\n",
            "creating build/temp.linux-x86_64-cpython-310/content/MaskDINO/maskdino\n",
            "creating build/temp.linux-x86_64-cpython-310/content/MaskDINO/maskdino/modeling\n",
            "creating build/temp.linux-x86_64-cpython-310/content/MaskDINO/maskdino/modeling/pixel_decoder\n",
            "creating build/temp.linux-x86_64-cpython-310/content/MaskDINO/maskdino/modeling/pixel_decoder/ops\n",
            "creating build/temp.linux-x86_64-cpython-310/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src\n",
            "creating build/temp.linux-x86_64-cpython-310/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cpu\n",
            "creating build/temp.linux-x86_64-cpython-310/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -DWITH_CUDA -I/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c /content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cpu/ms_deform_attn_cpu.cpp -o build/temp.linux-x86_64-cpython-310/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cpu/ms_deform_attn_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "/usr/local/cuda/bin/nvcc -DWITH_CUDA -I/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c /content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu -o build/temp.linux-x86_64-cpython-310/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 -std=c++17\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_im2col_cuda.cuh(266)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_im2col_cuda(cudaStream_t, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 69 of /content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_im2col_cuda.cuh(767)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 139 of /content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_im2col_cuda.cuh(877)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 139 of /content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_im2col_cuda.cuh(336)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 139 of /content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_im2col_cuda.cuh(441)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 139 of /content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_im2col_cuda.cuh(549)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 139 of /content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_im2col_cuda.cuh(654)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 139 of /content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kat::Tensor ms_deform_attn_cuda_forward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:39:61:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   39 |     AT_ASSERTM(value.type().is_cuda(), \"value must\u001b[01;35m\u001b[K be a CUDA t\u001b[m\u001b[Kensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:40:70:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   40 |     AT_ASSERTM(spatial_shapes.type().is_cuda(), \"s\u001b[01;35m\u001b[Kpatial_shapes must be\u001b[m\u001b[K a CUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:41:73:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   41 |     AT_ASSERTM(level_start_index.type().is_cuda(),\u001b[01;35m\u001b[K \"level_start_index must\u001b[m\u001b[K be a CUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:42:68:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   42 |     AT_ASSERTM(sampling_loc.type().is_cuda(), \"sam\u001b[01;35m\u001b[Kpling_loc must be a\u001b[m\u001b[K CUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:43:67:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   43 |     AT_ASSERTM(attn_weight.type().is_cuda(), \"attn\u001b[01;35m\u001b[K_weight must be a \u001b[m\u001b[KCUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_\u001b[01;35m\u001b[KTYPES(value.ty\u001b[m\u001b[Kpe(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                              \u001b[01;35m\u001b[K~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:163:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K         \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:109:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  109 | \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "      | \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:1064:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:1150:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:1193:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:1226:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:1309:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:1467:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:2348:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:2434:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:2477:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:2509:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:2591:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:69:2748:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   69 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::vector<at::Tensor> ms_deform_attn_cuda_backward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:105:61:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  105 |     AT_ASSERTM(value.type().is_cuda(), \"value must\u001b[01;35m\u001b[K be a CUDA t\u001b[m\u001b[Kensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:106:70:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  106 |     AT_ASSERTM(spatial_shapes.type().is_cuda(), \"s\u001b[01;35m\u001b[Kpatial_shapes must be\u001b[m\u001b[K a CUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:107:73:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  107 |     AT_ASSERTM(level_start_index.type().is_cuda(),\u001b[01;35m\u001b[K \"level_start_index must\u001b[m\u001b[K be a CUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:108:68:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  108 |     AT_ASSERTM(sampling_loc.type().is_cuda(), \"sam\u001b[01;35m\u001b[Kpling_loc must be a\u001b[m\u001b[K CUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:109:67:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  109 |     AT_ASSERTM(attn_weight.type().is_cuda(), \"attn\u001b[01;35m\u001b[K_weight must be a \u001b[m\u001b[KCUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:110:67:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  110 |     AT_ASSERTM(grad_output.type().is_cuda(), \"grad\u001b[01;35m\u001b[K_output must be a \u001b[m\u001b[KCUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_\u001b[01;35m\u001b[KTYPES(value.ty\u001b[m\u001b[Kpe(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                              \u001b[01;35m\u001b[K~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:164:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K         \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:109:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  109 | \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "      | \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:1074:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:1100:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:1186:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:1229:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:1262:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:1345:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:1506:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:1590:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:1678:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:2620:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:2645:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:2731:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:2774:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:2806:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:2888:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:3048:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:3131:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.cu:139:3218:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  139 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -DWITH_CUDA -I/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c /content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/vision.cpp -o build/temp.linux-x86_64-cpython-310/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/vision.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "In file included from \u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/vision.cpp:16\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/ms_deform_attn.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kat::Tensor ms_deform_attn_forward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/ms_deform_attn.h:34:19:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   34 |     if (\u001b[01;35m\u001b[Kvalue.type()\u001b[m\u001b[K.is_cuda())\n",
            "      |         \u001b[01;35m\u001b[K~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cpu/ms_deform_attn_cpu.h:17\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/ms_deform_attn.h:18\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/vision.cpp:16\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/vision.cpp:16\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/ms_deform_attn.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::vector<at::Tensor> ms_deform_attn_backward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/ms_deform_attn.h:56:19:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   56 |     if (\u001b[01;35m\u001b[Kvalue.type()\u001b[m\u001b[K.is_cuda())\n",
            "      |         \u001b[01;35m\u001b[K~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cpu/ms_deform_attn_cpu.h:17\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/ms_deform_attn.h:18\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/vision.cpp:16\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cpu/ms_deform_attn_cpu.o build/temp.linux-x86_64-cpython-310/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/cuda/ms_deform_attn_cuda.o build/temp.linux-x86_64-cpython-310/content/MaskDINO/maskdino/modeling/pixel_decoder/ops/src/vision.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/MultiScaleDeformableAttention.cpython-310-x86_64-linux-gnu.so\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating MultiScaleDeformableAttention.egg-info\n",
            "writing MultiScaleDeformableAttention.egg-info/PKG-INFO\n",
            "writing dependency_links to MultiScaleDeformableAttention.egg-info/dependency_links.txt\n",
            "writing top-level names to MultiScaleDeformableAttention.egg-info/top_level.txt\n",
            "writing manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "reading manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "writing manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/modules\n",
            "copying build/lib.linux-x86_64-cpython-310/modules/__init__.py -> build/bdist.linux-x86_64/egg/modules\n",
            "copying build/lib.linux-x86_64-cpython-310/modules/ms_deform_attn.py -> build/bdist.linux-x86_64/egg/modules\n",
            "creating build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-310/functions/__init__.py -> build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-310/functions/ms_deform_attn_func.py -> build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-310/MultiScaleDeformableAttention.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "byte-compiling build/bdist.linux-x86_64/egg/modules/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/modules/ms_deform_attn.py to ms_deform_attn.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/functions/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/functions/ms_deform_attn_func.py to ms_deform_attn_func.cpython-310.pyc\n",
            "creating stub loader for MultiScaleDeformableAttention.cpython-310-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/MultiScaleDeformableAttention.py to MultiScaleDeformableAttention.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.MultiScaleDeformableAttention.cpython-310: module references __file__\n",
            "creating dist\n",
            "creating 'dist/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "creating /usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "Extracting MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding MultiScaleDeformableAttention 1.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "Processing dependencies for MultiScaleDeformableAttention==1.0\n",
            "Finished processing dependencies for MultiScaleDeformableAttention==1.0\n"
          ]
        }
      ],
      "source": [
        "%cd maskdino/modeling/pixel_decoder/ops\n",
        "!sh make.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqEU-9BsuOMZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9bc9278-4b21-44a3-ac6b-8a290e2012d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MaskDINO\n"
          ]
        }
      ],
      "source": [
        "%cd ../../../.."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset Preparation"
      ],
      "metadata": {
        "id": "uWyM2PwUHMvX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL9L_W4WNYqy"
      },
      "source": [
        "## Copy dataset folder from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCpOy9pMueW0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pv9hzwIAue3j"
      },
      "outputs": [],
      "source": [
        "%cp -av /content/drive/MyDrive/dataset_EfficientPS/data_maskdino/ /content/MaskDINO/datasets/coco/  #change the source based on your dataset directory in Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtK5QMYY2ZIG"
      },
      "source": [
        "## (OPTIONAL) Parts of program to be modified if you use custom dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vr-wAoh3Uxx"
      },
      "source": [
        "### 1. ImageFile.py in PIL (change TRUCATED from False to True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZjEs8JO2kEL"
      },
      "source": [
        "```\n",
        "#\n",
        "# The Python Imaging Library.\n",
        "# $Id$\n",
        "#\n",
        "# base class for image file handlers\n",
        "#\n",
        "# history:\n",
        "# 1995-09-09 fl   Created\n",
        "# 1996-03-11 fl   Fixed load mechanism.\n",
        "# 1996-04-15 fl   Added pcx/xbm decoders.\n",
        "# 1996-04-30 fl   Added encoders.\n",
        "# 1996-12-14 fl   Added load helpers\n",
        "# 1997-01-11 fl   Use encode_to_file where possible\n",
        "# 1997-08-27 fl   Flush output in _save\n",
        "# 1998-03-05 fl   Use memory mapping for some modes\n",
        "# 1999-02-04 fl   Use memory mapping also for \"I;16\" and \"I;16B\"\n",
        "# 1999-05-31 fl   Added image parser\n",
        "# 2000-10-12 fl   Set readonly flag on memory-mapped images\n",
        "# 2002-03-20 fl   Use better messages for common decoder errors\n",
        "# 2003-04-21 fl   Fall back on mmap/map_buffer if map is not available\n",
        "# 2003-10-30 fl   Added StubImageFile class\n",
        "# 2004-02-25 fl   Made incremental parser more robust\n",
        "#\n",
        "# Copyright (c) 1997-2004 by Secret Labs AB\n",
        "# Copyright (c) 1995-2004 by Fredrik Lundh\n",
        "#\n",
        "# See the README file for information on usage and redistribution.\n",
        "#\n",
        "\n",
        "import io\n",
        "import itertools\n",
        "import struct\n",
        "import sys\n",
        "\n",
        "from . import Image\n",
        "from ._util import is_path\n",
        "\n",
        "MAXBLOCK = 65536\n",
        "\n",
        "SAFEBLOCK = 1024 * 1024\n",
        "\n",
        "LOAD_TRUNCATED_IMAGES = True\n",
        "\"\"\"Whether or not to load truncated image files. User code may change this.\"\"\"\n",
        "\n",
        "ERRORS = {\n",
        "    -1: \"image buffer overrun error\",\n",
        "    -2: \"decoding error\",\n",
        "    -3: \"unknown error\",\n",
        "    -8: \"bad configuration\",\n",
        "    -9: \"out of memory error\",\n",
        "}\n",
        "\"\"\"\n",
        "Dict of known error codes returned from :meth:`.PyDecoder.decode`,\n",
        ":meth:`.PyEncoder.encode` :meth:`.PyEncoder.encode_to_pyfd` and\n",
        ":meth:`.PyEncoder.encode_to_file`.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "#\n",
        "# --------------------------------------------------------------------\n",
        "# Helpers\n",
        "\n",
        "\n",
        "def raise_oserror(error):\n",
        "    try:\n",
        "        msg = Image.core.getcodecstatus(error)\n",
        "    except AttributeError:\n",
        "        msg = ERRORS.get(error)\n",
        "    if not msg:\n",
        "        msg = f\"decoder error {error}\"\n",
        "    msg += \" when reading image file\"\n",
        "    raise OSError(msg)\n",
        "\n",
        "\n",
        "def _tilesort(t):\n",
        "    # sort on offset\n",
        "    return t[2]\n",
        "\n",
        "\n",
        "#\n",
        "# --------------------------------------------------------------------\n",
        "# ImageFile base class\n",
        "\n",
        "\n",
        "class ImageFile(Image.Image):\n",
        "    \"\"\"Base class for image file format handlers.\"\"\"\n",
        "\n",
        "    def __init__(self, fp=None, filename=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self._min_frame = 0\n",
        "\n",
        "        self.custom_mimetype = None\n",
        "\n",
        "        self.tile = None\n",
        "        \"\"\" A list of tile descriptors, or ``None`` \"\"\"\n",
        "\n",
        "        self.readonly = 1  # until we know better\n",
        "\n",
        "        self.decoderconfig = ()\n",
        "        self.decodermaxblock = MAXBLOCK\n",
        "\n",
        "        if is_path(fp):\n",
        "            # filename\n",
        "            self.fp = open(fp, \"rb\")\n",
        "            self.filename = fp\n",
        "            self._exclusive_fp = True\n",
        "        else:\n",
        "            # stream\n",
        "            self.fp = fp\n",
        "            self.filename = filename\n",
        "            # can be overridden\n",
        "            self._exclusive_fp = None\n",
        "\n",
        "        try:\n",
        "            try:\n",
        "                self._open()\n",
        "            except (\n",
        "                IndexError,  # end of data\n",
        "                TypeError,  # end of data (ord)\n",
        "                KeyError,  # unsupported mode\n",
        "                EOFError,  # got header but not the first frame\n",
        "                struct.error,\n",
        "            ) as v:\n",
        "                raise SyntaxError(v) from v\n",
        "\n",
        "            if not self.mode or self.size[0] <= 0 or self.size[1] <= 0:\n",
        "                msg = \"not identified by this driver\"\n",
        "                raise SyntaxError(msg)\n",
        "        except BaseException:\n",
        "            # close the file only if we have opened it this constructor\n",
        "            if self._exclusive_fp:\n",
        "                self.fp.close()\n",
        "            raise\n",
        "\n",
        "    def get_format_mimetype(self):\n",
        "        if self.custom_mimetype:\n",
        "            return self.custom_mimetype\n",
        "        if self.format is not None:\n",
        "            return Image.MIME.get(self.format.upper())\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        self.tile = []\n",
        "        super().__setstate__(state)\n",
        "\n",
        "    def verify(self):\n",
        "        \"\"\"Check file integrity\"\"\"\n",
        "\n",
        "        # raise exception if something's wrong.  must be called\n",
        "        # directly after open, and closes file when finished.\n",
        "        if self._exclusive_fp:\n",
        "            self.fp.close()\n",
        "        self.fp = None\n",
        "\n",
        "    def load(self):\n",
        "        \"\"\"Load image data based on tile list\"\"\"\n",
        "\n",
        "        if self.tile is None:\n",
        "            msg = \"cannot load this image\"\n",
        "            raise OSError(msg)\n",
        "\n",
        "        pixel = Image.Image.load(self)\n",
        "        if not self.tile:\n",
        "            return pixel\n",
        "\n",
        "        self.map = None\n",
        "        use_mmap = self.filename and len(self.tile) == 1\n",
        "        # As of pypy 2.1.0, memory mapping was failing here.\n",
        "        use_mmap = use_mmap and not hasattr(sys, \"pypy_version_info\")\n",
        "\n",
        "        readonly = 0\n",
        "\n",
        "        # look for read/seek overrides\n",
        "        try:\n",
        "            read = self.load_read\n",
        "            # don't use mmap if there are custom read/seek functions\n",
        "            use_mmap = False\n",
        "        except AttributeError:\n",
        "            read = self.fp.read\n",
        "\n",
        "        try:\n",
        "            seek = self.load_seek\n",
        "            use_mmap = False\n",
        "        except AttributeError:\n",
        "            seek = self.fp.seek\n",
        "\n",
        "        if use_mmap:\n",
        "            # try memory mapping\n",
        "            decoder_name, extents, offset, args = self.tile[0]\n",
        "            if (\n",
        "                decoder_name == \"raw\"\n",
        "                and len(args) >= 3\n",
        "                and args[0] == self.mode\n",
        "                and args[0] in Image._MAPMODES\n",
        "            ):\n",
        "                try:\n",
        "                    # use mmap, if possible\n",
        "                    import mmap\n",
        "\n",
        "                    with open(self.filename) as fp:\n",
        "                        self.map = mmap.mmap(fp.fileno(), 0, access=mmap.ACCESS_READ)\n",
        "                    if offset + self.size[1] * args[1] > self.map.size():\n",
        "                        # buffer is not large enough\n",
        "                        raise OSError\n",
        "                    self.im = Image.core.map_buffer(\n",
        "                        self.map, self.size, decoder_name, offset, args\n",
        "                    )\n",
        "                    readonly = 1\n",
        "                    # After trashing self.im,\n",
        "                    # we might need to reload the palette data.\n",
        "                    if self.palette:\n",
        "                        self.palette.dirty = 1\n",
        "                except (AttributeError, OSError, ImportError):\n",
        "                    self.map = None\n",
        "\n",
        "        self.load_prepare()\n",
        "        err_code = -3  # initialize to unknown error\n",
        "        if not self.map:\n",
        "            # sort tiles in file order\n",
        "            self.tile.sort(key=_tilesort)\n",
        "\n",
        "            try:\n",
        "                # FIXME: This is a hack to handle TIFF's JpegTables tag.\n",
        "                prefix = self.tile_prefix\n",
        "            except AttributeError:\n",
        "                prefix = b\"\"\n",
        "\n",
        "            # Remove consecutive duplicates that only differ by their offset\n",
        "            self.tile = [\n",
        "                list(tiles)[-1]\n",
        "                for _, tiles in itertools.groupby(\n",
        "                    self.tile, lambda tile: (tile[0], tile[1], tile[3])\n",
        "                )\n",
        "            ]\n",
        "            for decoder_name, extents, offset, args in self.tile:\n",
        "                seek(offset)\n",
        "                decoder = Image._getdecoder(\n",
        "                    self.mode, decoder_name, args, self.decoderconfig\n",
        "                )\n",
        "                try:\n",
        "                    decoder.setimage(self.im, extents)\n",
        "                    if decoder.pulls_fd:\n",
        "                        decoder.setfd(self.fp)\n",
        "                        err_code = decoder.decode(b\"\")[1]\n",
        "                    else:\n",
        "                        b = prefix\n",
        "                        while True:\n",
        "                            try:\n",
        "                                s = read(self.decodermaxblock)\n",
        "                            except (IndexError, struct.error) as e:\n",
        "                                # truncated png/gif\n",
        "                                if LOAD_TRUNCATED_IMAGES:\n",
        "                                    break\n",
        "                                else:\n",
        "                                    msg = \"image file is truncated\"\n",
        "                                    raise OSError(msg) from e\n",
        "\n",
        "                            if not s:  # truncated jpeg\n",
        "                                if LOAD_TRUNCATED_IMAGES:\n",
        "                                    break\n",
        "                                else:\n",
        "                                    msg = (\n",
        "                                        \"image file is truncated \"\n",
        "                                        f\"({len(b)} bytes not processed)\"\n",
        "                                    )\n",
        "                                    raise OSError(msg)\n",
        "\n",
        "                            b = b + s\n",
        "                            n, err_code = decoder.decode(b)\n",
        "                            if n < 0:\n",
        "                                break\n",
        "                            b = b[n:]\n",
        "                finally:\n",
        "                    # Need to cleanup here to prevent leaks\n",
        "                    decoder.cleanup()\n",
        "\n",
        "        self.tile = []\n",
        "        self.readonly = readonly\n",
        "\n",
        "        self.load_end()\n",
        "\n",
        "        if self._exclusive_fp and self._close_exclusive_fp_after_loading:\n",
        "            self.fp.close()\n",
        "        self.fp = None\n",
        "\n",
        "        if not self.map and not LOAD_TRUNCATED_IMAGES and err_code < 0:\n",
        "            # still raised if decoder fails to return anything\n",
        "            raise_oserror(err_code)\n",
        "\n",
        "        return Image.Image.load(self)\n",
        "\n",
        "    def load_prepare(self):\n",
        "        # create image memory if necessary\n",
        "        if not self.im or self.im.mode != self.mode or self.im.size != self.size:\n",
        "            self.im = Image.core.new(self.mode, self.size)\n",
        "        # create palette (optional)\n",
        "        if self.mode == \"P\":\n",
        "            Image.Image.load(self)\n",
        "\n",
        "    def load_end(self):\n",
        "        # may be overridden\n",
        "        pass\n",
        "\n",
        "    # may be defined for contained formats\n",
        "    # def load_seek(self, pos):\n",
        "    #     pass\n",
        "\n",
        "    # may be defined for blocked formats (e.g. PNG)\n",
        "    # def load_read(self, bytes):\n",
        "    #     pass\n",
        "\n",
        "    def _seek_check(self, frame):\n",
        "        if (\n",
        "            frame < self._min_frame\n",
        "            # Only check upper limit on frames if additional seek operations\n",
        "            # are not required to do so\n",
        "            or (\n",
        "                not (hasattr(self, \"_n_frames\") and self._n_frames is None)\n",
        "                and frame >= self.n_frames + self._min_frame\n",
        "            )\n",
        "        ):\n",
        "            msg = \"attempt to seek outside sequence\"\n",
        "            raise EOFError(msg)\n",
        "\n",
        "        return self.tell() != frame\n",
        "\n",
        "\n",
        "class StubImageFile(ImageFile):\n",
        "    \"\"\"\n",
        "    Base class for stub image loaders.\n",
        "\n",
        "    A stub loader is an image loader that can identify files of a\n",
        "    certain format, but relies on external code to load the file.\n",
        "    \"\"\"\n",
        "\n",
        "    def _open(self):\n",
        "        msg = \"StubImageFile subclass must implement _open\"\n",
        "        raise NotImplementedError(msg)\n",
        "\n",
        "    def load(self):\n",
        "        loader = self._load()\n",
        "        if loader is None:\n",
        "            msg = f\"cannot find loader for this {self.format} file\"\n",
        "            raise OSError(msg)\n",
        "        image = loader.load(self)\n",
        "        assert image is not None\n",
        "        # become the other object (!)\n",
        "        self.__class__ = image.__class__\n",
        "        self.__dict__ = image.__dict__\n",
        "        return image.load()\n",
        "\n",
        "    def _load(self):\n",
        "        \"\"\"(Hook) Find actual image loader.\"\"\"\n",
        "        msg = \"StubImageFile subclass must implement _load\"\n",
        "        raise NotImplementedError(msg)\n",
        "\n",
        "\n",
        "class Parser:\n",
        "    \"\"\"\n",
        "    Incremental image parser.  This class implements the standard\n",
        "    feed/close consumer interface.\n",
        "    \"\"\"\n",
        "\n",
        "    incremental = None\n",
        "    image = None\n",
        "    data = None\n",
        "    decoder = None\n",
        "    offset = 0\n",
        "    finished = 0\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        (Consumer) Reset the parser.  Note that you can only call this\n",
        "        method immediately after you've created a parser; parser\n",
        "        instances cannot be reused.\n",
        "        \"\"\"\n",
        "        assert self.data is None, \"cannot reuse parsers\"\n",
        "\n",
        "    def feed(self, data):\n",
        "        \"\"\"\n",
        "        (Consumer) Feed data to the parser.\n",
        "\n",
        "        :param data: A string buffer.\n",
        "        :exception OSError: If the parser failed to parse the image file.\n",
        "        \"\"\"\n",
        "        # collect data\n",
        "\n",
        "        if self.finished:\n",
        "            return\n",
        "\n",
        "        if self.data is None:\n",
        "            self.data = data\n",
        "        else:\n",
        "            self.data = self.data + data\n",
        "\n",
        "        # parse what we have\n",
        "        if self.decoder:\n",
        "\n",
        "            if self.offset > 0:\n",
        "                # skip header\n",
        "                skip = min(len(self.data), self.offset)\n",
        "                self.data = self.data[skip:]\n",
        "                self.offset = self.offset - skip\n",
        "                if self.offset > 0 or not self.data:\n",
        "                    return\n",
        "\n",
        "            n, e = self.decoder.decode(self.data)\n",
        "\n",
        "            if n < 0:\n",
        "                # end of stream\n",
        "                self.data = None\n",
        "                self.finished = 1\n",
        "                if e < 0:\n",
        "                    # decoding error\n",
        "                    self.image = None\n",
        "                    raise_oserror(e)\n",
        "                else:\n",
        "                    # end of image\n",
        "                    return\n",
        "            self.data = self.data[n:]\n",
        "\n",
        "        elif self.image:\n",
        "\n",
        "            # if we end up here with no decoder, this file cannot\n",
        "            # be incrementally parsed.  wait until we've gotten all\n",
        "            # available data\n",
        "            pass\n",
        "\n",
        "        else:\n",
        "\n",
        "            # attempt to open this file\n",
        "            try:\n",
        "                with io.BytesIO(self.data) as fp:\n",
        "                    im = Image.open(fp)\n",
        "            except OSError:\n",
        "                # traceback.print_exc()\n",
        "                pass  # not enough data\n",
        "            else:\n",
        "                flag = hasattr(im, \"load_seek\") or hasattr(im, \"load_read\")\n",
        "                if flag or len(im.tile) != 1:\n",
        "                    # custom load code, or multiple tiles\n",
        "                    self.decode = None\n",
        "                else:\n",
        "                    # initialize decoder\n",
        "                    im.load_prepare()\n",
        "                    d, e, o, a = im.tile[0]\n",
        "                    im.tile = []\n",
        "                    self.decoder = Image._getdecoder(im.mode, d, a, im.decoderconfig)\n",
        "                    self.decoder.setimage(im.im, e)\n",
        "\n",
        "                    # calculate decoder offset\n",
        "                    self.offset = o\n",
        "                    if self.offset <= len(self.data):\n",
        "                        self.data = self.data[self.offset :]\n",
        "                        self.offset = 0\n",
        "\n",
        "                self.image = im\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        self.close()\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"\n",
        "        (Consumer) Close the stream.\n",
        "\n",
        "        :returns: An image object.\n",
        "        :exception OSError: If the parser failed to parse the image file either\n",
        "                            because it cannot be identified or cannot be\n",
        "                            decoded.\n",
        "        \"\"\"\n",
        "        # finish decoding\n",
        "        if self.decoder:\n",
        "            # get rid of what's left in the buffers\n",
        "            self.feed(b\"\")\n",
        "            self.data = self.decoder = None\n",
        "            if not self.finished:\n",
        "                msg = \"image was incomplete\"\n",
        "                raise OSError(msg)\n",
        "        if not self.image:\n",
        "            msg = \"cannot parse this image\"\n",
        "            raise OSError(msg)\n",
        "        if self.data:\n",
        "            # incremental parsing not possible; reopen the file\n",
        "            # not that we have all data\n",
        "            with io.BytesIO(self.data) as fp:\n",
        "                try:\n",
        "                    self.image = Image.open(fp)\n",
        "                finally:\n",
        "                    self.image.load()\n",
        "        return self.image\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def _save(im, fp, tile, bufsize=0):\n",
        "    \"\"\"Helper to save image based on tile list\n",
        "\n",
        "    :param im: Image object.\n",
        "    :param fp: File object.\n",
        "    :param tile: Tile list.\n",
        "    :param bufsize: Optional buffer size\n",
        "    \"\"\"\n",
        "\n",
        "    im.load()\n",
        "    if not hasattr(im, \"encoderconfig\"):\n",
        "        im.encoderconfig = ()\n",
        "    tile.sort(key=_tilesort)\n",
        "    # FIXME: make MAXBLOCK a configuration parameter\n",
        "    # It would be great if we could have the encoder specify what it needs\n",
        "    # But, it would need at least the image size in most cases. RawEncode is\n",
        "    # a tricky case.\n",
        "    bufsize = max(MAXBLOCK, bufsize, im.size[0] * 4)  # see RawEncode.c\n",
        "    try:\n",
        "        fh = fp.fileno()\n",
        "        fp.flush()\n",
        "        _encode_tile(im, fp, tile, bufsize, fh)\n",
        "    except (AttributeError, io.UnsupportedOperation) as exc:\n",
        "        _encode_tile(im, fp, tile, bufsize, None, exc)\n",
        "    if hasattr(fp, \"flush\"):\n",
        "        fp.flush()\n",
        "\n",
        "\n",
        "def _encode_tile(im, fp, tile, bufsize, fh, exc=None):\n",
        "    for e, b, o, a in tile:\n",
        "        if o > 0:\n",
        "            fp.seek(o)\n",
        "        encoder = Image._getencoder(im.mode, e, a, im.encoderconfig)\n",
        "        try:\n",
        "            encoder.setimage(im.im, b)\n",
        "            if encoder.pushes_fd:\n",
        "                encoder.setfd(fp)\n",
        "                l, s = encoder.encode_to_pyfd()\n",
        "            else:\n",
        "                if exc:\n",
        "                    # compress to Python file-compatible object\n",
        "                    while True:\n",
        "                        l, s, d = encoder.encode(bufsize)\n",
        "                        fp.write(d)\n",
        "                        if s:\n",
        "                            break\n",
        "                else:\n",
        "                    # slight speedup: compress to real file object\n",
        "                    s = encoder.encode_to_file(fh, bufsize)\n",
        "            if s < 0:\n",
        "                msg = f\"encoder error {s} when writing image file\"\n",
        "                raise OSError(msg) from exc\n",
        "        finally:\n",
        "            encoder.cleanup()\n",
        "\n",
        "\n",
        "def _safe_read(fp, size):\n",
        "    \"\"\"\n",
        "    Reads large blocks in a safe way.  Unlike fp.read(n), this function\n",
        "    doesn't trust the user.  If the requested size is larger than\n",
        "    SAFEBLOCK, the file is read block by block.\n",
        "\n",
        "    :param fp: File handle.  Must implement a <b>read</b> method.\n",
        "    :param size: Number of bytes to read.\n",
        "    :returns: A string containing <i>size</i> bytes of data.\n",
        "\n",
        "    Raises an OSError if the file is truncated and the read cannot be completed\n",
        "\n",
        "    \"\"\"\n",
        "    if size <= 0:\n",
        "        return b\"\"\n",
        "    if size <= SAFEBLOCK:\n",
        "        data = fp.read(size)\n",
        "        if len(data) < size:\n",
        "            msg = \"Truncated File Read\"\n",
        "            raise OSError(msg)\n",
        "        return data\n",
        "    data = []\n",
        "    remaining_size = size\n",
        "    while remaining_size > 0:\n",
        "        block = fp.read(min(remaining_size, SAFEBLOCK))\n",
        "        if not block:\n",
        "            break\n",
        "        data.append(block)\n",
        "        remaining_size -= len(block)\n",
        "    if sum(len(d) for d in data) < size:\n",
        "        msg = \"Truncated File Read\"\n",
        "        raise OSError(msg)\n",
        "    return b\"\".join(data)\n",
        "\n",
        "\n",
        "class PyCodecState:\n",
        "    def __init__(self):\n",
        "        self.xsize = 0\n",
        "        self.ysize = 0\n",
        "        self.xoff = 0\n",
        "        self.yoff = 0\n",
        "\n",
        "    def extents(self):\n",
        "        return self.xoff, self.yoff, self.xoff + self.xsize, self.yoff + self.ysize\n",
        "\n",
        "\n",
        "class PyCodec:\n",
        "    def __init__(self, mode, *args):\n",
        "        self.im = None\n",
        "        self.state = PyCodecState()\n",
        "        self.fd = None\n",
        "        self.mode = mode\n",
        "        self.init(args)\n",
        "\n",
        "    def init(self, args):\n",
        "        \"\"\"\n",
        "        Override to perform codec specific initialization\n",
        "\n",
        "        :param args: Array of args items from the tile entry\n",
        "        :returns: None\n",
        "        \"\"\"\n",
        "        self.args = args\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"\n",
        "        Override to perform codec specific cleanup\n",
        "\n",
        "        :returns: None\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def setfd(self, fd):\n",
        "        \"\"\"\n",
        "        Called from ImageFile to set the Python file-like object\n",
        "\n",
        "        :param fd: A Python file-like object\n",
        "        :returns: None\n",
        "        \"\"\"\n",
        "        self.fd = fd\n",
        "\n",
        "    def setimage(self, im, extents=None):\n",
        "        \"\"\"\n",
        "        Called from ImageFile to set the core output image for the codec\n",
        "\n",
        "        :param im: A core image object\n",
        "        :param extents: a 4 tuple of (x0, y0, x1, y1) defining the rectangle\n",
        "            for this tile\n",
        "        :returns: None\n",
        "        \"\"\"\n",
        "\n",
        "        # following c code\n",
        "        self.im = im\n",
        "\n",
        "        if extents:\n",
        "            (x0, y0, x1, y1) = extents\n",
        "        else:\n",
        "            (x0, y0, x1, y1) = (0, 0, 0, 0)\n",
        "\n",
        "        if x0 == 0 and x1 == 0:\n",
        "            self.state.xsize, self.state.ysize = self.im.size\n",
        "        else:\n",
        "            self.state.xoff = x0\n",
        "            self.state.yoff = y0\n",
        "            self.state.xsize = x1 - x0\n",
        "            self.state.ysize = y1 - y0\n",
        "\n",
        "        if self.state.xsize <= 0 or self.state.ysize <= 0:\n",
        "            msg = \"Size cannot be negative\"\n",
        "            raise ValueError(msg)\n",
        "\n",
        "        if (\n",
        "            self.state.xsize + self.state.xoff > self.im.size[0]\n",
        "            or self.state.ysize + self.state.yoff > self.im.size[1]\n",
        "        ):\n",
        "            msg = \"Tile cannot extend outside image\"\n",
        "            raise ValueError(msg)\n",
        "\n",
        "\n",
        "class PyDecoder(PyCodec):\n",
        "    \"\"\"\n",
        "    Python implementation of a format decoder. Override this class and\n",
        "    add the decoding logic in the :meth:`decode` method.\n",
        "\n",
        "    See :ref:`Writing Your Own File Codec in Python<file-codecs-py>`\n",
        "    \"\"\"\n",
        "\n",
        "    _pulls_fd = False\n",
        "\n",
        "    @property\n",
        "    def pulls_fd(self):\n",
        "        return self._pulls_fd\n",
        "\n",
        "    def decode(self, buffer):\n",
        "        \"\"\"\n",
        "        Override to perform the decoding process.\n",
        "\n",
        "        :param buffer: A bytes object with the data to be decoded.\n",
        "        :returns: A tuple of ``(bytes consumed, errcode)``.\n",
        "            If finished with decoding return -1 for the bytes consumed.\n",
        "            Err codes are from :data:`.ImageFile.ERRORS`.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def set_as_raw(self, data, rawmode=None):\n",
        "        \"\"\"\n",
        "        Convenience method to set the internal image from a stream of raw data\n",
        "\n",
        "        :param data: Bytes to be set\n",
        "        :param rawmode: The rawmode to be used for the decoder.\n",
        "            If not specified, it will default to the mode of the image\n",
        "        :returns: None\n",
        "        \"\"\"\n",
        "\n",
        "        if not rawmode:\n",
        "            rawmode = self.mode\n",
        "        d = Image._getdecoder(self.mode, \"raw\", rawmode)\n",
        "        d.setimage(self.im, self.state.extents())\n",
        "        s = d.decode(data)\n",
        "\n",
        "        if s[0] >= 0:\n",
        "            msg = \"not enough image data\"\n",
        "            raise ValueError(msg)\n",
        "        if s[1] != 0:\n",
        "            msg = \"cannot decode image data\"\n",
        "            raise ValueError(msg)\n",
        "\n",
        "\n",
        "class PyEncoder(PyCodec):\n",
        "    \"\"\"\n",
        "    Python implementation of a format encoder. Override this class and\n",
        "    add the decoding logic in the :meth:`encode` method.\n",
        "\n",
        "    See :ref:`Writing Your Own File Codec in Python<file-codecs-py>`\n",
        "    \"\"\"\n",
        "\n",
        "    _pushes_fd = False\n",
        "\n",
        "    @property\n",
        "    def pushes_fd(self):\n",
        "        return self._pushes_fd\n",
        "\n",
        "    def encode(self, bufsize):\n",
        "        \"\"\"\n",
        "        Override to perform the encoding process.\n",
        "\n",
        "        :param bufsize: Buffer size.\n",
        "        :returns: A tuple of ``(bytes encoded, errcode, bytes)``.\n",
        "            If finished with encoding return 1 for the error code.\n",
        "            Err codes are from :data:`.ImageFile.ERRORS`.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def encode_to_pyfd(self):\n",
        "        \"\"\"\n",
        "        If ``pushes_fd`` is ``True``, then this method will be used,\n",
        "        and ``encode()`` will only be called once.\n",
        "\n",
        "        :returns: A tuple of ``(bytes consumed, errcode)``.\n",
        "            Err codes are from :data:`.ImageFile.ERRORS`.\n",
        "        \"\"\"\n",
        "        if not self.pushes_fd:\n",
        "            return 0, -8  # bad configuration\n",
        "        bytes_consumed, errcode, data = self.encode(0)\n",
        "        if data:\n",
        "            self.fd.write(data)\n",
        "        return bytes_consumed, errcode\n",
        "\n",
        "    def encode_to_file(self, fh, bufsize):\n",
        "        \"\"\"\n",
        "        :param fh: File handle.\n",
        "        :param bufsize: Buffer size.\n",
        "\n",
        "        :returns: If finished successfully, return 0.\n",
        "            Otherwise, return an error code. Err codes are from\n",
        "            :data:`.ImageFile.ERRORS`.\n",
        "        \"\"\"\n",
        "        errcode = 0\n",
        "        while errcode == 0:\n",
        "            status, errcode, buf = self.encode(bufsize)\n",
        "            if status > 0:\n",
        "                fh.write(buf[status:])\n",
        "        return errcode\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L1VAzgh3Zgv"
      },
      "source": [
        "### 2. builtin_meta.py in detectron2/data/datasets (edit the categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Number of class in MaskDINO config file"
      ],
      "metadata": {
        "id": "_dWiLG45IvSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Panoptic API"
      ],
      "metadata": {
        "id": "iGDXz7cmHnpr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFIKiU3OVbjm"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/cocodataset/panopticapi.gitb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NH0C9KwNoqN"
      },
      "source": [
        "## Dataset Preprocessing to prepare semantic annotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-55DOe5yfiE1",
        "outputId": "b2c4133e-0eb1-4130-94ab-49591845c0b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 13: 11, 14: 12, 15: 13, 16: 14, 17: 15, 18: 16, 19: 17, 20: 18, 21: 19, 22: 20, 23: 21, 24: 22, 25: 23, 27: 24, 28: 25, 31: 26, 32: 27, 33: 28, 34: 29, 35: 30, 36: 31, 37: 32, 38: 33, 39: 34, 40: 35, 41: 36, 42: 37, 43: 38, 44: 39, 46: 40, 47: 41, 48: 42, 49: 43, 50: 44, 51: 45, 52: 46, 53: 47, 54: 48, 55: 49, 56: 50, 57: 51, 58: 52, 59: 53, 60: 54, 61: 55, 62: 56, 63: 57, 64: 58, 65: 59, 67: 60, 70: 61, 72: 62, 73: 63, 74: 64, 75: 65, 76: 66, 77: 67, 78: 68, 79: 69, 80: 70, 81: 71, 82: 72, 84: 73, 85: 74, 86: 75, 87: 76, 88: 77, 89: 78, 90: 79, 92: 80, 93: 81, 95: 82, 100: 83, 107: 84, 109: 85, 112: 86, 118: 87, 119: 88, 122: 89, 125: 90, 128: 91, 130: 92, 133: 93, 138: 94, 141: 95, 144: 96, 145: 97, 147: 98, 148: 99, 149: 100, 151: 101, 154: 102, 155: 103, 156: 104, 159: 105, 161: 106, 166: 107, 168: 108, 171: 109, 175: 110, 176: 111, 177: 112, 178: 113, 180: 114, 181: 115, 184: 116, 185: 117, 186: 118, 187: 119, 188: 120, 189: 121, 190: 122, 191: 123, 192: 124, 193: 125, 194: 126, 195: 127, 196: 128, 197: 129, 198: 130, 199: 131, 200: 132, 201: 133, 202: 134, 203: 135, 204: 136, 205: 137, 206: 138, 207: 139, 208: 140, 209: 141}\n",
            "Start writing to datasets/coco/panoptic_semseg_val2017 ...\n",
            "Finished. time: 8.40s\n",
            "{1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 13: 11, 14: 12, 15: 13, 16: 14, 17: 15, 18: 16, 19: 17, 20: 18, 21: 19, 22: 20, 23: 21, 24: 22, 25: 23, 27: 24, 28: 25, 31: 26, 32: 27, 33: 28, 34: 29, 35: 30, 36: 31, 37: 32, 38: 33, 39: 34, 40: 35, 41: 36, 42: 37, 43: 38, 44: 39, 46: 40, 47: 41, 48: 42, 49: 43, 50: 44, 51: 45, 52: 46, 53: 47, 54: 48, 55: 49, 56: 50, 57: 51, 58: 52, 59: 53, 60: 54, 61: 55, 62: 56, 63: 57, 64: 58, 65: 59, 67: 60, 70: 61, 72: 62, 73: 63, 74: 64, 75: 65, 76: 66, 77: 67, 78: 68, 79: 69, 80: 70, 81: 71, 82: 72, 84: 73, 85: 74, 86: 75, 87: 76, 88: 77, 89: 78, 90: 79, 92: 80, 93: 81, 95: 82, 100: 83, 107: 84, 109: 85, 112: 86, 118: 87, 119: 88, 122: 89, 125: 90, 128: 91, 130: 92, 133: 93, 138: 94, 141: 95, 144: 96, 145: 97, 147: 98, 148: 99, 149: 100, 151: 101, 154: 102, 155: 103, 156: 104, 159: 105, 161: 106, 166: 107, 168: 108, 171: 109, 175: 110, 176: 111, 177: 112, 178: 113, 180: 114, 181: 115, 184: 116, 185: 117, 186: 118, 187: 119, 188: 120, 189: 121, 190: 122, 191: 123, 192: 124, 193: 125, 194: 126, 195: 127, 196: 128, 197: 129, 198: 130, 199: 131, 200: 132, 201: 133, 202: 134, 203: 135, 204: 136, 205: 137, 206: 138, 207: 139, 208: 140, 209: 141}\n",
            "Start writing to datasets/coco/panoptic_semseg_train2017 ...\n",
            "Finished. time: 10.39s\n"
          ]
        }
      ],
      "source": [
        "!python datasets/prepare_coco_semantic_annos_from_panoptic_annos.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "K6OaMsJCJgQn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8I8tgAdvr4T"
      },
      "source": [
        "## (Optional) Additional Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRgFgQf0vgrT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import albumentations as A\n",
        "\n",
        "augmentation_pipeline_decrease_contrast = A.Compose([              #change based on your custom pipeline\n",
        "    A.RandomBrightnessContrast(p=1, contrast_limit=(-0.5, -0.5)),\n",
        "])\n",
        "\n",
        "augmentation_pipeline_increase_contrast = A.Compose([\n",
        "    A.RandomBrightnessContrast(p=1, contrast_limit=(0.5, 0.5)),\n",
        "])\n",
        "\n",
        "# Path to the folder containing images\n",
        "folder_path = \"datasets/coco/train2017/\"\n",
        "\n",
        "# Iterate through the images in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Assuming images are JPG or PNG format\n",
        "        # Read the image\n",
        "        image_path = os.path.join(folder_path, filename)\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Apply augmentation pipeline to the image\n",
        "        augmented_image = augmentation_pipeline_increase_contrast(image=image)['image']\n",
        "\n",
        "        # Save the augmented image with the same filename\n",
        "        cv2.imwrite(image_path, augmented_image)\n",
        "\n",
        "        print(f\"Augmented image saved: {image_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RtRBnK5zSDu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import albumentations as A\n",
        "\n",
        "augmentation_pipeline_decrease_contrast = A.Compose([           #change based on your custom pipeline\n",
        "    A.RandomBrightnessContrast(p=1, contrast_limit=(-0.5, -0.5)),\n",
        "])\n",
        "\n",
        "augmentation_pipeline_increase_contrast = A.Compose([\n",
        "    A.RandomBrightnessContrast(p=1, contrast_limit=(0.5, 0.5)),\n",
        "])\n",
        "\n",
        "# Path to the folder containing images\n",
        "folder_path = \"datasets/coco/val2017/\"\n",
        "\n",
        "# Iterate through the images in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Assuming images are JPG or PNG format\n",
        "        # Read the image\n",
        "        image_path = os.path.join(folder_path, filename)\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Apply augmentation pipeline to the image\n",
        "        augmented_image = augmentation_pipeline_increase_contrast(image=image)['image']\n",
        "\n",
        "        # Save the augmented image with the same filename\n",
        "        cv2.imwrite(image_path, augmented_image)\n",
        "\n",
        "        print(f\"Augmented image saved: {image_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSyUMmCBPaTT"
      },
      "source": [
        "## Training from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucpXKX8cej38",
        "outputId": "c42e6099-1e8a-4408-c041-ffc63b3d9a7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/MaskDINO/maskdino/modeling/criterion.py:344: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  if self.dn is not \"no\" and mask_dict is not None:\n",
            "Command Line Args: Namespace(config_file='configs/coco/panoptic-segmentation/maskdino_R50_bs16_50ep_3s_dowsample1_2048.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:6076', opts=['SOLVER.IMS_PER_BATCH', '2', 'SOLVER.BASE_LR', '0.0000125'], EVAL_FLAG=1)\n",
            "pwd: /content/MaskDINO\n",
            "\u001b[32m[01/31 10:30:49 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
            "\u001b[32m[01/31 10:30:50 detectron2]: \u001b[0mEnvironment info:\n",
            "-------------------------------  -----------------------------------------------------------------\n",
            "sys.platform                     linux\n",
            "Python                           3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "numpy                            1.23.5\n",
            "detectron2                       0.6 @/usr/local/lib/python3.10/dist-packages/detectron2\n",
            "Compiler                         GCC 11.4\n",
            "CUDA compiler                    CUDA 12.2\n",
            "detectron2 arch flags            7.0\n",
            "DETECTRON2_ENV_MODULE            <not set>\n",
            "PyTorch                          2.1.0+cu121 @/usr/local/lib/python3.10/dist-packages/torch\n",
            "PyTorch debug build              False\n",
            "torch._C._GLIBCXX_USE_CXX11_ABI  False\n",
            "GPU available                    Yes\n",
            "GPU 0                            Tesla V100-SXM2-16GB (arch=7.0)\n",
            "Driver version                   535.104.05\n",
            "CUDA_HOME                        /usr/local/cuda\n",
            "Pillow                           9.4.0\n",
            "torchvision                      0.16.0+cu121 @/usr/local/lib/python3.10/dist-packages/torchvision\n",
            "torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0\n",
            "fvcore                           0.1.5.post20221221\n",
            "iopath                           0.1.9\n",
            "cv2                              4.8.0\n",
            "-------------------------------  -----------------------------------------------------------------\n",
            "PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "\u001b[32m[01/31 10:30:50 detectron2]: \u001b[0mCommand line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/maskdino_R50_bs16_50ep_3s_dowsample1_2048.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:6076', opts=['SOLVER.IMS_PER_BATCH', '2', 'SOLVER.BASE_LR', '0.0000125'], EVAL_FLAG=1)\n",
            "\u001b[32m[01/31 10:30:50 detectron2]: \u001b[0mContents of args.config_file=configs/coco/panoptic-segmentation/maskdino_R50_bs16_50ep_3s_dowsample1_2048.yaml:\n",
            "\u001b[38;5;204m_BASE_\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mBase-COCO-PanopticSegmentation.yaml\u001b[39m\n",
            "\u001b[38;5;204mMODEL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMETA_ARCHITECTURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINO\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSEM_SEG_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINOHead\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIGNORE_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m142\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONVS_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mGN\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;245m# pixel decoder\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPIXEL_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINOEncoder\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTOTAL_NUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres2\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres3\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres4\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres5\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres3\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres4\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres5\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOMMON_STRIDE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_ENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFEATURE_ORDER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mlow2high\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMaskDINO\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINODecoder\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEEP_SUPERVISION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNO_OBJECT_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLASS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOX_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mGIOU_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mHIDDEN_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_OBJECT_QUERIES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m300\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNHEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENFORCE_INPUT_PROJ\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m9\u001b[39m\u001b[38;5;15m  \u001b[39m\u001b[38;5;245m# 9+1, 9 decoder layers, add one for the loss on learnable query\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRAIN_NUM_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12544\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOVERSAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIMPORTANCE_SAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.75\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mEVAL_FLAG\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIAL_PRED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTWO_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mseg\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN_NUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIALIZE_BOX_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186mno\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPANO_BOX_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSEMANTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mINSTANCE_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANOPTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOVERLAP_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOBJECT_MASK_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.25\u001b[39m\n",
            "\n",
            "\u001b[38;5;204mSOLVER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mAMP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mEVAL_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5000\u001b[39m\n",
            "\u001b[38;5;245m#  EVAL_FLAG: 1\u001b[39m\n",
            "\n",
            "\u001b[32m[01/31 10:30:50 detectron2]: \u001b[0mRunning with full config:\n",
            "\u001b[38;5;204mCUDNN_BENCHMARK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;204mDATALOADER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mASPECT_RATIO_GROUPING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mFILTER_EMPTY_ANNOTATIONS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mNUM_WORKERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mREPEAT_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSAMPLER_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrainingSampler\u001b[39m\n",
            "\u001b[38;5;204mDATASETS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPRECOMPUTED_PROPOSAL_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPRECOMPUTED_PROPOSAL_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPROPOSAL_FILES_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPROPOSAL_FILES_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mcoco_2017_val_panoptic_with_sem_seg\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mTRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mcoco_2017_train_panoptic\u001b[39m\n",
            "\u001b[38;5;204mDefault_loading\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;204mGLOBAL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mHACK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;204mINPUT\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCOLOR_AUG_SSD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCROP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSINGLE_CATEGORY_MAX_AREA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.9\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.9\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mrelative_range\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mDATASET_MAPPER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mcoco_panoptic_lsj\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mFORMAT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRGB\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mIMAGE_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMASK_FORMAT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mpolygon\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_SIZE_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1333\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_SIZE_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1333\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SIZE_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m800\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SIZE_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m800\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SIZE_TRAIN_SAMPLING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mchoice\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRANDOM_FLIP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhorizontal\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;204mMODEL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mANCHOR_GENERATOR\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mANGLES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-90\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m90\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPECT_RATIOS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mDefaultAnchorGenerator\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOFFSET\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m128\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFREEZE_AT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mbuild_resnet_backbone\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mDEVICE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mcuda\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mFPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFUSE_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msum\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mKEYPOINT_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mLOAD_PROPOSALS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMASK_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMETA_ARCHITECTURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINO\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMaskDINO\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOX_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOX_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLASS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_BOX_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_CLASS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_DICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_GIOU_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_MASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m9\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEEP_SUPERVISION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mseg\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN_NOISE_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN_NUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENFORCE_INPUT_PROJ\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mEVAL_FLAG\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mGIOU_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mHIDDEN_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIMPORTANCE_SAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.75\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIALIZE_BOX_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186mno\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIAL_PRED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLEARN_TGT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNHEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNO_OBJECT_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_OBJECT_QUERIES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m300\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOVERSAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPANO_BOX_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRED_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSEMANTIC_CE_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mINSTANCE_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOBJECT_MASK_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.25\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOVERLAP_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANOPTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANO_TEMPERATURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.06\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANO_TRANSFORM_EVAL\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSEMANTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mTEST_FOUCUS_ON_BOX\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRAIN_NUM_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12544\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINODecoder\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTWO_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPANOPTIC_FPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOMBINE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mINSTANCES_CONFIDENCE_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOVERLAP_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSTUFF_AREA_LIMIT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4096\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINSTANCE_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPIXEL_MEAN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m123.675\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m116.28\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m103.53\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPIXEL_STD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m58.395\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m57.12\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m57.375\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPROPOSAL_GENERATOR\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMIN_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRPN\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRESNETS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORM_MODULATED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORM_NUM_GROUPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORM_ON_PER_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEPTH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m50\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFrozenBN\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_GROUPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOUT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES2_OUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES4_DILATION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES5_DILATION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES5_MULTI_GRID\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSTEM_OUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSTEM_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mbasic\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSTRIDE_IN_1X1\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mWIDTH_PER_GROUP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRETINANET\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m&id002\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFOCAL_LOSS_ALPHA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.25\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFOCAL_LOSS_GAMMA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNMS_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m80\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CONVS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRIOR_PROB\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.01\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSCORE_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSMOOTH_L1_LOSS_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTOPK_CANDIDATES_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_BOX_CASCADE_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m&id001\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m20.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m20.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m30.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m30.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m15.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m15.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOUS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_BOX_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m*id001\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLS_AGNOSTIC_BBOX_REG\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFC_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFED_LOSS_FREQ_WEIGHT_POWER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFED_LOSS_NUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m50\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_FC\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSMOOTH_L1_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRAIN_ON_PRED_BOXES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_FED_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_SIGMOID_CE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBATCH_SIZE_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRes5ROIHeads\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNMS_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m80\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOSITIVE_FRACTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.25\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPROPOSAL_APPEND_GT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSCORE_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_KEYPOINT_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIMS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMIN_KEYPOINTS_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mKRCNNConvDeconvUpsampleHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_KEYPOINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m17\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_MASK_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLS_AGNOSTIC_MASK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskRCNNConvUpsampleHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBATCH_SIZE_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m*id002\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOUNDARY_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIMS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mHEAD_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mStandardRPNHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNMS_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOSITIVE_FRACTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOST_NMS_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOST_NMS_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NMS_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NMS_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSMOOTH_L1_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSEM_SEG_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPP_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPP_DILATIONS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m18\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPP_DROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOMMON_STRIDE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONVS_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFEATURE_ORDER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mlow2high\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIGNORE_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhard_pixel_mining\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINOHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mGN\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m142\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPIXEL_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINOEncoder\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPROJECT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m48\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPROJECT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTOTAL_NUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_ENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_DEPTHWISE_SEPARABLE_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSWIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mAPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mATTN_DROP_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEPTHS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROP_PATH_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROP_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mEMBED_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m96\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMLP_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m24\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOUT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPATCH_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPATCH_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRETRAIN_IMG_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m224\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mQKV_BIAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mQK_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mnull\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_CHECKPOINT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mWINDOW_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m7\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mdetectron2://ImageNetPretrained/torchvision/R-50.pkl\u001b[39m\n",
            "\u001b[38;5;204mOUTPUT_DIR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m./output\u001b[39m\n",
            "\u001b[38;5;204mSEED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;204mSOLVER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mAMP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE_MULTIPLIER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.25e-05\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR_END\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBIAS_LR_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCHECKPOINT_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCLIP_GRADIENTS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLIP_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfull_model\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLIP_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.01\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mGAMMA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mIMS_PER_BATCH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mLR_SCHEDULER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mWarmupMultiStepLR\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m368750\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMOMENTUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.9\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mNESTEROV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mNUM_DECAYS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mOPTIMIZER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mADAMW\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPOLY_LR_CONSTANT_ENDING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPOLY_LR_POWER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.9\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mREFERENCE_WORLD_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRESCALE_INTERVAL\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSTEPS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m327778\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m355092\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_ITERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_METHOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mlinear\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY_BIAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mnull\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY_EMBED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mAUG\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFLIP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMAX_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMIN_SIZES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m400\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m500\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m600\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m700\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m800\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m900\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1200\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mDETECTIONS_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mEVAL_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mEXPECTED_RESULTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mKEYPOINT_OKS_SIGMAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPRECISE_BN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m200\u001b[39m\n",
            "\u001b[38;5;204mVERSION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;204mVIS_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\n",
            "\u001b[32m[01/31 10:30:50 detectron2]: \u001b[0mFull config saved to ./output/config.yaml\n",
            "\u001b[32m[01/31 10:30:50 d2.utils.env]: \u001b[0mUsing a generated random seed 50359810\n",
            "Command cfg: CUDNN_BENCHMARK: False\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: True\n",
            "  FILTER_EMPTY_ANNOTATIONS: True\n",
            "  NUM_WORKERS: 4\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: ()\n",
            "  PROPOSAL_FILES_TRAIN: ()\n",
            "  TEST: ('coco_2017_val_panoptic_with_sem_seg',)\n",
            "  TRAIN: ('coco_2017_train_panoptic',)\n",
            "Default_loading: True\n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  COLOR_AUG_SSD: False\n",
            "  CROP:\n",
            "    ENABLED: False\n",
            "    SINGLE_CATEGORY_MAX_AREA: 1.0\n",
            "    SIZE: [0.9, 0.9]\n",
            "    TYPE: relative_range\n",
            "  DATASET_MAPPER_NAME: coco_panoptic_lsj\n",
            "  FORMAT: RGB\n",
            "  IMAGE_SIZE: 1024\n",
            "  MASK_FORMAT: polygon\n",
            "  MAX_SCALE: 2.0\n",
            "  MAX_SIZE_TEST: 1333\n",
            "  MAX_SIZE_TRAIN: 1333\n",
            "  MIN_SCALE: 0.1\n",
            "  MIN_SIZE_TEST: 800\n",
            "  MIN_SIZE_TRAIN: (800,)\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "  RANDOM_FLIP: horizontal\n",
            "  SIZE_DIVISIBILITY: -1\n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES: [[-90, 0, 90]]\n",
            "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]\n",
            "    NAME: DefaultAnchorGenerator\n",
            "    OFFSET: 0.0\n",
            "    SIZES: [[32, 64, 128, 256, 512]]\n",
            "  BACKBONE:\n",
            "    FREEZE_AT: 0\n",
            "    NAME: build_resnet_backbone\n",
            "  DEVICE: cuda\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES: []\n",
            "    NORM: \n",
            "    OUT_CHANNELS: 256\n",
            "  KEYPOINT_ON: False\n",
            "  LOAD_PROPOSALS: False\n",
            "  MASK_ON: False\n",
            "  META_ARCHITECTURE: MaskDINO\n",
            "  MaskDINO:\n",
            "    BOX_LOSS: True\n",
            "    BOX_WEIGHT: 5.0\n",
            "    CLASS_WEIGHT: 4.0\n",
            "    COST_BOX_WEIGHT: 5.0\n",
            "    COST_CLASS_WEIGHT: 4.0\n",
            "    COST_DICE_WEIGHT: 5.0\n",
            "    COST_GIOU_WEIGHT: 2.0\n",
            "    COST_MASK_WEIGHT: 5.0\n",
            "    DEC_LAYERS: 9\n",
            "    DEEP_SUPERVISION: True\n",
            "    DICE_WEIGHT: 5.0\n",
            "    DIM_FEEDFORWARD: 2048\n",
            "    DN: seg\n",
            "    DN_NOISE_SCALE: 0.4\n",
            "    DN_NUM: 100\n",
            "    DROPOUT: 0.0\n",
            "    ENC_LAYERS: 0\n",
            "    ENFORCE_INPUT_PROJ: False\n",
            "    EVAL_FLAG: 1\n",
            "    GIOU_WEIGHT: 2.0\n",
            "    HIDDEN_DIM: 256\n",
            "    IMPORTANCE_SAMPLE_RATIO: 0.75\n",
            "    INITIALIZE_BOX_TYPE: no\n",
            "    INITIAL_PRED: True\n",
            "    LEARN_TGT: False\n",
            "    MASK_WEIGHT: 5.0\n",
            "    NHEADS: 8\n",
            "    NO_OBJECT_WEIGHT: 0.1\n",
            "    NUM_OBJECT_QUERIES: 300\n",
            "    OVERSAMPLE_RATIO: 3.0\n",
            "    PANO_BOX_LOSS: False\n",
            "    PRED_CONV: False\n",
            "    PRE_NORM: False\n",
            "    SEMANTIC_CE_LOSS: False\n",
            "    SIZE_DIVISIBILITY: 32\n",
            "    TEST:\n",
            "      INSTANCE_ON: True\n",
            "      OBJECT_MASK_THRESHOLD: 0.25\n",
            "      OVERLAP_THRESHOLD: 0.8\n",
            "      PANOPTIC_ON: True\n",
            "      PANO_TEMPERATURE: 0.06\n",
            "      PANO_TRANSFORM_EVAL: True\n",
            "      SEMANTIC_ON: True\n",
            "      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: False\n",
            "      TEST_FOUCUS_ON_BOX: False\n",
            "    TRAIN_NUM_POINTS: 12544\n",
            "    TRANSFORMER_DECODER_NAME: MaskDINODecoder\n",
            "    TWO_STAGE: True\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: True\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN: [123.675, 116.28, 103.53]\n",
            "  PIXEL_STD: [58.395, 57.12, 57.375]\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: RPN\n",
            "  RESNETS:\n",
            "    DEFORM_MODULATED: False\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
            "    DEPTH: 50\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 1\n",
            "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES4_DILATION: 1\n",
            "    RES5_DILATION: 1\n",
            "    RES5_MULTI_GRID: [1, 1, 1]\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STEM_TYPE: basic\n",
            "    STRIDE_IN_1X1: False\n",
            "    WIDTH_PER_GROUP: 64\n",
            "  RETINANET:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.4, 0.5]\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NORM: \n",
            "    NUM_CLASSES: 80\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))\n",
            "    IOUS: (0.5, 0.6, 0.7)\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
            "    CLS_AGNOSTIC_BBOX_REG: False\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    FED_LOSS_FREQ_WEIGHT_POWER: 0.5\n",
            "    FED_LOSS_NUM_CLASSES: 50\n",
            "    NAME: \n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "    TRAIN_ON_PRED_BOXES: False\n",
            "    USE_FED_LOSS: False\n",
            "    USE_SIGMOID_CE: False\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 512\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, 1]\n",
            "    IOU_THRESHOLDS: [0.5]\n",
            "    NAME: Res5ROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 80\n",
            "    POSITIVE_FRACTION: 0.25\n",
            "    PROPOSAL_APPEND_GT: True\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: False\n",
            "    CONV_DIM: 256\n",
            "    NAME: MaskRCNNConvUpsampleHead\n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    BOUNDARY_THRESH: -1\n",
            "    CONV_DIMS: [-1]\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.3, 0.7]\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 1000\n",
            "    POST_NMS_TOPK_TRAIN: 2000\n",
            "    PRE_NMS_TOPK_TEST: 6000\n",
            "    PRE_NMS_TOPK_TRAIN: 12000\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "  SEM_SEG_HEAD:\n",
            "    ASPP_CHANNELS: 256\n",
            "    ASPP_DILATIONS: [6, 12, 18]\n",
            "    ASPP_DROPOUT: 0.1\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 256\n",
            "    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES: ['res3', 'res4', 'res5']\n",
            "    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8\n",
            "    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4\n",
            "    DIM_FEEDFORWARD: 2048\n",
            "    FEATURE_ORDER: low2high\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    LOSS_TYPE: hard_pixel_mining\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MASK_DIM: 256\n",
            "    NAME: MaskDINOHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 142\n",
            "    NUM_FEATURE_LEVELS: 3\n",
            "    PIXEL_DECODER_NAME: MaskDINOEncoder\n",
            "    PROJECT_CHANNELS: [48]\n",
            "    PROJECT_FEATURES: ['res2']\n",
            "    TOTAL_NUM_FEATURE_LEVELS: 4\n",
            "    TRANSFORMER_ENC_LAYERS: 6\n",
            "    USE_DEPTHWISE_SEPARABLE_CONV: False\n",
            "  SWIN:\n",
            "    APE: False\n",
            "    ATTN_DROP_RATE: 0.0\n",
            "    DEPTHS: [2, 2, 6, 2]\n",
            "    DROP_PATH_RATE: 0.3\n",
            "    DROP_RATE: 0.0\n",
            "    EMBED_DIM: 96\n",
            "    MLP_RATIO: 4.0\n",
            "    NUM_HEADS: [3, 6, 12, 24]\n",
            "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    PATCH_NORM: True\n",
            "    PATCH_SIZE: 4\n",
            "    PRETRAIN_IMG_SIZE: 224\n",
            "    QKV_BIAS: True\n",
            "    QK_SCALE: None\n",
            "    USE_CHECKPOINT: False\n",
            "    WINDOW_SIZE: 7\n",
            "  WEIGHTS: detectron2://ImageNetPretrained/torchvision/R-50.pkl\n",
            "OUTPUT_DIR: ./output\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  AMP:\n",
            "    ENABLED: True\n",
            "  BACKBONE_MULTIPLIER: 0.1\n",
            "  BASE_LR: 1.25e-05\n",
            "  BASE_LR_END: 0.0\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 5000\n",
            "  CLIP_GRADIENTS:\n",
            "    CLIP_TYPE: full_model\n",
            "    CLIP_VALUE: 0.01\n",
            "    ENABLED: True\n",
            "    NORM_TYPE: 2.0\n",
            "  GAMMA: 0.1\n",
            "  IMS_PER_BATCH: 2\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 368750\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: False\n",
            "  NUM_DECAYS: 3\n",
            "  OPTIMIZER: ADAMW\n",
            "  POLY_LR_CONSTANT_ENDING: 0.0\n",
            "  POLY_LR_POWER: 0.9\n",
            "  REFERENCE_WORLD_SIZE: 0\n",
            "  RESCALE_INTERVAL: False\n",
            "  STEPS: (327778, 355092)\n",
            "  WARMUP_FACTOR: 1.0\n",
            "  WARMUP_ITERS: 10\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.05\n",
            "  WEIGHT_DECAY_BIAS: None\n",
            "  WEIGHT_DECAY_EMBED: 0.0\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: False\n",
            "    FLIP: True\n",
            "    MAX_SIZE: 4000\n",
            "    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)\n",
            "  DETECTIONS_PER_IMAGE: 100\n",
            "  EVAL_PERIOD: 5000\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: False\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VIS_PERIOD: 0\n",
            "criterion.weight_dict  {'loss_ce': 4.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_ce_interm': 4.0, 'loss_mask_interm': 5.0, 'loss_dice_interm': 5.0, 'loss_bbox_interm': 5.0, 'loss_giou_interm': 2.0, 'loss_ce_dn': 4.0, 'loss_mask_dn': 5.0, 'loss_dice_dn': 5.0, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_ce_interm_dn': 4.0, 'loss_mask_interm_dn': 5.0, 'loss_dice_interm_dn': 5.0, 'loss_bbox_interm_dn': 5.0, 'loss_giou_interm_dn': 2.0, 'loss_ce_0': 4.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_ce_interm_0': 4.0, 'loss_mask_interm_0': 5.0, 'loss_dice_interm_0': 5.0, 'loss_bbox_interm_0': 5.0, 'loss_giou_interm_0': 2.0, 'loss_ce_dn_0': 4.0, 'loss_mask_dn_0': 5.0, 'loss_dice_dn_0': 5.0, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_ce_interm_dn_0': 4.0, 'loss_mask_interm_dn_0': 5.0, 'loss_dice_interm_dn_0': 5.0, 'loss_bbox_interm_dn_0': 5.0, 'loss_giou_interm_dn_0': 2.0, 'loss_ce_1': 4.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_ce_interm_1': 4.0, 'loss_mask_interm_1': 5.0, 'loss_dice_interm_1': 5.0, 'loss_bbox_interm_1': 5.0, 'loss_giou_interm_1': 2.0, 'loss_ce_dn_1': 4.0, 'loss_mask_dn_1': 5.0, 'loss_dice_dn_1': 5.0, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_ce_interm_dn_1': 4.0, 'loss_mask_interm_dn_1': 5.0, 'loss_dice_interm_dn_1': 5.0, 'loss_bbox_interm_dn_1': 5.0, 'loss_giou_interm_dn_1': 2.0, 'loss_ce_2': 4.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_ce_interm_2': 4.0, 'loss_mask_interm_2': 5.0, 'loss_dice_interm_2': 5.0, 'loss_bbox_interm_2': 5.0, 'loss_giou_interm_2': 2.0, 'loss_ce_dn_2': 4.0, 'loss_mask_dn_2': 5.0, 'loss_dice_dn_2': 5.0, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_ce_interm_dn_2': 4.0, 'loss_mask_interm_dn_2': 5.0, 'loss_dice_interm_dn_2': 5.0, 'loss_bbox_interm_dn_2': 5.0, 'loss_giou_interm_dn_2': 2.0, 'loss_ce_3': 4.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_ce_interm_3': 4.0, 'loss_mask_interm_3': 5.0, 'loss_dice_interm_3': 5.0, 'loss_bbox_interm_3': 5.0, 'loss_giou_interm_3': 2.0, 'loss_ce_dn_3': 4.0, 'loss_mask_dn_3': 5.0, 'loss_dice_dn_3': 5.0, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_ce_interm_dn_3': 4.0, 'loss_mask_interm_dn_3': 5.0, 'loss_dice_interm_dn_3': 5.0, 'loss_bbox_interm_dn_3': 5.0, 'loss_giou_interm_dn_3': 2.0, 'loss_ce_4': 4.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_ce_interm_4': 4.0, 'loss_mask_interm_4': 5.0, 'loss_dice_interm_4': 5.0, 'loss_bbox_interm_4': 5.0, 'loss_giou_interm_4': 2.0, 'loss_ce_dn_4': 4.0, 'loss_mask_dn_4': 5.0, 'loss_dice_dn_4': 5.0, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0, 'loss_ce_interm_dn_4': 4.0, 'loss_mask_interm_dn_4': 5.0, 'loss_dice_interm_dn_4': 5.0, 'loss_bbox_interm_dn_4': 5.0, 'loss_giou_interm_dn_4': 2.0, 'loss_ce_5': 4.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_bbox_5': 5.0, 'loss_giou_5': 2.0, 'loss_ce_interm_5': 4.0, 'loss_mask_interm_5': 5.0, 'loss_dice_interm_5': 5.0, 'loss_bbox_interm_5': 5.0, 'loss_giou_interm_5': 2.0, 'loss_ce_dn_5': 4.0, 'loss_mask_dn_5': 5.0, 'loss_dice_dn_5': 5.0, 'loss_bbox_dn_5': 5.0, 'loss_giou_dn_5': 2.0, 'loss_ce_interm_dn_5': 4.0, 'loss_mask_interm_dn_5': 5.0, 'loss_dice_interm_dn_5': 5.0, 'loss_bbox_interm_dn_5': 5.0, 'loss_giou_interm_dn_5': 2.0, 'loss_ce_6': 4.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_bbox_6': 5.0, 'loss_giou_6': 2.0, 'loss_ce_interm_6': 4.0, 'loss_mask_interm_6': 5.0, 'loss_dice_interm_6': 5.0, 'loss_bbox_interm_6': 5.0, 'loss_giou_interm_6': 2.0, 'loss_ce_dn_6': 4.0, 'loss_mask_dn_6': 5.0, 'loss_dice_dn_6': 5.0, 'loss_bbox_dn_6': 5.0, 'loss_giou_dn_6': 2.0, 'loss_ce_interm_dn_6': 4.0, 'loss_mask_interm_dn_6': 5.0, 'loss_dice_interm_dn_6': 5.0, 'loss_bbox_interm_dn_6': 5.0, 'loss_giou_interm_dn_6': 2.0, 'loss_ce_7': 4.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_bbox_7': 5.0, 'loss_giou_7': 2.0, 'loss_ce_interm_7': 4.0, 'loss_mask_interm_7': 5.0, 'loss_dice_interm_7': 5.0, 'loss_bbox_interm_7': 5.0, 'loss_giou_interm_7': 2.0, 'loss_ce_dn_7': 4.0, 'loss_mask_dn_7': 5.0, 'loss_dice_dn_7': 5.0, 'loss_bbox_dn_7': 5.0, 'loss_giou_dn_7': 2.0, 'loss_ce_interm_dn_7': 4.0, 'loss_mask_interm_dn_7': 5.0, 'loss_dice_interm_dn_7': 5.0, 'loss_bbox_interm_dn_7': 5.0, 'loss_giou_interm_dn_7': 2.0, 'loss_ce_8': 4.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_bbox_8': 5.0, 'loss_giou_8': 2.0, 'loss_ce_interm_8': 4.0, 'loss_mask_interm_8': 5.0, 'loss_dice_interm_8': 5.0, 'loss_bbox_interm_8': 5.0, 'loss_giou_interm_8': 2.0, 'loss_ce_dn_8': 4.0, 'loss_mask_dn_8': 5.0, 'loss_dice_dn_8': 5.0, 'loss_bbox_dn_8': 5.0, 'loss_giou_dn_8': 2.0, 'loss_ce_interm_dn_8': 4.0, 'loss_mask_interm_dn_8': 5.0, 'loss_dice_interm_dn_8': 5.0, 'loss_bbox_interm_dn_8': 5.0, 'loss_giou_interm_dn_8': 2.0}\n",
            "\u001b[32m[01/31 10:30:51 d2.engine.defaults]: \u001b[0mModel:\n",
            "MaskDINO(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskDINOHead(\n",
            "    (pixel_decoder): MaskDINOEncoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (3): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MaskDINODecoder(\n",
            "      (enc_output): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-3): 4 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=142, bias=True)\n",
            "      (label_enc): Embedding(142, 256)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (decoder): TransformerDecoder(\n",
            "        (layers): ModuleList(\n",
            "          (0-8): 9 x DeformableTransformerDecoderLayer(\n",
            "            (cross_attn): MSDeformAttn(\n",
            "              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
            "              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "            )\n",
            "            (dropout2): Dropout(p=0.0, inplace=False)\n",
            "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "            (dropout4): Dropout(p=0.0, inplace=False)\n",
            "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ref_point_head): MLP(\n",
            "          (layers): ModuleList(\n",
            "            (0): Linear(in_features=512, out_features=256, bias=True)\n",
            "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (bbox_embed): ModuleList(\n",
            "          (0-8): 9 x MLP(\n",
            "            (layers): ModuleList(\n",
            "              (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (_bbox_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "          (2): Linear(in_features=256, out_features=4, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (bbox_embed): ModuleList(\n",
            "        (0-8): 9 x MLP(\n",
            "          (layers): ModuleList(\n",
            "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 4.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks', 'boxes']\n",
            "      weight_dict: {'loss_ce': 4.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_ce_interm': 4.0, 'loss_mask_interm': 5.0, 'loss_dice_interm': 5.0, 'loss_bbox_interm': 5.0, 'loss_giou_interm': 2.0, 'loss_ce_dn': 4.0, 'loss_mask_dn': 5.0, 'loss_dice_dn': 5.0, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_ce_interm_dn': 4.0, 'loss_mask_interm_dn': 5.0, 'loss_dice_interm_dn': 5.0, 'loss_bbox_interm_dn': 5.0, 'loss_giou_interm_dn': 2.0, 'loss_ce_0': 4.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_ce_interm_0': 4.0, 'loss_mask_interm_0': 5.0, 'loss_dice_interm_0': 5.0, 'loss_bbox_interm_0': 5.0, 'loss_giou_interm_0': 2.0, 'loss_ce_dn_0': 4.0, 'loss_mask_dn_0': 5.0, 'loss_dice_dn_0': 5.0, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_ce_interm_dn_0': 4.0, 'loss_mask_interm_dn_0': 5.0, 'loss_dice_interm_dn_0': 5.0, 'loss_bbox_interm_dn_0': 5.0, 'loss_giou_interm_dn_0': 2.0, 'loss_ce_1': 4.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_ce_interm_1': 4.0, 'loss_mask_interm_1': 5.0, 'loss_dice_interm_1': 5.0, 'loss_bbox_interm_1': 5.0, 'loss_giou_interm_1': 2.0, 'loss_ce_dn_1': 4.0, 'loss_mask_dn_1': 5.0, 'loss_dice_dn_1': 5.0, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_ce_interm_dn_1': 4.0, 'loss_mask_interm_dn_1': 5.0, 'loss_dice_interm_dn_1': 5.0, 'loss_bbox_interm_dn_1': 5.0, 'loss_giou_interm_dn_1': 2.0, 'loss_ce_2': 4.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_ce_interm_2': 4.0, 'loss_mask_interm_2': 5.0, 'loss_dice_interm_2': 5.0, 'loss_bbox_interm_2': 5.0, 'loss_giou_interm_2': 2.0, 'loss_ce_dn_2': 4.0, 'loss_mask_dn_2': 5.0, 'loss_dice_dn_2': 5.0, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_ce_interm_dn_2': 4.0, 'loss_mask_interm_dn_2': 5.0, 'loss_dice_interm_dn_2': 5.0, 'loss_bbox_interm_dn_2': 5.0, 'loss_giou_interm_dn_2': 2.0, 'loss_ce_3': 4.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_ce_interm_3': 4.0, 'loss_mask_interm_3': 5.0, 'loss_dice_interm_3': 5.0, 'loss_bbox_interm_3': 5.0, 'loss_giou_interm_3': 2.0, 'loss_ce_dn_3': 4.0, 'loss_mask_dn_3': 5.0, 'loss_dice_dn_3': 5.0, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_ce_interm_dn_3': 4.0, 'loss_mask_interm_dn_3': 5.0, 'loss_dice_interm_dn_3': 5.0, 'loss_bbox_interm_dn_3': 5.0, 'loss_giou_interm_dn_3': 2.0, 'loss_ce_4': 4.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_ce_interm_4': 4.0, 'loss_mask_interm_4': 5.0, 'loss_dice_interm_4': 5.0, 'loss_bbox_interm_4': 5.0, 'loss_giou_interm_4': 2.0, 'loss_ce_dn_4': 4.0, 'loss_mask_dn_4': 5.0, 'loss_dice_dn_4': 5.0, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0, 'loss_ce_interm_dn_4': 4.0, 'loss_mask_interm_dn_4': 5.0, 'loss_dice_interm_dn_4': 5.0, 'loss_bbox_interm_dn_4': 5.0, 'loss_giou_interm_dn_4': 2.0, 'loss_ce_5': 4.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_bbox_5': 5.0, 'loss_giou_5': 2.0, 'loss_ce_interm_5': 4.0, 'loss_mask_interm_5': 5.0, 'loss_dice_interm_5': 5.0, 'loss_bbox_interm_5': 5.0, 'loss_giou_interm_5': 2.0, 'loss_ce_dn_5': 4.0, 'loss_mask_dn_5': 5.0, 'loss_dice_dn_5': 5.0, 'loss_bbox_dn_5': 5.0, 'loss_giou_dn_5': 2.0, 'loss_ce_interm_dn_5': 4.0, 'loss_mask_interm_dn_5': 5.0, 'loss_dice_interm_dn_5': 5.0, 'loss_bbox_interm_dn_5': 5.0, 'loss_giou_interm_dn_5': 2.0, 'loss_ce_6': 4.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_bbox_6': 5.0, 'loss_giou_6': 2.0, 'loss_ce_interm_6': 4.0, 'loss_mask_interm_6': 5.0, 'loss_dice_interm_6': 5.0, 'loss_bbox_interm_6': 5.0, 'loss_giou_interm_6': 2.0, 'loss_ce_dn_6': 4.0, 'loss_mask_dn_6': 5.0, 'loss_dice_dn_6': 5.0, 'loss_bbox_dn_6': 5.0, 'loss_giou_dn_6': 2.0, 'loss_ce_interm_dn_6': 4.0, 'loss_mask_interm_dn_6': 5.0, 'loss_dice_interm_dn_6': 5.0, 'loss_bbox_interm_dn_6': 5.0, 'loss_giou_interm_dn_6': 2.0, 'loss_ce_7': 4.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_bbox_7': 5.0, 'loss_giou_7': 2.0, 'loss_ce_interm_7': 4.0, 'loss_mask_interm_7': 5.0, 'loss_dice_interm_7': 5.0, 'loss_bbox_interm_7': 5.0, 'loss_giou_interm_7': 2.0, 'loss_ce_dn_7': 4.0, 'loss_mask_dn_7': 5.0, 'loss_dice_dn_7': 5.0, 'loss_bbox_dn_7': 5.0, 'loss_giou_dn_7': 2.0, 'loss_ce_interm_dn_7': 4.0, 'loss_mask_interm_dn_7': 5.0, 'loss_dice_interm_dn_7': 5.0, 'loss_bbox_interm_dn_7': 5.0, 'loss_giou_interm_dn_7': 2.0, 'loss_ce_8': 4.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_bbox_8': 5.0, 'loss_giou_8': 2.0, 'loss_ce_interm_8': 4.0, 'loss_mask_interm_8': 5.0, 'loss_dice_interm_8': 5.0, 'loss_bbox_interm_8': 5.0, 'loss_giou_interm_8': 2.0, 'loss_ce_dn_8': 4.0, 'loss_mask_dn_8': 5.0, 'loss_dice_dn_8': 5.0, 'loss_bbox_dn_8': 5.0, 'loss_giou_dn_8': 2.0, 'loss_ce_interm_dn_8': 4.0, 'loss_mask_interm_dn_8': 5.0, 'loss_dice_interm_dn_8': 5.0, 'loss_bbox_interm_dn_8': 5.0, 'loss_giou_interm_dn_8': 2.0}\n",
            "      num_classes: 142\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")\n",
            "\u001b[32m[01/31 10:30:51 maskdino.data.dataset_mappers.coco_panoptic_new_baseline_dataset_mapper]: \u001b[0m[COCOPanopticNewBaselineDatasetMapper] Full TransformGens used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=1024, target_width=1024), FixedSizeCrop(crop_size=(1024, 1024))]\n",
            "\u001b[32m[01/31 10:30:51 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
            "\u001b[32m[01/31 10:30:51 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[01/31 10:30:51 d2.data.common]: \u001b[0mSerializing 3372 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/31 10:30:51 d2.data.common]: \u001b[0mSerialized dataset takes 2.18 MiB\n",
            "\u001b[32m[01/31 10:30:51 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=2\n",
            "\u001b[32m[01/31 10:30:51 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/torchvision/R-50.pkl ...\n",
            "R-50.pkl: 102MB [00:00, 123MB/s]                \n",
            "\u001b[32m[01/31 10:30:52 fvcore.common.checkpoint]: \u001b[0m[Checkpointer] Loading from /root/.torch/iopath_cache/detectron2/ImageNetPretrained/torchvision/R-50.pkl ...\n",
            "\u001b[32m[01/31 10:30:52 fvcore.common.checkpoint]: \u001b[0mReading a file from 'torchvision'\n",
            "\u001b[32m[01/31 10:30:52 d2.checkpoint.c2_model_loading]: \u001b[0mFollowing weights matched with submodule backbone - Total num: 53\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/31 10:30:52 fvcore.common.checkpoint]: \u001b[0mSome model parameters or buffers are not found in the checkpoint:\n",
            "\u001b[34mcriterion.empty_weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.adapter_1.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.3.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.input_proj.3.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.layer_1.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.pixel_decoder.transformer.level_embed\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor._bbox_embed.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor._bbox_embed.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor._bbox_embed.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.0.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.0.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.0.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.1.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.1.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.1.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.2.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.2.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.2.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.3.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.3.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.3.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.4.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.4.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.4.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.5.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.5.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.5.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.6.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.6.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.6.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.7.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.7.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.7.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.8.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.8.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.bbox_embed.8.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.class_embed.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.0.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.0.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.0.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.1.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.1.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.1.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.2.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.2.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.2.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.3.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.3.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.3.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.4.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.4.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.4.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.5.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.5.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.5.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.6.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.6.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.6.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.7.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.7.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.7.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.8.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.8.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.bbox_embed.8.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.0.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.1.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.2.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.3.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.4.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.5.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.6.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.7.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.cross_attn.attention_weights.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.cross_attn.output_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.cross_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.cross_attn.value_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.linear1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.linear2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.norm1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.norm2.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.norm3.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.self_attn.out_proj.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.layers.8.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.norm.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.ref_point_head.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder.ref_point_head.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.decoder_norm.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.enc_output.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.enc_output_norm.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.label_enc.weight\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}\u001b[0m\n",
            "\u001b[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}\u001b[0m\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/31 10:30:53 fvcore.common.checkpoint]: \u001b[0mThe checkpoint state_dict contains keys that are not used by the model:\n",
            "  \u001b[35mstem.fc.{bias, weight}\u001b[0m\n",
            "\u001b[32m[01/31 10:30:53 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "\u001b[32m[01/31 10:31:17 d2.utils.events]: \u001b[0m eta: 3 days, 15:14:02  iter: 19  total_loss: 3.054e+04  loss_ce: 2702  loss_mask: 3.365  loss_dice: 4.738  loss_bbox: 2.3  loss_giou: 1.968  loss_ce_dn: 121.8  loss_mask_dn: 3.457  loss_dice_dn: 4.699  loss_bbox_dn: 0.5488  loss_giou_dn: 0.9076  loss_ce_0: 2526  loss_mask_0: 2.624  loss_dice_0: 4.712  loss_bbox_0: 2.334  loss_giou_0: 1.985  loss_ce_dn_0: 117.5  loss_mask_dn_0: 3.47  loss_dice_dn_0: 4.681  loss_bbox_dn_0: 0.5489  loss_giou_dn_0: 0.9068  loss_ce_1: 2733  loss_mask_1: 2.697  loss_dice_1: 4.705  loss_bbox_1: 2.28  loss_giou_1: 1.953  loss_ce_dn_1: 136.7  loss_mask_dn_1: 3.443  loss_dice_dn_1: 4.672  loss_bbox_dn_1: 0.5489  loss_giou_dn_1: 0.9069  loss_ce_2: 2601  loss_mask_2: 3.133  loss_dice_2: 4.702  loss_bbox_2: 2.418  loss_giou_2: 1.929  loss_ce_dn_2: 121.5  loss_mask_dn_2: 3.642  loss_dice_dn_2: 4.688  loss_bbox_dn_2: 0.5489  loss_giou_dn_2: 0.907  loss_ce_3: 2798  loss_mask_3: 2.759  loss_dice_3: 4.652  loss_bbox_3: 2.269  loss_giou_3: 1.855  loss_ce_dn_3: 130.8  loss_mask_dn_3: 3.076  loss_dice_dn_3: 4.663  loss_bbox_dn_3: 0.5489  loss_giou_dn_3: 0.907  loss_ce_4: 2738  loss_mask_4: 2.691  loss_dice_4: 4.705  loss_bbox_4: 2.271  loss_giou_4: 1.903  loss_ce_dn_4: 121.3  loss_mask_dn_4: 3.054  loss_dice_dn_4: 4.696  loss_bbox_dn_4: 0.5489  loss_giou_dn_4: 0.9071  loss_ce_5: 2814  loss_mask_5: 2.269  loss_dice_5: 4.714  loss_bbox_5: 2.219  loss_giou_5: 1.951  loss_ce_dn_5: 120  loss_mask_dn_5: 2.544  loss_dice_dn_5: 4.728  loss_bbox_dn_5: 0.5489  loss_giou_dn_5: 0.9072  loss_ce_6: 2671  loss_mask_6: 3.269  loss_dice_6: 4.687  loss_bbox_6: 2.205  loss_giou_6: 1.93  loss_ce_dn_6: 116.4  loss_mask_dn_6: 3.519  loss_dice_dn_6: 4.664  loss_bbox_dn_6: 0.5489  loss_giou_dn_6: 0.9073  loss_ce_7: 2544  loss_mask_7: 3.864  loss_dice_7: 4.654  loss_bbox_7: 2.151  loss_giou_7: 2.023  loss_ce_dn_7: 115  loss_mask_dn_7: 4.014  loss_dice_dn_7: 4.627  loss_bbox_dn_7: 0.5488  loss_giou_dn_7: 0.9074  loss_ce_8: 2729  loss_mask_8: 3.837  loss_dice_8: 4.646  loss_bbox_8: 2.306  loss_giou_8: 1.976  loss_ce_dn_8: 124.2  loss_mask_dn_8: 4.218  loss_dice_dn_8: 4.63  loss_bbox_dn_8: 0.5488  loss_giou_dn_8: 0.9075  loss_ce_interm: 2526  loss_mask_interm: 2.625  loss_dice_interm: 4.709  loss_bbox_interm: 2.334  loss_giou_interm: 1.98    time: 0.8460  last_time: 0.8626  data_time: 0.0212  last_data_time: 0.0117   lr: 1.25e-05  max_mem: 11715M\n",
            "2024-01-31 10:31:18.027665: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-31 10:31:18.027729: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-31 10:31:18.029033: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-31 10:31:19.269615: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[32m[01/31 10:31:37 d2.utils.events]: \u001b[0m eta: 3 days, 15:18:40  iter: 39  total_loss: 1.966e+04  loss_ce: 1394  loss_mask: 1.465  loss_dice: 4.808  loss_bbox: 4.295  loss_giou: 2.462  loss_ce_dn: 74.82  loss_mask_dn: 1.56  loss_dice_dn: 4.844  loss_bbox_dn: 0.5754  loss_giou_dn: 0.8668  loss_ce_0: 2273  loss_mask_0: 0.9013  loss_dice_0: 4.88  loss_bbox_0: 4.397  loss_giou_0: 2.429  loss_ce_dn_0: 128.3  loss_mask_dn_0: 1.377  loss_dice_dn_0: 4.821  loss_bbox_dn_0: 0.5753  loss_giou_dn_0: 0.8598  loss_ce_1: 2052  loss_mask_1: 1.063  loss_dice_1: 4.825  loss_bbox_1: 4.417  loss_giou_1: 2.38  loss_ce_dn_1: 111.2  loss_mask_dn_1: 1.205  loss_dice_dn_1: 4.842  loss_bbox_dn_1: 0.5752  loss_giou_dn_1: 0.86  loss_ce_2: 1795  loss_mask_2: 1.212  loss_dice_2: 4.852  loss_bbox_2: 4.347  loss_giou_2: 2.477  loss_ce_dn_2: 92.23  loss_mask_dn_2: 1.339  loss_dice_dn_2: 4.826  loss_bbox_dn_2: 0.5752  loss_giou_dn_2: 0.8606  loss_ce_3: 1685  loss_mask_3: 1.041  loss_dice_3: 4.857  loss_bbox_3: 4.313  loss_giou_3: 2.449  loss_ce_dn_3: 81.73  loss_mask_dn_3: 1.162  loss_dice_dn_3: 4.836  loss_bbox_dn_3: 0.5752  loss_giou_dn_3: 0.8612  loss_ce_4: 1489  loss_mask_4: 1.062  loss_dice_4: 4.879  loss_bbox_4: 4.308  loss_giou_4: 2.434  loss_ce_dn_4: 72.49  loss_mask_dn_4: 1.026  loss_dice_dn_4: 4.874  loss_bbox_dn_4: 0.5752  loss_giou_dn_4: 0.862  loss_ce_5: 1454  loss_mask_5: 1.17  loss_dice_5: 4.858  loss_bbox_5: 4.307  loss_giou_5: 2.45  loss_ce_dn_5: 72.36  loss_mask_dn_5: 1.074  loss_dice_dn_5: 4.888  loss_bbox_dn_5: 0.5753  loss_giou_dn_5: 0.8628  loss_ce_6: 1427  loss_mask_6: 1.036  loss_dice_6: 4.874  loss_bbox_6: 4.316  loss_giou_6: 2.458  loss_ce_dn_6: 70.32  loss_mask_dn_6: 1.076  loss_dice_dn_6: 4.901  loss_bbox_dn_6: 0.5753  loss_giou_dn_6: 0.8636  loss_ce_7: 1399  loss_mask_7: 1.273  loss_dice_7: 4.821  loss_bbox_7: 4.311  loss_giou_7: 2.483  loss_ce_dn_7: 72.01  loss_mask_dn_7: 1.268  loss_dice_dn_7: 4.827  loss_bbox_dn_7: 0.5753  loss_giou_dn_7: 0.8646  loss_ce_8: 1396  loss_mask_8: 1.325  loss_dice_8: 4.801  loss_bbox_8: 4.29  loss_giou_8: 2.407  loss_ce_dn_8: 74.4  loss_mask_dn_8: 1.346  loss_dice_dn_8: 4.807  loss_bbox_dn_8: 0.5753  loss_giou_dn_8: 0.8657  loss_ce_interm: 2273  loss_mask_interm: 0.8978  loss_dice_interm: 4.88  loss_bbox_interm: 4.397  loss_giou_interm: 2.429    time: 0.8521  last_time: 0.8549  data_time: 0.0106  last_data_time: 0.0138   lr: 1.25e-05  max_mem: 11835M\n",
            "\u001b[32m[01/31 10:31:54 d2.utils.events]: \u001b[0m eta: 3 days, 15:26:54  iter: 59  total_loss: 1.363e+04  loss_ce: 771.3  loss_mask: 0.9064  loss_dice: 4.847  loss_bbox: 12.48  loss_giou: 2.746  loss_ce_dn: 54.5  loss_mask_dn: 0.9148  loss_dice_dn: 4.872  loss_bbox_dn: 0.6105  loss_giou_dn: 0.9156  loss_ce_0: 2203  loss_mask_0: 0.9987  loss_dice_0: 4.882  loss_bbox_0: 12.57  loss_giou_0: 2.741  loss_ce_dn_0: 126.5  loss_mask_dn_0: 0.8672  loss_dice_dn_0: 4.851  loss_bbox_dn_0: 0.6128  loss_giou_dn_0: 0.9043  loss_ce_1: 1397  loss_mask_1: 1.004  loss_dice_1: 4.891  loss_bbox_1: 12.7  loss_giou_1: 2.741  loss_ce_dn_1: 88.22  loss_mask_dn_1: 0.9146  loss_dice_dn_1: 4.841  loss_bbox_dn_1: 0.6122  loss_giou_dn_1: 0.9052  loss_ce_2: 1029  loss_mask_2: 0.8865  loss_dice_2: 4.914  loss_bbox_2: 12.5  loss_giou_2: 2.744  loss_ce_dn_2: 65.56  loss_mask_dn_2: 0.8897  loss_dice_dn_2: 4.877  loss_bbox_dn_2: 0.6118  loss_giou_dn_2: 0.9061  loss_ce_3: 881.3  loss_mask_3: 0.9559  loss_dice_3: 4.906  loss_bbox_3: 12.6  loss_giou_3: 2.742  loss_ce_dn_3: 57.75  loss_mask_dn_3: 0.8869  loss_dice_dn_3: 4.869  loss_bbox_dn_3: 0.6114  loss_giou_dn_3: 0.9071  loss_ce_4: 803.7  loss_mask_4: 0.8494  loss_dice_4: 4.901  loss_bbox_4: 12.5  loss_giou_4: 2.743  loss_ce_dn_4: 53.44  loss_mask_dn_4: 0.8457  loss_dice_dn_4: 4.921  loss_bbox_dn_4: 0.6111  loss_giou_dn_4: 0.9083  loss_ce_5: 803.8  loss_mask_5: 0.8854  loss_dice_5: 4.879  loss_bbox_5: 12.48  loss_giou_5: 2.746  loss_ce_dn_5: 55.32  loss_mask_dn_5: 0.9339  loss_dice_dn_5: 4.894  loss_bbox_dn_5: 0.6108  loss_giou_dn_5: 0.9095  loss_ce_6: 790.2  loss_mask_6: 0.8491  loss_dice_6: 4.904  loss_bbox_6: 12.48  loss_giou_6: 2.744  loss_ce_dn_6: 53.07  loss_mask_dn_6: 0.8757  loss_dice_dn_6: 4.909  loss_bbox_dn_6: 0.6106  loss_giou_dn_6: 0.9108  loss_ce_7: 778.4  loss_mask_7: 0.9469  loss_dice_7: 4.82  loss_bbox_7: 12.49  loss_giou_7: 2.745  loss_ce_dn_7: 53.91  loss_mask_dn_7: 0.8985  loss_dice_dn_7: 4.838  loss_bbox_dn_7: 0.6106  loss_giou_dn_7: 0.9122  loss_ce_8: 743.2  loss_mask_8: 0.9602  loss_dice_8: 4.842  loss_bbox_8: 12.49  loss_giou_8: 2.746  loss_ce_dn_8: 55.81  loss_mask_dn_8: 0.9948  loss_dice_dn_8: 4.806  loss_bbox_dn_8: 0.6105  loss_giou_dn_8: 0.914  loss_ce_interm: 2203  loss_mask_interm: 1.012  loss_dice_interm: 4.882  loss_bbox_interm: 12.58  loss_giou_interm: 2.741    time: 0.8532  last_time: 0.8388  data_time: 0.0107  last_data_time: 0.0127   lr: 1.25e-05  max_mem: 11835M\n",
            "\u001b[32m[01/31 10:32:11 d2.utils.events]: \u001b[0m eta: 3 days, 15:19:43  iter: 79  total_loss: 1.333e+04  loss_ce: 708.4  loss_mask: 1.285  loss_dice: 4.713  loss_bbox: 8.973  loss_giou: 2.809  loss_ce_dn: 45.22  loss_mask_dn: 1.378  loss_dice_dn: 4.792  loss_bbox_dn: 0.7626  loss_giou_dn: 0.8869  loss_ce_0: 2524  loss_mask_0: 1.389  loss_dice_0: 4.762  loss_bbox_0: 9.031  loss_giou_0: 2.812  loss_ce_dn_0: 131.7  loss_mask_dn_0: 1.348  loss_dice_dn_0: 4.757  loss_bbox_dn_0: 0.7574  loss_giou_dn_0: 0.8688  loss_ce_1: 1138  loss_mask_1: 1.413  loss_dice_1: 4.756  loss_bbox_1: 9.02  loss_giou_1: 2.813  loss_ce_dn_1: 79.02  loss_mask_dn_1: 1.405  loss_dice_dn_1: 4.691  loss_bbox_dn_1: 0.7578  loss_giou_dn_1: 0.8697  loss_ce_2: 852.4  loss_mask_2: 1.351  loss_dice_2: 4.747  loss_bbox_2: 9.002  loss_giou_2: 2.806  loss_ce_dn_2: 55.79  loss_mask_dn_2: 1.347  loss_dice_dn_2: 4.738  loss_bbox_dn_2: 0.7583  loss_giou_dn_2: 0.8707  loss_ce_3: 783.4  loss_mask_3: 1.416  loss_dice_3: 4.747  loss_bbox_3: 9.005  loss_giou_3: 2.807  loss_ce_dn_3: 48.64  loss_mask_dn_3: 1.424  loss_dice_dn_3: 4.718  loss_bbox_dn_3: 0.7588  loss_giou_dn_3: 0.8718  loss_ce_4: 746.6  loss_mask_4: 1.369  loss_dice_4: 4.805  loss_bbox_4: 9.006  loss_giou_4: 2.807  loss_ce_dn_4: 47.03  loss_mask_dn_4: 1.435  loss_dice_dn_4: 4.831  loss_bbox_dn_4: 0.7593  loss_giou_dn_4: 0.8731  loss_ce_5: 764.1  loss_mask_5: 1.318  loss_dice_5: 4.772  loss_bbox_5: 9.009  loss_giou_5: 2.809  loss_ce_dn_5: 48.23  loss_mask_dn_5: 1.37  loss_dice_dn_5: 4.807  loss_bbox_dn_5: 0.7598  loss_giou_dn_5: 0.8744  loss_ce_6: 748.6  loss_mask_6: 1.331  loss_dice_6: 4.765  loss_bbox_6: 8.987  loss_giou_6: 2.807  loss_ce_dn_6: 48.48  loss_mask_dn_6: 1.385  loss_dice_dn_6: 4.74  loss_bbox_dn_6: 0.7605  loss_giou_dn_6: 0.8762  loss_ce_7: 736.7  loss_mask_7: 1.357  loss_dice_7: 4.689  loss_bbox_7: 8.984  loss_giou_7: 2.808  loss_ce_dn_7: 49.79  loss_mask_dn_7: 1.394  loss_dice_dn_7: 4.713  loss_bbox_dn_7: 0.7612  loss_giou_dn_7: 0.8788  loss_ce_8: 708.8  loss_mask_8: 1.43  loss_dice_8: 4.705  loss_bbox_8: 8.997  loss_giou_8: 2.809  loss_ce_dn_8: 48.62  loss_mask_dn_8: 1.423  loss_dice_dn_8: 4.681  loss_bbox_dn_8: 0.7619  loss_giou_dn_8: 0.8823  loss_ce_interm: 2524  loss_mask_interm: 1.384  loss_dice_interm: 4.762  loss_bbox_interm: 9.031  loss_giou_interm: 2.812    time: 0.8527  last_time: 0.8341  data_time: 0.0103  last_data_time: 0.0123   lr: 1.25e-05  max_mem: 11835M\n",
            "\u001b[32m[01/31 10:32:28 d2.utils.events]: \u001b[0m eta: 3 days, 15:19:26  iter: 99  total_loss: 1.112e+04  loss_ce: 567.3  loss_mask: 1.15  loss_dice: 4.729  loss_bbox: 13.14  loss_giou: 2.804  loss_ce_dn: 34.3  loss_mask_dn: 1.116  loss_dice_dn: 4.781  loss_bbox_dn: 0.6273  loss_giou_dn: 0.8733  loss_ce_0: 2271  loss_mask_0: 1.149  loss_dice_0: 4.713  loss_bbox_0: 13.13  loss_giou_0: 2.807  loss_ce_dn_0: 120  loss_mask_dn_0: 1.133  loss_dice_dn_0: 4.765  loss_bbox_dn_0: 0.6275  loss_giou_dn_0: 0.8571  loss_ce_1: 835.6  loss_mask_1: 1.142  loss_dice_1: 4.698  loss_bbox_1: 13.13  loss_giou_1: 2.811  loss_ce_dn_1: 59.47  loss_mask_dn_1: 1.149  loss_dice_dn_1: 4.705  loss_bbox_dn_1: 0.6273  loss_giou_dn_1: 0.8586  loss_ce_2: 660.2  loss_mask_2: 1.101  loss_dice_2: 4.726  loss_bbox_2: 13.14  loss_giou_2: 2.804  loss_ce_dn_2: 42.4  loss_mask_dn_2: 1.108  loss_dice_dn_2: 4.731  loss_bbox_dn_2: 0.6271  loss_giou_dn_2: 0.86  loss_ce_3: 624  loss_mask_3: 1.136  loss_dice_3: 4.724  loss_bbox_3: 13.14  loss_giou_3: 2.801  loss_ce_dn_3: 37.26  loss_mask_dn_3: 1.155  loss_dice_dn_3: 4.705  loss_bbox_dn_3: 0.6269  loss_giou_dn_3: 0.8614  loss_ce_4: 600  loss_mask_4: 1.066  loss_dice_4: 4.792  loss_bbox_4: 13.14  loss_giou_4: 2.818  loss_ce_dn_4: 37.02  loss_mask_dn_4: 1.102  loss_dice_dn_4: 4.819  loss_bbox_dn_4: 0.6269  loss_giou_dn_4: 0.8631  loss_ce_5: 615.7  loss_mask_5: 1.086  loss_dice_5: 4.741  loss_bbox_5: 13.14  loss_giou_5: 2.852  loss_ce_dn_5: 37.1  loss_mask_dn_5: 1.119  loss_dice_dn_5: 4.783  loss_bbox_dn_5: 0.6268  loss_giou_dn_5: 0.8645  loss_ce_6: 605.3  loss_mask_6: 1.117  loss_dice_6: 4.736  loss_bbox_6: 13.14  loss_giou_6: 2.838  loss_ce_dn_6: 38.07  loss_mask_dn_6: 1.133  loss_dice_dn_6: 4.727  loss_bbox_dn_6: 0.6269  loss_giou_dn_6: 0.8663  loss_ce_7: 594.5  loss_mask_7: 1.183  loss_dice_7: 4.702  loss_bbox_7: 13.14  loss_giou_7: 2.834  loss_ce_dn_7: 38.07  loss_mask_dn_7: 1.118  loss_dice_dn_7: 4.724  loss_bbox_dn_7: 0.627  loss_giou_dn_7: 0.8685  loss_ce_8: 574.8  loss_mask_8: 1.198  loss_dice_8: 4.726  loss_bbox_8: 13.14  loss_giou_8: 2.806  loss_ce_dn_8: 36.72  loss_mask_dn_8: 1.157  loss_dice_dn_8: 4.699  loss_bbox_dn_8: 0.6271  loss_giou_dn_8: 0.8709  loss_ce_interm: 2271  loss_mask_interm: 1.152  loss_dice_interm: 4.714  loss_bbox_interm: 13.13  loss_giou_interm: 2.807    time: 0.8529  last_time: 0.8338  data_time: 0.0103  last_data_time: 0.0084   lr: 1.25e-05  max_mem: 11835M\n",
            "\u001b[32m[01/31 10:32:45 d2.utils.events]: \u001b[0m eta: 3 days, 15:19:42  iter: 119  total_loss: 1.069e+04  loss_ce: 542.3  loss_mask: 1.23  loss_dice: 4.757  loss_bbox: 10.58  loss_giou: 2.771  loss_ce_dn: 33.14  loss_mask_dn: 1.262  loss_dice_dn: 4.78  loss_bbox_dn: 0.6745  loss_giou_dn: 0.9239  loss_ce_0: 2367  loss_mask_0: 1.289  loss_dice_0: 4.766  loss_bbox_0: 10.56  loss_giou_0: 2.789  loss_ce_dn_0: 135.4  loss_mask_dn_0: 1.227  loss_dice_dn_0: 4.778  loss_bbox_dn_0: 0.6734  loss_giou_dn_0: 0.918  loss_ce_1: 714.5  loss_mask_1: 1.353  loss_dice_1: 4.707  loss_bbox_1: 10.57  loss_giou_1: 2.781  loss_ce_dn_1: 57.74  loss_mask_dn_1: 1.251  loss_dice_dn_1: 4.737  loss_bbox_dn_1: 0.6735  loss_giou_dn_1: 0.9185  loss_ce_2: 591.4  loss_mask_2: 1.244  loss_dice_2: 4.746  loss_bbox_2: 10.56  loss_giou_2: 2.776  loss_ce_dn_2: 41.19  loss_mask_dn_2: 1.287  loss_dice_dn_2: 4.76  loss_bbox_dn_2: 0.6736  loss_giou_dn_2: 0.9189  loss_ce_3: 574.6  loss_mask_3: 1.265  loss_dice_3: 4.739  loss_bbox_3: 10.67  loss_giou_3: 2.776  loss_ce_dn_3: 36.76  loss_mask_dn_3: 1.308  loss_dice_dn_3: 4.738  loss_bbox_dn_3: 0.6737  loss_giou_dn_3: 0.9195  loss_ce_4: 562.7  loss_mask_4: 1.234  loss_dice_4: 4.807  loss_bbox_4: 10.58  loss_giou_4: 2.776  loss_ce_dn_4: 36.22  loss_mask_dn_4: 1.291  loss_dice_dn_4: 4.818  loss_bbox_dn_4: 0.6738  loss_giou_dn_4: 0.92  loss_ce_5: 572.2  loss_mask_5: 1.227  loss_dice_5: 4.754  loss_bbox_5: 10.57  loss_giou_5: 2.779  loss_ce_dn_5: 35.98  loss_mask_dn_5: 1.297  loss_dice_dn_5: 4.79  loss_bbox_dn_5: 0.6739  loss_giou_dn_5: 0.9205  loss_ce_6: 567.8  loss_mask_6: 1.237  loss_dice_6: 4.74  loss_bbox_6: 10.57  loss_giou_6: 2.773  loss_ce_dn_6: 36.69  loss_mask_dn_6: 1.31  loss_dice_dn_6: 4.752  loss_bbox_dn_6: 0.674  loss_giou_dn_6: 0.9212  loss_ce_7: 560.7  loss_mask_7: 1.294  loss_dice_7: 4.723  loss_bbox_7: 10.57  loss_giou_7: 2.775  loss_ce_dn_7: 36.39  loss_mask_dn_7: 1.24  loss_dice_dn_7: 4.75  loss_bbox_dn_7: 0.6742  loss_giou_dn_7: 0.9221  loss_ce_8: 551.2  loss_mask_8: 1.316  loss_dice_8: 4.747  loss_bbox_8: 10.57  loss_giou_8: 2.775  loss_ce_dn_8: 34.87  loss_mask_dn_8: 1.23  loss_dice_dn_8: 4.735  loss_bbox_dn_8: 0.6744  loss_giou_dn_8: 0.923  loss_ce_interm: 2367  loss_mask_interm: 1.294  loss_dice_interm: 4.768  loss_bbox_interm: 10.56  loss_giou_interm: 2.789    time: 0.8537  last_time: 0.8951  data_time: 0.0108  last_data_time: 0.0130   lr: 1.25e-05  max_mem: 11835M\n",
            "\u001b[32m[01/31 10:33:02 d2.utils.events]: \u001b[0m eta: 3 days, 15:32:20  iter: 139  total_loss: 9862  loss_ce: 493.5  loss_mask: 1.203  loss_dice: 4.666  loss_bbox: 12.59  loss_giou: 2.814  loss_ce_dn: 24.98  loss_mask_dn: 1.189  loss_dice_dn: 4.695  loss_bbox_dn: 0.6355  loss_giou_dn: 0.9085  loss_ce_0: 2245  loss_mask_0: 1.179  loss_dice_0: 4.679  loss_bbox_0: 12.65  loss_giou_0: 2.803  loss_ce_dn_0: 114.8  loss_mask_dn_0: 1.199  loss_dice_dn_0: 4.742  loss_bbox_dn_0: 0.6343  loss_giou_dn_0: 0.9016  loss_ce_1: 616  loss_mask_1: 1.231  loss_dice_1: 4.638  loss_bbox_1: 12.61  loss_giou_1: 2.825  loss_ce_dn_1: 44.47  loss_mask_dn_1: 1.174  loss_dice_dn_1: 4.672  loss_bbox_dn_1: 0.6343  loss_giou_dn_1: 0.9015  loss_ce_2: 528.4  loss_mask_2: 1.174  loss_dice_2: 4.67  loss_bbox_2: 12.6  loss_giou_2: 2.825  loss_ce_dn_2: 31.27  loss_mask_dn_2: 1.172  loss_dice_dn_2: 4.668  loss_bbox_dn_2: 0.6343  loss_giou_dn_2: 0.9016  loss_ce_3: 519.3  loss_mask_3: 1.184  loss_dice_3: 4.669  loss_bbox_3: 12.6  loss_giou_3: 2.825  loss_ce_dn_3: 27.29  loss_mask_dn_3: 1.238  loss_dice_dn_3: 4.653  loss_bbox_dn_3: 0.6344  loss_giou_dn_3: 0.902  loss_ce_4: 510.5  loss_mask_4: 1.159  loss_dice_4: 4.718  loss_bbox_4: 12.6  loss_giou_4: 2.814  loss_ce_dn_4: 27.26  loss_mask_dn_4: 1.175  loss_dice_dn_4: 4.744  loss_bbox_dn_4: 0.6345  loss_giou_dn_4: 0.9025  loss_ce_5: 518  loss_mask_5: 1.193  loss_dice_5: 4.672  loss_bbox_5: 12.6  loss_giou_5: 2.804  loss_ce_dn_5: 26.76  loss_mask_dn_5: 1.173  loss_dice_dn_5: 4.702  loss_bbox_dn_5: 0.6346  loss_giou_dn_5: 0.9032  loss_ce_6: 513.1  loss_mask_6: 1.188  loss_dice_6: 4.664  loss_bbox_6: 12.6  loss_giou_6: 2.804  loss_ce_dn_6: 27.46  loss_mask_dn_6: 1.185  loss_dice_dn_6: 4.669  loss_bbox_dn_6: 0.6347  loss_giou_dn_6: 0.9043  loss_ce_7: 508.5  loss_mask_7: 1.228  loss_dice_7: 4.653  loss_bbox_7: 12.6  loss_giou_7: 2.814  loss_ce_dn_7: 26.53  loss_mask_dn_7: 1.163  loss_dice_dn_7: 4.647  loss_bbox_dn_7: 0.6349  loss_giou_dn_7: 0.9056  loss_ce_8: 501.6  loss_mask_8: 1.211  loss_dice_8: 4.671  loss_bbox_8: 12.64  loss_giou_8: 2.805  loss_ce_dn_8: 26.04  loss_mask_dn_8: 1.153  loss_dice_dn_8: 4.659  loss_bbox_dn_8: 0.6352  loss_giou_dn_8: 0.9068  loss_ce_interm: 2245  loss_mask_interm: 1.179  loss_dice_interm: 4.681  loss_bbox_interm: 12.65  loss_giou_interm: 2.803    time: 0.8540  last_time: 0.8710  data_time: 0.0097  last_data_time: 0.0075   lr: 1.25e-05  max_mem: 11835M\n",
            "\u001b[32m[01/31 10:33:20 d2.utils.events]: \u001b[0m eta: 3 days, 15:32:18  iter: 159  total_loss: 9068  loss_ce: 456.1  loss_mask: 1.507  loss_dice: 4.736  loss_bbox: 9.099  loss_giou: 2.646  loss_ce_dn: 21.6  loss_mask_dn: 1.545  loss_dice_dn: 4.742  loss_bbox_dn: 0.731  loss_giou_dn: 0.9866  loss_ce_0: 2090  loss_mask_0: 1.455  loss_dice_0: 4.707  loss_bbox_0: 9.204  loss_giou_0: 2.668  loss_ce_dn_0: 116.3  loss_mask_dn_0: 1.52  loss_dice_dn_0: 4.715  loss_bbox_dn_0: 0.7302  loss_giou_dn_0: 0.9841  loss_ce_1: 531.6  loss_mask_1: 1.514  loss_dice_1: 4.721  loss_bbox_1: 9.096  loss_giou_1: 2.663  loss_ce_dn_1: 41.68  loss_mask_dn_1: 1.541  loss_dice_dn_1: 4.733  loss_bbox_dn_1: 0.7302  loss_giou_dn_1: 0.9842  loss_ce_2: 478.5  loss_mask_2: 1.488  loss_dice_2: 4.722  loss_bbox_2: 9.079  loss_giou_2: 2.65  loss_ce_dn_2: 27.72  loss_mask_dn_2: 1.539  loss_dice_dn_2: 4.753  loss_bbox_dn_2: 0.7302  loss_giou_dn_2: 0.9844  loss_ce_3: 472.3  loss_mask_3: 1.528  loss_dice_3: 4.736  loss_bbox_3: 9.081  loss_giou_3: 2.65  loss_ce_dn_3: 24.22  loss_mask_dn_3: 1.549  loss_dice_dn_3: 4.728  loss_bbox_dn_3: 0.7302  loss_giou_dn_3: 0.9847  loss_ce_4: 469.4  loss_mask_4: 1.517  loss_dice_4: 4.758  loss_bbox_4: 9.078  loss_giou_4: 2.647  loss_ce_dn_4: 23.9  loss_mask_dn_4: 1.516  loss_dice_dn_4: 4.755  loss_bbox_dn_4: 0.7303  loss_giou_dn_4: 0.985  loss_ce_5: 474.1  loss_mask_5: 1.489  loss_dice_5: 4.757  loss_bbox_5: 9.08  loss_giou_5: 2.651  loss_ce_dn_5: 23.23  loss_mask_dn_5: 1.49  loss_dice_dn_5: 4.75  loss_bbox_dn_5: 0.7304  loss_giou_dn_5: 0.9852  loss_ce_6: 469.6  loss_mask_6: 1.534  loss_dice_6: 4.756  loss_bbox_6: 9.08  loss_giou_6: 2.651  loss_ce_dn_6: 23.77  loss_mask_dn_6: 1.51  loss_dice_dn_6: 4.737  loss_bbox_dn_6: 0.7306  loss_giou_dn_6: 0.9856  loss_ce_7: 465.1  loss_mask_7: 1.537  loss_dice_7: 4.736  loss_bbox_7: 9.08  loss_giou_7: 2.635  loss_ce_dn_7: 23.01  loss_mask_dn_7: 1.512  loss_dice_dn_7: 4.72  loss_bbox_dn_7: 0.7307  loss_giou_dn_7: 0.986  loss_ce_8: 462.5  loss_mask_8: 1.522  loss_dice_8: 4.744  loss_bbox_8: 9.106  loss_giou_8: 2.643  loss_ce_dn_8: 22.41  loss_mask_dn_8: 1.51  loss_dice_dn_8: 4.716  loss_bbox_dn_8: 0.7309  loss_giou_dn_8: 0.9863  loss_ce_interm: 2090  loss_mask_interm: 1.458  loss_dice_interm: 4.707  loss_bbox_interm: 9.204  loss_giou_interm: 2.668    time: 0.8558  last_time: 0.8837  data_time: 0.0100  last_data_time: 0.0108   lr: 1.25e-05  max_mem: 11835M\n",
            "\u001b[32m[01/31 10:33:37 d2.utils.events]: \u001b[0m eta: 3 days, 15:33:11  iter: 179  total_loss: 7997  loss_ce: 390  loss_mask: 1.022  loss_dice: 4.77  loss_bbox: 11.96  loss_giou: 2.868  loss_ce_dn: 20.49  loss_mask_dn: 0.9998  loss_dice_dn: 4.781  loss_bbox_dn: 0.5002  loss_giou_dn: 0.95  loss_ce_0: 1877  loss_mask_0: 1.016  loss_dice_0: 4.749  loss_bbox_0: 12.01  loss_giou_0: 2.877  loss_ce_dn_0: 119.6  loss_mask_dn_0: 1.013  loss_dice_dn_0: 4.782  loss_bbox_dn_0: 0.5016  loss_giou_dn_0: 0.9494  loss_ce_1: 433.1  loss_mask_1: 1.009  loss_dice_1: 4.758  loss_bbox_1: 11.95  loss_giou_1: 2.867  loss_ce_dn_1: 34.82  loss_mask_dn_1: 1.031  loss_dice_dn_1: 4.76  loss_bbox_dn_1: 0.5012  loss_giou_dn_1: 0.9492  loss_ce_2: 403.7  loss_mask_2: 1.011  loss_dice_2: 4.76  loss_bbox_2: 11.95  loss_giou_2: 2.864  loss_ce_dn_2: 24.7  loss_mask_dn_2: 1.004  loss_dice_dn_2: 4.776  loss_bbox_dn_2: 0.5009  loss_giou_dn_2: 0.9489  loss_ce_3: 400.3  loss_mask_3: 1.02  loss_dice_3: 4.772  loss_bbox_3: 11.96  loss_giou_3: 2.866  loss_ce_dn_3: 21.82  loss_mask_dn_3: 1.035  loss_dice_dn_3: 4.755  loss_bbox_dn_3: 0.5007  loss_giou_dn_3: 0.9488  loss_ce_4: 398.3  loss_mask_4: 1.017  loss_dice_4: 4.788  loss_bbox_4: 11.96  loss_giou_4: 2.856  loss_ce_dn_4: 21.81  loss_mask_dn_4: 1.006  loss_dice_dn_4: 4.78  loss_bbox_dn_4: 0.5004  loss_giou_dn_4: 0.9487  loss_ce_5: 400.9  loss_mask_5: 0.9857  loss_dice_5: 4.775  loss_bbox_5: 11.97  loss_giou_5: 2.86  loss_ce_dn_5: 21.16  loss_mask_dn_5: 0.9963  loss_dice_dn_5: 4.776  loss_bbox_dn_5: 0.5003  loss_giou_dn_5: 0.9486  loss_ce_6: 397.4  loss_mask_6: 1.014  loss_dice_6: 4.773  loss_bbox_6: 11.97  loss_giou_6: 2.883  loss_ce_dn_6: 22.32  loss_mask_dn_6: 1.02  loss_dice_dn_6: 4.775  loss_bbox_dn_6: 0.5002  loss_giou_dn_6: 0.9488  loss_ce_7: 395.8  loss_mask_7: 1.014  loss_dice_7: 4.767  loss_bbox_7: 11.96  loss_giou_7: 2.861  loss_ce_dn_7: 21.48  loss_mask_dn_7: 1.017  loss_dice_dn_7: 4.747  loss_bbox_dn_7: 0.5001  loss_giou_dn_7: 0.9491  loss_ce_8: 394.1  loss_mask_8: 1.015  loss_dice_8: 4.776  loss_bbox_8: 11.95  loss_giou_8: 2.857  loss_ce_dn_8: 21.2  loss_mask_dn_8: 1.015  loss_dice_dn_8: 4.754  loss_bbox_dn_8: 0.5001  loss_giou_dn_8: 0.9495  loss_ce_interm: 1877  loss_mask_interm: 1.017  loss_dice_interm: 4.751  loss_bbox_interm: 12.01  loss_giou_interm: 2.876    time: 0.8562  last_time: 0.8633  data_time: 0.0105  last_data_time: 0.0070   lr: 1.25e-05  max_mem: 11835M\n",
            "\u001b[32m[01/31 10:33:54 d2.utils.events]: \u001b[0m eta: 3 days, 15:31:29  iter: 199  total_loss: 7754  loss_ce: 376.1  loss_mask: 1.18  loss_dice: 4.734  loss_bbox: 10.79  loss_giou: 2.821  loss_ce_dn: 18.01  loss_mask_dn: 1.195  loss_dice_dn: 4.717  loss_bbox_dn: 0.6482  loss_giou_dn: 0.9189  loss_ce_0: 1823  loss_mask_0: 1.148  loss_dice_0: 4.723  loss_bbox_0: 10.84  loss_giou_0: 2.819  loss_ce_dn_0: 120.1  loss_mask_dn_0: 1.18  loss_dice_dn_0: 4.745  loss_bbox_dn_0: 0.6478  loss_giou_dn_0: 0.9129  loss_ce_1: 411.2  loss_mask_1: 1.198  loss_dice_1: 4.735  loss_bbox_1: 10.83  loss_giou_1: 2.82  loss_ce_dn_1: 31.75  loss_mask_dn_1: 1.177  loss_dice_dn_1: 4.729  loss_bbox_dn_1: 0.6477  loss_giou_dn_1: 0.9134  loss_ce_2: 387.3  loss_mask_2: 1.187  loss_dice_2: 4.718  loss_bbox_2: 10.82  loss_giou_2: 2.82  loss_ce_dn_2: 22.1  loss_mask_dn_2: 1.194  loss_dice_dn_2: 4.745  loss_bbox_dn_2: 0.6476  loss_giou_dn_2: 0.9139  loss_ce_3: 384.7  loss_mask_3: 1.184  loss_dice_3: 4.736  loss_bbox_3: 10.82  loss_giou_3: 2.82  loss_ce_dn_3: 19.57  loss_mask_dn_3: 1.169  loss_dice_dn_3: 4.736  loss_bbox_dn_3: 0.6475  loss_giou_dn_3: 0.9145  loss_ce_4: 383.5  loss_mask_4: 1.202  loss_dice_4: 4.761  loss_bbox_4: 10.82  loss_giou_4: 2.82  loss_ce_dn_4: 19.17  loss_mask_dn_4: 1.179  loss_dice_dn_4: 4.739  loss_bbox_dn_4: 0.6475  loss_giou_dn_4: 0.9151  loss_ce_5: 385.2  loss_mask_5: 1.22  loss_dice_5: 4.737  loss_bbox_5: 10.82  loss_giou_5: 2.82  loss_ce_dn_5: 18.7  loss_mask_dn_5: 1.187  loss_dice_dn_5: 4.737  loss_bbox_dn_5: 0.6476  loss_giou_dn_5: 0.9156  loss_ce_6: 381.9  loss_mask_6: 1.183  loss_dice_6: 4.732  loss_bbox_6: 10.82  loss_giou_6: 2.822  loss_ce_dn_6: 19.62  loss_mask_dn_6: 1.208  loss_dice_dn_6: 4.734  loss_bbox_dn_6: 0.6477  loss_giou_dn_6: 0.9164  loss_ce_7: 381  loss_mask_7: 1.202  loss_dice_7: 4.737  loss_bbox_7: 10.81  loss_giou_7: 2.821  loss_ce_dn_7: 18.91  loss_mask_dn_7: 1.179  loss_dice_dn_7: 4.7  loss_bbox_dn_7: 0.6479  loss_giou_dn_7: 0.9172  loss_ce_8: 379.4  loss_mask_8: 1.197  loss_dice_8: 4.745  loss_bbox_8: 10.8  loss_giou_8: 2.821  loss_ce_dn_8: 18.53  loss_mask_dn_8: 1.184  loss_dice_dn_8: 4.699  loss_bbox_dn_8: 0.648  loss_giou_dn_8: 0.918  loss_ce_interm: 1823  loss_mask_interm: 1.147  loss_dice_interm: 4.722  loss_bbox_interm: 10.84  loss_giou_interm: 2.819    time: 0.8561  last_time: 0.8414  data_time: 0.0105  last_data_time: 0.0127   lr: 1.25e-05  max_mem: 12042M\n",
            "\u001b[32m[01/31 10:34:11 d2.utils.events]: \u001b[0m eta: 3 days, 15:34:43  iter: 219  total_loss: 7431  loss_ce: 363  loss_mask: 1.136  loss_dice: 4.633  loss_bbox: 9.815  loss_giou: 2.877  loss_ce_dn: 15.54  loss_mask_dn: 1.15  loss_dice_dn: 4.637  loss_bbox_dn: 0.6308  loss_giou_dn: 0.9049  loss_ce_0: 1782  loss_mask_0: 1.19  loss_dice_0: 4.658  loss_bbox_0: 9.842  loss_giou_0: 2.874  loss_ce_dn_0: 118.1  loss_mask_dn_0: 1.166  loss_dice_dn_0: 4.689  loss_bbox_dn_0: 0.6345  loss_giou_dn_0: 0.9019  loss_ce_1: 387.9  loss_mask_1: 1.156  loss_dice_1: 4.658  loss_bbox_1: 9.838  loss_giou_1: 2.873  loss_ce_dn_1: 27.79  loss_mask_dn_1: 1.125  loss_dice_dn_1: 4.668  loss_bbox_dn_1: 0.6339  loss_giou_dn_1: 0.9019  loss_ce_2: 372  loss_mask_2: 1.137  loss_dice_2: 4.659  loss_bbox_2: 9.828  loss_giou_2: 2.874  loss_ce_dn_2: 18.87  loss_mask_dn_2: 1.135  loss_dice_dn_2: 4.666  loss_bbox_dn_2: 0.6333  loss_giou_dn_2: 0.9021  loss_ce_3: 368.6  loss_mask_3: 1.15  loss_dice_3: 4.662  loss_bbox_3: 9.844  loss_giou_3: 2.874  loss_ce_dn_3: 16.83  loss_mask_dn_3: 1.159  loss_dice_dn_3: 4.673  loss_bbox_dn_3: 0.6327  loss_giou_dn_3: 0.9024  loss_ce_4: 368.4  loss_mask_4: 1.122  loss_dice_4: 4.673  loss_bbox_4: 9.824  loss_giou_4: 2.875  loss_ce_dn_4: 16.62  loss_mask_dn_4: 1.172  loss_dice_dn_4: 4.661  loss_bbox_dn_4: 0.6323  loss_giou_dn_4: 0.9026  loss_ce_5: 369.1  loss_mask_5: 1.109  loss_dice_5: 4.678  loss_bbox_5: 9.831  loss_giou_5: 2.877  loss_ce_dn_5: 16.23  loss_mask_dn_5: 1.138  loss_dice_dn_5: 4.649  loss_bbox_dn_5: 0.632  loss_giou_dn_5: 0.9029  loss_ce_6: 367.2  loss_mask_6: 1.132  loss_dice_6: 4.663  loss_bbox_6: 9.835  loss_giou_6: 2.876  loss_ce_dn_6: 16.94  loss_mask_dn_6: 1.147  loss_dice_dn_6: 4.66  loss_bbox_dn_6: 0.6317  loss_giou_dn_6: 0.9033  loss_ce_7: 366.1  loss_mask_7: 1.121  loss_dice_7: 4.657  loss_bbox_7: 9.825  loss_giou_7: 2.877  loss_ce_dn_7: 16.36  loss_mask_dn_7: 1.21  loss_dice_dn_7: 4.615  loss_bbox_dn_7: 0.6313  loss_giou_dn_7: 0.9038  loss_ce_8: 365.1  loss_mask_8: 1.118  loss_dice_8: 4.666  loss_bbox_8: 9.842  loss_giou_8: 2.878  loss_ce_dn_8: 16.09  loss_mask_dn_8: 1.197  loss_dice_dn_8: 4.631  loss_bbox_dn_8: 0.631  loss_giou_dn_8: 0.9042  loss_ce_interm: 1782  loss_mask_interm: 1.189  loss_dice_interm: 4.66  loss_bbox_interm: 9.842  loss_giou_interm: 2.874    time: 0.8569  last_time: 0.8594  data_time: 0.0109  last_data_time: 0.0081   lr: 1.25e-05  max_mem: 12042M\n",
            "\u001b[32m[01/31 10:34:29 d2.utils.events]: \u001b[0m eta: 3 days, 15:33:33  iter: 239  total_loss: 5916  loss_ce: 280.8  loss_mask: 1.264  loss_dice: 4.751  loss_bbox: 9.992  loss_giou: 2.709  loss_ce_dn: 15.52  loss_mask_dn: 1.255  loss_dice_dn: 4.735  loss_bbox_dn: 0.7159  loss_giou_dn: 1.195  loss_ce_0: 1398  loss_mask_0: 1.159  loss_dice_0: 4.757  loss_bbox_0: 10  loss_giou_0: 2.701  loss_ce_dn_0: 125.1  loss_mask_dn_0: 1.311  loss_dice_dn_0: 4.724  loss_bbox_dn_0: 0.7176  loss_giou_dn_0: 1.191  loss_ce_1: 294.6  loss_mask_1: 1.279  loss_dice_1: 4.752  loss_bbox_1: 9.985  loss_giou_1: 2.711  loss_ce_dn_1: 28.38  loss_mask_dn_1: 1.285  loss_dice_dn_1: 4.732  loss_bbox_dn_1: 0.7173  loss_giou_dn_1: 1.191  loss_ce_2: 286.3  loss_mask_2: 1.234  loss_dice_2: 4.75  loss_bbox_2: 9.989  loss_giou_2: 2.711  loss_ce_dn_2: 19.36  loss_mask_dn_2: 1.281  loss_dice_dn_2: 4.75  loss_bbox_dn_2: 0.7171  loss_giou_dn_2: 1.191  loss_ce_3: 283.8  loss_mask_3: 1.24  loss_dice_3: 4.743  loss_bbox_3: 9.99  loss_giou_3: 2.711  loss_ce_dn_3: 16.77  loss_mask_dn_3: 1.267  loss_dice_dn_3: 4.75  loss_bbox_dn_3: 0.7168  loss_giou_dn_3: 1.192  loss_ce_4: 284.4  loss_mask_4: 1.26  loss_dice_4: 4.763  loss_bbox_4: 9.99  loss_giou_4: 2.712  loss_ce_dn_4: 16.57  loss_mask_dn_4: 1.267  loss_dice_dn_4: 4.735  loss_bbox_dn_4: 0.7166  loss_giou_dn_4: 1.192  loss_ce_5: 284.6  loss_mask_5: 1.246  loss_dice_5: 4.753  loss_bbox_5: 9.97  loss_giou_5: 2.7  loss_ce_dn_5: 16.15  loss_mask_dn_5: 1.295  loss_dice_dn_5: 4.747  loss_bbox_dn_5: 0.7164  loss_giou_dn_5: 1.193  loss_ce_6: 282.9  loss_mask_6: 1.294  loss_dice_6: 4.748  loss_bbox_6: 9.97  loss_giou_6: 2.707  loss_ce_dn_6: 16.89  loss_mask_dn_6: 1.274  loss_dice_dn_6: 4.752  loss_bbox_dn_6: 0.7162  loss_giou_dn_6: 1.193  loss_ce_7: 282.4  loss_mask_7: 1.24  loss_dice_7: 4.756  loss_bbox_7: 9.967  loss_giou_7: 2.701  loss_ce_dn_7: 16.25  loss_mask_dn_7: 1.276  loss_dice_dn_7: 4.721  loss_bbox_dn_7: 0.716  loss_giou_dn_7: 1.194  loss_ce_8: 281.9  loss_mask_8: 1.266  loss_dice_8: 4.752  loss_bbox_8: 9.982  loss_giou_8: 2.701  loss_ce_dn_8: 15.97  loss_mask_dn_8: 1.271  loss_dice_dn_8: 4.719  loss_bbox_dn_8: 0.7159  loss_giou_dn_8: 1.194  loss_ce_interm: 1398  loss_mask_interm: 1.153  loss_dice_interm: 4.757  loss_bbox_interm: 10.03  loss_giou_interm: 2.701    time: 0.8566  last_time: 0.8706  data_time: 0.0101  last_data_time: 0.0141   lr: 1.25e-05  max_mem: 12042M\n",
            "\u001b[32m[01/31 10:34:46 d2.utils.events]: \u001b[0m eta: 3 days, 15:32:02  iter: 259  total_loss: 4737  loss_ce: 217.3  loss_mask: 1.062  loss_dice: 4.763  loss_bbox: 11.46  loss_giou: 2.66  loss_ce_dn: 13.07  loss_mask_dn: 1.068  loss_dice_dn: 4.771  loss_bbox_dn: 0.7059  loss_giou_dn: 0.9224  loss_ce_0: 1099  loss_mask_0: 1.103  loss_dice_0: 4.775  loss_bbox_0: 11.47  loss_giou_0: 2.663  loss_ce_dn_0: 122.7  loss_mask_dn_0: 1.081  loss_dice_dn_0: 4.773  loss_bbox_dn_0: 0.7088  loss_giou_dn_0: 0.9071  loss_ce_1: 224.6  loss_mask_1: 1.082  loss_dice_1: 4.764  loss_bbox_1: 11.49  loss_giou_1: 2.672  loss_ce_dn_1: 23.79  loss_mask_dn_1: 1.045  loss_dice_dn_1: 4.763  loss_bbox_dn_1: 0.7081  loss_giou_dn_1: 0.9084  loss_ce_2: 220.5  loss_mask_2: 1.078  loss_dice_2: 4.769  loss_bbox_2: 11.47  loss_giou_2: 2.654  loss_ce_dn_2: 16.09  loss_mask_dn_2: 1.078  loss_dice_dn_2: 4.776  loss_bbox_dn_2: 0.7075  loss_giou_dn_2: 0.9096  loss_ce_3: 219.2  loss_mask_3: 1.088  loss_dice_3: 4.759  loss_bbox_3: 11.47  loss_giou_3: 2.668  loss_ce_dn_3: 14.42  loss_mask_dn_3: 1.083  loss_dice_dn_3: 4.763  loss_bbox_dn_3: 0.707  loss_giou_dn_3: 0.9111  loss_ce_4: 219.6  loss_mask_4: 1.08  loss_dice_4: 4.772  loss_bbox_4: 11.47  loss_giou_4: 2.665  loss_ce_dn_4: 14.13  loss_mask_dn_4: 1.1  loss_dice_dn_4: 4.745  loss_bbox_dn_4: 0.7067  loss_giou_dn_4: 0.9126  loss_ce_5: 219.5  loss_mask_5: 1.062  loss_dice_5: 4.766  loss_bbox_5: 11.47  loss_giou_5: 2.663  loss_ce_dn_5: 13.84  loss_mask_dn_5: 1.098  loss_dice_dn_5: 4.77  loss_bbox_dn_5: 0.7064  loss_giou_dn_5: 0.9143  loss_ce_6: 218.5  loss_mask_6: 1.059  loss_dice_6: 4.757  loss_bbox_6: 11.46  loss_giou_6: 2.655  loss_ce_dn_6: 14.17  loss_mask_dn_6: 1.063  loss_dice_dn_6: 4.761  loss_bbox_dn_6: 0.7061  loss_giou_dn_6: 0.9167  loss_ce_7: 218.2  loss_mask_7: 1.062  loss_dice_7: 4.774  loss_bbox_7: 11.46  loss_giou_7: 2.652  loss_ce_dn_7: 13.65  loss_mask_dn_7: 1.112  loss_dice_dn_7: 4.744  loss_bbox_dn_7: 0.7059  loss_giou_dn_7: 0.9195  loss_ce_8: 217.9  loss_mask_8: 1.061  loss_dice_8: 4.775  loss_bbox_8: 11.46  loss_giou_8: 2.653  loss_ce_dn_8: 13.42  loss_mask_dn_8: 1.11  loss_dice_dn_8: 4.737  loss_bbox_dn_8: 0.7058  loss_giou_dn_8: 0.9209  loss_ce_interm: 1099  loss_mask_interm: 1.106  loss_dice_interm: 4.776  loss_bbox_interm: 11.47  loss_giou_interm: 2.663    time: 0.8565  last_time: 0.8424  data_time: 0.0107  last_data_time: 0.0111   lr: 1.25e-05  max_mem: 12042M\n",
            "\u001b[32m[01/31 10:35:03 d2.utils.events]: \u001b[0m eta: 3 days, 15:32:58  iter: 279  total_loss: 4905  loss_ce: 226.3  loss_mask: 1.154  loss_dice: 4.741  loss_bbox: 11.15  loss_giou: 2.765  loss_ce_dn: 11.25  loss_mask_dn: 1.137  loss_dice_dn: 4.727  loss_bbox_dn: 0.5834  loss_giou_dn: 0.8961  loss_ce_0: 1154  loss_mask_0: 1.178  loss_dice_0: 4.762  loss_bbox_0: 11.2  loss_giou_0: 2.773  loss_ce_dn_0: 115.3  loss_mask_dn_0: 1.155  loss_dice_dn_0: 4.757  loss_bbox_dn_0: 0.5829  loss_giou_dn_0: 0.8693  loss_ce_1: 231.9  loss_mask_1: 1.173  loss_dice_1: 4.74  loss_bbox_1: 11.19  loss_giou_1: 2.77  loss_ce_dn_1: 19.84  loss_mask_dn_1: 1.135  loss_dice_dn_1: 4.724  loss_bbox_dn_1: 0.5826  loss_giou_dn_1: 0.871  loss_ce_2: 229  loss_mask_2: 1.165  loss_dice_2: 4.745  loss_bbox_2: 11.19  loss_giou_2: 2.77  loss_ce_dn_2: 13.83  loss_mask_dn_2: 1.148  loss_dice_dn_2: 4.724  loss_bbox_dn_2: 0.5825  loss_giou_dn_2: 0.8726  loss_ce_3: 227.8  loss_mask_3: 1.183  loss_dice_3: 4.744  loss_bbox_3: 11.22  loss_giou_3: 2.764  loss_ce_dn_3: 12.5  loss_mask_dn_3: 1.182  loss_dice_dn_3: 4.752  loss_bbox_dn_3: 0.5825  loss_giou_dn_3: 0.8745  loss_ce_4: 228.3  loss_mask_4: 1.17  loss_dice_4: 4.762  loss_bbox_4: 11.18  loss_giou_4: 2.764  loss_ce_dn_4: 12.21  loss_mask_dn_4: 1.157  loss_dice_dn_4: 4.737  loss_bbox_dn_4: 0.5825  loss_giou_dn_4: 0.8761  loss_ce_5: 228.2  loss_mask_5: 1.141  loss_dice_5: 4.74  loss_bbox_5: 11.17  loss_giou_5: 2.764  loss_ce_dn_5: 11.94  loss_mask_dn_5: 1.155  loss_dice_dn_5: 4.72  loss_bbox_dn_5: 0.5825  loss_giou_dn_5: 0.8781  loss_ce_6: 227.2  loss_mask_6: 1.14  loss_dice_6: 4.735  loss_bbox_6: 11.21  loss_giou_6: 2.764  loss_ce_dn_6: 12.1  loss_mask_dn_6: 1.161  loss_dice_dn_6: 4.735  loss_bbox_dn_6: 0.5827  loss_giou_dn_6: 0.8846  loss_ce_7: 227.1  loss_mask_7: 1.15  loss_dice_7: 4.752  loss_bbox_7: 11.2  loss_giou_7: 2.757  loss_ce_dn_7: 11.71  loss_mask_dn_7: 1.164  loss_dice_dn_7: 4.723  loss_bbox_dn_7: 0.5828  loss_giou_dn_7: 0.8898  loss_ce_8: 226.8  loss_mask_8: 1.154  loss_dice_8: 4.751  loss_bbox_8: 11.2  loss_giou_8: 2.755  loss_ce_dn_8: 11.48  loss_mask_dn_8: 1.129  loss_dice_dn_8: 4.724  loss_bbox_dn_8: 0.5831  loss_giou_dn_8: 0.8927  loss_ce_interm: 1154  loss_mask_interm: 1.172  loss_dice_interm: 4.762  loss_bbox_interm: 11.2  loss_giou_interm: 2.773    time: 0.8562  last_time: 0.8551  data_time: 0.0101  last_data_time: 0.0088   lr: 1.25e-05  max_mem: 12042M\n",
            "\u001b[32m[01/31 10:35:20 d2.utils.events]: \u001b[0m eta: 3 days, 15:30:40  iter: 299  total_loss: 4263  loss_ce: 193.7  loss_mask: 1.092  loss_dice: 4.669  loss_bbox: 11.99  loss_giou: 2.725  loss_ce_dn: 10.54  loss_mask_dn: 1.103  loss_dice_dn: 4.639  loss_bbox_dn: 0.7061  loss_giou_dn: 0.9051  loss_ce_0: 986.8  loss_mask_0: 1.119  loss_dice_0: 4.661  loss_bbox_0: 11.97  loss_giou_0: 2.722  loss_ce_dn_0: 122.4  loss_mask_dn_0: 1.089  loss_dice_dn_0: 4.715  loss_bbox_dn_0: 0.7088  loss_giou_dn_0: 0.8559  loss_ce_1: 198.4  loss_mask_1: 1.098  loss_dice_1: 4.67  loss_bbox_1: 11.97  loss_giou_1: 2.723  loss_ce_dn_1: 17.55  loss_mask_dn_1: 1.093  loss_dice_dn_1: 4.653  loss_bbox_dn_1: 0.7081  loss_giou_dn_1: 0.8572  loss_ce_2: 195.9  loss_mask_2: 1.089  loss_dice_2: 4.67  loss_bbox_2: 11.97  loss_giou_2: 2.722  loss_ce_dn_2: 12.23  loss_mask_dn_2: 1.092  loss_dice_dn_2: 4.652  loss_bbox_dn_2: 0.7076  loss_giou_dn_2: 0.8611  loss_ce_3: 194.8  loss_mask_3: 1.092  loss_dice_3: 4.672  loss_bbox_3: 11.97  loss_giou_3: 2.724  loss_ce_dn_3: 11.18  loss_mask_dn_3: 1.091  loss_dice_dn_3: 4.693  loss_bbox_dn_3: 0.7072  loss_giou_dn_3: 0.8649  loss_ce_4: 195.2  loss_mask_4: 1.078  loss_dice_4: 4.68  loss_bbox_4: 11.97  loss_giou_4: 2.723  loss_ce_dn_4: 11.14  loss_mask_dn_4: 1.133  loss_dice_dn_4: 4.653  loss_bbox_dn_4: 0.7068  loss_giou_dn_4: 0.8704  loss_ce_5: 195.1  loss_mask_5: 1.083  loss_dice_5: 4.668  loss_bbox_5: 11.97  loss_giou_5: 2.723  loss_ce_dn_5: 10.99  loss_mask_dn_5: 1.107  loss_dice_dn_5: 4.651  loss_bbox_dn_5: 0.7066  loss_giou_dn_5: 0.8769  loss_ce_6: 194.4  loss_mask_6: 1.089  loss_dice_6: 4.659  loss_bbox_6: 11.97  loss_giou_6: 2.726  loss_ce_dn_6: 11.53  loss_mask_dn_6: 1.099  loss_dice_dn_6: 4.667  loss_bbox_dn_6: 0.7064  loss_giou_dn_6: 0.8865  loss_ce_7: 194.2  loss_mask_7: 1.062  loss_dice_7: 4.677  loss_bbox_7: 11.97  loss_giou_7: 2.726  loss_ce_dn_7: 11.03  loss_mask_dn_7: 1.16  loss_dice_dn_7: 4.627  loss_bbox_dn_7: 0.7062  loss_giou_dn_7: 0.8931  loss_ce_8: 194.1  loss_mask_8: 1.085  loss_dice_8: 4.674  loss_bbox_8: 11.97  loss_giou_8: 2.726  loss_ce_dn_8: 10.86  loss_mask_dn_8: 1.121  loss_dice_dn_8: 4.641  loss_bbox_dn_8: 0.7061  loss_giou_dn_8: 0.8991  loss_ce_interm: 986.8  loss_mask_interm: 1.109  loss_dice_interm: 4.659  loss_bbox_interm: 11.97  loss_giou_interm: 2.722    time: 0.8562  last_time: 0.8647  data_time: 0.0103  last_data_time: 0.0109   lr: 1.25e-05  max_mem: 12042M\n",
            "\u001b[32m[01/31 10:35:37 d2.utils.events]: \u001b[0m eta: 3 days, 15:32:24  iter: 319  total_loss: 4258  loss_ce: 195.1  loss_mask: 1.383  loss_dice: 4.606  loss_bbox: 9.699  loss_giou: 2.601  loss_ce_dn: 8.582  loss_mask_dn: 1.391  loss_dice_dn: 4.608  loss_bbox_dn: 0.8348  loss_giou_dn: 0.9302  loss_ce_0: 997  loss_mask_0: 1.39  loss_dice_0: 4.639  loss_bbox_0: 9.718  loss_giou_0: 2.597  loss_ce_dn_0: 115.5  loss_mask_dn_0: 1.355  loss_dice_dn_0: 4.65  loss_bbox_dn_0: 0.8332  loss_giou_dn_0: 0.9146  loss_ce_1: 198.5  loss_mask_1: 1.397  loss_dice_1: 4.602  loss_bbox_1: 9.714  loss_giou_1: 2.598  loss_ce_dn_1: 15.33  loss_mask_dn_1: 1.36  loss_dice_dn_1: 4.619  loss_bbox_dn_1: 0.8332  loss_giou_dn_1: 0.9157  loss_ce_2: 196.8  loss_mask_2: 1.394  loss_dice_2: 4.614  loss_bbox_2: 9.71  loss_giou_2: 2.598  loss_ce_dn_2: 10.15  loss_mask_dn_2: 1.362  loss_dice_dn_2: 4.625  loss_bbox_dn_2: 0.8333  loss_giou_dn_2: 0.9167  loss_ce_3: 196  loss_mask_3: 1.388  loss_dice_3: 4.604  loss_bbox_3: 9.708  loss_giou_3: 2.598  loss_ce_dn_3: 9.228  loss_mask_dn_3: 1.412  loss_dice_dn_3: 4.622  loss_bbox_dn_3: 0.8334  loss_giou_dn_3: 0.918  loss_ce_4: 196.3  loss_mask_4: 1.39  loss_dice_4: 4.61  loss_bbox_4: 9.695  loss_giou_4: 2.598  loss_ce_dn_4: 9.095  loss_mask_dn_4: 1.374  loss_dice_dn_4: 4.586  loss_bbox_dn_4: 0.8335  loss_giou_dn_4: 0.9194  loss_ce_5: 196.3  loss_mask_5: 1.421  loss_dice_5: 4.606  loss_bbox_5: 9.705  loss_giou_5: 2.603  loss_ce_dn_5: 8.99  loss_mask_dn_5: 1.394  loss_dice_dn_5: 4.616  loss_bbox_dn_5: 0.8336  loss_giou_dn_5: 0.9209  loss_ce_6: 195.6  loss_mask_6: 1.394  loss_dice_6: 4.601  loss_bbox_6: 9.703  loss_giou_6: 2.605  loss_ce_dn_6: 9.311  loss_mask_dn_6: 1.39  loss_dice_dn_6: 4.62  loss_bbox_dn_6: 0.8339  loss_giou_dn_6: 0.9228  loss_ce_7: 195.4  loss_mask_7: 1.412  loss_dice_7: 4.63  loss_bbox_7: 9.702  loss_giou_7: 2.605  loss_ce_dn_7: 8.961  loss_mask_dn_7: 1.39  loss_dice_dn_7: 4.59  loss_bbox_dn_7: 0.8341  loss_giou_dn_7: 0.925  loss_ce_8: 195.3  loss_mask_8: 1.392  loss_dice_8: 4.614  loss_bbox_8: 9.7  loss_giou_8: 2.606  loss_ce_dn_8: 8.81  loss_mask_dn_8: 1.384  loss_dice_dn_8: 4.592  loss_bbox_dn_8: 0.8344  loss_giou_dn_8: 0.9275  loss_ce_interm: 997  loss_mask_interm: 1.391  loss_dice_interm: 4.642  loss_bbox_interm: 9.718  loss_giou_interm: 2.597    time: 0.8562  last_time: 0.8269  data_time: 0.0095  last_data_time: 0.0086   lr: 1.25e-05  max_mem: 12042M\n",
            "\u001b[32m[01/31 10:35:55 d2.utils.events]: \u001b[0m eta: 3 days, 15:33:21  iter: 339  total_loss: 3092  loss_ce: 136.4  loss_mask: 1.116  loss_dice: 4.777  loss_bbox: 12.52  loss_giou: 2.72  loss_ce_dn: 8.312  loss_mask_dn: 1.128  loss_dice_dn: 4.777  loss_bbox_dn: 0.6871  loss_giou_dn: 0.9834  loss_ce_0: 694.1  loss_mask_0: 1.125  loss_dice_0: 4.78  loss_bbox_0: 12.54  loss_giou_0: 2.711  loss_ce_dn_0: 122  loss_mask_dn_0: 1.03  loss_dice_dn_0: 4.768  loss_bbox_dn_0: 0.6836  loss_giou_dn_0: 0.9636  loss_ce_1: 139.2  loss_mask_1: 1.143  loss_dice_1: 4.772  loss_bbox_1: 12.53  loss_giou_1: 2.72  loss_ce_dn_1: 14.11  loss_mask_dn_1: 1.062  loss_dice_dn_1: 4.749  loss_bbox_dn_1: 0.6838  loss_giou_dn_1: 0.9654  loss_ce_2: 137.4  loss_mask_2: 1.139  loss_dice_2: 4.774  loss_bbox_2: 12.53  loss_giou_2: 2.719  loss_ce_dn_2: 9.682  loss_mask_dn_2: 1.108  loss_dice_dn_2: 4.781  loss_bbox_dn_2: 0.684  loss_giou_dn_2: 0.9672  loss_ce_3: 137  loss_mask_3: 1.119  loss_dice_3: 4.78  loss_bbox_3: 12.52  loss_giou_3: 2.719  loss_ce_dn_3: 8.854  loss_mask_dn_3: 1.125  loss_dice_dn_3: 4.777  loss_bbox_dn_3: 0.6843  loss_giou_dn_3: 0.9694  loss_ce_4: 137.2  loss_mask_4: 1.132  loss_dice_4: 4.78  loss_bbox_4: 12.53  loss_giou_4: 2.719  loss_ce_dn_4: 8.858  loss_mask_dn_4: 1.126  loss_dice_dn_4: 4.762  loss_bbox_dn_4: 0.6846  loss_giou_dn_4: 0.9712  loss_ce_5: 137.1  loss_mask_5: 1.132  loss_dice_5: 4.776  loss_bbox_5: 12.53  loss_giou_5: 2.72  loss_ce_dn_5: 8.744  loss_mask_dn_5: 1.122  loss_dice_dn_5: 4.779  loss_bbox_dn_5: 0.685  loss_giou_dn_5: 0.9731  loss_ce_6: 136.9  loss_mask_6: 1.124  loss_dice_6: 4.776  loss_bbox_6: 12.52  loss_giou_6: 2.72  loss_ce_dn_6: 9.07  loss_mask_dn_6: 1.142  loss_dice_dn_6: 4.775  loss_bbox_dn_6: 0.6854  loss_giou_dn_6: 0.9754  loss_ce_7: 136.7  loss_mask_7: 1.126  loss_dice_7: 4.782  loss_bbox_7: 12.52  loss_giou_7: 2.72  loss_ce_dn_7: 8.711  loss_mask_dn_7: 1.139  loss_dice_dn_7: 4.773  loss_bbox_dn_7: 0.6859  loss_giou_dn_7: 0.9781  loss_ce_8: 136.6  loss_mask_8: 1.127  loss_dice_8: 4.781  loss_bbox_8: 12.52  loss_giou_8: 2.72  loss_ce_dn_8: 8.597  loss_mask_dn_8: 1.117  loss_dice_dn_8: 4.762  loss_bbox_dn_8: 0.6865  loss_giou_dn_8: 0.9807  loss_ce_interm: 694.1  loss_mask_interm: 1.115  loss_dice_interm: 4.781  loss_bbox_interm: 12.54  loss_giou_interm: 2.711    time: 0.8566  last_time: 0.8977  data_time: 0.0113  last_data_time: 0.0086   lr: 1.25e-05  max_mem: 12042M\n",
            "\u001b[32m[01/31 10:36:12 d2.utils.events]: \u001b[0m eta: 3 days, 15:33:04  iter: 359  total_loss: 2786  loss_ce: 121.6  loss_mask: 1.088  loss_dice: 4.756  loss_bbox: 13.06  loss_giou: 2.605  loss_ce_dn: 7.02  loss_mask_dn: 1.082  loss_dice_dn: 4.746  loss_bbox_dn: 0.5998  loss_giou_dn: 0.8843  loss_ce_0: 618.9  loss_mask_0: 1.096  loss_dice_0: 4.755  loss_bbox_0: 13.06  loss_giou_0: 2.624  loss_ce_dn_0: 115.9  loss_mask_dn_0: 1.031  loss_dice_dn_0: 4.707  loss_bbox_dn_0: 0.5997  loss_giou_dn_0: 0.8765  loss_ce_1: 123.3  loss_mask_1: 1.082  loss_dice_1: 4.748  loss_bbox_1: 13.06  loss_giou_1: 2.611  loss_ce_dn_1: 11.55  loss_mask_dn_1: 1.053  loss_dice_dn_1: 4.743  loss_bbox_dn_1: 0.5995  loss_giou_dn_1: 0.8764  loss_ce_2: 122.3  loss_mask_2: 1.097  loss_dice_2: 4.75  loss_bbox_2: 13.06  loss_giou_2: 2.613  loss_ce_dn_2: 8.052  loss_mask_dn_2: 1.073  loss_dice_dn_2: 4.75  loss_bbox_dn_2: 0.5993  loss_giou_dn_2: 0.8766  loss_ce_3: 121.9  loss_mask_3: 1.1  loss_dice_3: 4.754  loss_bbox_3: 13.06  loss_giou_3: 2.608  loss_ce_dn_3: 7.488  loss_mask_dn_3: 1.058  loss_dice_dn_3: 4.768  loss_bbox_dn_3: 0.5992  loss_giou_dn_3: 0.8771  loss_ce_4: 122.1  loss_mask_4: 1.091  loss_dice_4: 4.761  loss_bbox_4: 13.06  loss_giou_4: 2.613  loss_ce_dn_4: 7.427  loss_mask_dn_4: 1.058  loss_dice_dn_4: 4.748  loss_bbox_dn_4: 0.5992  loss_giou_dn_4: 0.8775  loss_ce_5: 122.1  loss_mask_5: 1.073  loss_dice_5: 4.759  loss_bbox_5: 13.06  loss_giou_5: 2.608  loss_ce_dn_5: 7.362  loss_mask_dn_5: 1.087  loss_dice_dn_5: 4.761  loss_bbox_dn_5: 0.5992  loss_giou_dn_5: 0.8782  loss_ce_6: 121.9  loss_mask_6: 1.091  loss_dice_6: 4.749  loss_bbox_6: 13.06  loss_giou_6: 2.608  loss_ce_dn_6: 7.638  loss_mask_dn_6: 1.077  loss_dice_dn_6: 4.755  loss_bbox_dn_6: 0.5993  loss_giou_dn_6: 0.8788  loss_ce_7: 121.8  loss_mask_7: 1.081  loss_dice_7: 4.764  loss_bbox_7: 13.06  loss_giou_7: 2.608  loss_ce_dn_7: 7.327  loss_mask_dn_7: 1.109  loss_dice_dn_7: 4.754  loss_bbox_dn_7: 0.5994  loss_giou_dn_7: 0.8804  loss_ce_8: 121.7  loss_mask_8: 1.08  loss_dice_8: 4.757  loss_bbox_8: 13.06  loss_giou_8: 2.608  loss_ce_dn_8: 7.212  loss_mask_dn_8: 1.098  loss_dice_dn_8: 4.752  loss_bbox_dn_8: 0.5996  loss_giou_dn_8: 0.8822  loss_ce_interm: 618.9  loss_mask_interm: 1.097  loss_dice_interm: 4.754  loss_bbox_interm: 13.06  loss_giou_interm: 2.624    time: 0.8568  last_time: 0.8464  data_time: 0.0109  last_data_time: 0.0103   lr: 1.25e-05  max_mem: 12042M\n",
            "\u001b[32m[01/31 10:36:29 d2.utils.events]: \u001b[0m eta: 3 days, 15:34:30  iter: 379  total_loss: 3124  loss_ce: 135.8  loss_mask: 1.282  loss_dice: 4.588  loss_bbox: 11.89  loss_giou: 2.626  loss_ce_dn: 6.044  loss_mask_dn: 1.283  loss_dice_dn: 4.569  loss_bbox_dn: 0.7837  loss_giou_dn: 0.8928  loss_ce_0: 693.9  loss_mask_0: 1.304  loss_dice_0: 4.604  loss_bbox_0: 11.88  loss_giou_0: 2.61  loss_ce_dn_0: 112.8  loss_mask_dn_0: 1.377  loss_dice_dn_0: 4.603  loss_bbox_dn_0: 0.7808  loss_giou_dn_0: 0.8651  loss_ce_1: 137.8  loss_mask_1: 1.313  loss_dice_1: 4.595  loss_bbox_1: 11.88  loss_giou_1: 2.672  loss_ce_dn_1: 10.03  loss_mask_dn_1: 1.313  loss_dice_dn_1: 4.574  loss_bbox_dn_1: 0.7808  loss_giou_dn_1: 0.8667  loss_ce_2: 136.6  loss_mask_2: 1.308  loss_dice_2: 4.591  loss_bbox_2: 11.88  loss_giou_2: 2.644  loss_ce_dn_2: 6.873  loss_mask_dn_2: 1.292  loss_dice_dn_2: 4.577  loss_bbox_dn_2: 0.7808  loss_giou_dn_2: 0.8687  loss_ce_3: 136.3  loss_mask_3: 1.281  loss_dice_3: 4.587  loss_bbox_3: 11.88  loss_giou_3: 2.64  loss_ce_dn_3: 6.447  loss_mask_dn_3: 1.269  loss_dice_dn_3: 4.598  loss_bbox_dn_3: 0.781  loss_giou_dn_3: 0.8713  loss_ce_4: 136.5  loss_mask_4: 1.281  loss_dice_4: 4.595  loss_bbox_4: 11.88  loss_giou_4: 2.641  loss_ce_dn_4: 6.39  loss_mask_dn_4: 1.28  loss_dice_dn_4: 4.564  loss_bbox_dn_4: 0.7811  loss_giou_dn_4: 0.8736  loss_ce_5: 136.5  loss_mask_5: 1.285  loss_dice_5: 4.592  loss_bbox_5: 11.88  loss_giou_5: 2.641  loss_ce_dn_5: 6.382  loss_mask_dn_5: 1.282  loss_dice_dn_5: 4.583  loss_bbox_dn_5: 0.7815  loss_giou_dn_5: 0.8769  loss_ce_6: 136.3  loss_mask_6: 1.301  loss_dice_6: 4.578  loss_bbox_6: 11.88  loss_giou_6: 2.646  loss_ce_dn_6: 6.492  loss_mask_dn_6: 1.29  loss_dice_dn_6: 4.568  loss_bbox_dn_6: 0.7819  loss_giou_dn_6: 0.8807  loss_ce_7: 136.1  loss_mask_7: 1.282  loss_dice_7: 4.595  loss_bbox_7: 11.88  loss_giou_7: 2.646  loss_ce_dn_7: 6.291  loss_mask_dn_7: 1.295  loss_dice_dn_7: 4.563  loss_bbox_dn_7: 0.7824  loss_giou_dn_7: 0.8835  loss_ce_8: 136  loss_mask_8: 1.31  loss_dice_8: 4.592  loss_bbox_8: 11.88  loss_giou_8: 2.642  loss_ce_dn_8: 6.182  loss_mask_dn_8: 1.289  loss_dice_dn_8: 4.569  loss_bbox_dn_8: 0.783  loss_giou_dn_8: 0.8865  loss_ce_interm: 693.9  loss_mask_interm: 1.3  loss_dice_interm: 4.603  loss_bbox_interm: 11.88  loss_giou_interm: 2.61    time: 0.8570  last_time: 0.8629  data_time: 0.0101  last_data_time: 0.0088   lr: 1.25e-05  max_mem: 12042M\n",
            "\u001b[32m[01/31 10:36:46 d2.utils.events]: \u001b[0m eta: 3 days, 15:34:13  iter: 399  total_loss: 2579  loss_ce: 108.9  loss_mask: 1.15  loss_dice: 4.619  loss_bbox: 12.87  loss_giou: 2.652  loss_ce_dn: 6.057  loss_mask_dn: 1.126  loss_dice_dn: 4.627  loss_bbox_dn: 0.6158  loss_giou_dn: 0.8823  loss_ce_0: 555.2  loss_mask_0: 1.154  loss_dice_0: 4.651  loss_bbox_0: 12.89  loss_giou_0: 2.654  loss_ce_dn_0: 121.6  loss_mask_dn_0: 1.111  loss_dice_dn_0: 4.667  loss_bbox_dn_0: 0.6146  loss_giou_dn_0: 0.8592  loss_ce_1: 109.8  loss_mask_1: 1.15  loss_dice_1: 4.609  loss_bbox_1: 12.87  loss_giou_1: 2.651  loss_ce_dn_1: 9.8  loss_mask_dn_1: 1.093  loss_dice_dn_1: 4.625  loss_bbox_dn_1: 0.6145  loss_giou_dn_1: 0.8602  loss_ce_2: 109.2  loss_mask_2: 1.155  loss_dice_2: 4.614  loss_bbox_2: 12.87  loss_giou_2: 2.665  loss_ce_dn_2: 6.954  loss_mask_dn_2: 1.111  loss_dice_dn_2: 4.626  loss_bbox_dn_2: 0.6143  loss_giou_dn_2: 0.8618  loss_ce_3: 109.1  loss_mask_3: 1.146  loss_dice_3: 4.614  loss_bbox_3: 12.87  loss_giou_3: 2.652  loss_ce_dn_3: 6.429  loss_mask_dn_3: 1.1  loss_dice_dn_3: 4.648  loss_bbox_dn_3: 0.6142  loss_giou_dn_3: 0.8634  loss_ce_4: 109.2  loss_mask_4: 1.147  loss_dice_4: 4.624  loss_bbox_4: 12.87  loss_giou_4: 2.652  loss_ce_dn_4: 6.393  loss_mask_dn_4: 1.124  loss_dice_dn_4: 4.617  loss_bbox_dn_4: 0.6142  loss_giou_dn_4: 0.8661  loss_ce_5: 109.2  loss_mask_5: 1.145  loss_dice_5: 4.622  loss_bbox_5: 12.87  loss_giou_5: 2.652  loss_ce_dn_5: 6.346  loss_mask_dn_5: 1.143  loss_dice_dn_5: 4.62  loss_bbox_dn_5: 0.6142  loss_giou_dn_5: 0.8706  loss_ce_6: 109  loss_mask_6: 1.156  loss_dice_6: 4.608  loss_bbox_6: 12.87  loss_giou_6: 2.652  loss_ce_dn_6: 6.482  loss_mask_dn_6: 1.145  loss_dice_dn_6: 4.608  loss_bbox_dn_6: 0.6145  loss_giou_dn_6: 0.8747  loss_ce_7: 109  loss_mask_7: 1.15  loss_dice_7: 4.625  loss_bbox_7: 12.87  loss_giou_7: 2.663  loss_ce_dn_7: 6.309  loss_mask_dn_7: 1.149  loss_dice_dn_7: 4.619  loss_bbox_dn_7: 0.6149  loss_giou_dn_7: 0.8768  loss_ce_8: 108.9  loss_mask_8: 1.149  loss_dice_8: 4.625  loss_bbox_8: 12.87  loss_giou_8: 2.663  loss_ce_dn_8: 6.197  loss_mask_dn_8: 1.135  loss_dice_dn_8: 4.612  loss_bbox_dn_8: 0.6153  loss_giou_dn_8: 0.8789  loss_ce_interm: 555.2  loss_mask_interm: 1.151  loss_dice_interm: 4.651  loss_bbox_interm: 12.89  loss_giou_interm: 2.654    time: 0.8568  last_time: 0.8680  data_time: 0.0099  last_data_time: 0.0163   lr: 1.25e-05  max_mem: 12042M\n",
            "\u001b[32m[01/31 10:36:57 d2.engine.hooks]: \u001b[0mOverall training speed: 411 iterations in 0:05:52 (0.8571 s / it)\n",
            "\u001b[32m[01/31 10:36:57 d2.engine.hooks]: \u001b[0mTotal training time: 0:05:55 (0:00:03 on hooks)\n",
            "\u001b[32m[01/31 10:36:57 d2.utils.events]: \u001b[0m eta: 3 days, 15:32:05  iter: 413  total_loss: 2519  loss_ce: 106.1  loss_mask: 1.344  loss_dice: 4.633  loss_bbox: 11.73  loss_giou: 2.677  loss_ce_dn: 5.526  loss_mask_dn: 1.345  loss_dice_dn: 4.625  loss_bbox_dn: 0.7617  loss_giou_dn: 0.8961  loss_ce_0: 539.3  loss_mask_0: 1.284  loss_dice_0: 4.637  loss_bbox_0: 11.74  loss_giou_0: 2.678  loss_ce_dn_0: 124.9  loss_mask_dn_0: 1.209  loss_dice_dn_0: 4.638  loss_bbox_dn_0: 0.7594  loss_giou_dn_0: 0.8585  loss_ce_1: 107.1  loss_mask_1: 1.351  loss_dice_1: 4.614  loss_bbox_1: 11.73  loss_giou_1: 2.677  loss_ce_dn_1: 9.005  loss_mask_dn_1: 1.281  loss_dice_dn_1: 4.626  loss_bbox_dn_1: 0.7597  loss_giou_dn_1: 0.8595  loss_ce_2: 106.5  loss_mask_2: 1.34  loss_dice_2: 4.615  loss_bbox_2: 11.73  loss_giou_2: 2.677  loss_ce_dn_2: 6.365  loss_mask_dn_2: 1.324  loss_dice_dn_2: 4.637  loss_bbox_dn_2: 0.7599  loss_giou_dn_2: 0.861  loss_ce_3: 106.3  loss_mask_3: 1.343  loss_dice_3: 4.613  loss_bbox_3: 11.73  loss_giou_3: 2.677  loss_ce_dn_3: 5.949  loss_mask_dn_3: 1.32  loss_dice_dn_3: 4.651  loss_bbox_dn_3: 0.7602  loss_giou_dn_3: 0.8646  loss_ce_4: 106.4  loss_mask_4: 1.338  loss_dice_4: 4.627  loss_bbox_4: 11.73  loss_giou_4: 2.678  loss_ce_dn_4: 5.871  loss_mask_dn_4: 1.343  loss_dice_dn_4: 4.619  loss_bbox_dn_4: 0.7602  loss_giou_dn_4: 0.8687  loss_ce_5: 106.4  loss_mask_5: 1.335  loss_dice_5: 4.622  loss_bbox_5: 11.73  loss_giou_5: 2.68  loss_ce_dn_5: 5.799  loss_mask_dn_5: 1.324  loss_dice_dn_5: 4.629  loss_bbox_dn_5: 0.7604  loss_giou_dn_5: 0.8725  loss_ce_6: 106.2  loss_mask_6: 1.349  loss_dice_6: 4.619  loss_bbox_6: 11.73  loss_giou_6: 2.674  loss_ce_dn_6: 5.933  loss_mask_dn_6: 1.334  loss_dice_dn_6: 4.598  loss_bbox_dn_6: 0.7606  loss_giou_dn_6: 0.877  loss_ce_7: 106.2  loss_mask_7: 1.337  loss_dice_7: 4.638  loss_bbox_7: 11.72  loss_giou_7: 2.679  loss_ce_dn_7: 5.798  loss_mask_dn_7: 1.36  loss_dice_dn_7: 4.613  loss_bbox_dn_7: 0.7609  loss_giou_dn_7: 0.8834  loss_ce_8: 106.1  loss_mask_8: 1.349  loss_dice_8: 4.63  loss_bbox_8: 11.73  loss_giou_8: 2.679  loss_ce_dn_8: 5.648  loss_mask_dn_8: 1.331  loss_dice_dn_8: 4.607  loss_bbox_dn_8: 0.7612  loss_giou_dn_8: 0.89  loss_ce_interm: 539.3  loss_mask_interm: 1.282  loss_dice_interm: 4.637  loss_bbox_interm: 11.74  loss_giou_interm: 2.678    time: 0.8565  last_time: 0.8665  data_time: 0.0099  last_data_time: 0.0085   lr: 1.25e-05  max_mem: 12042M\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/MaskDINO/train_net.py\", line 377, in <module>\n",
            "    launch(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/engine/launch.py\", line 84, in launch\n",
            "    main_func(*args)\n",
            "  File \"/content/MaskDINO/train_net.py\", line 364, in main\n",
            "    return trainer.train()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/engine/defaults.py\", line 486, in train\n",
            "    super().train(self.start_iter, self.max_iter)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/engine/train_loop.py\", line 155, in train\n",
            "    self.run_step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/engine/defaults.py\", line 496, in run_step\n",
            "    self._trainer.run_step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/engine/train_loop.py\", line 494, in run_step\n",
            "    loss_dict = self.model(data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/MaskDINO/maskdino/maskdino.py\", line 267, in forward\n",
            "    losses = self.criterion(outputs, targets,mask_dict)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/MaskDINO/maskdino/modeling/criterion.py\", line 393, in forward\n",
            "    l_dict = self.get_loss(loss, aux_outputs, targets, indices, num_masks)\n",
            "  File \"/content/MaskDINO/maskdino/modeling/criterion.py\", line 332, in get_loss\n",
            "    return loss_map[loss](outputs, targets, indices, num_masks)\n",
            "  File \"/content/MaskDINO/maskdino/modeling/criterion.py\", line 218, in loss_boxes\n",
            "    loss_giou = 1 - torch.diag(box_ops.generalized_box_iou(\n",
            "  File \"/content/MaskDINO/maskdino/utils/box_ops.py\", line 54, in generalized_box_iou\n",
            "    iou, union = box_iou(boxes1, boxes2)\n",
            "  File \"/content/MaskDINO/maskdino/utils/box_ops.py\", line 25, in box_iou\n",
            "    area1 = box_area(boxes1)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python train_net.py --num-gpus 1 --config-file configs/coco/panoptic-segmentation/maskdino_R50_bs16_50ep_3s_dowsample1_2048.yaml \\\n",
        "  SOLVER.IMS_PER_BATCH 2 SOLVER.BASE_LR 0.0000125 #change args based on your custom training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl1r2Ls9Pgmz"
      },
      "source": [
        "## Training with model weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2QM7Gh5X_1n",
        "outputId": "613ed335-523f-42dd-8ff9-9d74eb6efd9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Command Line Args: Namespace(config_file='configs/coco/panoptic-segmentation/maskdino_R50_bs16_50ep_3s_dowsample1_2048.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:6679', opts=['SOLVER.IMS_PER_BATCH', '2', 'SOLVER.BASE_LR', '0.0000125', 'MODEL.WEIGHTS', '../drive/MyDrive/AAAAA/v3-67_model_0004999.pth'], EVAL_FLAG=1)\n",
            "pwd: /content/MaskDINO\n",
            "\u001b[32m[02/20 07:29:03 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
            "\u001b[32m[02/20 07:29:03 detectron2]: \u001b[0mEnvironment info:\n",
            "-------------------------------  -----------------------------------------------------------------\n",
            "sys.platform                     linux\n",
            "Python                           3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "numpy                            1.25.2\n",
            "detectron2                       0.6 @/usr/local/lib/python3.10/dist-packages/detectron2\n",
            "Compiler                         GCC 11.4\n",
            "CUDA compiler                    CUDA 12.2\n",
            "detectron2 arch flags            7.0\n",
            "DETECTRON2_ENV_MODULE            <not set>\n",
            "PyTorch                          2.1.0+cu121 @/usr/local/lib/python3.10/dist-packages/torch\n",
            "PyTorch debug build              False\n",
            "torch._C._GLIBCXX_USE_CXX11_ABI  False\n",
            "GPU available                    Yes\n",
            "GPU 0                            Tesla V100-SXM2-16GB (arch=7.0)\n",
            "Driver version                   535.104.05\n",
            "CUDA_HOME                        /usr/local/cuda\n",
            "Pillow                           9.4.0\n",
            "torchvision                      0.16.0+cu121 @/usr/local/lib/python3.10/dist-packages/torchvision\n",
            "torchvision arch flags           5.0, 6.0, 7.0, 7.5, 8.0, 8.6, 9.0\n",
            "fvcore                           0.1.5.post20221221\n",
            "iopath                           0.1.9\n",
            "cv2                              4.8.0\n",
            "-------------------------------  -----------------------------------------------------------------\n",
            "PyTorch built with:\n",
            "  - GCC 9.3\n",
            "  - C++ Version: 201703\n",
            "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX512\n",
            "  - CUDA Runtime 12.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
            "  - CuDNN 8.9.2\n",
            "  - Magma 2.6.1\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
            "\n",
            "\u001b[32m[02/20 07:29:03 detectron2]: \u001b[0mCommand line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/maskdino_R50_bs16_50ep_3s_dowsample1_2048.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:6679', opts=['SOLVER.IMS_PER_BATCH', '2', 'SOLVER.BASE_LR', '0.0000125', 'MODEL.WEIGHTS', '../drive/MyDrive/AAAAA/v3-67_model_0004999.pth'], EVAL_FLAG=1)\n",
            "\u001b[32m[02/20 07:29:03 detectron2]: \u001b[0mContents of args.config_file=configs/coco/panoptic-segmentation/maskdino_R50_bs16_50ep_3s_dowsample1_2048.yaml:\n",
            "\u001b[38;5;204m_BASE_\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mBase-COCO-PanopticSegmentation.yaml\u001b[39m\n",
            "\u001b[38;5;204mMODEL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMETA_ARCHITECTURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINO\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSEM_SEG_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINOHead\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIGNORE_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m142\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONVS_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mGN\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;245m# pixel decoder\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPIXEL_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINOEncoder\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTOTAL_NUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres2\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres3\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres4\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres5\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres3\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres4\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mres5\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOMMON_STRIDE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_ENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFEATURE_ORDER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mlow2high\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMaskDINO\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mMaskDINODecoder\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEEP_SUPERVISION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNO_OBJECT_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLASS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOX_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mGIOU_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mHIDDEN_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_OBJECT_QUERIES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m300\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNHEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENFORCE_INPUT_PROJ\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m9\u001b[39m\u001b[38;5;15m  \u001b[39m\u001b[38;5;245m# 9+1, 9 decoder layers, add one for the loss on learnable query\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRAIN_NUM_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12544\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOVERSAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIMPORTANCE_SAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.75\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mEVAL_FLAG\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIAL_PRED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTWO_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mseg\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN_NUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIALIZE_BOX_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186mno\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPANO_BOX_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSEMANTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mINSTANCE_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANOPTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOVERLAP_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOBJECT_MASK_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.25\u001b[39m\n",
            "\n",
            "\u001b[38;5;204mSOLVER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mAMP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
            "\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mEVAL_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5000\u001b[39m\n",
            "\u001b[38;5;245m#  EVAL_FLAG: 1\u001b[39m\n",
            "\n",
            "\u001b[32m[02/20 07:29:03 detectron2]: \u001b[0mRunning with full config:\n",
            "\u001b[38;5;204mCUDNN_BENCHMARK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;204mDATALOADER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mASPECT_RATIO_GROUPING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mFILTER_EMPTY_ANNOTATIONS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mNUM_WORKERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mREPEAT_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSAMPLER_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrainingSampler\u001b[39m\n",
            "\u001b[38;5;204mDATASETS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPRECOMPUTED_PROPOSAL_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPRECOMPUTED_PROPOSAL_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPROPOSAL_FILES_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPROPOSAL_FILES_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mcoco_2017_val_panoptic_with_sem_seg\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mTRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mcoco_2017_train_panoptic\u001b[39m\n",
            "\u001b[38;5;204mDefault_loading\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;204mGLOBAL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mHACK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;204mINPUT\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCOLOR_AUG_SSD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCROP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSINGLE_CATEGORY_MAX_AREA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.9\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.9\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mrelative_range\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mDATASET_MAPPER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mcoco_panoptic_lsj\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mFORMAT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRGB\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mIMAGE_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMASK_FORMAT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mpolygon\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_SIZE_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1333\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_SIZE_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1333\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SIZE_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m800\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SIZE_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m800\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMIN_SIZE_TRAIN_SAMPLING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mchoice\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRANDOM_FLIP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhorizontal\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;204mMODEL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mANCHOR_GENERATOR\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mANGLES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-90\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m90\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPECT_RATIOS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mDefaultAnchorGenerator\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOFFSET\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m128\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFREEZE_AT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mbuild_resnet_backbone\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mDEVICE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mcuda\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mFPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFUSE_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msum\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mKEYPOINT_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mLOAD_PROPOSALS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMASK_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMETA_ARCHITECTURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINO\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMaskDINO\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOX_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOX_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLASS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_BOX_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_CLASS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_DICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_GIOU_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOST_MASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m9\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEEP_SUPERVISION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mseg\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN_NOISE_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDN_NUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENFORCE_INPUT_PROJ\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mEVAL_FLAG\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mGIOU_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mHIDDEN_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIMPORTANCE_SAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.75\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIALIZE_BOX_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186mno\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINITIAL_PRED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLEARN_TGT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNHEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNO_OBJECT_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_OBJECT_QUERIES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m300\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOVERSAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPANO_BOX_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRED_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSEMANTIC_CE_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mINSTANCE_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOBJECT_MASK_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.25\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOVERLAP_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANOPTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANO_TEMPERATURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.06\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mPANO_TRANSFORM_EVAL\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSEMANTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mTEST_FOUCUS_ON_BOX\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRAIN_NUM_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12544\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINODecoder\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTWO_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPANOPTIC_FPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOMBINE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mINSTANCES_CONFIDENCE_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mOVERLAP_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;204mSTUFF_AREA_LIMIT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4096\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mINSTANCE_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPIXEL_MEAN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m123.675\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m116.28\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m103.53\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPIXEL_STD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m58.395\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m57.12\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m57.375\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPROPOSAL_GENERATOR\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMIN_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRPN\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRESNETS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORM_MODULATED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORM_NUM_GROUPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORM_ON_PER_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEPTH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m50\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFrozenBN\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_GROUPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOUT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES2_OUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES4_DILATION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES5_DILATION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mRES5_MULTI_GRID\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSTEM_OUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSTEM_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mbasic\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSTRIDE_IN_1X1\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mWIDTH_PER_GROUP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRETINANET\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m&id002\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFOCAL_LOSS_ALPHA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.25\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFOCAL_LOSS_GAMMA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNMS_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m80\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CONVS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRIOR_PROB\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.01\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSCORE_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSMOOTH_L1_LOSS_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTOPK_CANDIDATES_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_BOX_CASCADE_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m&id001\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m20.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m20.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m30.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m30.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m15.0\u001b[39m\n",
            "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m15.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOUS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_BOX_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m*id001\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLS_AGNOSTIC_BBOX_REG\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFC_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFED_LOSS_FREQ_WEIGHT_POWER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFED_LOSS_NUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m50\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_FC\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSMOOTH_L1_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRAIN_ON_PRED_BOXES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_FED_LOSS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_SIGMOID_CE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBATCH_SIZE_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRes5ROIHeads\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNMS_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m80\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOSITIVE_FRACTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.25\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPROPOSAL_APPEND_GT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSCORE_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_KEYPOINT_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIMS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMIN_KEYPOINTS_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mKRCNNConvDeconvUpsampleHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_KEYPOINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m17\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mROI_MASK_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLS_AGNOSTIC_MASK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskRCNNConvUpsampleHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBATCH_SIZE_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m*id002\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mBOUNDARY_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONV_DIMS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mHEAD_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mStandardRPNHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNMS_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOSITIVE_FRACTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOST_NMS_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPOST_NMS_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NMS_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRE_NMS_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mSMOOTH_L1_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSEM_SEG_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPP_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPP_DILATIONS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m18\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mASPP_DROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCOMMON_STRIDE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCONVS_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFEATURE_ORDER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mlow2high\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIGNORE_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhard_pixel_mining\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMASK_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINOHead\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mGN\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m142\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPIXEL_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskDINOEncoder\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPROJECT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m48\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPROJECT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTOTAL_NUM_FEATURE_LEVELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mTRANSFORMER_ENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_DEPTHWISE_SEPARABLE_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSWIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mAPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mATTN_DROP_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDEPTHS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROP_PATH_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mDROP_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mEMBED_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m96\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMLP_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m24\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mOUT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPATCH_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPATCH_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mPRETRAIN_IMG_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m224\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mQKV_BIAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mQK_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mnull\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mUSE_CHECKPOINT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mWINDOW_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m7\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m../drive/MyDrive/AAAAA/v3-67_model_0004999.pth\u001b[39m\n",
            "\u001b[38;5;204mOUTPUT_DIR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m./output\u001b[39m\n",
            "\u001b[38;5;204mSEED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
            "\u001b[38;5;204mSOLVER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mAMP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBACKBONE_MULTIPLIER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.25e-05\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBASE_LR_END\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mBIAS_LR_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCHECKPOINT_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mCLIP_GRADIENTS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLIP_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfull_model\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mCLIP_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.01\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNORM_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mGAMMA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mIMS_PER_BATCH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mLR_SCHEDULER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mWarmupMultiStepLR\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMAX_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m368750\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mMOMENTUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.9\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mNESTEROV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mNUM_DECAYS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mOPTIMIZER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mADAMW\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPOLY_LR_CONSTANT_ENDING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPOLY_LR_POWER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.9\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mREFERENCE_WORLD_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mRESCALE_INTERVAL\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mSTEPS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m327778\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m355092\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_ITERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWARMUP_METHOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mlinear\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY_BIAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mnull\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY_EMBED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mWEIGHT_DECAY_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
            "\u001b[38;5;204mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mAUG\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mFLIP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMAX_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mMIN_SIZES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m400\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m500\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m600\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m700\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m800\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m900\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1100\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1200\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mDETECTIONS_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mEVAL_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5000\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mEXPECTED_RESULTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mKEYPOINT_OKS_SIGMAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
            "\u001b[38;5;15m  \u001b[39m\u001b[38;5;204mPRECISE_BN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
            "\u001b[38;5;15m    \u001b[39m\u001b[38;5;204mNUM_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m200\u001b[39m\n",
            "\u001b[38;5;204mVERSION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
            "\u001b[38;5;204mVIS_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
            "\n",
            "\u001b[32m[02/20 07:29:04 detectron2]: \u001b[0mFull config saved to ./output/config.yaml\n",
            "\u001b[32m[02/20 07:29:04 d2.utils.env]: \u001b[0mUsing a generated random seed 4184349\n",
            "Command cfg: CUDNN_BENCHMARK: False\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: True\n",
            "  FILTER_EMPTY_ANNOTATIONS: True\n",
            "  NUM_WORKERS: 4\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: ()\n",
            "  PROPOSAL_FILES_TRAIN: ()\n",
            "  TEST: ('coco_2017_val_panoptic_with_sem_seg',)\n",
            "  TRAIN: ('coco_2017_train_panoptic',)\n",
            "Default_loading: True\n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  COLOR_AUG_SSD: False\n",
            "  CROP:\n",
            "    ENABLED: False\n",
            "    SINGLE_CATEGORY_MAX_AREA: 1.0\n",
            "    SIZE: [0.9, 0.9]\n",
            "    TYPE: relative_range\n",
            "  DATASET_MAPPER_NAME: coco_panoptic_lsj\n",
            "  FORMAT: RGB\n",
            "  IMAGE_SIZE: 1024\n",
            "  MASK_FORMAT: polygon\n",
            "  MAX_SCALE: 2.0\n",
            "  MAX_SIZE_TEST: 1333\n",
            "  MAX_SIZE_TRAIN: 1333\n",
            "  MIN_SCALE: 0.1\n",
            "  MIN_SIZE_TEST: 800\n",
            "  MIN_SIZE_TRAIN: (800,)\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "  RANDOM_FLIP: horizontal\n",
            "  SIZE_DIVISIBILITY: -1\n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES: [[-90, 0, 90]]\n",
            "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]\n",
            "    NAME: DefaultAnchorGenerator\n",
            "    OFFSET: 0.0\n",
            "    SIZES: [[32, 64, 128, 256, 512]]\n",
            "  BACKBONE:\n",
            "    FREEZE_AT: 0\n",
            "    NAME: build_resnet_backbone\n",
            "  DEVICE: cuda\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES: []\n",
            "    NORM: \n",
            "    OUT_CHANNELS: 256\n",
            "  KEYPOINT_ON: False\n",
            "  LOAD_PROPOSALS: False\n",
            "  MASK_ON: False\n",
            "  META_ARCHITECTURE: MaskDINO\n",
            "  MaskDINO:\n",
            "    BOX_LOSS: True\n",
            "    BOX_WEIGHT: 5.0\n",
            "    CLASS_WEIGHT: 4.0\n",
            "    COST_BOX_WEIGHT: 5.0\n",
            "    COST_CLASS_WEIGHT: 4.0\n",
            "    COST_DICE_WEIGHT: 5.0\n",
            "    COST_GIOU_WEIGHT: 2.0\n",
            "    COST_MASK_WEIGHT: 5.0\n",
            "    DEC_LAYERS: 9\n",
            "    DEEP_SUPERVISION: True\n",
            "    DICE_WEIGHT: 5.0\n",
            "    DIM_FEEDFORWARD: 2048\n",
            "    DN: seg\n",
            "    DN_NOISE_SCALE: 0.4\n",
            "    DN_NUM: 100\n",
            "    DROPOUT: 0.0\n",
            "    ENC_LAYERS: 0\n",
            "    ENFORCE_INPUT_PROJ: False\n",
            "    EVAL_FLAG: 1\n",
            "    GIOU_WEIGHT: 2.0\n",
            "    HIDDEN_DIM: 256\n",
            "    IMPORTANCE_SAMPLE_RATIO: 0.75\n",
            "    INITIALIZE_BOX_TYPE: no\n",
            "    INITIAL_PRED: True\n",
            "    LEARN_TGT: False\n",
            "    MASK_WEIGHT: 5.0\n",
            "    NHEADS: 8\n",
            "    NO_OBJECT_WEIGHT: 0.1\n",
            "    NUM_OBJECT_QUERIES: 300\n",
            "    OVERSAMPLE_RATIO: 3.0\n",
            "    PANO_BOX_LOSS: False\n",
            "    PRED_CONV: False\n",
            "    PRE_NORM: False\n",
            "    SEMANTIC_CE_LOSS: False\n",
            "    SIZE_DIVISIBILITY: 32\n",
            "    TEST:\n",
            "      INSTANCE_ON: True\n",
            "      OBJECT_MASK_THRESHOLD: 0.25\n",
            "      OVERLAP_THRESHOLD: 0.8\n",
            "      PANOPTIC_ON: True\n",
            "      PANO_TEMPERATURE: 0.06\n",
            "      PANO_TRANSFORM_EVAL: True\n",
            "      SEMANTIC_ON: True\n",
            "      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: False\n",
            "      TEST_FOUCUS_ON_BOX: False\n",
            "    TRAIN_NUM_POINTS: 12544\n",
            "    TRANSFORMER_DECODER_NAME: MaskDINODecoder\n",
            "    TWO_STAGE: True\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: True\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN: [123.675, 116.28, 103.53]\n",
            "  PIXEL_STD: [58.395, 57.12, 57.375]\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: RPN\n",
            "  RESNETS:\n",
            "    DEFORM_MODULATED: False\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
            "    DEPTH: 50\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 1\n",
            "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES4_DILATION: 1\n",
            "    RES5_DILATION: 1\n",
            "    RES5_MULTI_GRID: [1, 1, 1]\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STEM_TYPE: basic\n",
            "    STRIDE_IN_1X1: False\n",
            "    WIDTH_PER_GROUP: 64\n",
            "  RETINANET:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.4, 0.5]\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NORM: \n",
            "    NUM_CLASSES: 80\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))\n",
            "    IOUS: (0.5, 0.6, 0.7)\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
            "    CLS_AGNOSTIC_BBOX_REG: False\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    FED_LOSS_FREQ_WEIGHT_POWER: 0.5\n",
            "    FED_LOSS_NUM_CLASSES: 50\n",
            "    NAME: \n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "    TRAIN_ON_PRED_BOXES: False\n",
            "    USE_FED_LOSS: False\n",
            "    USE_SIGMOID_CE: False\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 512\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, 1]\n",
            "    IOU_THRESHOLDS: [0.5]\n",
            "    NAME: Res5ROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 80\n",
            "    POSITIVE_FRACTION: 0.25\n",
            "    PROPOSAL_APPEND_GT: True\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: False\n",
            "    CONV_DIM: 256\n",
            "    NAME: MaskRCNNConvUpsampleHead\n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    BOUNDARY_THRESH: -1\n",
            "    CONV_DIMS: [-1]\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.3, 0.7]\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 1000\n",
            "    POST_NMS_TOPK_TRAIN: 2000\n",
            "    PRE_NMS_TOPK_TEST: 6000\n",
            "    PRE_NMS_TOPK_TRAIN: 12000\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "  SEM_SEG_HEAD:\n",
            "    ASPP_CHANNELS: 256\n",
            "    ASPP_DILATIONS: [6, 12, 18]\n",
            "    ASPP_DROPOUT: 0.1\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 256\n",
            "    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES: ['res3', 'res4', 'res5']\n",
            "    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8\n",
            "    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4\n",
            "    DIM_FEEDFORWARD: 2048\n",
            "    FEATURE_ORDER: low2high\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    LOSS_TYPE: hard_pixel_mining\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MASK_DIM: 256\n",
            "    NAME: MaskDINOHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 142\n",
            "    NUM_FEATURE_LEVELS: 3\n",
            "    PIXEL_DECODER_NAME: MaskDINOEncoder\n",
            "    PROJECT_CHANNELS: [48]\n",
            "    PROJECT_FEATURES: ['res2']\n",
            "    TOTAL_NUM_FEATURE_LEVELS: 4\n",
            "    TRANSFORMER_ENC_LAYERS: 6\n",
            "    USE_DEPTHWISE_SEPARABLE_CONV: False\n",
            "  SWIN:\n",
            "    APE: False\n",
            "    ATTN_DROP_RATE: 0.0\n",
            "    DEPTHS: [2, 2, 6, 2]\n",
            "    DROP_PATH_RATE: 0.3\n",
            "    DROP_RATE: 0.0\n",
            "    EMBED_DIM: 96\n",
            "    MLP_RATIO: 4.0\n",
            "    NUM_HEADS: [3, 6, 12, 24]\n",
            "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    PATCH_NORM: True\n",
            "    PATCH_SIZE: 4\n",
            "    PRETRAIN_IMG_SIZE: 224\n",
            "    QKV_BIAS: True\n",
            "    QK_SCALE: None\n",
            "    USE_CHECKPOINT: False\n",
            "    WINDOW_SIZE: 7\n",
            "  WEIGHTS: ../drive/MyDrive/AAAAA/v3-67_model_0004999.pth\n",
            "OUTPUT_DIR: ./output\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  AMP:\n",
            "    ENABLED: True\n",
            "  BACKBONE_MULTIPLIER: 0.1\n",
            "  BASE_LR: 1.25e-05\n",
            "  BASE_LR_END: 0.0\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 5000\n",
            "  CLIP_GRADIENTS:\n",
            "    CLIP_TYPE: full_model\n",
            "    CLIP_VALUE: 0.01\n",
            "    ENABLED: True\n",
            "    NORM_TYPE: 2.0\n",
            "  GAMMA: 0.1\n",
            "  IMS_PER_BATCH: 2\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 368750\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: False\n",
            "  NUM_DECAYS: 3\n",
            "  OPTIMIZER: ADAMW\n",
            "  POLY_LR_CONSTANT_ENDING: 0.0\n",
            "  POLY_LR_POWER: 0.9\n",
            "  REFERENCE_WORLD_SIZE: 0\n",
            "  RESCALE_INTERVAL: False\n",
            "  STEPS: (327778, 355092)\n",
            "  WARMUP_FACTOR: 1.0\n",
            "  WARMUP_ITERS: 10\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.05\n",
            "  WEIGHT_DECAY_BIAS: None\n",
            "  WEIGHT_DECAY_EMBED: 0.0\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: False\n",
            "    FLIP: True\n",
            "    MAX_SIZE: 4000\n",
            "    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)\n",
            "  DETECTIONS_PER_IMAGE: 100\n",
            "  EVAL_PERIOD: 5000\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: False\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VIS_PERIOD: 0\n",
            "criterion.weight_dict  {'loss_ce': 4.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_ce_interm': 4.0, 'loss_mask_interm': 5.0, 'loss_dice_interm': 5.0, 'loss_bbox_interm': 5.0, 'loss_giou_interm': 2.0, 'loss_ce_dn': 4.0, 'loss_mask_dn': 5.0, 'loss_dice_dn': 5.0, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_ce_interm_dn': 4.0, 'loss_mask_interm_dn': 5.0, 'loss_dice_interm_dn': 5.0, 'loss_bbox_interm_dn': 5.0, 'loss_giou_interm_dn': 2.0, 'loss_ce_0': 4.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_ce_interm_0': 4.0, 'loss_mask_interm_0': 5.0, 'loss_dice_interm_0': 5.0, 'loss_bbox_interm_0': 5.0, 'loss_giou_interm_0': 2.0, 'loss_ce_dn_0': 4.0, 'loss_mask_dn_0': 5.0, 'loss_dice_dn_0': 5.0, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_ce_interm_dn_0': 4.0, 'loss_mask_interm_dn_0': 5.0, 'loss_dice_interm_dn_0': 5.0, 'loss_bbox_interm_dn_0': 5.0, 'loss_giou_interm_dn_0': 2.0, 'loss_ce_1': 4.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_ce_interm_1': 4.0, 'loss_mask_interm_1': 5.0, 'loss_dice_interm_1': 5.0, 'loss_bbox_interm_1': 5.0, 'loss_giou_interm_1': 2.0, 'loss_ce_dn_1': 4.0, 'loss_mask_dn_1': 5.0, 'loss_dice_dn_1': 5.0, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_ce_interm_dn_1': 4.0, 'loss_mask_interm_dn_1': 5.0, 'loss_dice_interm_dn_1': 5.0, 'loss_bbox_interm_dn_1': 5.0, 'loss_giou_interm_dn_1': 2.0, 'loss_ce_2': 4.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_ce_interm_2': 4.0, 'loss_mask_interm_2': 5.0, 'loss_dice_interm_2': 5.0, 'loss_bbox_interm_2': 5.0, 'loss_giou_interm_2': 2.0, 'loss_ce_dn_2': 4.0, 'loss_mask_dn_2': 5.0, 'loss_dice_dn_2': 5.0, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_ce_interm_dn_2': 4.0, 'loss_mask_interm_dn_2': 5.0, 'loss_dice_interm_dn_2': 5.0, 'loss_bbox_interm_dn_2': 5.0, 'loss_giou_interm_dn_2': 2.0, 'loss_ce_3': 4.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_ce_interm_3': 4.0, 'loss_mask_interm_3': 5.0, 'loss_dice_interm_3': 5.0, 'loss_bbox_interm_3': 5.0, 'loss_giou_interm_3': 2.0, 'loss_ce_dn_3': 4.0, 'loss_mask_dn_3': 5.0, 'loss_dice_dn_3': 5.0, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_ce_interm_dn_3': 4.0, 'loss_mask_interm_dn_3': 5.0, 'loss_dice_interm_dn_3': 5.0, 'loss_bbox_interm_dn_3': 5.0, 'loss_giou_interm_dn_3': 2.0, 'loss_ce_4': 4.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_ce_interm_4': 4.0, 'loss_mask_interm_4': 5.0, 'loss_dice_interm_4': 5.0, 'loss_bbox_interm_4': 5.0, 'loss_giou_interm_4': 2.0, 'loss_ce_dn_4': 4.0, 'loss_mask_dn_4': 5.0, 'loss_dice_dn_4': 5.0, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0, 'loss_ce_interm_dn_4': 4.0, 'loss_mask_interm_dn_4': 5.0, 'loss_dice_interm_dn_4': 5.0, 'loss_bbox_interm_dn_4': 5.0, 'loss_giou_interm_dn_4': 2.0, 'loss_ce_5': 4.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_bbox_5': 5.0, 'loss_giou_5': 2.0, 'loss_ce_interm_5': 4.0, 'loss_mask_interm_5': 5.0, 'loss_dice_interm_5': 5.0, 'loss_bbox_interm_5': 5.0, 'loss_giou_interm_5': 2.0, 'loss_ce_dn_5': 4.0, 'loss_mask_dn_5': 5.0, 'loss_dice_dn_5': 5.0, 'loss_bbox_dn_5': 5.0, 'loss_giou_dn_5': 2.0, 'loss_ce_interm_dn_5': 4.0, 'loss_mask_interm_dn_5': 5.0, 'loss_dice_interm_dn_5': 5.0, 'loss_bbox_interm_dn_5': 5.0, 'loss_giou_interm_dn_5': 2.0, 'loss_ce_6': 4.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_bbox_6': 5.0, 'loss_giou_6': 2.0, 'loss_ce_interm_6': 4.0, 'loss_mask_interm_6': 5.0, 'loss_dice_interm_6': 5.0, 'loss_bbox_interm_6': 5.0, 'loss_giou_interm_6': 2.0, 'loss_ce_dn_6': 4.0, 'loss_mask_dn_6': 5.0, 'loss_dice_dn_6': 5.0, 'loss_bbox_dn_6': 5.0, 'loss_giou_dn_6': 2.0, 'loss_ce_interm_dn_6': 4.0, 'loss_mask_interm_dn_6': 5.0, 'loss_dice_interm_dn_6': 5.0, 'loss_bbox_interm_dn_6': 5.0, 'loss_giou_interm_dn_6': 2.0, 'loss_ce_7': 4.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_bbox_7': 5.0, 'loss_giou_7': 2.0, 'loss_ce_interm_7': 4.0, 'loss_mask_interm_7': 5.0, 'loss_dice_interm_7': 5.0, 'loss_bbox_interm_7': 5.0, 'loss_giou_interm_7': 2.0, 'loss_ce_dn_7': 4.0, 'loss_mask_dn_7': 5.0, 'loss_dice_dn_7': 5.0, 'loss_bbox_dn_7': 5.0, 'loss_giou_dn_7': 2.0, 'loss_ce_interm_dn_7': 4.0, 'loss_mask_interm_dn_7': 5.0, 'loss_dice_interm_dn_7': 5.0, 'loss_bbox_interm_dn_7': 5.0, 'loss_giou_interm_dn_7': 2.0, 'loss_ce_8': 4.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_bbox_8': 5.0, 'loss_giou_8': 2.0, 'loss_ce_interm_8': 4.0, 'loss_mask_interm_8': 5.0, 'loss_dice_interm_8': 5.0, 'loss_bbox_interm_8': 5.0, 'loss_giou_interm_8': 2.0, 'loss_ce_dn_8': 4.0, 'loss_mask_dn_8': 5.0, 'loss_dice_dn_8': 5.0, 'loss_bbox_dn_8': 5.0, 'loss_giou_dn_8': 2.0, 'loss_ce_interm_dn_8': 4.0, 'loss_mask_interm_dn_8': 5.0, 'loss_dice_interm_dn_8': 5.0, 'loss_bbox_interm_dn_8': 5.0, 'loss_giou_interm_dn_8': 2.0}\n",
            "\u001b[32m[02/20 07:29:05 d2.engine.defaults]: \u001b[0mModel:\n",
            "MaskDINO(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (sem_seg_head): MaskDINOHead(\n",
            "    (pixel_decoder): MaskDINOEncoder(\n",
            "      (input_proj): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "        (3): Sequential(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
            "        (encoder): MSDeformAttnTransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
            "              (self_attn): MSDeformAttn(\n",
            "                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "              (dropout3): Dropout(p=0.0, inplace=False)\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 128\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (adapter_1): Conv2d(\n",
            "        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): MaskDINODecoder(\n",
            "      (enc_output): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-3): 4 x Sequential()\n",
            "      )\n",
            "      (class_embed): Linear(in_features=256, out_features=142, bias=True)\n",
            "      (label_enc): Embedding(142, 256)\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (decoder): TransformerDecoder(\n",
            "        (layers): ModuleList(\n",
            "          (0-8): 9 x DeformableTransformerDecoderLayer(\n",
            "            (cross_attn): MSDeformAttn(\n",
            "              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
            "              (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            )\n",
            "            (dropout1): Dropout(p=0.0, inplace=False)\n",
            "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (self_attn): MultiheadAttention(\n",
            "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "            )\n",
            "            (dropout2): Dropout(p=0.0, inplace=False)\n",
            "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "            (dropout3): Dropout(p=0.0, inplace=False)\n",
            "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "            (dropout4): Dropout(p=0.0, inplace=False)\n",
            "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ref_point_head): MLP(\n",
            "          (layers): ModuleList(\n",
            "            (0): Linear(in_features=512, out_features=256, bias=True)\n",
            "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (bbox_embed): ModuleList(\n",
            "          (0-8): 9 x MLP(\n",
            "            (layers): ModuleList(\n",
            "              (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "              (2): Linear(in_features=256, out_features=4, bias=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (_bbox_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "          (2): Linear(in_features=256, out_features=4, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (bbox_embed): ModuleList(\n",
            "        (0-8): 9 x MLP(\n",
            "          (layers): ModuleList(\n",
            "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 4.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: ['labels', 'masks', 'boxes']\n",
            "      weight_dict: {'loss_ce': 4.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_bbox': 5.0, 'loss_giou': 2.0, 'loss_ce_interm': 4.0, 'loss_mask_interm': 5.0, 'loss_dice_interm': 5.0, 'loss_bbox_interm': 5.0, 'loss_giou_interm': 2.0, 'loss_ce_dn': 4.0, 'loss_mask_dn': 5.0, 'loss_dice_dn': 5.0, 'loss_bbox_dn': 5.0, 'loss_giou_dn': 2.0, 'loss_ce_interm_dn': 4.0, 'loss_mask_interm_dn': 5.0, 'loss_dice_interm_dn': 5.0, 'loss_bbox_interm_dn': 5.0, 'loss_giou_interm_dn': 2.0, 'loss_ce_0': 4.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_bbox_0': 5.0, 'loss_giou_0': 2.0, 'loss_ce_interm_0': 4.0, 'loss_mask_interm_0': 5.0, 'loss_dice_interm_0': 5.0, 'loss_bbox_interm_0': 5.0, 'loss_giou_interm_0': 2.0, 'loss_ce_dn_0': 4.0, 'loss_mask_dn_0': 5.0, 'loss_dice_dn_0': 5.0, 'loss_bbox_dn_0': 5.0, 'loss_giou_dn_0': 2.0, 'loss_ce_interm_dn_0': 4.0, 'loss_mask_interm_dn_0': 5.0, 'loss_dice_interm_dn_0': 5.0, 'loss_bbox_interm_dn_0': 5.0, 'loss_giou_interm_dn_0': 2.0, 'loss_ce_1': 4.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_bbox_1': 5.0, 'loss_giou_1': 2.0, 'loss_ce_interm_1': 4.0, 'loss_mask_interm_1': 5.0, 'loss_dice_interm_1': 5.0, 'loss_bbox_interm_1': 5.0, 'loss_giou_interm_1': 2.0, 'loss_ce_dn_1': 4.0, 'loss_mask_dn_1': 5.0, 'loss_dice_dn_1': 5.0, 'loss_bbox_dn_1': 5.0, 'loss_giou_dn_1': 2.0, 'loss_ce_interm_dn_1': 4.0, 'loss_mask_interm_dn_1': 5.0, 'loss_dice_interm_dn_1': 5.0, 'loss_bbox_interm_dn_1': 5.0, 'loss_giou_interm_dn_1': 2.0, 'loss_ce_2': 4.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_bbox_2': 5.0, 'loss_giou_2': 2.0, 'loss_ce_interm_2': 4.0, 'loss_mask_interm_2': 5.0, 'loss_dice_interm_2': 5.0, 'loss_bbox_interm_2': 5.0, 'loss_giou_interm_2': 2.0, 'loss_ce_dn_2': 4.0, 'loss_mask_dn_2': 5.0, 'loss_dice_dn_2': 5.0, 'loss_bbox_dn_2': 5.0, 'loss_giou_dn_2': 2.0, 'loss_ce_interm_dn_2': 4.0, 'loss_mask_interm_dn_2': 5.0, 'loss_dice_interm_dn_2': 5.0, 'loss_bbox_interm_dn_2': 5.0, 'loss_giou_interm_dn_2': 2.0, 'loss_ce_3': 4.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_bbox_3': 5.0, 'loss_giou_3': 2.0, 'loss_ce_interm_3': 4.0, 'loss_mask_interm_3': 5.0, 'loss_dice_interm_3': 5.0, 'loss_bbox_interm_3': 5.0, 'loss_giou_interm_3': 2.0, 'loss_ce_dn_3': 4.0, 'loss_mask_dn_3': 5.0, 'loss_dice_dn_3': 5.0, 'loss_bbox_dn_3': 5.0, 'loss_giou_dn_3': 2.0, 'loss_ce_interm_dn_3': 4.0, 'loss_mask_interm_dn_3': 5.0, 'loss_dice_interm_dn_3': 5.0, 'loss_bbox_interm_dn_3': 5.0, 'loss_giou_interm_dn_3': 2.0, 'loss_ce_4': 4.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_bbox_4': 5.0, 'loss_giou_4': 2.0, 'loss_ce_interm_4': 4.0, 'loss_mask_interm_4': 5.0, 'loss_dice_interm_4': 5.0, 'loss_bbox_interm_4': 5.0, 'loss_giou_interm_4': 2.0, 'loss_ce_dn_4': 4.0, 'loss_mask_dn_4': 5.0, 'loss_dice_dn_4': 5.0, 'loss_bbox_dn_4': 5.0, 'loss_giou_dn_4': 2.0, 'loss_ce_interm_dn_4': 4.0, 'loss_mask_interm_dn_4': 5.0, 'loss_dice_interm_dn_4': 5.0, 'loss_bbox_interm_dn_4': 5.0, 'loss_giou_interm_dn_4': 2.0, 'loss_ce_5': 4.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_bbox_5': 5.0, 'loss_giou_5': 2.0, 'loss_ce_interm_5': 4.0, 'loss_mask_interm_5': 5.0, 'loss_dice_interm_5': 5.0, 'loss_bbox_interm_5': 5.0, 'loss_giou_interm_5': 2.0, 'loss_ce_dn_5': 4.0, 'loss_mask_dn_5': 5.0, 'loss_dice_dn_5': 5.0, 'loss_bbox_dn_5': 5.0, 'loss_giou_dn_5': 2.0, 'loss_ce_interm_dn_5': 4.0, 'loss_mask_interm_dn_5': 5.0, 'loss_dice_interm_dn_5': 5.0, 'loss_bbox_interm_dn_5': 5.0, 'loss_giou_interm_dn_5': 2.0, 'loss_ce_6': 4.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_bbox_6': 5.0, 'loss_giou_6': 2.0, 'loss_ce_interm_6': 4.0, 'loss_mask_interm_6': 5.0, 'loss_dice_interm_6': 5.0, 'loss_bbox_interm_6': 5.0, 'loss_giou_interm_6': 2.0, 'loss_ce_dn_6': 4.0, 'loss_mask_dn_6': 5.0, 'loss_dice_dn_6': 5.0, 'loss_bbox_dn_6': 5.0, 'loss_giou_dn_6': 2.0, 'loss_ce_interm_dn_6': 4.0, 'loss_mask_interm_dn_6': 5.0, 'loss_dice_interm_dn_6': 5.0, 'loss_bbox_interm_dn_6': 5.0, 'loss_giou_interm_dn_6': 2.0, 'loss_ce_7': 4.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_bbox_7': 5.0, 'loss_giou_7': 2.0, 'loss_ce_interm_7': 4.0, 'loss_mask_interm_7': 5.0, 'loss_dice_interm_7': 5.0, 'loss_bbox_interm_7': 5.0, 'loss_giou_interm_7': 2.0, 'loss_ce_dn_7': 4.0, 'loss_mask_dn_7': 5.0, 'loss_dice_dn_7': 5.0, 'loss_bbox_dn_7': 5.0, 'loss_giou_dn_7': 2.0, 'loss_ce_interm_dn_7': 4.0, 'loss_mask_interm_dn_7': 5.0, 'loss_dice_interm_dn_7': 5.0, 'loss_bbox_interm_dn_7': 5.0, 'loss_giou_interm_dn_7': 2.0, 'loss_ce_8': 4.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_bbox_8': 5.0, 'loss_giou_8': 2.0, 'loss_ce_interm_8': 4.0, 'loss_mask_interm_8': 5.0, 'loss_dice_interm_8': 5.0, 'loss_bbox_interm_8': 5.0, 'loss_giou_interm_8': 2.0, 'loss_ce_dn_8': 4.0, 'loss_mask_dn_8': 5.0, 'loss_dice_dn_8': 5.0, 'loss_bbox_dn_8': 5.0, 'loss_giou_dn_8': 2.0, 'loss_ce_interm_dn_8': 4.0, 'loss_mask_interm_dn_8': 5.0, 'loss_dice_interm_dn_8': 5.0, 'loss_bbox_interm_dn_8': 5.0, 'loss_giou_interm_dn_8': 2.0}\n",
            "      num_classes: 142\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")\n",
            "\u001b[32m[02/20 07:29:05 maskdino.data.dataset_mappers.coco_panoptic_new_baseline_dataset_mapper]: \u001b[0m[COCOPanopticNewBaselineDatasetMapper] Full TransformGens used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=1024, target_width=1024), FixedSizeCrop(crop_size=(1024, 1024))]\n",
            "\u001b[32m[02/20 07:29:05 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
            "\u001b[32m[02/20 07:29:05 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[02/20 07:29:05 d2.data.common]: \u001b[0mSerializing 7542 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[02/20 07:29:05 d2.data.common]: \u001b[0mSerialized dataset takes 6.51 MiB\n",
            "\u001b[32m[02/20 07:29:05 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=2\n",
            "\u001b[32m[02/20 07:29:06 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from ../drive/MyDrive/AAAAA/v3-67_model_0004999.pth ...\n",
            "\u001b[32m[02/20 07:29:06 fvcore.common.checkpoint]: \u001b[0m[Checkpointer] Loading from ../drive/MyDrive/AAAAA/v3-67_model_0004999.pth ...\n",
            "\u001b[32m[02/20 07:29:06 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "\u001b[32m[02/20 07:29:29 d2.utils.events]: \u001b[0m eta: 3 days, 16:01:47  iter: 19  total_loss: 56.66  loss_ce: 0.7181  loss_mask: 0.2404  loss_dice: 0.5247  loss_bbox: 0.2605  loss_giou: 0.6602  loss_ce_dn: 0.1218  loss_mask_dn: 0.2448  loss_dice_dn: 0.5775  loss_bbox_dn: 0.2203  loss_giou_dn: 0.5455  loss_ce_0: 1.48  loss_mask_0: 0.2571  loss_dice_0: 0.5382  loss_bbox_0: 0.3637  loss_giou_0: 0.9022  loss_ce_dn_0: 0.749  loss_mask_dn_0: 0.96  loss_dice_dn_0: 2.851  loss_bbox_dn_0: 0.6909  loss_giou_dn_0: 0.9541  loss_ce_1: 1.422  loss_mask_1: 0.2556  loss_dice_1: 0.4593  loss_bbox_1: 0.2574  loss_giou_1: 0.7883  loss_ce_dn_1: 0.2568  loss_mask_dn_1: 0.3186  loss_dice_dn_1: 0.7261  loss_bbox_dn_1: 0.3781  loss_giou_dn_1: 0.6975  loss_ce_2: 1.177  loss_mask_2: 0.2556  loss_dice_2: 0.5787  loss_bbox_2: 0.1991  loss_giou_2: 0.7207  loss_ce_dn_2: 0.2085  loss_mask_dn_2: 0.278  loss_dice_dn_2: 0.6581  loss_bbox_dn_2: 0.297  loss_giou_dn_2: 0.6209  loss_ce_3: 0.9301  loss_mask_3: 0.2559  loss_dice_3: 0.5874  loss_bbox_3: 0.2233  loss_giou_3: 0.7154  loss_ce_dn_3: 0.1727  loss_mask_dn_3: 0.267  loss_dice_dn_3: 0.607  loss_bbox_dn_3: 0.2564  loss_giou_dn_3: 0.5954  loss_ce_4: 0.8657  loss_mask_4: 0.2544  loss_dice_4: 0.5158  loss_bbox_4: 0.2261  loss_giou_4: 0.6938  loss_ce_dn_4: 0.1459  loss_mask_dn_4: 0.2577  loss_dice_dn_4: 0.5911  loss_bbox_dn_4: 0.2452  loss_giou_dn_4: 0.573  loss_ce_5: 0.7934  loss_mask_5: 0.2509  loss_dice_5: 0.5387  loss_bbox_5: 0.2186  loss_giou_5: 0.6977  loss_ce_dn_5: 0.1369  loss_mask_dn_5: 0.2554  loss_dice_dn_5: 0.5826  loss_bbox_dn_5: 0.2401  loss_giou_dn_5: 0.5678  loss_ce_6: 0.7587  loss_mask_6: 0.2418  loss_dice_6: 0.5654  loss_bbox_6: 0.2269  loss_giou_6: 0.6813  loss_ce_dn_6: 0.1428  loss_mask_dn_6: 0.247  loss_dice_dn_6: 0.5603  loss_bbox_dn_6: 0.2246  loss_giou_dn_6: 0.5535  loss_ce_7: 0.7312  loss_mask_7: 0.2434  loss_dice_7: 0.5146  loss_bbox_7: 0.2195  loss_giou_7: 0.6682  loss_ce_dn_7: 0.1317  loss_mask_dn_7: 0.2464  loss_dice_dn_7: 0.5908  loss_bbox_dn_7: 0.2206  loss_giou_dn_7: 0.5528  loss_ce_8: 0.694  loss_mask_8: 0.235  loss_dice_8: 0.5097  loss_bbox_8: 0.2458  loss_giou_8: 0.6603  loss_ce_dn_8: 0.1255  loss_mask_dn_8: 0.2431  loss_dice_dn_8: 0.5874  loss_bbox_dn_8: 0.2211  loss_giou_dn_8: 0.547  loss_ce_interm: 1.484  loss_mask_interm: 0.2541  loss_dice_interm: 0.501  loss_bbox_interm: 0.3644  loss_giou_interm: 0.893    time: 0.8688  last_time: 0.8589  data_time: 0.0316  last_data_time: 0.0088   lr: 1.25e-05  max_mem: 11782M\n",
            "2024-02-20 07:29:29.691643: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-20 07:29:29.691704: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-20 07:29:29.693205: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-20 07:29:31.062447: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[32m[02/20 07:29:49 d2.utils.events]: \u001b[0m eta: 3 days, 16:29:35  iter: 39  total_loss: 51.34  loss_ce: 0.6642  loss_mask: 0.228  loss_dice: 0.6493  loss_bbox: 0.151  loss_giou: 0.7405  loss_ce_dn: 0.1392  loss_mask_dn: 0.1969  loss_dice_dn: 0.688  loss_bbox_dn: 0.1708  loss_giou_dn: 0.6989  loss_ce_0: 1.328  loss_mask_0: 0.2154  loss_dice_0: 0.732  loss_bbox_0: 0.3315  loss_giou_0: 0.9585  loss_ce_dn_0: 0.6853  loss_mask_dn_0: 0.6908  loss_dice_dn_0: 2.623  loss_bbox_dn_0: 0.5787  loss_giou_dn_0: 1.043  loss_ce_1: 1.24  loss_mask_1: 0.2221  loss_dice_1: 0.7266  loss_bbox_1: 0.2177  loss_giou_1: 0.8147  loss_ce_dn_1: 0.2458  loss_mask_dn_1: 0.2217  loss_dice_dn_1: 0.7748  loss_bbox_dn_1: 0.2996  loss_giou_dn_1: 0.8271  loss_ce_2: 1.104  loss_mask_2: 0.2231  loss_dice_2: 0.7723  loss_bbox_2: 0.1986  loss_giou_2: 0.8154  loss_ce_dn_2: 0.2042  loss_mask_dn_2: 0.2046  loss_dice_dn_2: 0.7289  loss_bbox_dn_2: 0.243  loss_giou_dn_2: 0.7701  loss_ce_3: 0.9404  loss_mask_3: 0.2235  loss_dice_3: 0.7139  loss_bbox_3: 0.1708  loss_giou_3: 0.7587  loss_ce_dn_3: 0.1783  loss_mask_dn_3: 0.2021  loss_dice_dn_3: 0.6972  loss_bbox_dn_3: 0.2213  loss_giou_dn_3: 0.7351  loss_ce_4: 0.7735  loss_mask_4: 0.2325  loss_dice_4: 0.7177  loss_bbox_4: 0.1603  loss_giou_4: 0.7491  loss_ce_dn_4: 0.166  loss_mask_dn_4: 0.1997  loss_dice_dn_4: 0.6909  loss_bbox_dn_4: 0.1888  loss_giou_dn_4: 0.7168  loss_ce_5: 0.7702  loss_mask_5: 0.2282  loss_dice_5: 0.7265  loss_bbox_5: 0.1699  loss_giou_5: 0.7478  loss_ce_dn_5: 0.1546  loss_mask_dn_5: 0.2016  loss_dice_dn_5: 0.7133  loss_bbox_dn_5: 0.1835  loss_giou_dn_5: 0.7163  loss_ce_6: 0.7034  loss_mask_6: 0.2261  loss_dice_6: 0.6326  loss_bbox_6: 0.1585  loss_giou_6: 0.75  loss_ce_dn_6: 0.1494  loss_mask_dn_6: 0.1965  loss_dice_dn_6: 0.6927  loss_bbox_dn_6: 0.171  loss_giou_dn_6: 0.7051  loss_ce_7: 0.645  loss_mask_7: 0.2266  loss_dice_7: 0.6346  loss_bbox_7: 0.1567  loss_giou_7: 0.7439  loss_ce_dn_7: 0.1434  loss_mask_dn_7: 0.1948  loss_dice_dn_7: 0.6917  loss_bbox_dn_7: 0.1695  loss_giou_dn_7: 0.7015  loss_ce_8: 0.6276  loss_mask_8: 0.2267  loss_dice_8: 0.6749  loss_bbox_8: 0.1506  loss_giou_8: 0.7413  loss_ce_dn_8: 0.1382  loss_mask_dn_8: 0.1957  loss_dice_dn_8: 0.6744  loss_bbox_dn_8: 0.1712  loss_giou_dn_8: 0.6994  loss_ce_interm: 1.323  loss_mask_interm: 0.2206  loss_dice_interm: 0.7054  loss_bbox_interm: 0.3289  loss_giou_interm: 0.9609    time: 0.8653  last_time: 0.8511  data_time: 0.0098  last_data_time: 0.0110   lr: 1.25e-05  max_mem: 11782M\n",
            "\u001b[32m[02/20 07:30:06 d2.utils.events]: \u001b[0m eta: 3 days, 16:43:27  iter: 59  total_loss: 68.4  loss_ce: 0.8874  loss_mask: 0.3444  loss_dice: 0.8203  loss_bbox: 0.2685  loss_giou: 0.6901  loss_ce_dn: 0.1308  loss_mask_dn: 0.3477  loss_dice_dn: 0.8789  loss_bbox_dn: 0.261  loss_giou_dn: 0.5407  loss_ce_0: 1.444  loss_mask_0: 0.3521  loss_dice_0: 0.9544  loss_bbox_0: 0.4636  loss_giou_0: 0.8915  loss_ce_dn_0: 0.6942  loss_mask_dn_0: 0.9166  loss_dice_dn_0: 2.948  loss_bbox_dn_0: 0.792  loss_giou_dn_0: 0.9214  loss_ce_1: 1.334  loss_mask_1: 0.3788  loss_dice_1: 0.9151  loss_bbox_1: 0.3333  loss_giou_1: 0.7706  loss_ce_dn_1: 0.2438  loss_mask_dn_1: 0.4111  loss_dice_dn_1: 1.018  loss_bbox_dn_1: 0.4407  loss_giou_dn_1: 0.6767  loss_ce_2: 1.199  loss_mask_2: 0.3398  loss_dice_2: 0.9094  loss_bbox_2: 0.277  loss_giou_2: 0.7236  loss_ce_dn_2: 0.1948  loss_mask_dn_2: 0.377  loss_dice_dn_2: 0.9587  loss_bbox_dn_2: 0.3384  loss_giou_dn_2: 0.5971  loss_ce_3: 1.023  loss_mask_3: 0.334  loss_dice_3: 0.8837  loss_bbox_3: 0.29  loss_giou_3: 0.7146  loss_ce_dn_3: 0.155  loss_mask_dn_3: 0.3586  loss_dice_dn_3: 0.9262  loss_bbox_dn_3: 0.3107  loss_giou_dn_3: 0.5779  loss_ce_4: 0.9399  loss_mask_4: 0.3513  loss_dice_4: 0.8647  loss_bbox_4: 0.3009  loss_giou_4: 0.7336  loss_ce_dn_4: 0.1479  loss_mask_dn_4: 0.3557  loss_dice_dn_4: 0.902  loss_bbox_dn_4: 0.2866  loss_giou_dn_4: 0.5594  loss_ce_5: 0.917  loss_mask_5: 0.3544  loss_dice_5: 0.8653  loss_bbox_5: 0.2839  loss_giou_5: 0.728  loss_ce_dn_5: 0.1301  loss_mask_dn_5: 0.3505  loss_dice_dn_5: 0.9062  loss_bbox_dn_5: 0.2781  loss_giou_dn_5: 0.5463  loss_ce_6: 0.9083  loss_mask_6: 0.3476  loss_dice_6: 0.8706  loss_bbox_6: 0.2737  loss_giou_6: 0.6887  loss_ce_dn_6: 0.129  loss_mask_dn_6: 0.3441  loss_dice_dn_6: 0.8858  loss_bbox_dn_6: 0.2709  loss_giou_dn_6: 0.5455  loss_ce_7: 0.8635  loss_mask_7: 0.3554  loss_dice_7: 0.8685  loss_bbox_7: 0.2709  loss_giou_7: 0.7111  loss_ce_dn_7: 0.1213  loss_mask_dn_7: 0.3492  loss_dice_dn_7: 0.8642  loss_bbox_dn_7: 0.2675  loss_giou_dn_7: 0.5403  loss_ce_8: 0.8927  loss_mask_8: 0.3485  loss_dice_8: 0.86  loss_bbox_8: 0.2708  loss_giou_8: 0.7192  loss_ce_dn_8: 0.1307  loss_mask_dn_8: 0.3494  loss_dice_dn_8: 0.8781  loss_bbox_dn_8: 0.2624  loss_giou_dn_8: 0.5416  loss_ce_interm: 1.452  loss_mask_interm: 0.3521  loss_dice_interm: 0.9133  loss_bbox_interm: 0.4669  loss_giou_interm: 0.8872    time: 0.8663  last_time: 0.8782  data_time: 0.0106  last_data_time: 0.0125   lr: 1.25e-05  max_mem: 11782M\n",
            "\u001b[32m[02/20 07:30:24 d2.utils.events]: \u001b[0m eta: 3 days, 16:49:04  iter: 79  total_loss: 60.76  loss_ce: 0.8072  loss_mask: 0.2225  loss_dice: 0.5408  loss_bbox: 0.2362  loss_giou: 0.6598  loss_ce_dn: 0.1044  loss_mask_dn: 0.2516  loss_dice_dn: 0.5938  loss_bbox_dn: 0.1847  loss_giou_dn: 0.5215  loss_ce_0: 1.416  loss_mask_0: 0.2521  loss_dice_0: 0.6218  loss_bbox_0: 0.3846  loss_giou_0: 0.9262  loss_ce_dn_0: 0.6997  loss_mask_dn_0: 0.8914  loss_dice_dn_0: 2.93  loss_bbox_dn_0: 0.5736  loss_giou_dn_0: 0.9384  loss_ce_1: 1.347  loss_mask_1: 0.2436  loss_dice_1: 0.5546  loss_bbox_1: 0.2954  loss_giou_1: 0.756  loss_ce_dn_1: 0.2354  loss_mask_dn_1: 0.3092  loss_dice_dn_1: 0.688  loss_bbox_dn_1: 0.3112  loss_giou_dn_1: 0.6568  loss_ce_2: 1.19  loss_mask_2: 0.2701  loss_dice_2: 0.4948  loss_bbox_2: 0.2572  loss_giou_2: 0.7048  loss_ce_dn_2: 0.1904  loss_mask_dn_2: 0.2763  loss_dice_dn_2: 0.6588  loss_bbox_dn_2: 0.2464  loss_giou_dn_2: 0.5844  loss_ce_3: 1.01  loss_mask_3: 0.2395  loss_dice_3: 0.5403  loss_bbox_3: 0.2514  loss_giou_3: 0.7041  loss_ce_dn_3: 0.147  loss_mask_dn_3: 0.2598  loss_dice_dn_3: 0.622  loss_bbox_dn_3: 0.2192  loss_giou_dn_3: 0.5621  loss_ce_4: 0.9543  loss_mask_4: 0.2443  loss_dice_4: 0.5949  loss_bbox_4: 0.2482  loss_giou_4: 0.6965  loss_ce_dn_4: 0.1354  loss_mask_dn_4: 0.2652  loss_dice_dn_4: 0.6091  loss_bbox_dn_4: 0.2029  loss_giou_dn_4: 0.5396  loss_ce_5: 0.8961  loss_mask_5: 0.22  loss_dice_5: 0.5797  loss_bbox_5: 0.246  loss_giou_5: 0.673  loss_ce_dn_5: 0.1216  loss_mask_dn_5: 0.2551  loss_dice_dn_5: 0.5834  loss_bbox_dn_5: 0.201  loss_giou_dn_5: 0.5355  loss_ce_6: 0.8626  loss_mask_6: 0.2237  loss_dice_6: 0.5571  loss_bbox_6: 0.2612  loss_giou_6: 0.678  loss_ce_dn_6: 0.1201  loss_mask_dn_6: 0.2597  loss_dice_dn_6: 0.5786  loss_bbox_dn_6: 0.1911  loss_giou_dn_6: 0.5207  loss_ce_7: 0.8434  loss_mask_7: 0.2176  loss_dice_7: 0.542  loss_bbox_7: 0.2452  loss_giou_7: 0.6611  loss_ce_dn_7: 0.1124  loss_mask_dn_7: 0.2529  loss_dice_dn_7: 0.5877  loss_bbox_dn_7: 0.1874  loss_giou_dn_7: 0.5212  loss_ce_8: 0.8224  loss_mask_8: 0.2263  loss_dice_8: 0.5519  loss_bbox_8: 0.2432  loss_giou_8: 0.6448  loss_ce_dn_8: 0.1047  loss_mask_dn_8: 0.2478  loss_dice_dn_8: 0.5871  loss_bbox_dn_8: 0.1845  loss_giou_dn_8: 0.5224  loss_ce_interm: 1.415  loss_mask_interm: 0.249  loss_dice_interm: 0.6206  loss_bbox_interm: 0.384  loss_giou_interm: 0.9175    time: 0.8701  last_time: 0.8645  data_time: 0.0136  last_data_time: 0.0102   lr: 1.25e-05  max_mem: 11782M\n",
            "\u001b[32m[02/20 07:30:41 d2.utils.events]: \u001b[0m eta: 3 days, 16:46:31  iter: 99  total_loss: 62.77  loss_ce: 0.749  loss_mask: 0.3483  loss_dice: 0.9595  loss_bbox: 0.3013  loss_giou: 0.6985  loss_ce_dn: 0.1047  loss_mask_dn: 0.3382  loss_dice_dn: 0.9403  loss_bbox_dn: 0.2246  loss_giou_dn: 0.5523  loss_ce_0: 1.462  loss_mask_0: 0.357  loss_dice_0: 1.056  loss_bbox_0: 0.4738  loss_giou_0: 0.928  loss_ce_dn_0: 0.7244  loss_mask_dn_0: 0.8874  loss_dice_dn_0: 2.915  loss_bbox_dn_0: 0.6701  loss_giou_dn_0: 0.9135  loss_ce_1: 1.337  loss_mask_1: 0.3447  loss_dice_1: 0.9921  loss_bbox_1: 0.3933  loss_giou_1: 0.7714  loss_ce_dn_1: 0.2321  loss_mask_dn_1: 0.3881  loss_dice_dn_1: 1.037  loss_bbox_dn_1: 0.4062  loss_giou_dn_1: 0.685  loss_ce_2: 1.183  loss_mask_2: 0.3391  loss_dice_2: 0.9455  loss_bbox_2: 0.3612  loss_giou_2: 0.7549  loss_ce_dn_2: 0.1825  loss_mask_dn_2: 0.3601  loss_dice_dn_2: 0.9689  loss_bbox_dn_2: 0.362  loss_giou_dn_2: 0.6261  loss_ce_3: 1.034  loss_mask_3: 0.3414  loss_dice_3: 0.9476  loss_bbox_3: 0.3474  loss_giou_3: 0.7104  loss_ce_dn_3: 0.1496  loss_mask_dn_3: 0.3372  loss_dice_dn_3: 0.9682  loss_bbox_dn_3: 0.2946  loss_giou_dn_3: 0.5953  loss_ce_4: 0.8838  loss_mask_4: 0.3302  loss_dice_4: 0.998  loss_bbox_4: 0.3395  loss_giou_4: 0.6733  loss_ce_dn_4: 0.1359  loss_mask_dn_4: 0.3398  loss_dice_dn_4: 0.9673  loss_bbox_dn_4: 0.2581  loss_giou_dn_4: 0.5752  loss_ce_5: 0.8302  loss_mask_5: 0.3221  loss_dice_5: 0.9701  loss_bbox_5: 0.3255  loss_giou_5: 0.6908  loss_ce_dn_5: 0.1258  loss_mask_dn_5: 0.3336  loss_dice_dn_5: 0.9596  loss_bbox_dn_5: 0.2513  loss_giou_dn_5: 0.5675  loss_ce_6: 0.8  loss_mask_6: 0.3411  loss_dice_6: 0.9869  loss_bbox_6: 0.3144  loss_giou_6: 0.6812  loss_ce_dn_6: 0.1131  loss_mask_dn_6: 0.3375  loss_dice_dn_6: 0.9474  loss_bbox_dn_6: 0.2342  loss_giou_dn_6: 0.5607  loss_ce_7: 0.7697  loss_mask_7: 0.3457  loss_dice_7: 0.9955  loss_bbox_7: 0.3035  loss_giou_7: 0.7062  loss_ce_dn_7: 0.1097  loss_mask_dn_7: 0.343  loss_dice_dn_7: 0.9445  loss_bbox_dn_7: 0.2299  loss_giou_dn_7: 0.5559  loss_ce_8: 0.7803  loss_mask_8: 0.3418  loss_dice_8: 0.9073  loss_bbox_8: 0.3008  loss_giou_8: 0.6786  loss_ce_dn_8: 0.1019  loss_mask_dn_8: 0.3371  loss_dice_dn_8: 0.9467  loss_bbox_dn_8: 0.2261  loss_giou_dn_8: 0.553  loss_ce_interm: 1.482  loss_mask_interm: 0.3498  loss_dice_interm: 1.046  loss_bbox_interm: 0.4798  loss_giou_interm: 0.9378    time: 0.8697  last_time: 0.8554  data_time: 0.0109  last_data_time: 0.0102   lr: 1.25e-05  max_mem: 11782M\n",
            "\u001b[32m[02/20 07:30:58 d2.utils.events]: \u001b[0m eta: 3 days, 16:43:19  iter: 119  total_loss: 54.98  loss_ce: 0.7929  loss_mask: 0.2694  loss_dice: 0.6851  loss_bbox: 0.2402  loss_giou: 0.6312  loss_ce_dn: 0.1085  loss_mask_dn: 0.2651  loss_dice_dn: 0.6927  loss_bbox_dn: 0.2449  loss_giou_dn: 0.6021  loss_ce_0: 1.493  loss_mask_0: 0.3102  loss_dice_0: 0.7441  loss_bbox_0: 0.3622  loss_giou_0: 0.8949  loss_ce_dn_0: 0.6944  loss_mask_dn_0: 0.8943  loss_dice_dn_0: 3.012  loss_bbox_dn_0: 0.5862  loss_giou_dn_0: 0.9618  loss_ce_1: 1.354  loss_mask_1: 0.2773  loss_dice_1: 0.7426  loss_bbox_1: 0.3261  loss_giou_1: 0.7856  loss_ce_dn_1: 0.2373  loss_mask_dn_1: 0.3011  loss_dice_dn_1: 0.8125  loss_bbox_dn_1: 0.3296  loss_giou_dn_1: 0.7272  loss_ce_2: 1.157  loss_mask_2: 0.2463  loss_dice_2: 0.7058  loss_bbox_2: 0.3119  loss_giou_2: 0.7072  loss_ce_dn_2: 0.1853  loss_mask_dn_2: 0.2794  loss_dice_dn_2: 0.7227  loss_bbox_dn_2: 0.2858  loss_giou_dn_2: 0.6727  loss_ce_3: 0.9608  loss_mask_3: 0.2837  loss_dice_3: 0.7148  loss_bbox_3: 0.2801  loss_giou_3: 0.7135  loss_ce_dn_3: 0.162  loss_mask_dn_3: 0.2718  loss_dice_dn_3: 0.7153  loss_bbox_dn_3: 0.2733  loss_giou_dn_3: 0.6455  loss_ce_4: 0.9332  loss_mask_4: 0.2742  loss_dice_4: 0.7223  loss_bbox_4: 0.2421  loss_giou_4: 0.6875  loss_ce_dn_4: 0.1378  loss_mask_dn_4: 0.2654  loss_dice_dn_4: 0.7071  loss_bbox_dn_4: 0.2539  loss_giou_dn_4: 0.6257  loss_ce_5: 0.861  loss_mask_5: 0.2696  loss_dice_5: 0.6914  loss_bbox_5: 0.2504  loss_giou_5: 0.6604  loss_ce_dn_5: 0.1246  loss_mask_dn_5: 0.2722  loss_dice_dn_5: 0.6975  loss_bbox_dn_5: 0.2524  loss_giou_dn_5: 0.6206  loss_ce_6: 0.8385  loss_mask_6: 0.2771  loss_dice_6: 0.6923  loss_bbox_6: 0.24  loss_giou_6: 0.6397  loss_ce_dn_6: 0.1226  loss_mask_dn_6: 0.2641  loss_dice_dn_6: 0.6906  loss_bbox_dn_6: 0.2459  loss_giou_dn_6: 0.6056  loss_ce_7: 0.7959  loss_mask_7: 0.2809  loss_dice_7: 0.6803  loss_bbox_7: 0.2425  loss_giou_7: 0.6374  loss_ce_dn_7: 0.1114  loss_mask_dn_7: 0.2701  loss_dice_dn_7: 0.6804  loss_bbox_dn_7: 0.246  loss_giou_dn_7: 0.6056  loss_ce_8: 0.7945  loss_mask_8: 0.2733  loss_dice_8: 0.685  loss_bbox_8: 0.2429  loss_giou_8: 0.6326  loss_ce_dn_8: 0.112  loss_mask_dn_8: 0.2648  loss_dice_dn_8: 0.6955  loss_bbox_dn_8: 0.2465  loss_giou_dn_8: 0.6038  loss_ce_interm: 1.457  loss_mask_interm: 0.3133  loss_dice_interm: 0.7398  loss_bbox_interm: 0.3656  loss_giou_interm: 0.9006    time: 0.8690  last_time: 0.8788  data_time: 0.0115  last_data_time: 0.0084   lr: 1.25e-05  max_mem: 11823M\n",
            "\u001b[32m[02/20 07:31:16 d2.utils.events]: \u001b[0m eta: 3 days, 16:40:50  iter: 139  total_loss: 61.11  loss_ce: 0.6281  loss_mask: 0.2283  loss_dice: 0.6907  loss_bbox: 0.2217  loss_giou: 0.794  loss_ce_dn: 0.09148  loss_mask_dn: 0.2154  loss_dice_dn: 0.6885  loss_bbox_dn: 0.2095  loss_giou_dn: 0.6704  loss_ce_0: 1.358  loss_mask_0: 0.2794  loss_dice_0: 0.7506  loss_bbox_0: 0.4037  loss_giou_0: 0.985  loss_ce_dn_0: 0.7856  loss_mask_dn_0: 0.881  loss_dice_dn_0: 3.188  loss_bbox_dn_0: 0.6285  loss_giou_dn_0: 1.09  loss_ce_1: 1.247  loss_mask_1: 0.2513  loss_dice_1: 0.7308  loss_bbox_1: 0.3196  loss_giou_1: 0.8667  loss_ce_dn_1: 0.2146  loss_mask_dn_1: 0.3201  loss_dice_dn_1: 0.8024  loss_bbox_dn_1: 0.3604  loss_giou_dn_1: 0.8259  loss_ce_2: 1.085  loss_mask_2: 0.2418  loss_dice_2: 0.6791  loss_bbox_2: 0.275  loss_giou_2: 0.8377  loss_ce_dn_2: 0.1665  loss_mask_dn_2: 0.2569  loss_dice_dn_2: 0.7395  loss_bbox_dn_2: 0.2868  loss_giou_dn_2: 0.7419  loss_ce_3: 0.9133  loss_mask_3: 0.2286  loss_dice_3: 0.6793  loss_bbox_3: 0.2438  loss_giou_3: 0.8254  loss_ce_dn_3: 0.1315  loss_mask_dn_3: 0.2391  loss_dice_dn_3: 0.6864  loss_bbox_dn_3: 0.252  loss_giou_dn_3: 0.7085  loss_ce_4: 0.8167  loss_mask_4: 0.2248  loss_dice_4: 0.6498  loss_bbox_4: 0.2346  loss_giou_4: 0.8126  loss_ce_dn_4: 0.1153  loss_mask_dn_4: 0.2354  loss_dice_dn_4: 0.6773  loss_bbox_dn_4: 0.2252  loss_giou_dn_4: 0.6897  loss_ce_5: 0.749  loss_mask_5: 0.2241  loss_dice_5: 0.6569  loss_bbox_5: 0.2225  loss_giou_5: 0.8086  loss_ce_dn_5: 0.1131  loss_mask_dn_5: 0.2274  loss_dice_dn_5: 0.6739  loss_bbox_dn_5: 0.2195  loss_giou_dn_5: 0.6792  loss_ce_6: 0.7117  loss_mask_6: 0.2306  loss_dice_6: 0.7067  loss_bbox_6: 0.2194  loss_giou_6: 0.8053  loss_ce_dn_6: 0.09952  loss_mask_dn_6: 0.2284  loss_dice_dn_6: 0.6786  loss_bbox_dn_6: 0.2133  loss_giou_dn_6: 0.6762  loss_ce_7: 0.6523  loss_mask_7: 0.2181  loss_dice_7: 0.6176  loss_bbox_7: 0.2169  loss_giou_7: 0.7942  loss_ce_dn_7: 0.09777  loss_mask_dn_7: 0.2167  loss_dice_dn_7: 0.6915  loss_bbox_dn_7: 0.2114  loss_giou_dn_7: 0.6759  loss_ce_8: 0.6294  loss_mask_8: 0.2218  loss_dice_8: 0.6348  loss_bbox_8: 0.2158  loss_giou_8: 0.799  loss_ce_dn_8: 0.09246  loss_mask_dn_8: 0.2172  loss_dice_dn_8: 0.6735  loss_bbox_dn_8: 0.209  loss_giou_dn_8: 0.6713  loss_ce_interm: 1.37  loss_mask_interm: 0.2752  loss_dice_interm: 0.7575  loss_bbox_interm: 0.4094  loss_giou_interm: 0.985    time: 0.8680  last_time: 0.8719  data_time: 0.0112  last_data_time: 0.0124   lr: 1.25e-05  max_mem: 11823M\n",
            "\u001b[32m[02/20 07:31:33 d2.utils.events]: \u001b[0m eta: 3 days, 16:51:47  iter: 159  total_loss: 63.31  loss_ce: 0.7861  loss_mask: 0.2693  loss_dice: 0.8795  loss_bbox: 0.266  loss_giou: 0.6716  loss_ce_dn: 0.1349  loss_mask_dn: 0.2471  loss_dice_dn: 0.8509  loss_bbox_dn: 0.2256  loss_giou_dn: 0.5078  loss_ce_0: 1.372  loss_mask_0: 0.2878  loss_dice_0: 0.9469  loss_bbox_0: 0.4058  loss_giou_0: 0.8887  loss_ce_dn_0: 0.7235  loss_mask_dn_0: 0.8375  loss_dice_dn_0: 2.928  loss_bbox_dn_0: 0.5682  loss_giou_dn_0: 0.8819  loss_ce_1: 1.331  loss_mask_1: 0.3099  loss_dice_1: 0.9072  loss_bbox_1: 0.3009  loss_giou_1: 0.7622  loss_ce_dn_1: 0.2497  loss_mask_dn_1: 0.3097  loss_dice_dn_1: 0.9455  loss_bbox_dn_1: 0.3325  loss_giou_dn_1: 0.6317  loss_ce_2: 1.179  loss_mask_2: 0.2941  loss_dice_2: 0.8628  loss_bbox_2: 0.2833  loss_giou_2: 0.6953  loss_ce_dn_2: 0.1987  loss_mask_dn_2: 0.2939  loss_dice_dn_2: 0.8979  loss_bbox_dn_2: 0.2739  loss_giou_dn_2: 0.5907  loss_ce_3: 0.9671  loss_mask_3: 0.2896  loss_dice_3: 0.9476  loss_bbox_3: 0.2838  loss_giou_3: 0.683  loss_ce_dn_3: 0.1783  loss_mask_dn_3: 0.2737  loss_dice_dn_3: 0.8704  loss_bbox_dn_3: 0.2558  loss_giou_dn_3: 0.5604  loss_ce_4: 0.9204  loss_mask_4: 0.2672  loss_dice_4: 0.7946  loss_bbox_4: 0.2875  loss_giou_4: 0.6895  loss_ce_dn_4: 0.1653  loss_mask_dn_4: 0.2636  loss_dice_dn_4: 0.8819  loss_bbox_dn_4: 0.2338  loss_giou_dn_4: 0.5382  loss_ce_5: 0.8558  loss_mask_5: 0.2818  loss_dice_5: 0.8796  loss_bbox_5: 0.2689  loss_giou_5: 0.6791  loss_ce_dn_5: 0.1536  loss_mask_dn_5: 0.2638  loss_dice_dn_5: 0.8647  loss_bbox_dn_5: 0.2333  loss_giou_dn_5: 0.5275  loss_ce_6: 0.8379  loss_mask_6: 0.264  loss_dice_6: 0.8114  loss_bbox_6: 0.2717  loss_giou_6: 0.6738  loss_ce_dn_6: 0.1358  loss_mask_dn_6: 0.2509  loss_dice_dn_6: 0.8696  loss_bbox_dn_6: 0.2296  loss_giou_dn_6: 0.5192  loss_ce_7: 0.8216  loss_mask_7: 0.2748  loss_dice_7: 0.8734  loss_bbox_7: 0.277  loss_giou_7: 0.6913  loss_ce_dn_7: 0.1315  loss_mask_dn_7: 0.2454  loss_dice_dn_7: 0.8721  loss_bbox_dn_7: 0.2281  loss_giou_dn_7: 0.5146  loss_ce_8: 0.8176  loss_mask_8: 0.2668  loss_dice_8: 0.8916  loss_bbox_8: 0.2664  loss_giou_8: 0.6719  loss_ce_dn_8: 0.1354  loss_mask_dn_8: 0.2472  loss_dice_dn_8: 0.8748  loss_bbox_dn_8: 0.2244  loss_giou_dn_8: 0.5074  loss_ce_interm: 1.359  loss_mask_interm: 0.2923  loss_dice_interm: 0.9438  loss_bbox_interm: 0.4073  loss_giou_interm: 0.8978    time: 0.8688  last_time: 0.8702  data_time: 0.0127  last_data_time: 0.0155   lr: 1.25e-05  max_mem: 11823M\n",
            "\u001b[32m[02/20 07:31:51 d2.utils.events]: \u001b[0m eta: 3 days, 16:52:40  iter: 179  total_loss: 60.42  loss_ce: 0.7304  loss_mask: 0.2357  loss_dice: 0.682  loss_bbox: 0.2798  loss_giou: 0.6151  loss_ce_dn: 0.1017  loss_mask_dn: 0.2201  loss_dice_dn: 0.7858  loss_bbox_dn: 0.2233  loss_giou_dn: 0.5009  loss_ce_0: 1.447  loss_mask_0: 0.2405  loss_dice_0: 0.7781  loss_bbox_0: 0.3919  loss_giou_0: 0.8403  loss_ce_dn_0: 0.7321  loss_mask_dn_0: 0.7489  loss_dice_dn_0: 3.043  loss_bbox_dn_0: 0.5612  loss_giou_dn_0: 0.934  loss_ce_1: 1.32  loss_mask_1: 0.22  loss_dice_1: 0.7378  loss_bbox_1: 0.3296  loss_giou_1: 0.716  loss_ce_dn_1: 0.2592  loss_mask_dn_1: 0.3141  loss_dice_dn_1: 0.8933  loss_bbox_dn_1: 0.3131  loss_giou_dn_1: 0.6672  loss_ce_2: 1.1  loss_mask_2: 0.2267  loss_dice_2: 0.7512  loss_bbox_2: 0.2724  loss_giou_2: 0.6642  loss_ce_dn_2: 0.1876  loss_mask_dn_2: 0.2571  loss_dice_dn_2: 0.8321  loss_bbox_dn_2: 0.2736  loss_giou_dn_2: 0.5903  loss_ce_3: 0.9123  loss_mask_3: 0.224  loss_dice_3: 0.7451  loss_bbox_3: 0.2774  loss_giou_3: 0.6377  loss_ce_dn_3: 0.1616  loss_mask_dn_3: 0.2429  loss_dice_dn_3: 0.796  loss_bbox_dn_3: 0.2492  loss_giou_dn_3: 0.5486  loss_ce_4: 0.8627  loss_mask_4: 0.2194  loss_dice_4: 0.7113  loss_bbox_4: 0.254  loss_giou_4: 0.6326  loss_ce_dn_4: 0.1377  loss_mask_dn_4: 0.2344  loss_dice_dn_4: 0.7825  loss_bbox_dn_4: 0.2375  loss_giou_dn_4: 0.5346  loss_ce_5: 0.7079  loss_mask_5: 0.2411  loss_dice_5: 0.7648  loss_bbox_5: 0.2748  loss_giou_5: 0.6398  loss_ce_dn_5: 0.1233  loss_mask_dn_5: 0.2324  loss_dice_dn_5: 0.7749  loss_bbox_dn_5: 0.2321  loss_giou_dn_5: 0.5273  loss_ce_6: 0.6874  loss_mask_6: 0.2353  loss_dice_6: 0.7714  loss_bbox_6: 0.2781  loss_giou_6: 0.632  loss_ce_dn_6: 0.1116  loss_mask_dn_6: 0.2208  loss_dice_dn_6: 0.7937  loss_bbox_dn_6: 0.2282  loss_giou_dn_6: 0.5158  loss_ce_7: 0.7226  loss_mask_7: 0.2347  loss_dice_7: 0.7178  loss_bbox_7: 0.2631  loss_giou_7: 0.6197  loss_ce_dn_7: 0.1032  loss_mask_dn_7: 0.2216  loss_dice_dn_7: 0.775  loss_bbox_dn_7: 0.2267  loss_giou_dn_7: 0.5095  loss_ce_8: 0.6972  loss_mask_8: 0.2326  loss_dice_8: 0.7791  loss_bbox_8: 0.2796  loss_giou_8: 0.6194  loss_ce_dn_8: 0.1017  loss_mask_dn_8: 0.2209  loss_dice_dn_8: 0.7805  loss_bbox_dn_8: 0.2226  loss_giou_dn_8: 0.504  loss_ce_interm: 1.451  loss_mask_interm: 0.2372  loss_dice_interm: 0.7467  loss_bbox_interm: 0.38  loss_giou_interm: 0.8311    time: 0.8698  last_time: 0.8810  data_time: 0.0118  last_data_time: 0.0081   lr: 1.25e-05  max_mem: 11823M\n",
            "\u001b[32m[02/20 07:32:08 d2.utils.events]: \u001b[0m eta: 3 days, 16:52:23  iter: 199  total_loss: 60.2  loss_ce: 0.7577  loss_mask: 0.2849  loss_dice: 0.6652  loss_bbox: 0.2536  loss_giou: 0.8005  loss_ce_dn: 0.1593  loss_mask_dn: 0.2894  loss_dice_dn: 0.6761  loss_bbox_dn: 0.2201  loss_giou_dn: 0.6975  loss_ce_0: 1.442  loss_mask_0: 0.2965  loss_dice_0: 0.6532  loss_bbox_0: 0.4212  loss_giou_0: 1.011  loss_ce_dn_0: 0.7453  loss_mask_dn_0: 0.966  loss_dice_dn_0: 3.017  loss_bbox_dn_0: 0.6292  loss_giou_dn_0: 1.095  loss_ce_1: 1.272  loss_mask_1: 0.3126  loss_dice_1: 0.6334  loss_bbox_1: 0.2992  loss_giou_1: 0.9082  loss_ce_dn_1: 0.3132  loss_mask_dn_1: 0.3699  loss_dice_dn_1: 0.8127  loss_bbox_dn_1: 0.3554  loss_giou_dn_1: 0.8296  loss_ce_2: 1.109  loss_mask_2: 0.302  loss_dice_2: 0.6601  loss_bbox_2: 0.2926  loss_giou_2: 0.8301  loss_ce_dn_2: 0.2542  loss_mask_dn_2: 0.3164  loss_dice_dn_2: 0.7514  loss_bbox_dn_2: 0.2887  loss_giou_dn_2: 0.7576  loss_ce_3: 0.9865  loss_mask_3: 0.3153  loss_dice_3: 0.6304  loss_bbox_3: 0.2994  loss_giou_3: 0.8554  loss_ce_dn_3: 0.2142  loss_mask_dn_3: 0.3103  loss_dice_dn_3: 0.7349  loss_bbox_dn_3: 0.2581  loss_giou_dn_3: 0.743  loss_ce_4: 0.9212  loss_mask_4: 0.2976  loss_dice_4: 0.65  loss_bbox_4: 0.2544  loss_giou_4: 0.8547  loss_ce_dn_4: 0.1913  loss_mask_dn_4: 0.3096  loss_dice_dn_4: 0.7132  loss_bbox_dn_4: 0.2373  loss_giou_dn_4: 0.7258  loss_ce_5: 0.8512  loss_mask_5: 0.2833  loss_dice_5: 0.7  loss_bbox_5: 0.2525  loss_giou_5: 0.8179  loss_ce_dn_5: 0.1756  loss_mask_dn_5: 0.2976  loss_dice_dn_5: 0.6832  loss_bbox_dn_5: 0.2257  loss_giou_dn_5: 0.7172  loss_ce_6: 0.8397  loss_mask_6: 0.2829  loss_dice_6: 0.6608  loss_bbox_6: 0.244  loss_giou_6: 0.8199  loss_ce_dn_6: 0.16  loss_mask_dn_6: 0.2963  loss_dice_dn_6: 0.6636  loss_bbox_dn_6: 0.2249  loss_giou_dn_6: 0.7074  loss_ce_7: 0.7783  loss_mask_7: 0.2878  loss_dice_7: 0.6615  loss_bbox_7: 0.267  loss_giou_7: 0.8088  loss_ce_dn_7: 0.1631  loss_mask_dn_7: 0.2913  loss_dice_dn_7: 0.6731  loss_bbox_dn_7: 0.2216  loss_giou_dn_7: 0.7036  loss_ce_8: 0.7604  loss_mask_8: 0.2784  loss_dice_8: 0.6477  loss_bbox_8: 0.2518  loss_giou_8: 0.8033  loss_ce_dn_8: 0.1593  loss_mask_dn_8: 0.2888  loss_dice_dn_8: 0.6544  loss_bbox_dn_8: 0.2231  loss_giou_dn_8: 0.699  loss_ce_interm: 1.438  loss_mask_interm: 0.2876  loss_dice_interm: 0.655  loss_bbox_interm: 0.4172  loss_giou_interm: 1.007    time: 0.8696  last_time: 0.8800  data_time: 0.0110  last_data_time: 0.0097   lr: 1.25e-05  max_mem: 11823M\n",
            "\u001b[32m[02/20 07:32:26 d2.utils.events]: \u001b[0m eta: 3 days, 16:51:15  iter: 219  total_loss: 63.44  loss_ce: 0.8052  loss_mask: 0.2764  loss_dice: 0.7752  loss_bbox: 0.2207  loss_giou: 0.5797  loss_ce_dn: 0.1256  loss_mask_dn: 0.2663  loss_dice_dn: 0.8007  loss_bbox_dn: 0.2455  loss_giou_dn: 0.5056  loss_ce_0: 1.504  loss_mask_0: 0.3008  loss_dice_0: 0.8842  loss_bbox_0: 0.3997  loss_giou_0: 0.8475  loss_ce_dn_0: 0.7679  loss_mask_dn_0: 0.9891  loss_dice_dn_0: 3.151  loss_bbox_dn_0: 0.5853  loss_giou_dn_0: 0.9285  loss_ce_1: 1.347  loss_mask_1: 0.309  loss_dice_1: 0.8279  loss_bbox_1: 0.3181  loss_giou_1: 0.683  loss_ce_dn_1: 0.2683  loss_mask_dn_1: 0.352  loss_dice_dn_1: 0.9293  loss_bbox_dn_1: 0.3545  loss_giou_dn_1: 0.6634  loss_ce_2: 1.213  loss_mask_2: 0.3045  loss_dice_2: 0.8524  loss_bbox_2: 0.2689  loss_giou_2: 0.6474  loss_ce_dn_2: 0.2121  loss_mask_dn_2: 0.2914  loss_dice_dn_2: 0.8782  loss_bbox_dn_2: 0.2948  loss_giou_dn_2: 0.5814  loss_ce_3: 1.071  loss_mask_3: 0.3002  loss_dice_3: 0.8147  loss_bbox_3: 0.2438  loss_giou_3: 0.6348  loss_ce_dn_3: 0.1815  loss_mask_dn_3: 0.2795  loss_dice_dn_3: 0.8249  loss_bbox_dn_3: 0.2637  loss_giou_dn_3: 0.5478  loss_ce_4: 0.9731  loss_mask_4: 0.3165  loss_dice_4: 0.8163  loss_bbox_4: 0.2367  loss_giou_4: 0.5966  loss_ce_dn_4: 0.1677  loss_mask_dn_4: 0.283  loss_dice_dn_4: 0.8122  loss_bbox_dn_4: 0.2513  loss_giou_dn_4: 0.5225  loss_ce_5: 0.8549  loss_mask_5: 0.2867  loss_dice_5: 0.8274  loss_bbox_5: 0.2302  loss_giou_5: 0.5885  loss_ce_dn_5: 0.1468  loss_mask_dn_5: 0.276  loss_dice_dn_5: 0.7813  loss_bbox_dn_5: 0.2474  loss_giou_dn_5: 0.5188  loss_ce_6: 0.8271  loss_mask_6: 0.2901  loss_dice_6: 0.8361  loss_bbox_6: 0.2189  loss_giou_6: 0.5797  loss_ce_dn_6: 0.1361  loss_mask_dn_6: 0.2716  loss_dice_dn_6: 0.8154  loss_bbox_dn_6: 0.2451  loss_giou_dn_6: 0.5133  loss_ce_7: 0.804  loss_mask_7: 0.2868  loss_dice_7: 0.791  loss_bbox_7: 0.2181  loss_giou_7: 0.5875  loss_ce_dn_7: 0.129  loss_mask_dn_7: 0.2662  loss_dice_dn_7: 0.8114  loss_bbox_dn_7: 0.2457  loss_giou_dn_7: 0.5121  loss_ce_8: 0.8118  loss_mask_8: 0.2891  loss_dice_8: 0.8223  loss_bbox_8: 0.2164  loss_giou_8: 0.5848  loss_ce_dn_8: 0.1208  loss_mask_dn_8: 0.2679  loss_dice_dn_8: 0.8017  loss_bbox_dn_8: 0.2465  loss_giou_dn_8: 0.5055  loss_ce_interm: 1.493  loss_mask_interm: 0.294  loss_dice_interm: 0.8685  loss_bbox_interm: 0.3935  loss_giou_interm: 0.8499    time: 0.8690  last_time: 0.8715  data_time: 0.0122  last_data_time: 0.0132   lr: 1.25e-05  max_mem: 11823M\n",
            "\u001b[32m[02/20 07:32:43 d2.utils.events]: \u001b[0m eta: 3 days, 16:51:04  iter: 239  total_loss: 62.53  loss_ce: 0.7508  loss_mask: 0.2557  loss_dice: 0.8787  loss_bbox: 0.302  loss_giou: 0.7289  loss_ce_dn: 0.08952  loss_mask_dn: 0.2587  loss_dice_dn: 0.8685  loss_bbox_dn: 0.2443  loss_giou_dn: 0.5742  loss_ce_0: 1.416  loss_mask_0: 0.2848  loss_dice_0: 0.9279  loss_bbox_0: 0.392  loss_giou_0: 0.8865  loss_ce_dn_0: 0.6799  loss_mask_dn_0: 0.9411  loss_dice_dn_0: 2.772  loss_bbox_dn_0: 0.5839  loss_giou_dn_0: 0.934  loss_ce_1: 1.36  loss_mask_1: 0.303  loss_dice_1: 0.8489  loss_bbox_1: 0.3462  loss_giou_1: 0.81  loss_ce_dn_1: 0.2082  loss_mask_dn_1: 0.3004  loss_dice_dn_1: 0.9567  loss_bbox_dn_1: 0.3416  loss_giou_dn_1: 0.713  loss_ce_2: 1.121  loss_mask_2: 0.2619  loss_dice_2: 0.8503  loss_bbox_2: 0.319  loss_giou_2: 0.765  loss_ce_dn_2: 0.1565  loss_mask_dn_2: 0.2878  loss_dice_dn_2: 0.9324  loss_bbox_dn_2: 0.2915  loss_giou_dn_2: 0.6423  loss_ce_3: 0.9472  loss_mask_3: 0.2557  loss_dice_3: 0.8726  loss_bbox_3: 0.3169  loss_giou_3: 0.7287  loss_ce_dn_3: 0.1294  loss_mask_dn_3: 0.268  loss_dice_dn_3: 0.8827  loss_bbox_dn_3: 0.2748  loss_giou_dn_3: 0.6091  loss_ce_4: 0.8672  loss_mask_4: 0.2477  loss_dice_4: 0.8917  loss_bbox_4: 0.3077  loss_giou_4: 0.7234  loss_ce_dn_4: 0.118  loss_mask_dn_4: 0.2591  loss_dice_dn_4: 0.8848  loss_bbox_dn_4: 0.2604  loss_giou_dn_4: 0.5868  loss_ce_5: 0.8116  loss_mask_5: 0.2638  loss_dice_5: 0.8344  loss_bbox_5: 0.2977  loss_giou_5: 0.7317  loss_ce_dn_5: 0.1093  loss_mask_dn_5: 0.2637  loss_dice_dn_5: 0.8891  loss_bbox_dn_5: 0.2566  loss_giou_dn_5: 0.5854  loss_ce_6: 0.7837  loss_mask_6: 0.2631  loss_dice_6: 0.8771  loss_bbox_6: 0.31  loss_giou_6: 0.7301  loss_ce_dn_6: 0.099  loss_mask_dn_6: 0.2642  loss_dice_dn_6: 0.8834  loss_bbox_dn_6: 0.2488  loss_giou_dn_6: 0.5795  loss_ce_7: 0.7602  loss_mask_7: 0.2559  loss_dice_7: 0.8834  loss_bbox_7: 0.2943  loss_giou_7: 0.7338  loss_ce_dn_7: 0.09354  loss_mask_dn_7: 0.2617  loss_dice_dn_7: 0.8621  loss_bbox_dn_7: 0.2441  loss_giou_dn_7: 0.5789  loss_ce_8: 0.7698  loss_mask_8: 0.2624  loss_dice_8: 0.8497  loss_bbox_8: 0.3062  loss_giou_8: 0.7289  loss_ce_dn_8: 0.09428  loss_mask_dn_8: 0.2604  loss_dice_dn_8: 0.8728  loss_bbox_dn_8: 0.2446  loss_giou_dn_8: 0.5754  loss_ce_interm: 1.382  loss_mask_interm: 0.2853  loss_dice_interm: 0.9046  loss_bbox_interm: 0.3947  loss_giou_interm: 0.8865    time: 0.8691  last_time: 0.8559  data_time: 0.0120  last_data_time: 0.0115   lr: 1.25e-05  max_mem: 11823M\n",
            "\u001b[32m[02/20 07:33:01 d2.utils.events]: \u001b[0m eta: 3 days, 16:49:23  iter: 259  total_loss: 62.55  loss_ce: 0.6913  loss_mask: 0.2574  loss_dice: 0.8159  loss_bbox: 0.2678  loss_giou: 0.7098  loss_ce_dn: 0.1269  loss_mask_dn: 0.2288  loss_dice_dn: 0.7076  loss_bbox_dn: 0.2819  loss_giou_dn: 0.5207  loss_ce_0: 1.451  loss_mask_0: 0.2772  loss_dice_0: 0.8674  loss_bbox_0: 0.4672  loss_giou_0: 0.868  loss_ce_dn_0: 0.7441  loss_mask_dn_0: 0.869  loss_dice_dn_0: 2.664  loss_bbox_dn_0: 0.7539  loss_giou_dn_0: 0.9256  loss_ce_1: 1.403  loss_mask_1: 0.2706  loss_dice_1: 0.8561  loss_bbox_1: 0.361  loss_giou_1: 0.7744  loss_ce_dn_1: 0.2632  loss_mask_dn_1: 0.3772  loss_dice_dn_1: 0.8787  loss_bbox_dn_1: 0.4294  loss_giou_dn_1: 0.6593  loss_ce_2: 1.201  loss_mask_2: 0.2807  loss_dice_2: 0.7953  loss_bbox_2: 0.3412  loss_giou_2: 0.7271  loss_ce_dn_2: 0.2083  loss_mask_dn_2: 0.29  loss_dice_dn_2: 0.7615  loss_bbox_dn_2: 0.35  loss_giou_dn_2: 0.5896  loss_ce_3: 1.01  loss_mask_3: 0.2805  loss_dice_3: 0.8218  loss_bbox_3: 0.3078  loss_giou_3: 0.7202  loss_ce_dn_3: 0.1773  loss_mask_dn_3: 0.2389  loss_dice_dn_3: 0.7309  loss_bbox_dn_3: 0.3124  loss_giou_dn_3: 0.557  loss_ce_4: 0.8835  loss_mask_4: 0.2515  loss_dice_4: 0.7701  loss_bbox_4: 0.2967  loss_giou_4: 0.7038  loss_ce_dn_4: 0.1591  loss_mask_dn_4: 0.2308  loss_dice_dn_4: 0.6961  loss_bbox_dn_4: 0.3001  loss_giou_dn_4: 0.5414  loss_ce_5: 0.7888  loss_mask_5: 0.2656  loss_dice_5: 0.8163  loss_bbox_5: 0.2869  loss_giou_5: 0.7166  loss_ce_dn_5: 0.1485  loss_mask_dn_5: 0.236  loss_dice_dn_5: 0.678  loss_bbox_dn_5: 0.2916  loss_giou_dn_5: 0.5394  loss_ce_6: 0.7537  loss_mask_6: 0.2771  loss_dice_6: 0.825  loss_bbox_6: 0.2809  loss_giou_6: 0.7082  loss_ce_dn_6: 0.1378  loss_mask_dn_6: 0.2419  loss_dice_dn_6: 0.7004  loss_bbox_dn_6: 0.2853  loss_giou_dn_6: 0.5286  loss_ce_7: 0.6925  loss_mask_7: 0.2594  loss_dice_7: 0.8231  loss_bbox_7: 0.2607  loss_giou_7: 0.7231  loss_ce_dn_7: 0.1339  loss_mask_dn_7: 0.2256  loss_dice_dn_7: 0.6935  loss_bbox_dn_7: 0.2832  loss_giou_dn_7: 0.5282  loss_ce_8: 0.7047  loss_mask_8: 0.2488  loss_dice_8: 0.8017  loss_bbox_8: 0.2706  loss_giou_8: 0.7111  loss_ce_dn_8: 0.1232  loss_mask_dn_8: 0.2294  loss_dice_dn_8: 0.7028  loss_bbox_dn_8: 0.2807  loss_giou_dn_8: 0.5203  loss_ce_interm: 1.451  loss_mask_interm: 0.272  loss_dice_interm: 0.8473  loss_bbox_interm: 0.4547  loss_giou_interm: 0.8633    time: 0.8691  last_time: 0.8653  data_time: 0.0112  last_data_time: 0.0113   lr: 1.25e-05  max_mem: 11823M\n",
            "\u001b[32m[02/20 07:33:18 d2.utils.events]: \u001b[0m eta: 3 days, 16:47:50  iter: 279  total_loss: 62.74  loss_ce: 0.8133  loss_mask: 0.2573  loss_dice: 0.8051  loss_bbox: 0.2137  loss_giou: 0.7192  loss_ce_dn: 0.131  loss_mask_dn: 0.2529  loss_dice_dn: 0.7267  loss_bbox_dn: 0.1988  loss_giou_dn: 0.5397  loss_ce_0: 1.452  loss_mask_0: 0.2855  loss_dice_0: 0.7704  loss_bbox_0: 0.3718  loss_giou_0: 0.9394  loss_ce_dn_0: 0.7053  loss_mask_dn_0: 0.95  loss_dice_dn_0: 3.052  loss_bbox_dn_0: 0.4866  loss_giou_dn_0: 0.9333  loss_ce_1: 1.396  loss_mask_1: 0.2658  loss_dice_1: 0.774  loss_bbox_1: 0.2423  loss_giou_1: 0.8266  loss_ce_dn_1: 0.2421  loss_mask_dn_1: 0.3057  loss_dice_dn_1: 0.8342  loss_bbox_dn_1: 0.2937  loss_giou_dn_1: 0.6928  loss_ce_2: 1.244  loss_mask_2: 0.2705  loss_dice_2: 0.7822  loss_bbox_2: 0.2155  loss_giou_2: 0.769  loss_ce_dn_2: 0.2011  loss_mask_dn_2: 0.2858  loss_dice_dn_2: 0.7694  loss_bbox_dn_2: 0.2387  loss_giou_dn_2: 0.6225  loss_ce_3: 1.072  loss_mask_3: 0.2614  loss_dice_3: 0.7704  loss_bbox_3: 0.1985  loss_giou_3: 0.7628  loss_ce_dn_3: 0.1664  loss_mask_dn_3: 0.2628  loss_dice_dn_3: 0.7514  loss_bbox_dn_3: 0.2248  loss_giou_dn_3: 0.5877  loss_ce_4: 0.9874  loss_mask_4: 0.2573  loss_dice_4: 0.7423  loss_bbox_4: 0.2162  loss_giou_4: 0.7307  loss_ce_dn_4: 0.1591  loss_mask_dn_4: 0.2519  loss_dice_dn_4: 0.7441  loss_bbox_dn_4: 0.2114  loss_giou_dn_4: 0.5636  loss_ce_5: 0.8583  loss_mask_5: 0.2697  loss_dice_5: 0.7405  loss_bbox_5: 0.2295  loss_giou_5: 0.735  loss_ce_dn_5: 0.1467  loss_mask_dn_5: 0.2579  loss_dice_dn_5: 0.7463  loss_bbox_dn_5: 0.2071  loss_giou_dn_5: 0.5575  loss_ce_6: 0.8024  loss_mask_6: 0.2728  loss_dice_6: 0.8014  loss_bbox_6: 0.2333  loss_giou_6: 0.7358  loss_ce_dn_6: 0.1349  loss_mask_dn_6: 0.2557  loss_dice_dn_6: 0.7371  loss_bbox_dn_6: 0.2  loss_giou_dn_6: 0.5514  loss_ce_7: 0.8056  loss_mask_7: 0.2532  loss_dice_7: 0.8026  loss_bbox_7: 0.2241  loss_giou_7: 0.7405  loss_ce_dn_7: 0.1336  loss_mask_dn_7: 0.2518  loss_dice_dn_7: 0.7342  loss_bbox_dn_7: 0.199  loss_giou_dn_7: 0.5446  loss_ce_8: 0.8112  loss_mask_8: 0.2634  loss_dice_8: 0.7805  loss_bbox_8: 0.2192  loss_giou_8: 0.7346  loss_ce_dn_8: 0.1321  loss_mask_dn_8: 0.2514  loss_dice_dn_8: 0.7317  loss_bbox_dn_8: 0.1995  loss_giou_dn_8: 0.541  loss_ce_interm: 1.453  loss_mask_interm: 0.2984  loss_dice_interm: 0.801  loss_bbox_interm: 0.3732  loss_giou_interm: 0.9221    time: 0.8690  last_time: 0.8861  data_time: 0.0115  last_data_time: 0.0157   lr: 1.25e-05  max_mem: 11823M\n",
            "\u001b[32m[02/20 07:33:35 d2.utils.events]: \u001b[0m eta: 3 days, 16:46:09  iter: 299  total_loss: 65.34  loss_ce: 0.8502  loss_mask: 0.3124  loss_dice: 0.6795  loss_bbox: 0.2594  loss_giou: 0.8091  loss_ce_dn: 0.1535  loss_mask_dn: 0.3068  loss_dice_dn: 0.7309  loss_bbox_dn: 0.2784  loss_giou_dn: 0.6889  loss_ce_0: 1.557  loss_mask_0: 0.3212  loss_dice_0: 0.7171  loss_bbox_0: 0.4585  loss_giou_0: 1.012  loss_ce_dn_0: 0.7818  loss_mask_dn_0: 0.9083  loss_dice_dn_0: 3.246  loss_bbox_dn_0: 0.6706  loss_giou_dn_0: 1.038  loss_ce_1: 1.506  loss_mask_1: 0.3157  loss_dice_1: 0.7184  loss_bbox_1: 0.3178  loss_giou_1: 0.9164  loss_ce_dn_1: 0.3039  loss_mask_dn_1: 0.4144  loss_dice_dn_1: 0.8243  loss_bbox_dn_1: 0.3887  loss_giou_dn_1: 0.8206  loss_ce_2: 1.323  loss_mask_2: 0.324  loss_dice_2: 0.6992  loss_bbox_2: 0.3057  loss_giou_2: 0.8054  loss_ce_dn_2: 0.2541  loss_mask_dn_2: 0.3785  loss_dice_dn_2: 0.7522  loss_bbox_dn_2: 0.3339  loss_giou_dn_2: 0.7456  loss_ce_3: 1.079  loss_mask_3: 0.3074  loss_dice_3: 0.7002  loss_bbox_3: 0.3044  loss_giou_3: 0.7799  loss_ce_dn_3: 0.2139  loss_mask_dn_3: 0.3378  loss_dice_dn_3: 0.7172  loss_bbox_dn_3: 0.3232  loss_giou_dn_3: 0.7194  loss_ce_4: 0.9761  loss_mask_4: 0.3079  loss_dice_4: 0.7047  loss_bbox_4: 0.2809  loss_giou_4: 0.8015  loss_ce_dn_4: 0.1974  loss_mask_dn_4: 0.3352  loss_dice_dn_4: 0.7153  loss_bbox_dn_4: 0.3039  loss_giou_dn_4: 0.7052  loss_ce_5: 0.9169  loss_mask_5: 0.308  loss_dice_5: 0.7165  loss_bbox_5: 0.2728  loss_giou_5: 0.7844  loss_ce_dn_5: 0.1882  loss_mask_dn_5: 0.317  loss_dice_dn_5: 0.7253  loss_bbox_dn_5: 0.3019  loss_giou_dn_5: 0.6978  loss_ce_6: 0.8329  loss_mask_6: 0.3141  loss_dice_6: 0.6984  loss_bbox_6: 0.2686  loss_giou_6: 0.8095  loss_ce_dn_6: 0.1675  loss_mask_dn_6: 0.3267  loss_dice_dn_6: 0.728  loss_bbox_dn_6: 0.2908  loss_giou_dn_6: 0.692  loss_ce_7: 0.8312  loss_mask_7: 0.3  loss_dice_7: 0.709  loss_bbox_7: 0.2772  loss_giou_7: 0.8086  loss_ce_dn_7: 0.1567  loss_mask_dn_7: 0.3146  loss_dice_dn_7: 0.7058  loss_bbox_dn_7: 0.28  loss_giou_dn_7: 0.6917  loss_ce_8: 0.815  loss_mask_8: 0.3123  loss_dice_8: 0.7009  loss_bbox_8: 0.2742  loss_giou_8: 0.8071  loss_ce_dn_8: 0.1532  loss_mask_dn_8: 0.3194  loss_dice_dn_8: 0.7283  loss_bbox_dn_8: 0.2785  loss_giou_dn_8: 0.6878  loss_ce_interm: 1.572  loss_mask_interm: 0.3253  loss_dice_interm: 0.6993  loss_bbox_interm: 0.4519  loss_giou_interm: 1.006    time: 0.8689  last_time: 0.8561  data_time: 0.0123  last_data_time: 0.0122   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:33:53 d2.utils.events]: \u001b[0m eta: 3 days, 16:48:31  iter: 319  total_loss: 64.96  loss_ce: 0.9345  loss_mask: 0.2787  loss_dice: 0.6932  loss_bbox: 0.3025  loss_giou: 0.8643  loss_ce_dn: 0.2265  loss_mask_dn: 0.2709  loss_dice_dn: 0.6555  loss_bbox_dn: 0.2486  loss_giou_dn: 0.7689  loss_ce_0: 1.544  loss_mask_0: 0.2844  loss_dice_0: 0.7287  loss_bbox_0: 0.4301  loss_giou_0: 1.02  loss_ce_dn_0: 0.739  loss_mask_dn_0: 0.9507  loss_dice_dn_0: 3.276  loss_bbox_dn_0: 0.6806  loss_giou_dn_0: 1.135  loss_ce_1: 1.407  loss_mask_1: 0.3123  loss_dice_1: 0.7168  loss_bbox_1: 0.3275  loss_giou_1: 0.9603  loss_ce_dn_1: 0.3218  loss_mask_dn_1: 0.3362  loss_dice_dn_1: 0.7458  loss_bbox_dn_1: 0.374  loss_giou_dn_1: 0.9219  loss_ce_2: 1.261  loss_mask_2: 0.2989  loss_dice_2: 0.6874  loss_bbox_2: 0.2903  loss_giou_2: 0.8938  loss_ce_dn_2: 0.2756  loss_mask_dn_2: 0.2959  loss_dice_dn_2: 0.7226  loss_bbox_dn_2: 0.3158  loss_giou_dn_2: 0.8394  loss_ce_3: 1.125  loss_mask_3: 0.2859  loss_dice_3: 0.6719  loss_bbox_3: 0.2955  loss_giou_3: 0.8843  loss_ce_dn_3: 0.2558  loss_mask_dn_3: 0.2929  loss_dice_dn_3: 0.6982  loss_bbox_dn_3: 0.2847  loss_giou_dn_3: 0.8143  loss_ce_4: 1.057  loss_mask_4: 0.2881  loss_dice_4: 0.6661  loss_bbox_4: 0.2709  loss_giou_4: 0.8535  loss_ce_dn_4: 0.2478  loss_mask_dn_4: 0.2888  loss_dice_dn_4: 0.7012  loss_bbox_dn_4: 0.2643  loss_giou_dn_4: 0.7873  loss_ce_5: 0.989  loss_mask_5: 0.2916  loss_dice_5: 0.6509  loss_bbox_5: 0.3025  loss_giou_5: 0.8683  loss_ce_dn_5: 0.2357  loss_mask_dn_5: 0.2879  loss_dice_dn_5: 0.6799  loss_bbox_dn_5: 0.2568  loss_giou_dn_5: 0.7827  loss_ce_6: 0.9597  loss_mask_6: 0.284  loss_dice_6: 0.6638  loss_bbox_6: 0.3024  loss_giou_6: 0.863  loss_ce_dn_6: 0.2293  loss_mask_dn_6: 0.284  loss_dice_dn_6: 0.6818  loss_bbox_dn_6: 0.2504  loss_giou_dn_6: 0.7742  loss_ce_7: 0.9601  loss_mask_7: 0.2816  loss_dice_7: 0.7209  loss_bbox_7: 0.3034  loss_giou_7: 0.8588  loss_ce_dn_7: 0.2278  loss_mask_dn_7: 0.2766  loss_dice_dn_7: 0.6821  loss_bbox_dn_7: 0.2478  loss_giou_dn_7: 0.7718  loss_ce_8: 0.9524  loss_mask_8: 0.2791  loss_dice_8: 0.6695  loss_bbox_8: 0.2982  loss_giou_8: 0.8556  loss_ce_dn_8: 0.2253  loss_mask_dn_8: 0.2742  loss_dice_dn_8: 0.6435  loss_bbox_dn_8: 0.248  loss_giou_dn_8: 0.7684  loss_ce_interm: 1.539  loss_mask_interm: 0.3098  loss_dice_interm: 0.741  loss_bbox_interm: 0.4284  loss_giou_interm: 1.01    time: 0.8695  last_time: 0.8752  data_time: 0.0135  last_data_time: 0.0192   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:34:10 d2.utils.events]: \u001b[0m eta: 3 days, 16:48:14  iter: 339  total_loss: 57.48  loss_ce: 0.6381  loss_mask: 0.2167  loss_dice: 0.7738  loss_bbox: 0.2171  loss_giou: 0.7628  loss_ce_dn: 0.1201  loss_mask_dn: 0.2278  loss_dice_dn: 0.6941  loss_bbox_dn: 0.189  loss_giou_dn: 0.519  loss_ce_0: 1.39  loss_mask_0: 0.2286  loss_dice_0: 0.7668  loss_bbox_0: 0.3696  loss_giou_0: 0.9208  loss_ce_dn_0: 0.7173  loss_mask_dn_0: 0.7143  loss_dice_dn_0: 2.592  loss_bbox_dn_0: 0.5453  loss_giou_dn_0: 0.9032  loss_ce_1: 1.279  loss_mask_1: 0.2338  loss_dice_1: 0.7883  loss_bbox_1: 0.2309  loss_giou_1: 0.8327  loss_ce_dn_1: 0.2544  loss_mask_dn_1: 0.2756  loss_dice_dn_1: 0.8504  loss_bbox_dn_1: 0.3224  loss_giou_dn_1: 0.6536  loss_ce_2: 1.049  loss_mask_2: 0.2282  loss_dice_2: 0.7487  loss_bbox_2: 0.2284  loss_giou_2: 0.7799  loss_ce_dn_2: 0.2074  loss_mask_dn_2: 0.2757  loss_dice_dn_2: 0.7291  loss_bbox_dn_2: 0.2686  loss_giou_dn_2: 0.5861  loss_ce_3: 0.9247  loss_mask_3: 0.2227  loss_dice_3: 0.7443  loss_bbox_3: 0.2134  loss_giou_3: 0.7712  loss_ce_dn_3: 0.1705  loss_mask_dn_3: 0.2628  loss_dice_dn_3: 0.7345  loss_bbox_dn_3: 0.2343  loss_giou_dn_3: 0.557  loss_ce_4: 0.8155  loss_mask_4: 0.2194  loss_dice_4: 0.739  loss_bbox_4: 0.2232  loss_giou_4: 0.776  loss_ce_dn_4: 0.1528  loss_mask_dn_4: 0.2378  loss_dice_dn_4: 0.6799  loss_bbox_dn_4: 0.2097  loss_giou_dn_4: 0.5345  loss_ce_5: 0.7394  loss_mask_5: 0.2198  loss_dice_5: 0.7624  loss_bbox_5: 0.2208  loss_giou_5: 0.767  loss_ce_dn_5: 0.1378  loss_mask_dn_5: 0.235  loss_dice_dn_5: 0.6724  loss_bbox_dn_5: 0.2015  loss_giou_dn_5: 0.5306  loss_ce_6: 0.6915  loss_mask_6: 0.2149  loss_dice_6: 0.757  loss_bbox_6: 0.2334  loss_giou_6: 0.7512  loss_ce_dn_6: 0.1281  loss_mask_dn_6: 0.2277  loss_dice_dn_6: 0.7001  loss_bbox_dn_6: 0.1872  loss_giou_dn_6: 0.525  loss_ce_7: 0.6927  loss_mask_7: 0.2091  loss_dice_7: 0.7582  loss_bbox_7: 0.2266  loss_giou_7: 0.7628  loss_ce_dn_7: 0.1252  loss_mask_dn_7: 0.2281  loss_dice_dn_7: 0.7135  loss_bbox_dn_7: 0.1861  loss_giou_dn_7: 0.5219  loss_ce_8: 0.6617  loss_mask_8: 0.2125  loss_dice_8: 0.7525  loss_bbox_8: 0.2252  loss_giou_8: 0.7766  loss_ce_dn_8: 0.1187  loss_mask_dn_8: 0.2288  loss_dice_dn_8: 0.7097  loss_bbox_dn_8: 0.1874  loss_giou_dn_8: 0.5184  loss_ce_interm: 1.396  loss_mask_interm: 0.2301  loss_dice_interm: 0.7736  loss_bbox_interm: 0.3607  loss_giou_interm: 0.9328    time: 0.8695  last_time: 0.8801  data_time: 0.0123  last_data_time: 0.0124   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:34:28 d2.utils.events]: \u001b[0m eta: 3 days, 16:45:56  iter: 359  total_loss: 66.94  loss_ce: 0.7566  loss_mask: 0.3747  loss_dice: 0.8037  loss_bbox: 0.3083  loss_giou: 0.6956  loss_ce_dn: 0.1529  loss_mask_dn: 0.39  loss_dice_dn: 0.7713  loss_bbox_dn: 0.2642  loss_giou_dn: 0.5817  loss_ce_0: 1.625  loss_mask_0: 0.3917  loss_dice_0: 0.8583  loss_bbox_0: 0.4461  loss_giou_0: 0.9117  loss_ce_dn_0: 0.7157  loss_mask_dn_0: 0.9782  loss_dice_dn_0: 3.016  loss_bbox_dn_0: 0.6837  loss_giou_dn_0: 0.9253  loss_ce_1: 1.457  loss_mask_1: 0.421  loss_dice_1: 0.804  loss_bbox_1: 0.3666  loss_giou_1: 0.8232  loss_ce_dn_1: 0.2867  loss_mask_dn_1: 0.4878  loss_dice_dn_1: 0.9298  loss_bbox_dn_1: 0.4294  loss_giou_dn_1: 0.7022  loss_ce_2: 1.227  loss_mask_2: 0.3693  loss_dice_2: 0.8106  loss_bbox_2: 0.3177  loss_giou_2: 0.7662  loss_ce_dn_2: 0.2369  loss_mask_dn_2: 0.4211  loss_dice_dn_2: 0.843  loss_bbox_dn_2: 0.3559  loss_giou_dn_2: 0.6316  loss_ce_3: 1.019  loss_mask_3: 0.3972  loss_dice_3: 0.8256  loss_bbox_3: 0.323  loss_giou_3: 0.7868  loss_ce_dn_3: 0.1983  loss_mask_dn_3: 0.4057  loss_dice_dn_3: 0.7978  loss_bbox_dn_3: 0.321  loss_giou_dn_3: 0.617  loss_ce_4: 0.8872  loss_mask_4: 0.3793  loss_dice_4: 0.8612  loss_bbox_4: 0.3167  loss_giou_4: 0.783  loss_ce_dn_4: 0.1787  loss_mask_dn_4: 0.3977  loss_dice_dn_4: 0.7973  loss_bbox_dn_4: 0.2949  loss_giou_dn_4: 0.5987  loss_ce_5: 0.8722  loss_mask_5: 0.3711  loss_dice_5: 0.8176  loss_bbox_5: 0.3136  loss_giou_5: 0.7736  loss_ce_dn_5: 0.1696  loss_mask_dn_5: 0.3975  loss_dice_dn_5: 0.7944  loss_bbox_dn_5: 0.2874  loss_giou_dn_5: 0.5909  loss_ce_6: 0.8133  loss_mask_6: 0.3686  loss_dice_6: 0.7899  loss_bbox_6: 0.3136  loss_giou_6: 0.7218  loss_ce_dn_6: 0.1601  loss_mask_dn_6: 0.3863  loss_dice_dn_6: 0.7761  loss_bbox_dn_6: 0.2761  loss_giou_dn_6: 0.583  loss_ce_7: 0.7573  loss_mask_7: 0.3683  loss_dice_7: 0.8099  loss_bbox_7: 0.3051  loss_giou_7: 0.7144  loss_ce_dn_7: 0.1545  loss_mask_dn_7: 0.3819  loss_dice_dn_7: 0.7735  loss_bbox_dn_7: 0.274  loss_giou_dn_7: 0.5834  loss_ce_8: 0.7442  loss_mask_8: 0.3752  loss_dice_8: 0.8089  loss_bbox_8: 0.3058  loss_giou_8: 0.7259  loss_ce_dn_8: 0.1549  loss_mask_dn_8: 0.3873  loss_dice_dn_8: 0.8006  loss_bbox_dn_8: 0.2643  loss_giou_dn_8: 0.5802  loss_ce_interm: 1.538  loss_mask_interm: 0.4321  loss_dice_interm: 0.8493  loss_bbox_interm: 0.4464  loss_giou_interm: 0.9149    time: 0.8695  last_time: 0.8825  data_time: 0.0112  last_data_time: 0.0201   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:34:45 d2.utils.events]: \u001b[0m eta: 3 days, 16:45:30  iter: 379  total_loss: 59.76  loss_ce: 0.7784  loss_mask: 0.2011  loss_dice: 0.8323  loss_bbox: 0.2798  loss_giou: 0.7457  loss_ce_dn: 0.1163  loss_mask_dn: 0.1991  loss_dice_dn: 0.8629  loss_bbox_dn: 0.1941  loss_giou_dn: 0.5563  loss_ce_0: 1.496  loss_mask_0: 0.2073  loss_dice_0: 0.8812  loss_bbox_0: 0.3625  loss_giou_0: 0.9154  loss_ce_dn_0: 0.6652  loss_mask_dn_0: 0.6767  loss_dice_dn_0: 3.016  loss_bbox_dn_0: 0.555  loss_giou_dn_0: 0.9011  loss_ce_1: 1.336  loss_mask_1: 0.1891  loss_dice_1: 0.8215  loss_bbox_1: 0.3026  loss_giou_1: 0.7904  loss_ce_dn_1: 0.2509  loss_mask_dn_1: 0.2474  loss_dice_dn_1: 0.9566  loss_bbox_dn_1: 0.2875  loss_giou_dn_1: 0.6618  loss_ce_2: 1.16  loss_mask_2: 0.1918  loss_dice_2: 0.8323  loss_bbox_2: 0.2677  loss_giou_2: 0.7247  loss_ce_dn_2: 0.1977  loss_mask_dn_2: 0.2105  loss_dice_dn_2: 0.9119  loss_bbox_dn_2: 0.2493  loss_giou_dn_2: 0.6186  loss_ce_3: 0.9746  loss_mask_3: 0.183  loss_dice_3: 0.7885  loss_bbox_3: 0.2842  loss_giou_3: 0.7457  loss_ce_dn_3: 0.1683  loss_mask_dn_3: 0.1954  loss_dice_dn_3: 0.8822  loss_bbox_dn_3: 0.2254  loss_giou_dn_3: 0.5909  loss_ce_4: 0.9199  loss_mask_4: 0.1791  loss_dice_4: 0.8465  loss_bbox_4: 0.2835  loss_giou_4: 0.7339  loss_ce_dn_4: 0.1541  loss_mask_dn_4: 0.197  loss_dice_dn_4: 0.8566  loss_bbox_dn_4: 0.2104  loss_giou_dn_4: 0.5802  loss_ce_5: 0.8156  loss_mask_5: 0.1918  loss_dice_5: 0.7786  loss_bbox_5: 0.2735  loss_giou_5: 0.7517  loss_ce_dn_5: 0.1313  loss_mask_dn_5: 0.2007  loss_dice_dn_5: 0.8614  loss_bbox_dn_5: 0.2054  loss_giou_dn_5: 0.5766  loss_ce_6: 0.7901  loss_mask_6: 0.1873  loss_dice_6: 0.8191  loss_bbox_6: 0.2766  loss_giou_6: 0.7601  loss_ce_dn_6: 0.1244  loss_mask_dn_6: 0.2026  loss_dice_dn_6: 0.8607  loss_bbox_dn_6: 0.1984  loss_giou_dn_6: 0.5646  loss_ce_7: 0.7942  loss_mask_7: 0.1963  loss_dice_7: 0.8472  loss_bbox_7: 0.2723  loss_giou_7: 0.7313  loss_ce_dn_7: 0.1199  loss_mask_dn_7: 0.203  loss_dice_dn_7: 0.8679  loss_bbox_dn_7: 0.1981  loss_giou_dn_7: 0.5612  loss_ce_8: 0.776  loss_mask_8: 0.1989  loss_dice_8: 0.8544  loss_bbox_8: 0.2883  loss_giou_8: 0.7519  loss_ce_dn_8: 0.1154  loss_mask_dn_8: 0.1994  loss_dice_dn_8: 0.8462  loss_bbox_dn_8: 0.1942  loss_giou_dn_8: 0.558  loss_ce_interm: 1.508  loss_mask_interm: 0.2022  loss_dice_interm: 0.8836  loss_bbox_interm: 0.3625  loss_giou_interm: 0.9091    time: 0.8696  last_time: 0.9165  data_time: 0.0120  last_data_time: 0.0168   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:35:03 d2.utils.events]: \u001b[0m eta: 3 days, 16:47:51  iter: 399  total_loss: 66.26  loss_ce: 0.8328  loss_mask: 0.2163  loss_dice: 0.9853  loss_bbox: 0.2796  loss_giou: 0.693  loss_ce_dn: 0.1234  loss_mask_dn: 0.1957  loss_dice_dn: 0.9247  loss_bbox_dn: 0.2124  loss_giou_dn: 0.5535  loss_ce_0: 1.503  loss_mask_0: 0.2547  loss_dice_0: 1.027  loss_bbox_0: 0.3959  loss_giou_0: 0.8636  loss_ce_dn_0: 0.6959  loss_mask_dn_0: 0.685  loss_dice_dn_0: 3.056  loss_bbox_dn_0: 0.5787  loss_giou_dn_0: 0.9058  loss_ce_1: 1.368  loss_mask_1: 0.257  loss_dice_1: 1.032  loss_bbox_1: 0.312  loss_giou_1: 0.7834  loss_ce_dn_1: 0.2498  loss_mask_dn_1: 0.2383  loss_dice_dn_1: 1.078  loss_bbox_dn_1: 0.3397  loss_giou_dn_1: 0.6619  loss_ce_2: 1.214  loss_mask_2: 0.243  loss_dice_2: 1.017  loss_bbox_2: 0.2681  loss_giou_2: 0.7557  loss_ce_dn_2: 0.2006  loss_mask_dn_2: 0.2196  loss_dice_dn_2: 1.009  loss_bbox_dn_2: 0.2656  loss_giou_dn_2: 0.5983  loss_ce_3: 1.114  loss_mask_3: 0.2184  loss_dice_3: 0.9931  loss_bbox_3: 0.2734  loss_giou_3: 0.741  loss_ce_dn_3: 0.1735  loss_mask_dn_3: 0.202  loss_dice_dn_3: 0.9409  loss_bbox_dn_3: 0.2396  loss_giou_dn_3: 0.5853  loss_ce_4: 1.038  loss_mask_4: 0.209  loss_dice_4: 0.9425  loss_bbox_4: 0.2767  loss_giou_4: 0.7097  loss_ce_dn_4: 0.157  loss_mask_dn_4: 0.2008  loss_dice_dn_4: 0.9445  loss_bbox_dn_4: 0.2235  loss_giou_dn_4: 0.5754  loss_ce_5: 0.9365  loss_mask_5: 0.2137  loss_dice_5: 0.9947  loss_bbox_5: 0.2701  loss_giou_5: 0.731  loss_ce_dn_5: 0.1402  loss_mask_dn_5: 0.1881  loss_dice_dn_5: 0.9516  loss_bbox_dn_5: 0.2165  loss_giou_dn_5: 0.5675  loss_ce_6: 0.8847  loss_mask_6: 0.211  loss_dice_6: 1.014  loss_bbox_6: 0.2681  loss_giou_6: 0.714  loss_ce_dn_6: 0.1278  loss_mask_dn_6: 0.1914  loss_dice_dn_6: 0.9367  loss_bbox_dn_6: 0.214  loss_giou_dn_6: 0.5603  loss_ce_7: 0.8605  loss_mask_7: 0.2181  loss_dice_7: 0.9926  loss_bbox_7: 0.2691  loss_giou_7: 0.7001  loss_ce_dn_7: 0.1264  loss_mask_dn_7: 0.1992  loss_dice_dn_7: 0.9303  loss_bbox_dn_7: 0.2147  loss_giou_dn_7: 0.5562  loss_ce_8: 0.8656  loss_mask_8: 0.2183  loss_dice_8: 1.002  loss_bbox_8: 0.2634  loss_giou_8: 0.6926  loss_ce_dn_8: 0.1256  loss_mask_dn_8: 0.1951  loss_dice_dn_8: 0.9171  loss_bbox_dn_8: 0.2131  loss_giou_dn_8: 0.5526  loss_ce_interm: 1.496  loss_mask_interm: 0.2576  loss_dice_interm: 1.028  loss_bbox_interm: 0.3929  loss_giou_interm: 0.8822    time: 0.8701  last_time: 0.8678  data_time: 0.0132  last_data_time: 0.0077   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:35:21 d2.utils.events]: \u001b[0m eta: 3 days, 16:49:17  iter: 419  total_loss: 61.43  loss_ce: 0.8037  loss_mask: 0.2289  loss_dice: 0.8201  loss_bbox: 0.2658  loss_giou: 0.6571  loss_ce_dn: 0.1447  loss_mask_dn: 0.2496  loss_dice_dn: 0.8536  loss_bbox_dn: 0.2668  loss_giou_dn: 0.5024  loss_ce_0: 1.504  loss_mask_0: 0.2879  loss_dice_0: 0.8866  loss_bbox_0: 0.4228  loss_giou_0: 0.8847  loss_ce_dn_0: 0.6969  loss_mask_dn_0: 0.7464  loss_dice_dn_0: 2.828  loss_bbox_dn_0: 0.6635  loss_giou_dn_0: 0.9094  loss_ce_1: 1.384  loss_mask_1: 0.2797  loss_dice_1: 0.875  loss_bbox_1: 0.3189  loss_giou_1: 0.7633  loss_ce_dn_1: 0.2748  loss_mask_dn_1: 0.3336  loss_dice_dn_1: 1.017  loss_bbox_dn_1: 0.3911  loss_giou_dn_1: 0.651  loss_ce_2: 1.269  loss_mask_2: 0.2701  loss_dice_2: 0.8428  loss_bbox_2: 0.2838  loss_giou_2: 0.7519  loss_ce_dn_2: 0.2336  loss_mask_dn_2: 0.2949  loss_dice_dn_2: 0.9647  loss_bbox_dn_2: 0.3127  loss_giou_dn_2: 0.5826  loss_ce_3: 1.129  loss_mask_3: 0.2612  loss_dice_3: 0.8364  loss_bbox_3: 0.2803  loss_giou_3: 0.726  loss_ce_dn_3: 0.189  loss_mask_dn_3: 0.2807  loss_dice_dn_3: 0.9057  loss_bbox_dn_3: 0.2937  loss_giou_dn_3: 0.5565  loss_ce_4: 1.012  loss_mask_4: 0.2321  loss_dice_4: 0.8516  loss_bbox_4: 0.2781  loss_giou_4: 0.6945  loss_ce_dn_4: 0.1676  loss_mask_dn_4: 0.2695  loss_dice_dn_4: 0.8737  loss_bbox_dn_4: 0.2844  loss_giou_dn_4: 0.5339  loss_ce_5: 0.9366  loss_mask_5: 0.2371  loss_dice_5: 0.8439  loss_bbox_5: 0.2809  loss_giou_5: 0.6792  loss_ce_dn_5: 0.1519  loss_mask_dn_5: 0.2673  loss_dice_dn_5: 0.8702  loss_bbox_dn_5: 0.283  loss_giou_dn_5: 0.5254  loss_ce_6: 0.8443  loss_mask_6: 0.2373  loss_dice_6: 0.8737  loss_bbox_6: 0.2799  loss_giou_6: 0.6867  loss_ce_dn_6: 0.1492  loss_mask_dn_6: 0.2525  loss_dice_dn_6: 0.8241  loss_bbox_dn_6: 0.2759  loss_giou_dn_6: 0.5129  loss_ce_7: 0.8363  loss_mask_7: 0.2352  loss_dice_7: 0.8314  loss_bbox_7: 0.2624  loss_giou_7: 0.6633  loss_ce_dn_7: 0.1493  loss_mask_dn_7: 0.2523  loss_dice_dn_7: 0.8284  loss_bbox_dn_7: 0.2709  loss_giou_dn_7: 0.5066  loss_ce_8: 0.8058  loss_mask_8: 0.2341  loss_dice_8: 0.8391  loss_bbox_8: 0.2658  loss_giou_8: 0.6462  loss_ce_dn_8: 0.1434  loss_mask_dn_8: 0.25  loss_dice_dn_8: 0.8588  loss_bbox_dn_8: 0.2663  loss_giou_dn_8: 0.5015  loss_ce_interm: 1.517  loss_mask_interm: 0.2882  loss_dice_interm: 0.8703  loss_bbox_interm: 0.4391  loss_giou_interm: 0.898    time: 0.8704  last_time: 0.8929  data_time: 0.0122  last_data_time: 0.0096   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:35:38 d2.utils.events]: \u001b[0m eta: 3 days, 16:50:26  iter: 439  total_loss: 69.04  loss_ce: 0.8563  loss_mask: 0.2586  loss_dice: 0.7755  loss_bbox: 0.295  loss_giou: 0.8308  loss_ce_dn: 0.1283  loss_mask_dn: 0.2637  loss_dice_dn: 0.8364  loss_bbox_dn: 0.2554  loss_giou_dn: 0.6382  loss_ce_0: 1.503  loss_mask_0: 0.288  loss_dice_0: 0.822  loss_bbox_0: 0.4255  loss_giou_0: 1.077  loss_ce_dn_0: 0.7705  loss_mask_dn_0: 0.811  loss_dice_dn_0: 3.381  loss_bbox_dn_0: 0.6278  loss_giou_dn_0: 0.94  loss_ce_1: 1.431  loss_mask_1: 0.2642  loss_dice_1: 0.8141  loss_bbox_1: 0.3068  loss_giou_1: 0.9316  loss_ce_dn_1: 0.2629  loss_mask_dn_1: 0.3286  loss_dice_dn_1: 0.9545  loss_bbox_dn_1: 0.382  loss_giou_dn_1: 0.7883  loss_ce_2: 1.274  loss_mask_2: 0.274  loss_dice_2: 0.7288  loss_bbox_2: 0.293  loss_giou_2: 0.9148  loss_ce_dn_2: 0.2194  loss_mask_dn_2: 0.311  loss_dice_dn_2: 0.8941  loss_bbox_dn_2: 0.3132  loss_giou_dn_2: 0.7296  loss_ce_3: 1.093  loss_mask_3: 0.2735  loss_dice_3: 0.7866  loss_bbox_3: 0.2996  loss_giou_3: 0.8917  loss_ce_dn_3: 0.1858  loss_mask_dn_3: 0.2976  loss_dice_dn_3: 0.8553  loss_bbox_dn_3: 0.2886  loss_giou_dn_3: 0.6907  loss_ce_4: 0.9805  loss_mask_4: 0.2701  loss_dice_4: 0.7774  loss_bbox_4: 0.2878  loss_giou_4: 0.8801  loss_ce_dn_4: 0.1693  loss_mask_dn_4: 0.2812  loss_dice_dn_4: 0.8123  loss_bbox_dn_4: 0.2764  loss_giou_dn_4: 0.6648  loss_ce_5: 0.9167  loss_mask_5: 0.2615  loss_dice_5: 0.7807  loss_bbox_5: 0.2819  loss_giou_5: 0.8402  loss_ce_dn_5: 0.1584  loss_mask_dn_5: 0.2749  loss_dice_dn_5: 0.8116  loss_bbox_dn_5: 0.2692  loss_giou_dn_5: 0.6524  loss_ce_6: 0.833  loss_mask_6: 0.2561  loss_dice_6: 0.768  loss_bbox_6: 0.2769  loss_giou_6: 0.8328  loss_ce_dn_6: 0.1523  loss_mask_dn_6: 0.27  loss_dice_dn_6: 0.8133  loss_bbox_dn_6: 0.2641  loss_giou_dn_6: 0.6458  loss_ce_7: 0.829  loss_mask_7: 0.2639  loss_dice_7: 0.798  loss_bbox_7: 0.2982  loss_giou_7: 0.838  loss_ce_dn_7: 0.1416  loss_mask_dn_7: 0.2682  loss_dice_dn_7: 0.8236  loss_bbox_dn_7: 0.262  loss_giou_dn_7: 0.6425  loss_ce_8: 0.8617  loss_mask_8: 0.253  loss_dice_8: 0.7929  loss_bbox_8: 0.2947  loss_giou_8: 0.8395  loss_ce_dn_8: 0.132  loss_mask_dn_8: 0.2652  loss_dice_dn_8: 0.8277  loss_bbox_dn_8: 0.2558  loss_giou_dn_8: 0.6387  loss_ce_interm: 1.493  loss_mask_interm: 0.2887  loss_dice_interm: 0.8595  loss_bbox_interm: 0.4275  loss_giou_interm: 1.067    time: 0.8711  last_time: 0.8767  data_time: 0.0123  last_data_time: 0.0130   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:35:56 d2.utils.events]: \u001b[0m eta: 3 days, 16:53:54  iter: 459  total_loss: 62.95  loss_ce: 0.7501  loss_mask: 0.2921  loss_dice: 0.7875  loss_bbox: 0.2998  loss_giou: 0.6715  loss_ce_dn: 0.09851  loss_mask_dn: 0.2967  loss_dice_dn: 0.7652  loss_bbox_dn: 0.2471  loss_giou_dn: 0.5767  loss_ce_0: 1.411  loss_mask_0: 0.3168  loss_dice_0: 0.8231  loss_bbox_0: 0.4323  loss_giou_0: 0.9336  loss_ce_dn_0: 0.7081  loss_mask_dn_0: 0.7888  loss_dice_dn_0: 2.861  loss_bbox_dn_0: 0.6473  loss_giou_dn_0: 0.9641  loss_ce_1: 1.359  loss_mask_1: 0.3274  loss_dice_1: 0.8214  loss_bbox_1: 0.3213  loss_giou_1: 0.8028  loss_ce_dn_1: 0.2503  loss_mask_dn_1: 0.36  loss_dice_dn_1: 0.9067  loss_bbox_dn_1: 0.3894  loss_giou_dn_1: 0.7176  loss_ce_2: 1.217  loss_mask_2: 0.2949  loss_dice_2: 0.7921  loss_bbox_2: 0.2782  loss_giou_2: 0.7441  loss_ce_dn_2: 0.1962  loss_mask_dn_2: 0.3189  loss_dice_dn_2: 0.8493  loss_bbox_dn_2: 0.3191  loss_giou_dn_2: 0.6358  loss_ce_3: 1.037  loss_mask_3: 0.2672  loss_dice_3: 0.782  loss_bbox_3: 0.333  loss_giou_3: 0.7355  loss_ce_dn_3: 0.1617  loss_mask_dn_3: 0.3303  loss_dice_dn_3: 0.7988  loss_bbox_dn_3: 0.2866  loss_giou_dn_3: 0.6125  loss_ce_4: 0.9177  loss_mask_4: 0.2643  loss_dice_4: 0.7686  loss_bbox_4: 0.3118  loss_giou_4: 0.6922  loss_ce_dn_4: 0.1463  loss_mask_dn_4: 0.3253  loss_dice_dn_4: 0.7947  loss_bbox_dn_4: 0.2685  loss_giou_dn_4: 0.5998  loss_ce_5: 0.8296  loss_mask_5: 0.279  loss_dice_5: 0.7855  loss_bbox_5: 0.3269  loss_giou_5: 0.7176  loss_ce_dn_5: 0.1259  loss_mask_dn_5: 0.319  loss_dice_dn_5: 0.793  loss_bbox_dn_5: 0.2664  loss_giou_dn_5: 0.5955  loss_ce_6: 0.802  loss_mask_6: 0.2847  loss_dice_6: 0.7785  loss_bbox_6: 0.3161  loss_giou_6: 0.7133  loss_ce_dn_6: 0.1113  loss_mask_dn_6: 0.316  loss_dice_dn_6: 0.7799  loss_bbox_dn_6: 0.256  loss_giou_dn_6: 0.5836  loss_ce_7: 0.8312  loss_mask_7: 0.2893  loss_dice_7: 0.7906  loss_bbox_7: 0.3004  loss_giou_7: 0.6868  loss_ce_dn_7: 0.102  loss_mask_dn_7: 0.308  loss_dice_dn_7: 0.7817  loss_bbox_dn_7: 0.2514  loss_giou_dn_7: 0.5789  loss_ce_8: 0.7721  loss_mask_8: 0.2973  loss_dice_8: 0.7801  loss_bbox_8: 0.2991  loss_giou_8: 0.7181  loss_ce_dn_8: 0.09972  loss_mask_dn_8: 0.3096  loss_dice_dn_8: 0.7914  loss_bbox_dn_8: 0.2483  loss_giou_dn_8: 0.5773  loss_ce_interm: 1.431  loss_mask_interm: 0.329  loss_dice_interm: 0.8638  loss_bbox_interm: 0.4305  loss_giou_interm: 0.9272    time: 0.8715  last_time: 0.8770  data_time: 0.0123  last_data_time: 0.0235   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:36:13 d2.utils.events]: \u001b[0m eta: 3 days, 16:57:51  iter: 479  total_loss: 56.9  loss_ce: 0.7254  loss_mask: 0.2298  loss_dice: 0.7741  loss_bbox: 0.2873  loss_giou: 0.6442  loss_ce_dn: 0.1016  loss_mask_dn: 0.2636  loss_dice_dn: 0.7693  loss_bbox_dn: 0.2118  loss_giou_dn: 0.497  loss_ce_0: 1.339  loss_mask_0: 0.2346  loss_dice_0: 0.8018  loss_bbox_0: 0.4171  loss_giou_0: 0.9367  loss_ce_dn_0: 0.7351  loss_mask_dn_0: 0.7868  loss_dice_dn_0: 2.636  loss_bbox_dn_0: 0.6006  loss_giou_dn_0: 0.8822  loss_ce_1: 1.307  loss_mask_1: 0.2664  loss_dice_1: 0.809  loss_bbox_1: 0.308  loss_giou_1: 0.7769  loss_ce_dn_1: 0.2371  loss_mask_dn_1: 0.3167  loss_dice_dn_1: 0.921  loss_bbox_dn_1: 0.3549  loss_giou_dn_1: 0.6379  loss_ce_2: 1.173  loss_mask_2: 0.2645  loss_dice_2: 0.7828  loss_bbox_2: 0.2922  loss_giou_2: 0.7234  loss_ce_dn_2: 0.1693  loss_mask_dn_2: 0.287  loss_dice_dn_2: 0.832  loss_bbox_dn_2: 0.2805  loss_giou_dn_2: 0.585  loss_ce_3: 0.9479  loss_mask_3: 0.238  loss_dice_3: 0.7835  loss_bbox_3: 0.301  loss_giou_3: 0.703  loss_ce_dn_3: 0.1296  loss_mask_dn_3: 0.274  loss_dice_dn_3: 0.7987  loss_bbox_dn_3: 0.2465  loss_giou_dn_3: 0.5471  loss_ce_4: 0.8842  loss_mask_4: 0.2476  loss_dice_4: 0.7448  loss_bbox_4: 0.2908  loss_giou_4: 0.6709  loss_ce_dn_4: 0.1162  loss_mask_dn_4: 0.2758  loss_dice_dn_4: 0.7776  loss_bbox_dn_4: 0.219  loss_giou_dn_4: 0.5206  loss_ce_5: 0.7989  loss_mask_5: 0.2436  loss_dice_5: 0.7512  loss_bbox_5: 0.3029  loss_giou_5: 0.655  loss_ce_dn_5: 0.09998  loss_mask_dn_5: 0.2697  loss_dice_dn_5: 0.7601  loss_bbox_dn_5: 0.214  loss_giou_dn_5: 0.5163  loss_ce_6: 0.7528  loss_mask_6: 0.2436  loss_dice_6: 0.7559  loss_bbox_6: 0.2911  loss_giou_6: 0.6514  loss_ce_dn_6: 0.1048  loss_mask_dn_6: 0.2656  loss_dice_dn_6: 0.7526  loss_bbox_dn_6: 0.2134  loss_giou_dn_6: 0.5071  loss_ce_7: 0.7357  loss_mask_7: 0.236  loss_dice_7: 0.7649  loss_bbox_7: 0.2876  loss_giou_7: 0.6367  loss_ce_dn_7: 0.1044  loss_mask_dn_7: 0.2619  loss_dice_dn_7: 0.7675  loss_bbox_dn_7: 0.2136  loss_giou_dn_7: 0.5008  loss_ce_8: 0.7266  loss_mask_8: 0.2323  loss_dice_8: 0.7577  loss_bbox_8: 0.2875  loss_giou_8: 0.6446  loss_ce_dn_8: 0.1023  loss_mask_dn_8: 0.2625  loss_dice_dn_8: 0.7698  loss_bbox_dn_8: 0.2128  loss_giou_dn_8: 0.498  loss_ce_interm: 1.331  loss_mask_interm: 0.2489  loss_dice_interm: 0.8299  loss_bbox_interm: 0.4257  loss_giou_interm: 0.93    time: 0.8716  last_time: 0.8582  data_time: 0.0126  last_data_time: 0.0097   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:36:31 d2.utils.events]: \u001b[0m eta: 3 days, 16:58:19  iter: 499  total_loss: 58.54  loss_ce: 0.6992  loss_mask: 0.1918  loss_dice: 0.8198  loss_bbox: 0.2131  loss_giou: 0.7118  loss_ce_dn: 0.1125  loss_mask_dn: 0.1967  loss_dice_dn: 0.7658  loss_bbox_dn: 0.224  loss_giou_dn: 0.5045  loss_ce_0: 1.355  loss_mask_0: 0.1909  loss_dice_0: 0.9062  loss_bbox_0: 0.3662  loss_giou_0: 0.9369  loss_ce_dn_0: 0.7214  loss_mask_dn_0: 0.6736  loss_dice_dn_0: 3.299  loss_bbox_dn_0: 0.5022  loss_giou_dn_0: 0.9277  loss_ce_1: 1.276  loss_mask_1: 0.1822  loss_dice_1: 0.8597  loss_bbox_1: 0.2952  loss_giou_1: 0.7856  loss_ce_dn_1: 0.227  loss_mask_dn_1: 0.2216  loss_dice_dn_1: 0.9062  loss_bbox_dn_1: 0.3242  loss_giou_dn_1: 0.6689  loss_ce_2: 1.073  loss_mask_2: 0.1804  loss_dice_2: 0.8472  loss_bbox_2: 0.2649  loss_giou_2: 0.7375  loss_ce_dn_2: 0.1733  loss_mask_dn_2: 0.2071  loss_dice_dn_2: 0.8232  loss_bbox_dn_2: 0.2772  loss_giou_dn_2: 0.5835  loss_ce_3: 0.9504  loss_mask_3: 0.1864  loss_dice_3: 0.861  loss_bbox_3: 0.2621  loss_giou_3: 0.7342  loss_ce_dn_3: 0.1511  loss_mask_dn_3: 0.2067  loss_dice_dn_3: 0.8132  loss_bbox_dn_3: 0.245  loss_giou_dn_3: 0.5494  loss_ce_4: 0.8649  loss_mask_4: 0.1818  loss_dice_4: 0.8336  loss_bbox_4: 0.2584  loss_giou_4: 0.7423  loss_ce_dn_4: 0.134  loss_mask_dn_4: 0.2093  loss_dice_dn_4: 0.8041  loss_bbox_dn_4: 0.2318  loss_giou_dn_4: 0.5285  loss_ce_5: 0.7581  loss_mask_5: 0.1934  loss_dice_5: 0.8335  loss_bbox_5: 0.2391  loss_giou_5: 0.7112  loss_ce_dn_5: 0.1215  loss_mask_dn_5: 0.2075  loss_dice_dn_5: 0.779  loss_bbox_dn_5: 0.2298  loss_giou_dn_5: 0.5215  loss_ce_6: 0.7304  loss_mask_6: 0.1828  loss_dice_6: 0.8672  loss_bbox_6: 0.2305  loss_giou_6: 0.7022  loss_ce_dn_6: 0.1153  loss_mask_dn_6: 0.2049  loss_dice_dn_6: 0.7997  loss_bbox_dn_6: 0.2271  loss_giou_dn_6: 0.5104  loss_ce_7: 0.7098  loss_mask_7: 0.1976  loss_dice_7: 0.9239  loss_bbox_7: 0.2235  loss_giou_7: 0.7019  loss_ce_dn_7: 0.1095  loss_mask_dn_7: 0.203  loss_dice_dn_7: 0.7846  loss_bbox_dn_7: 0.2239  loss_giou_dn_7: 0.506  loss_ce_8: 0.7012  loss_mask_8: 0.1918  loss_dice_8: 0.8385  loss_bbox_8: 0.2247  loss_giou_8: 0.7395  loss_ce_dn_8: 0.11  loss_mask_dn_8: 0.1993  loss_dice_dn_8: 0.7732  loss_bbox_dn_8: 0.223  loss_giou_dn_8: 0.5051  loss_ce_interm: 1.355  loss_mask_interm: 0.1931  loss_dice_interm: 0.914  loss_bbox_interm: 0.3659  loss_giou_interm: 0.9369    time: 0.8720  last_time: 0.8509  data_time: 0.0140  last_data_time: 0.0166   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:36:49 d2.utils.events]: \u001b[0m eta: 3 days, 16:58:10  iter: 519  total_loss: 64.47  loss_ce: 0.7859  loss_mask: 0.2216  loss_dice: 0.7352  loss_bbox: 0.2725  loss_giou: 0.8202  loss_ce_dn: 0.1137  loss_mask_dn: 0.2342  loss_dice_dn: 0.804  loss_bbox_dn: 0.2102  loss_giou_dn: 0.6481  loss_ce_0: 1.484  loss_mask_0: 0.2618  loss_dice_0: 0.8186  loss_bbox_0: 0.3957  loss_giou_0: 1.002  loss_ce_dn_0: 0.7171  loss_mask_dn_0: 0.8185  loss_dice_dn_0: 3.055  loss_bbox_dn_0: 0.5871  loss_giou_dn_0: 0.9535  loss_ce_1: 1.334  loss_mask_1: 0.2427  loss_dice_1: 0.7788  loss_bbox_1: 0.3074  loss_giou_1: 0.8442  loss_ce_dn_1: 0.2326  loss_mask_dn_1: 0.2766  loss_dice_dn_1: 0.9034  loss_bbox_dn_1: 0.3395  loss_giou_dn_1: 0.7362  loss_ce_2: 1.183  loss_mask_2: 0.2256  loss_dice_2: 0.7996  loss_bbox_2: 0.2863  loss_giou_2: 0.825  loss_ce_dn_2: 0.1848  loss_mask_dn_2: 0.248  loss_dice_dn_2: 0.8994  loss_bbox_dn_2: 0.2819  loss_giou_dn_2: 0.6929  loss_ce_3: 1.003  loss_mask_3: 0.2383  loss_dice_3: 0.7605  loss_bbox_3: 0.2715  loss_giou_3: 0.8292  loss_ce_dn_3: 0.1489  loss_mask_dn_3: 0.2455  loss_dice_dn_3: 0.8269  loss_bbox_dn_3: 0.2577  loss_giou_dn_3: 0.6731  loss_ce_4: 0.9428  loss_mask_4: 0.2327  loss_dice_4: 0.7476  loss_bbox_4: 0.2657  loss_giou_4: 0.8317  loss_ce_dn_4: 0.1353  loss_mask_dn_4: 0.2318  loss_dice_dn_4: 0.8128  loss_bbox_dn_4: 0.2291  loss_giou_dn_4: 0.6555  loss_ce_5: 0.8392  loss_mask_5: 0.2255  loss_dice_5: 0.7353  loss_bbox_5: 0.2798  loss_giou_5: 0.8229  loss_ce_dn_5: 0.1275  loss_mask_dn_5: 0.239  loss_dice_dn_5: 0.8158  loss_bbox_dn_5: 0.2094  loss_giou_dn_5: 0.6549  loss_ce_6: 0.8203  loss_mask_6: 0.2255  loss_dice_6: 0.7345  loss_bbox_6: 0.2732  loss_giou_6: 0.8163  loss_ce_dn_6: 0.1262  loss_mask_dn_6: 0.2396  loss_dice_dn_6: 0.8301  loss_bbox_dn_6: 0.2059  loss_giou_dn_6: 0.6532  loss_ce_7: 0.8302  loss_mask_7: 0.2191  loss_dice_7: 0.7993  loss_bbox_7: 0.2669  loss_giou_7: 0.8048  loss_ce_dn_7: 0.1175  loss_mask_dn_7: 0.237  loss_dice_dn_7: 0.8318  loss_bbox_dn_7: 0.2077  loss_giou_dn_7: 0.6528  loss_ce_8: 0.8208  loss_mask_8: 0.2108  loss_dice_8: 0.7716  loss_bbox_8: 0.2679  loss_giou_8: 0.8267  loss_ce_dn_8: 0.1178  loss_mask_dn_8: 0.2314  loss_dice_dn_8: 0.7958  loss_bbox_dn_8: 0.2091  loss_giou_dn_8: 0.6512  loss_ce_interm: 1.464  loss_mask_interm: 0.267  loss_dice_interm: 0.7994  loss_bbox_interm: 0.4018  loss_giou_interm: 0.9981    time: 0.8721  last_time: 0.8618  data_time: 0.0119  last_data_time: 0.0115   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:37:06 d2.utils.events]: \u001b[0m eta: 3 days, 16:57:59  iter: 539  total_loss: 55.14  loss_ce: 0.7466  loss_mask: 0.2355  loss_dice: 0.5923  loss_bbox: 0.221  loss_giou: 0.5832  loss_ce_dn: 0.08854  loss_mask_dn: 0.2074  loss_dice_dn: 0.6419  loss_bbox_dn: 0.2117  loss_giou_dn: 0.4659  loss_ce_0: 1.513  loss_mask_0: 0.2394  loss_dice_0: 0.632  loss_bbox_0: 0.3733  loss_giou_0: 0.8283  loss_ce_dn_0: 0.6795  loss_mask_dn_0: 0.8203  loss_dice_dn_0: 2.732  loss_bbox_dn_0: 0.6289  loss_giou_dn_0: 0.8723  loss_ce_1: 1.335  loss_mask_1: 0.2293  loss_dice_1: 0.6067  loss_bbox_1: 0.284  loss_giou_1: 0.6747  loss_ce_dn_1: 0.2295  loss_mask_dn_1: 0.2742  loss_dice_dn_1: 0.7489  loss_bbox_dn_1: 0.3297  loss_giou_dn_1: 0.6121  loss_ce_2: 1.124  loss_mask_2: 0.2397  loss_dice_2: 0.6104  loss_bbox_2: 0.2625  loss_giou_2: 0.631  loss_ce_dn_2: 0.1809  loss_mask_dn_2: 0.236  loss_dice_dn_2: 0.7084  loss_bbox_dn_2: 0.2695  loss_giou_dn_2: 0.5457  loss_ce_3: 0.9037  loss_mask_3: 0.2383  loss_dice_3: 0.6092  loss_bbox_3: 0.2652  loss_giou_3: 0.6089  loss_ce_dn_3: 0.1452  loss_mask_dn_3: 0.229  loss_dice_dn_3: 0.6965  loss_bbox_dn_3: 0.2402  loss_giou_dn_3: 0.5174  loss_ce_4: 0.9212  loss_mask_4: 0.2189  loss_dice_4: 0.6125  loss_bbox_4: 0.2552  loss_giou_4: 0.6046  loss_ce_dn_4: 0.1275  loss_mask_dn_4: 0.2288  loss_dice_dn_4: 0.6761  loss_bbox_dn_4: 0.2259  loss_giou_dn_4: 0.4848  loss_ce_5: 0.8335  loss_mask_5: 0.222  loss_dice_5: 0.6402  loss_bbox_5: 0.2427  loss_giou_5: 0.6108  loss_ce_dn_5: 0.1159  loss_mask_dn_5: 0.2125  loss_dice_dn_5: 0.667  loss_bbox_dn_5: 0.2264  loss_giou_dn_5: 0.477  loss_ce_6: 0.8023  loss_mask_6: 0.2294  loss_dice_6: 0.6059  loss_bbox_6: 0.233  loss_giou_6: 0.5893  loss_ce_dn_6: 0.1063  loss_mask_dn_6: 0.212  loss_dice_dn_6: 0.6572  loss_bbox_dn_6: 0.219  loss_giou_dn_6: 0.4679  loss_ce_7: 0.7456  loss_mask_7: 0.234  loss_dice_7: 0.6373  loss_bbox_7: 0.2216  loss_giou_7: 0.5985  loss_ce_dn_7: 0.09077  loss_mask_dn_7: 0.2117  loss_dice_dn_7: 0.6416  loss_bbox_dn_7: 0.215  loss_giou_dn_7: 0.466  loss_ce_8: 0.745  loss_mask_8: 0.2294  loss_dice_8: 0.6071  loss_bbox_8: 0.2193  loss_giou_8: 0.5847  loss_ce_dn_8: 0.09229  loss_mask_dn_8: 0.2112  loss_dice_dn_8: 0.6483  loss_bbox_dn_8: 0.2112  loss_giou_dn_8: 0.466  loss_ce_interm: 1.494  loss_mask_interm: 0.2309  loss_dice_interm: 0.6133  loss_bbox_interm: 0.3733  loss_giou_interm: 0.8354    time: 0.8720  last_time: 0.8451  data_time: 0.0113  last_data_time: 0.0112   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:37:24 d2.utils.events]: \u001b[0m eta: 3 days, 16:57:42  iter: 559  total_loss: 58.81  loss_ce: 0.6847  loss_mask: 0.2761  loss_dice: 0.7215  loss_bbox: 0.2732  loss_giou: 0.6481  loss_ce_dn: 0.1205  loss_mask_dn: 0.3121  loss_dice_dn: 0.6516  loss_bbox_dn: 0.2441  loss_giou_dn: 0.4936  loss_ce_0: 1.434  loss_mask_0: 0.3456  loss_dice_0: 0.7931  loss_bbox_0: 0.4369  loss_giou_0: 0.8579  loss_ce_dn_0: 0.7508  loss_mask_dn_0: 0.921  loss_dice_dn_0: 2.987  loss_bbox_dn_0: 0.6963  loss_giou_dn_0: 0.8839  loss_ce_1: 1.269  loss_mask_1: 0.3377  loss_dice_1: 0.7254  loss_bbox_1: 0.3506  loss_giou_1: 0.7458  loss_ce_dn_1: 0.2774  loss_mask_dn_1: 0.391  loss_dice_dn_1: 0.813  loss_bbox_dn_1: 0.408  loss_giou_dn_1: 0.6624  loss_ce_2: 1.055  loss_mask_2: 0.3058  loss_dice_2: 0.7486  loss_bbox_2: 0.2811  loss_giou_2: 0.6883  loss_ce_dn_2: 0.2103  loss_mask_dn_2: 0.3551  loss_dice_dn_2: 0.7567  loss_bbox_dn_2: 0.3246  loss_giou_dn_2: 0.5666  loss_ce_3: 0.9076  loss_mask_3: 0.2949  loss_dice_3: 0.6869  loss_bbox_3: 0.3028  loss_giou_3: 0.6833  loss_ce_dn_3: 0.1749  loss_mask_dn_3: 0.3251  loss_dice_dn_3: 0.7056  loss_bbox_dn_3: 0.2896  loss_giou_dn_3: 0.5437  loss_ce_4: 0.8093  loss_mask_4: 0.2818  loss_dice_4: 0.6793  loss_bbox_4: 0.2942  loss_giou_4: 0.6608  loss_ce_dn_4: 0.1607  loss_mask_dn_4: 0.3227  loss_dice_dn_4: 0.6826  loss_bbox_dn_4: 0.266  loss_giou_dn_4: 0.5187  loss_ce_5: 0.7733  loss_mask_5: 0.2914  loss_dice_5: 0.7084  loss_bbox_5: 0.2682  loss_giou_5: 0.6394  loss_ce_dn_5: 0.1468  loss_mask_dn_5: 0.3228  loss_dice_dn_5: 0.6756  loss_bbox_dn_5: 0.2558  loss_giou_dn_5: 0.5075  loss_ce_6: 0.7174  loss_mask_6: 0.2848  loss_dice_6: 0.6668  loss_bbox_6: 0.2636  loss_giou_6: 0.6468  loss_ce_dn_6: 0.1444  loss_mask_dn_6: 0.3192  loss_dice_dn_6: 0.674  loss_bbox_dn_6: 0.2513  loss_giou_dn_6: 0.4953  loss_ce_7: 0.6964  loss_mask_7: 0.2885  loss_dice_7: 0.729  loss_bbox_7: 0.2719  loss_giou_7: 0.6448  loss_ce_dn_7: 0.1317  loss_mask_dn_7: 0.3154  loss_dice_dn_7: 0.6802  loss_bbox_dn_7: 0.2471  loss_giou_dn_7: 0.4944  loss_ce_8: 0.6792  loss_mask_8: 0.2773  loss_dice_8: 0.6985  loss_bbox_8: 0.2631  loss_giou_8: 0.6425  loss_ce_dn_8: 0.1267  loss_mask_dn_8: 0.314  loss_dice_dn_8: 0.6649  loss_bbox_dn_8: 0.2462  loss_giou_dn_8: 0.4947  loss_ce_interm: 1.422  loss_mask_interm: 0.3639  loss_dice_interm: 0.7632  loss_bbox_interm: 0.4369  loss_giou_interm: 0.8626    time: 0.8720  last_time: 0.8643  data_time: 0.0126  last_data_time: 0.0122   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:37:41 d2.utils.events]: \u001b[0m eta: 3 days, 16:58:21  iter: 579  total_loss: 61.8  loss_ce: 0.8048  loss_mask: 0.2581  loss_dice: 0.7212  loss_bbox: 0.263  loss_giou: 0.7106  loss_ce_dn: 0.1365  loss_mask_dn: 0.2476  loss_dice_dn: 0.6958  loss_bbox_dn: 0.2149  loss_giou_dn: 0.555  loss_ce_0: 1.491  loss_mask_0: 0.2673  loss_dice_0: 0.7797  loss_bbox_0: 0.3555  loss_giou_0: 0.9897  loss_ce_dn_0: 0.7546  loss_mask_dn_0: 0.7679  loss_dice_dn_0: 2.947  loss_bbox_dn_0: 0.5402  loss_giou_dn_0: 0.9293  loss_ce_1: 1.341  loss_mask_1: 0.2712  loss_dice_1: 0.7369  loss_bbox_1: 0.2722  loss_giou_1: 0.7558  loss_ce_dn_1: 0.2819  loss_mask_dn_1: 0.3106  loss_dice_dn_1: 0.7784  loss_bbox_dn_1: 0.3048  loss_giou_dn_1: 0.6649  loss_ce_2: 1.24  loss_mask_2: 0.2368  loss_dice_2: 0.7028  loss_bbox_2: 0.2716  loss_giou_2: 0.7477  loss_ce_dn_2: 0.2267  loss_mask_dn_2: 0.2795  loss_dice_dn_2: 0.7493  loss_bbox_dn_2: 0.2663  loss_giou_dn_2: 0.6071  loss_ce_3: 1.017  loss_mask_3: 0.2429  loss_dice_3: 0.689  loss_bbox_3: 0.2801  loss_giou_3: 0.7268  loss_ce_dn_3: 0.1815  loss_mask_dn_3: 0.2722  loss_dice_dn_3: 0.7264  loss_bbox_dn_3: 0.2405  loss_giou_dn_3: 0.5753  loss_ce_4: 0.9595  loss_mask_4: 0.2419  loss_dice_4: 0.6739  loss_bbox_4: 0.2839  loss_giou_4: 0.7363  loss_ce_dn_4: 0.1707  loss_mask_dn_4: 0.2681  loss_dice_dn_4: 0.7306  loss_bbox_dn_4: 0.2326  loss_giou_dn_4: 0.5689  loss_ce_5: 0.8894  loss_mask_5: 0.2428  loss_dice_5: 0.6649  loss_bbox_5: 0.2825  loss_giou_5: 0.7329  loss_ce_dn_5: 0.1545  loss_mask_dn_5: 0.2541  loss_dice_dn_5: 0.7041  loss_bbox_dn_5: 0.2308  loss_giou_dn_5: 0.5683  loss_ce_6: 0.8642  loss_mask_6: 0.2452  loss_dice_6: 0.7114  loss_bbox_6: 0.2716  loss_giou_6: 0.7262  loss_ce_dn_6: 0.1417  loss_mask_dn_6: 0.2538  loss_dice_dn_6: 0.7047  loss_bbox_dn_6: 0.2192  loss_giou_dn_6: 0.5635  loss_ce_7: 0.8612  loss_mask_7: 0.2599  loss_dice_7: 0.7386  loss_bbox_7: 0.2751  loss_giou_7: 0.721  loss_ce_dn_7: 0.1407  loss_mask_dn_7: 0.258  loss_dice_dn_7: 0.6763  loss_bbox_dn_7: 0.2162  loss_giou_dn_7: 0.5612  loss_ce_8: 0.8267  loss_mask_8: 0.2533  loss_dice_8: 0.7264  loss_bbox_8: 0.2632  loss_giou_8: 0.7115  loss_ce_dn_8: 0.1427  loss_mask_dn_8: 0.2494  loss_dice_dn_8: 0.6933  loss_bbox_dn_8: 0.2157  loss_giou_dn_8: 0.5563  loss_ce_interm: 1.513  loss_mask_interm: 0.2714  loss_dice_interm: 0.7431  loss_bbox_interm: 0.3574  loss_giou_interm: 0.9626    time: 0.8720  last_time: 0.8495  data_time: 0.0120  last_data_time: 0.0124   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:37:58 d2.utils.events]: \u001b[0m eta: 3 days, 16:56:32  iter: 599  total_loss: 59.27  loss_ce: 0.682  loss_mask: 0.2558  loss_dice: 0.6984  loss_bbox: 0.2649  loss_giou: 0.635  loss_ce_dn: 0.09466  loss_mask_dn: 0.2512  loss_dice_dn: 0.7402  loss_bbox_dn: 0.2696  loss_giou_dn: 0.5288  loss_ce_0: 1.336  loss_mask_0: 0.2398  loss_dice_0: 0.7377  loss_bbox_0: 0.4127  loss_giou_0: 0.8947  loss_ce_dn_0: 0.7  loss_mask_dn_0: 0.7398  loss_dice_dn_0: 2.983  loss_bbox_dn_0: 0.6561  loss_giou_dn_0: 0.9762  loss_ce_1: 1.345  loss_mask_1: 0.2621  loss_dice_1: 0.8724  loss_bbox_1: 0.2975  loss_giou_1: 0.7757  loss_ce_dn_1: 0.2228  loss_mask_dn_1: 0.2792  loss_dice_dn_1: 0.8876  loss_bbox_dn_1: 0.394  loss_giou_dn_1: 0.7044  loss_ce_2: 1.187  loss_mask_2: 0.2412  loss_dice_2: 0.8575  loss_bbox_2: 0.284  loss_giou_2: 0.7116  loss_ce_dn_2: 0.1668  loss_mask_dn_2: 0.2555  loss_dice_dn_2: 0.8228  loss_bbox_dn_2: 0.3382  loss_giou_dn_2: 0.6194  loss_ce_3: 0.9971  loss_mask_3: 0.2717  loss_dice_3: 0.7515  loss_bbox_3: 0.2734  loss_giou_3: 0.6826  loss_ce_dn_3: 0.1376  loss_mask_dn_3: 0.2475  loss_dice_dn_3: 0.7681  loss_bbox_dn_3: 0.3109  loss_giou_dn_3: 0.5858  loss_ce_4: 0.8798  loss_mask_4: 0.2717  loss_dice_4: 0.7385  loss_bbox_4: 0.257  loss_giou_4: 0.6598  loss_ce_dn_4: 0.1229  loss_mask_dn_4: 0.2416  loss_dice_dn_4: 0.7642  loss_bbox_dn_4: 0.2863  loss_giou_dn_4: 0.5626  loss_ce_5: 0.8026  loss_mask_5: 0.279  loss_dice_5: 0.7219  loss_bbox_5: 0.2711  loss_giou_5: 0.6778  loss_ce_dn_5: 0.1209  loss_mask_dn_5: 0.2478  loss_dice_dn_5: 0.7432  loss_bbox_dn_5: 0.2845  loss_giou_dn_5: 0.5557  loss_ce_6: 0.7165  loss_mask_6: 0.2651  loss_dice_6: 0.7539  loss_bbox_6: 0.268  loss_giou_6: 0.6499  loss_ce_dn_6: 0.1061  loss_mask_dn_6: 0.2478  loss_dice_dn_6: 0.7349  loss_bbox_dn_6: 0.2732  loss_giou_dn_6: 0.541  loss_ce_7: 0.7043  loss_mask_7: 0.2451  loss_dice_7: 0.74  loss_bbox_7: 0.2644  loss_giou_7: 0.6496  loss_ce_dn_7: 0.1036  loss_mask_dn_7: 0.2548  loss_dice_dn_7: 0.742  loss_bbox_dn_7: 0.2733  loss_giou_dn_7: 0.5414  loss_ce_8: 0.682  loss_mask_8: 0.2468  loss_dice_8: 0.7351  loss_bbox_8: 0.2569  loss_giou_8: 0.6613  loss_ce_dn_8: 0.09656  loss_mask_dn_8: 0.2519  loss_dice_dn_8: 0.7388  loss_bbox_dn_8: 0.2708  loss_giou_dn_8: 0.5346  loss_ce_interm: 1.312  loss_mask_interm: 0.2411  loss_dice_interm: 0.7367  loss_bbox_interm: 0.4117  loss_giou_interm: 0.8792    time: 0.8717  last_time: 0.8684  data_time: 0.0111  last_data_time: 0.0083   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:38:16 d2.utils.events]: \u001b[0m eta: 3 days, 16:56:35  iter: 619  total_loss: 56.93  loss_ce: 0.8645  loss_mask: 0.175  loss_dice: 0.8822  loss_bbox: 0.2134  loss_giou: 0.7516  loss_ce_dn: 0.1539  loss_mask_dn: 0.157  loss_dice_dn: 0.8492  loss_bbox_dn: 0.1652  loss_giou_dn: 0.5183  loss_ce_0: 1.385  loss_mask_0: 0.2034  loss_dice_0: 0.8648  loss_bbox_0: 0.3146  loss_giou_0: 0.8933  loss_ce_dn_0: 0.743  loss_mask_dn_0: 0.6803  loss_dice_dn_0: 2.843  loss_bbox_dn_0: 0.4577  loss_giou_dn_0: 0.9114  loss_ce_1: 1.353  loss_mask_1: 0.1994  loss_dice_1: 0.8493  loss_bbox_1: 0.241  loss_giou_1: 0.7821  loss_ce_dn_1: 0.2764  loss_mask_dn_1: 0.2158  loss_dice_dn_1: 0.9828  loss_bbox_dn_1: 0.2474  loss_giou_dn_1: 0.6481  loss_ce_2: 1.173  loss_mask_2: 0.1718  loss_dice_2: 0.8925  loss_bbox_2: 0.2249  loss_giou_2: 0.7521  loss_ce_dn_2: 0.2205  loss_mask_dn_2: 0.2025  loss_dice_dn_2: 0.9169  loss_bbox_dn_2: 0.1949  loss_giou_dn_2: 0.5668  loss_ce_3: 1.024  loss_mask_3: 0.1738  loss_dice_3: 0.8415  loss_bbox_3: 0.239  loss_giou_3: 0.764  loss_ce_dn_3: 0.1977  loss_mask_dn_3: 0.1779  loss_dice_dn_3: 0.8743  loss_bbox_dn_3: 0.1635  loss_giou_dn_3: 0.5504  loss_ce_4: 0.9707  loss_mask_4: 0.1535  loss_dice_4: 0.8552  loss_bbox_4: 0.2119  loss_giou_4: 0.7543  loss_ce_dn_4: 0.1798  loss_mask_dn_4: 0.1712  loss_dice_dn_4: 0.8623  loss_bbox_dn_4: 0.1602  loss_giou_dn_4: 0.5399  loss_ce_5: 0.897  loss_mask_5: 0.175  loss_dice_5: 0.9385  loss_bbox_5: 0.2186  loss_giou_5: 0.7504  loss_ce_dn_5: 0.1705  loss_mask_dn_5: 0.161  loss_dice_dn_5: 0.8483  loss_bbox_dn_5: 0.1609  loss_giou_dn_5: 0.5296  loss_ce_6: 0.8906  loss_mask_6: 0.1763  loss_dice_6: 0.874  loss_bbox_6: 0.2087  loss_giou_6: 0.7607  loss_ce_dn_6: 0.1669  loss_mask_dn_6: 0.1612  loss_dice_dn_6: 0.8412  loss_bbox_dn_6: 0.1687  loss_giou_dn_6: 0.5238  loss_ce_7: 0.8793  loss_mask_7: 0.1763  loss_dice_7: 0.8809  loss_bbox_7: 0.2265  loss_giou_7: 0.7195  loss_ce_dn_7: 0.1612  loss_mask_dn_7: 0.1575  loss_dice_dn_7: 0.847  loss_bbox_dn_7: 0.1667  loss_giou_dn_7: 0.524  loss_ce_8: 0.8618  loss_mask_8: 0.1851  loss_dice_8: 0.9187  loss_bbox_8: 0.2277  loss_giou_8: 0.7487  loss_ce_dn_8: 0.156  loss_mask_dn_8: 0.1606  loss_dice_dn_8: 0.8462  loss_bbox_dn_8: 0.1661  loss_giou_dn_8: 0.5216  loss_ce_interm: 1.39  loss_mask_interm: 0.2111  loss_dice_interm: 0.8688  loss_bbox_interm: 0.3143  loss_giou_interm: 0.8937    time: 0.8717  last_time: 0.8768  data_time: 0.0121  last_data_time: 0.0110   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:38:34 d2.utils.events]: \u001b[0m eta: 3 days, 16:57:28  iter: 639  total_loss: 65.49  loss_ce: 0.8162  loss_mask: 0.2941  loss_dice: 0.7708  loss_bbox: 0.3196  loss_giou: 0.6746  loss_ce_dn: 0.153  loss_mask_dn: 0.3066  loss_dice_dn: 0.7681  loss_bbox_dn: 0.298  loss_giou_dn: 0.5862  loss_ce_0: 1.505  loss_mask_0: 0.3203  loss_dice_0: 0.8593  loss_bbox_0: 0.4409  loss_giou_0: 0.8383  loss_ce_dn_0: 0.7354  loss_mask_dn_0: 0.8677  loss_dice_dn_0: 3.074  loss_bbox_dn_0: 0.6422  loss_giou_dn_0: 0.9092  loss_ce_1: 1.362  loss_mask_1: 0.3072  loss_dice_1: 0.8752  loss_bbox_1: 0.3509  loss_giou_1: 0.7265  loss_ce_dn_1: 0.2603  loss_mask_dn_1: 0.3524  loss_dice_dn_1: 0.8583  loss_bbox_dn_1: 0.3892  loss_giou_dn_1: 0.6921  loss_ce_2: 1.238  loss_mask_2: 0.3227  loss_dice_2: 0.8296  loss_bbox_2: 0.3063  loss_giou_2: 0.7281  loss_ce_dn_2: 0.2125  loss_mask_dn_2: 0.333  loss_dice_dn_2: 0.8276  loss_bbox_dn_2: 0.3473  loss_giou_dn_2: 0.6403  loss_ce_3: 1.052  loss_mask_3: 0.3184  loss_dice_3: 0.8034  loss_bbox_3: 0.2981  loss_giou_3: 0.7088  loss_ce_dn_3: 0.1958  loss_mask_dn_3: 0.3065  loss_dice_dn_3: 0.7972  loss_bbox_dn_3: 0.3268  loss_giou_dn_3: 0.6136  loss_ce_4: 0.9806  loss_mask_4: 0.317  loss_dice_4: 0.8386  loss_bbox_4: 0.3109  loss_giou_4: 0.6874  loss_ce_dn_4: 0.1869  loss_mask_dn_4: 0.3032  loss_dice_dn_4: 0.8008  loss_bbox_dn_4: 0.3091  loss_giou_dn_4: 0.6052  loss_ce_5: 0.9029  loss_mask_5: 0.2978  loss_dice_5: 0.8156  loss_bbox_5: 0.3151  loss_giou_5: 0.6904  loss_ce_dn_5: 0.1719  loss_mask_dn_5: 0.3052  loss_dice_dn_5: 0.7742  loss_bbox_dn_5: 0.3065  loss_giou_dn_5: 0.5986  loss_ce_6: 0.8692  loss_mask_6: 0.2968  loss_dice_6: 0.7884  loss_bbox_6: 0.3292  loss_giou_6: 0.6796  loss_ce_dn_6: 0.1631  loss_mask_dn_6: 0.2982  loss_dice_dn_6: 0.7679  loss_bbox_dn_6: 0.3058  loss_giou_dn_6: 0.5933  loss_ce_7: 0.8459  loss_mask_7: 0.3012  loss_dice_7: 0.7792  loss_bbox_7: 0.3163  loss_giou_7: 0.6795  loss_ce_dn_7: 0.1619  loss_mask_dn_7: 0.3034  loss_dice_dn_7: 0.7628  loss_bbox_dn_7: 0.3025  loss_giou_dn_7: 0.5889  loss_ce_8: 0.8244  loss_mask_8: 0.2903  loss_dice_8: 0.7881  loss_bbox_8: 0.314  loss_giou_8: 0.6749  loss_ce_dn_8: 0.1539  loss_mask_dn_8: 0.3042  loss_dice_dn_8: 0.7556  loss_bbox_dn_8: 0.2984  loss_giou_dn_8: 0.5878  loss_ce_interm: 1.505  loss_mask_interm: 0.3263  loss_dice_interm: 0.7877  loss_bbox_interm: 0.4409  loss_giou_interm: 0.8405    time: 0.8719  last_time: 0.8555  data_time: 0.0125  last_data_time: 0.0113   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:38:51 d2.utils.events]: \u001b[0m eta: 3 days, 16:57:44  iter: 659  total_loss: 61  loss_ce: 0.8937  loss_mask: 0.2135  loss_dice: 0.6123  loss_bbox: 0.2284  loss_giou: 0.7156  loss_ce_dn: 0.168  loss_mask_dn: 0.2228  loss_dice_dn: 0.6531  loss_bbox_dn: 0.1832  loss_giou_dn: 0.6308  loss_ce_0: 1.479  loss_mask_0: 0.2336  loss_dice_0: 0.6852  loss_bbox_0: 0.3564  loss_giou_0: 0.9811  loss_ce_dn_0: 0.697  loss_mask_dn_0: 0.8323  loss_dice_dn_0: 3.01  loss_bbox_dn_0: 0.5555  loss_giou_dn_0: 0.9737  loss_ce_1: 1.417  loss_mask_1: 0.2374  loss_dice_1: 0.678  loss_bbox_1: 0.2403  loss_giou_1: 0.8455  loss_ce_dn_1: 0.3013  loss_mask_dn_1: 0.2699  loss_dice_dn_1: 0.7786  loss_bbox_dn_1: 0.3303  loss_giou_dn_1: 0.7814  loss_ce_2: 1.234  loss_mask_2: 0.2373  loss_dice_2: 0.6979  loss_bbox_2: 0.2261  loss_giou_2: 0.7789  loss_ce_dn_2: 0.2458  loss_mask_dn_2: 0.2457  loss_dice_dn_2: 0.7091  loss_bbox_dn_2: 0.2541  loss_giou_dn_2: 0.7117  loss_ce_3: 1.198  loss_mask_3: 0.229  loss_dice_3: 0.6849  loss_bbox_3: 0.2153  loss_giou_3: 0.7731  loss_ce_dn_3: 0.2128  loss_mask_dn_3: 0.2246  loss_dice_dn_3: 0.65  loss_bbox_dn_3: 0.2241  loss_giou_dn_3: 0.6782  loss_ce_4: 1.1  loss_mask_4: 0.2408  loss_dice_4: 0.6513  loss_bbox_4: 0.2203  loss_giou_4: 0.7516  loss_ce_dn_4: 0.2056  loss_mask_dn_4: 0.2121  loss_dice_dn_4: 0.6599  loss_bbox_dn_4: 0.2027  loss_giou_dn_4: 0.6489  loss_ce_5: 1.005  loss_mask_5: 0.233  loss_dice_5: 0.6398  loss_bbox_5: 0.2294  loss_giou_5: 0.76  loss_ce_dn_5: 0.1842  loss_mask_dn_5: 0.2213  loss_dice_dn_5: 0.6374  loss_bbox_dn_5: 0.1951  loss_giou_dn_5: 0.647  loss_ce_6: 0.9535  loss_mask_6: 0.2282  loss_dice_6: 0.6268  loss_bbox_6: 0.2221  loss_giou_6: 0.7065  loss_ce_dn_6: 0.1781  loss_mask_dn_6: 0.2234  loss_dice_dn_6: 0.6471  loss_bbox_dn_6: 0.1881  loss_giou_dn_6: 0.6321  loss_ce_7: 0.9176  loss_mask_7: 0.2274  loss_dice_7: 0.6265  loss_bbox_7: 0.2242  loss_giou_7: 0.7447  loss_ce_dn_7: 0.1714  loss_mask_dn_7: 0.2205  loss_dice_dn_7: 0.6544  loss_bbox_dn_7: 0.1858  loss_giou_dn_7: 0.6347  loss_ce_8: 0.8907  loss_mask_8: 0.2167  loss_dice_8: 0.602  loss_bbox_8: 0.218  loss_giou_8: 0.7444  loss_ce_dn_8: 0.1657  loss_mask_dn_8: 0.2202  loss_dice_dn_8: 0.6442  loss_bbox_dn_8: 0.183  loss_giou_dn_8: 0.6304  loss_ce_interm: 1.439  loss_mask_interm: 0.2275  loss_dice_interm: 0.6802  loss_bbox_interm: 0.3772  loss_giou_interm: 0.9964    time: 0.8721  last_time: 0.8766  data_time: 0.0124  last_data_time: 0.0143   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:39:09 d2.utils.events]: \u001b[0m eta: 3 days, 16:57:59  iter: 679  total_loss: 60.86  loss_ce: 0.9016  loss_mask: 0.2499  loss_dice: 0.6087  loss_bbox: 0.2314  loss_giou: 0.6833  loss_ce_dn: 0.1418  loss_mask_dn: 0.2678  loss_dice_dn: 0.6016  loss_bbox_dn: 0.254  loss_giou_dn: 0.6003  loss_ce_0: 1.439  loss_mask_0: 0.2429  loss_dice_0: 0.6186  loss_bbox_0: 0.3666  loss_giou_0: 0.9062  loss_ce_dn_0: 0.689  loss_mask_dn_0: 0.9505  loss_dice_dn_0: 3.01  loss_bbox_dn_0: 0.6741  loss_giou_dn_0: 0.9682  loss_ce_1: 1.398  loss_mask_1: 0.2518  loss_dice_1: 0.5891  loss_bbox_1: 0.2562  loss_giou_1: 0.8083  loss_ce_dn_1: 0.2647  loss_mask_dn_1: 0.2843  loss_dice_dn_1: 0.6951  loss_bbox_dn_1: 0.3813  loss_giou_dn_1: 0.7215  loss_ce_2: 1.259  loss_mask_2: 0.2616  loss_dice_2: 0.6616  loss_bbox_2: 0.2473  loss_giou_2: 0.7222  loss_ce_dn_2: 0.2082  loss_mask_dn_2: 0.2518  loss_dice_dn_2: 0.6859  loss_bbox_dn_2: 0.326  loss_giou_dn_2: 0.6727  loss_ce_3: 1.13  loss_mask_3: 0.2584  loss_dice_3: 0.6011  loss_bbox_3: 0.2226  loss_giou_3: 0.7287  loss_ce_dn_3: 0.1829  loss_mask_dn_3: 0.2597  loss_dice_dn_3: 0.6179  loss_bbox_dn_3: 0.293  loss_giou_dn_3: 0.6467  loss_ce_4: 1.03  loss_mask_4: 0.2597  loss_dice_4: 0.5969  loss_bbox_4: 0.2368  loss_giou_4: 0.747  loss_ce_dn_4: 0.1749  loss_mask_dn_4: 0.2572  loss_dice_dn_4: 0.6325  loss_bbox_dn_4: 0.2782  loss_giou_dn_4: 0.6233  loss_ce_5: 1.011  loss_mask_5: 0.2489  loss_dice_5: 0.605  loss_bbox_5: 0.239  loss_giou_5: 0.7086  loss_ce_dn_5: 0.1575  loss_mask_dn_5: 0.2564  loss_dice_dn_5: 0.607  loss_bbox_dn_5: 0.2694  loss_giou_dn_5: 0.6146  loss_ce_6: 0.98  loss_mask_6: 0.252  loss_dice_6: 0.5988  loss_bbox_6: 0.2338  loss_giou_6: 0.6792  loss_ce_dn_6: 0.1533  loss_mask_dn_6: 0.2551  loss_dice_dn_6: 0.6032  loss_bbox_dn_6: 0.2541  loss_giou_dn_6: 0.6052  loss_ce_7: 0.9268  loss_mask_7: 0.248  loss_dice_7: 0.6007  loss_bbox_7: 0.2281  loss_giou_7: 0.6789  loss_ce_dn_7: 0.1498  loss_mask_dn_7: 0.2545  loss_dice_dn_7: 0.5932  loss_bbox_dn_7: 0.2563  loss_giou_dn_7: 0.6032  loss_ce_8: 0.9247  loss_mask_8: 0.2487  loss_dice_8: 0.6157  loss_bbox_8: 0.2312  loss_giou_8: 0.7031  loss_ce_dn_8: 0.1478  loss_mask_dn_8: 0.2601  loss_dice_dn_8: 0.5929  loss_bbox_dn_8: 0.2553  loss_giou_dn_8: 0.5994  loss_ce_interm: 1.475  loss_mask_interm: 0.2451  loss_dice_interm: 0.6864  loss_bbox_interm: 0.3591  loss_giou_interm: 0.9025    time: 0.8723  last_time: 0.8899  data_time: 0.0122  last_data_time: 0.0099   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:39:26 d2.utils.events]: \u001b[0m eta: 3 days, 16:57:29  iter: 699  total_loss: 62.07  loss_ce: 0.7153  loss_mask: 0.2952  loss_dice: 0.6295  loss_bbox: 0.2719  loss_giou: 0.6404  loss_ce_dn: 0.1531  loss_mask_dn: 0.2992  loss_dice_dn: 0.5856  loss_bbox_dn: 0.1998  loss_giou_dn: 0.5904  loss_ce_0: 1.435  loss_mask_0: 0.3369  loss_dice_0: 0.6744  loss_bbox_0: 0.4121  loss_giou_0: 0.816  loss_ce_dn_0: 0.7079  loss_mask_dn_0: 0.9502  loss_dice_dn_0: 2.97  loss_bbox_dn_0: 0.6406  loss_giou_dn_0: 1.017  loss_ce_1: 1.437  loss_mask_1: 0.3073  loss_dice_1: 0.6488  loss_bbox_1: 0.3163  loss_giou_1: 0.6684  loss_ce_dn_1: 0.2515  loss_mask_dn_1: 0.3796  loss_dice_dn_1: 0.7413  loss_bbox_dn_1: 0.3311  loss_giou_dn_1: 0.7508  loss_ce_2: 1.215  loss_mask_2: 0.2927  loss_dice_2: 0.6653  loss_bbox_2: 0.2828  loss_giou_2: 0.6734  loss_ce_dn_2: 0.2157  loss_mask_dn_2: 0.3293  loss_dice_dn_2: 0.6499  loss_bbox_dn_2: 0.2787  loss_giou_dn_2: 0.6815  loss_ce_3: 1.041  loss_mask_3: 0.29  loss_dice_3: 0.6548  loss_bbox_3: 0.2596  loss_giou_3: 0.6478  loss_ce_dn_3: 0.1891  loss_mask_dn_3: 0.3231  loss_dice_dn_3: 0.6269  loss_bbox_dn_3: 0.2476  loss_giou_dn_3: 0.6372  loss_ce_4: 0.9223  loss_mask_4: 0.2901  loss_dice_4: 0.6398  loss_bbox_4: 0.2947  loss_giou_4: 0.6527  loss_ce_dn_4: 0.1801  loss_mask_dn_4: 0.3125  loss_dice_dn_4: 0.5952  loss_bbox_dn_4: 0.2241  loss_giou_dn_4: 0.6176  loss_ce_5: 0.8467  loss_mask_5: 0.305  loss_dice_5: 0.6306  loss_bbox_5: 0.2731  loss_giou_5: 0.6488  loss_ce_dn_5: 0.1675  loss_mask_dn_5: 0.3123  loss_dice_dn_5: 0.6047  loss_bbox_dn_5: 0.2187  loss_giou_dn_5: 0.6132  loss_ce_6: 0.7765  loss_mask_6: 0.2878  loss_dice_6: 0.648  loss_bbox_6: 0.2656  loss_giou_6: 0.6507  loss_ce_dn_6: 0.165  loss_mask_dn_6: 0.3042  loss_dice_dn_6: 0.59  loss_bbox_dn_6: 0.2046  loss_giou_dn_6: 0.5992  loss_ce_7: 0.7875  loss_mask_7: 0.2825  loss_dice_7: 0.6135  loss_bbox_7: 0.2729  loss_giou_7: 0.6412  loss_ce_dn_7: 0.1684  loss_mask_dn_7: 0.2948  loss_dice_dn_7: 0.6077  loss_bbox_dn_7: 0.2037  loss_giou_dn_7: 0.5967  loss_ce_8: 0.7242  loss_mask_8: 0.2891  loss_dice_8: 0.6773  loss_bbox_8: 0.2753  loss_giou_8: 0.652  loss_ce_dn_8: 0.1583  loss_mask_dn_8: 0.2967  loss_dice_dn_8: 0.574  loss_bbox_dn_8: 0.2012  loss_giou_dn_8: 0.5915  loss_ce_interm: 1.489  loss_mask_interm: 0.3178  loss_dice_interm: 0.6802  loss_bbox_interm: 0.4141  loss_giou_interm: 0.8257    time: 0.8723  last_time: 0.8745  data_time: 0.0115  last_data_time: 0.0090   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:39:44 d2.utils.events]: \u001b[0m eta: 3 days, 16:59:08  iter: 719  total_loss: 68.19  loss_ce: 0.8939  loss_mask: 0.2105  loss_dice: 0.7446  loss_bbox: 0.2523  loss_giou: 0.8807  loss_ce_dn: 0.1811  loss_mask_dn: 0.2585  loss_dice_dn: 0.7281  loss_bbox_dn: 0.221  loss_giou_dn: 0.6777  loss_ce_0: 1.521  loss_mask_0: 0.2429  loss_dice_0: 0.7209  loss_bbox_0: 0.3608  loss_giou_0: 1.072  loss_ce_dn_0: 0.7054  loss_mask_dn_0: 0.9052  loss_dice_dn_0: 3.025  loss_bbox_dn_0: 0.6454  loss_giou_dn_0: 1.054  loss_ce_1: 1.381  loss_mask_1: 0.2354  loss_dice_1: 0.7577  loss_bbox_1: 0.2771  loss_giou_1: 1.001  loss_ce_dn_1: 0.2867  loss_mask_dn_1: 0.2957  loss_dice_dn_1: 0.7677  loss_bbox_dn_1: 0.3645  loss_giou_dn_1: 0.8145  loss_ce_2: 1.284  loss_mask_2: 0.2186  loss_dice_2: 0.7231  loss_bbox_2: 0.2594  loss_giou_2: 0.9421  loss_ce_dn_2: 0.2308  loss_mask_dn_2: 0.2764  loss_dice_dn_2: 0.7299  loss_bbox_dn_2: 0.298  loss_giou_dn_2: 0.7405  loss_ce_3: 1.141  loss_mask_3: 0.2157  loss_dice_3: 0.7295  loss_bbox_3: 0.2744  loss_giou_3: 0.9101  loss_ce_dn_3: 0.2075  loss_mask_dn_3: 0.2629  loss_dice_dn_3: 0.7162  loss_bbox_dn_3: 0.269  loss_giou_dn_3: 0.7162  loss_ce_4: 1.083  loss_mask_4: 0.2115  loss_dice_4: 0.7046  loss_bbox_4: 0.2764  loss_giou_4: 0.873  loss_ce_dn_4: 0.2032  loss_mask_dn_4: 0.2711  loss_dice_dn_4: 0.7597  loss_bbox_dn_4: 0.246  loss_giou_dn_4: 0.702  loss_ce_5: 0.9434  loss_mask_5: 0.2034  loss_dice_5: 0.7421  loss_bbox_5: 0.2704  loss_giou_5: 0.8791  loss_ce_dn_5: 0.1989  loss_mask_dn_5: 0.2625  loss_dice_dn_5: 0.6872  loss_bbox_dn_5: 0.238  loss_giou_dn_5: 0.6968  loss_ce_6: 0.9745  loss_mask_6: 0.2115  loss_dice_6: 0.7186  loss_bbox_6: 0.2558  loss_giou_6: 0.8594  loss_ce_dn_6: 0.1929  loss_mask_dn_6: 0.2564  loss_dice_dn_6: 0.7045  loss_bbox_dn_6: 0.2191  loss_giou_dn_6: 0.6849  loss_ce_7: 0.8946  loss_mask_7: 0.2191  loss_dice_7: 0.7378  loss_bbox_7: 0.254  loss_giou_7: 0.8798  loss_ce_dn_7: 0.188  loss_mask_dn_7: 0.2616  loss_dice_dn_7: 0.7147  loss_bbox_dn_7: 0.2185  loss_giou_dn_7: 0.6826  loss_ce_8: 0.8656  loss_mask_8: 0.2147  loss_dice_8: 0.7212  loss_bbox_8: 0.2566  loss_giou_8: 0.8571  loss_ce_dn_8: 0.1865  loss_mask_dn_8: 0.2581  loss_dice_dn_8: 0.7268  loss_bbox_dn_8: 0.2205  loss_giou_dn_8: 0.6787  loss_ce_interm: 1.518  loss_mask_interm: 0.2373  loss_dice_interm: 0.7767  loss_bbox_interm: 0.3578  loss_giou_interm: 1.075    time: 0.8724  last_time: 0.8711  data_time: 0.0130  last_data_time: 0.0088   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:40:01 d2.utils.events]: \u001b[0m eta: 3 days, 16:57:33  iter: 739  total_loss: 60.72  loss_ce: 0.8562  loss_mask: 0.3052  loss_dice: 0.7761  loss_bbox: 0.208  loss_giou: 0.8216  loss_ce_dn: 0.1426  loss_mask_dn: 0.2866  loss_dice_dn: 0.8029  loss_bbox_dn: 0.1939  loss_giou_dn: 0.6668  loss_ce_0: 1.476  loss_mask_0: 0.3137  loss_dice_0: 0.9544  loss_bbox_0: 0.3731  loss_giou_0: 0.9761  loss_ce_dn_0: 0.7373  loss_mask_dn_0: 0.7473  loss_dice_dn_0: 3.157  loss_bbox_dn_0: 0.5776  loss_giou_dn_0: 1.005  loss_ce_1: 1.384  loss_mask_1: 0.2999  loss_dice_1: 0.879  loss_bbox_1: 0.2799  loss_giou_1: 0.908  loss_ce_dn_1: 0.2672  loss_mask_dn_1: 0.36  loss_dice_dn_1: 0.8896  loss_bbox_dn_1: 0.347  loss_giou_dn_1: 0.7776  loss_ce_2: 1.196  loss_mask_2: 0.3054  loss_dice_2: 0.8495  loss_bbox_2: 0.2212  loss_giou_2: 0.8792  loss_ce_dn_2: 0.2113  loss_mask_dn_2: 0.3007  loss_dice_dn_2: 0.8139  loss_bbox_dn_2: 0.2657  loss_giou_dn_2: 0.7155  loss_ce_3: 1.088  loss_mask_3: 0.3031  loss_dice_3: 0.7624  loss_bbox_3: 0.2114  loss_giou_3: 0.8648  loss_ce_dn_3: 0.1948  loss_mask_dn_3: 0.299  loss_dice_dn_3: 0.7869  loss_bbox_dn_3: 0.2475  loss_giou_dn_3: 0.7011  loss_ce_4: 0.9475  loss_mask_4: 0.2871  loss_dice_4: 0.8374  loss_bbox_4: 0.2054  loss_giou_4: 0.8389  loss_ce_dn_4: 0.1828  loss_mask_dn_4: 0.2903  loss_dice_dn_4: 0.7771  loss_bbox_dn_4: 0.2066  loss_giou_dn_4: 0.6753  loss_ce_5: 0.8338  loss_mask_5: 0.299  loss_dice_5: 0.7397  loss_bbox_5: 0.2057  loss_giou_5: 0.8718  loss_ce_dn_5: 0.1645  loss_mask_dn_5: 0.2863  loss_dice_dn_5: 0.793  loss_bbox_dn_5: 0.2089  loss_giou_dn_5: 0.6727  loss_ce_6: 0.9316  loss_mask_6: 0.3075  loss_dice_6: 0.8076  loss_bbox_6: 0.2086  loss_giou_6: 0.8667  loss_ce_dn_6: 0.1471  loss_mask_dn_6: 0.2841  loss_dice_dn_6: 0.7855  loss_bbox_dn_6: 0.1982  loss_giou_dn_6: 0.673  loss_ce_7: 0.9609  loss_mask_7: 0.307  loss_dice_7: 0.7349  loss_bbox_7: 0.2111  loss_giou_7: 0.8333  loss_ce_dn_7: 0.1461  loss_mask_dn_7: 0.2837  loss_dice_dn_7: 0.8076  loss_bbox_dn_7: 0.1969  loss_giou_dn_7: 0.6686  loss_ce_8: 0.8773  loss_mask_8: 0.3088  loss_dice_8: 0.8027  loss_bbox_8: 0.1943  loss_giou_8: 0.816  loss_ce_dn_8: 0.1433  loss_mask_dn_8: 0.2865  loss_dice_dn_8: 0.7451  loss_bbox_dn_8: 0.1935  loss_giou_dn_8: 0.6661  loss_ce_interm: 1.499  loss_mask_interm: 0.3088  loss_dice_interm: 0.952  loss_bbox_interm: 0.3908  loss_giou_interm: 0.9652    time: 0.8723  last_time: 0.8741  data_time: 0.0115  last_data_time: 0.0099   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:40:19 d2.utils.events]: \u001b[0m eta: 3 days, 16:57:46  iter: 759  total_loss: 61.87  loss_ce: 0.6934  loss_mask: 0.2765  loss_dice: 0.7581  loss_bbox: 0.292  loss_giou: 0.7264  loss_ce_dn: 0.118  loss_mask_dn: 0.31  loss_dice_dn: 0.7455  loss_bbox_dn: 0.2517  loss_giou_dn: 0.6415  loss_ce_0: 1.498  loss_mask_0: 0.2766  loss_dice_0: 0.7397  loss_bbox_0: 0.3923  loss_giou_0: 0.8712  loss_ce_dn_0: 0.6703  loss_mask_dn_0: 0.9001  loss_dice_dn_0: 3.065  loss_bbox_dn_0: 0.6564  loss_giou_dn_0: 1.003  loss_ce_1: 1.315  loss_mask_1: 0.2698  loss_dice_1: 0.7586  loss_bbox_1: 0.3355  loss_giou_1: 0.7635  loss_ce_dn_1: 0.2471  loss_mask_dn_1: 0.3485  loss_dice_dn_1: 0.9133  loss_bbox_dn_1: 0.3685  loss_giou_dn_1: 0.7839  loss_ce_2: 1.119  loss_mask_2: 0.2998  loss_dice_2: 0.7584  loss_bbox_2: 0.2791  loss_giou_2: 0.7374  loss_ce_dn_2: 0.2089  loss_mask_dn_2: 0.3291  loss_dice_dn_2: 0.8016  loss_bbox_dn_2: 0.3099  loss_giou_dn_2: 0.7153  loss_ce_3: 0.9569  loss_mask_3: 0.2941  loss_dice_3: 0.7456  loss_bbox_3: 0.2943  loss_giou_3: 0.7533  loss_ce_dn_3: 0.169  loss_mask_dn_3: 0.3127  loss_dice_dn_3: 0.7868  loss_bbox_dn_3: 0.282  loss_giou_dn_3: 0.6878  loss_ce_4: 0.8756  loss_mask_4: 0.2702  loss_dice_4: 0.7496  loss_bbox_4: 0.2924  loss_giou_4: 0.7565  loss_ce_dn_4: 0.1554  loss_mask_dn_4: 0.3113  loss_dice_dn_4: 0.7665  loss_bbox_dn_4: 0.272  loss_giou_dn_4: 0.6705  loss_ce_5: 0.7685  loss_mask_5: 0.2664  loss_dice_5: 0.726  loss_bbox_5: 0.2876  loss_giou_5: 0.7484  loss_ce_dn_5: 0.1432  loss_mask_dn_5: 0.3139  loss_dice_dn_5: 0.7568  loss_bbox_dn_5: 0.2628  loss_giou_dn_5: 0.6635  loss_ce_6: 0.7565  loss_mask_6: 0.2624  loss_dice_6: 0.6938  loss_bbox_6: 0.2755  loss_giou_6: 0.7319  loss_ce_dn_6: 0.1278  loss_mask_dn_6: 0.3092  loss_dice_dn_6: 0.768  loss_bbox_dn_6: 0.2553  loss_giou_dn_6: 0.6529  loss_ce_7: 0.7086  loss_mask_7: 0.2719  loss_dice_7: 0.715  loss_bbox_7: 0.2966  loss_giou_7: 0.7426  loss_ce_dn_7: 0.1247  loss_mask_dn_7: 0.3119  loss_dice_dn_7: 0.7508  loss_bbox_dn_7: 0.2525  loss_giou_dn_7: 0.6465  loss_ce_8: 0.7064  loss_mask_8: 0.2746  loss_dice_8: 0.7352  loss_bbox_8: 0.295  loss_giou_8: 0.7375  loss_ce_dn_8: 0.1194  loss_mask_dn_8: 0.3146  loss_dice_dn_8: 0.7452  loss_bbox_dn_8: 0.251  loss_giou_dn_8: 0.6421  loss_ce_interm: 1.494  loss_mask_interm: 0.2816  loss_dice_interm: 0.7492  loss_bbox_interm: 0.3915  loss_giou_interm: 0.8689    time: 0.8723  last_time: 0.8679  data_time: 0.0131  last_data_time: 0.0113   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:40:36 d2.utils.events]: \u001b[0m eta: 3 days, 16:56:32  iter: 779  total_loss: 58.88  loss_ce: 0.6872  loss_mask: 0.2209  loss_dice: 0.6244  loss_bbox: 0.2258  loss_giou: 0.6999  loss_ce_dn: 0.1617  loss_mask_dn: 0.2188  loss_dice_dn: 0.556  loss_bbox_dn: 0.1805  loss_giou_dn: 0.6573  loss_ce_0: 1.434  loss_mask_0: 0.2437  loss_dice_0: 0.6383  loss_bbox_0: 0.3658  loss_giou_0: 0.9188  loss_ce_dn_0: 0.7328  loss_mask_dn_0: 0.9169  loss_dice_dn_0: 2.692  loss_bbox_dn_0: 0.5952  loss_giou_dn_0: 1.036  loss_ce_1: 1.289  loss_mask_1: 0.242  loss_dice_1: 0.6162  loss_bbox_1: 0.2838  loss_giou_1: 0.826  loss_ce_dn_1: 0.2613  loss_mask_dn_1: 0.2849  loss_dice_dn_1: 0.6642  loss_bbox_dn_1: 0.3288  loss_giou_dn_1: 0.7924  loss_ce_2: 1.148  loss_mask_2: 0.2482  loss_dice_2: 0.6136  loss_bbox_2: 0.2299  loss_giou_2: 0.7655  loss_ce_dn_2: 0.2249  loss_mask_dn_2: 0.263  loss_dice_dn_2: 0.5856  loss_bbox_dn_2: 0.2623  loss_giou_dn_2: 0.7278  loss_ce_3: 0.9243  loss_mask_3: 0.2336  loss_dice_3: 0.6239  loss_bbox_3: 0.2376  loss_giou_3: 0.7472  loss_ce_dn_3: 0.1938  loss_mask_dn_3: 0.2232  loss_dice_dn_3: 0.5859  loss_bbox_dn_3: 0.2312  loss_giou_dn_3: 0.6971  loss_ce_4: 0.8427  loss_mask_4: 0.2206  loss_dice_4: 0.6038  loss_bbox_4: 0.2045  loss_giou_4: 0.7192  loss_ce_dn_4: 0.1723  loss_mask_dn_4: 0.2203  loss_dice_dn_4: 0.568  loss_bbox_dn_4: 0.203  loss_giou_dn_4: 0.6686  loss_ce_5: 0.7912  loss_mask_5: 0.2266  loss_dice_5: 0.6632  loss_bbox_5: 0.2139  loss_giou_5: 0.72  loss_ce_dn_5: 0.1621  loss_mask_dn_5: 0.2189  loss_dice_dn_5: 0.5556  loss_bbox_dn_5: 0.1914  loss_giou_dn_5: 0.6726  loss_ce_6: 0.7362  loss_mask_6: 0.2296  loss_dice_6: 0.6249  loss_bbox_6: 0.203  loss_giou_6: 0.7168  loss_ce_dn_6: 0.1495  loss_mask_dn_6: 0.2259  loss_dice_dn_6: 0.5447  loss_bbox_dn_6: 0.186  loss_giou_dn_6: 0.6702  loss_ce_7: 0.7454  loss_mask_7: 0.2219  loss_dice_7: 0.6515  loss_bbox_7: 0.2274  loss_giou_7: 0.7098  loss_ce_dn_7: 0.162  loss_mask_dn_7: 0.2168  loss_dice_dn_7: 0.5203  loss_bbox_dn_7: 0.181  loss_giou_dn_7: 0.6614  loss_ce_8: 0.6892  loss_mask_8: 0.2189  loss_dice_8: 0.6192  loss_bbox_8: 0.2136  loss_giou_8: 0.6979  loss_ce_dn_8: 0.16  loss_mask_dn_8: 0.2172  loss_dice_dn_8: 0.5512  loss_bbox_dn_8: 0.1801  loss_giou_dn_8: 0.6547  loss_ce_interm: 1.464  loss_mask_interm: 0.2419  loss_dice_interm: 0.6646  loss_bbox_interm: 0.3375  loss_giou_interm: 0.935    time: 0.8721  last_time: 0.8735  data_time: 0.0112  last_data_time: 0.0114   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:40:54 d2.utils.events]: \u001b[0m eta: 3 days, 16:56:15  iter: 799  total_loss: 60.2  loss_ce: 0.6099  loss_mask: 0.2844  loss_dice: 0.7671  loss_bbox: 0.273  loss_giou: 0.7132  loss_ce_dn: 0.1261  loss_mask_dn: 0.278  loss_dice_dn: 0.7475  loss_bbox_dn: 0.269  loss_giou_dn: 0.6262  loss_ce_0: 1.536  loss_mask_0: 0.2735  loss_dice_0: 0.7613  loss_bbox_0: 0.4439  loss_giou_0: 0.9058  loss_ce_dn_0: 0.7777  loss_mask_dn_0: 0.9527  loss_dice_dn_0: 3.015  loss_bbox_dn_0: 0.7404  loss_giou_dn_0: 0.9957  loss_ce_1: 1.395  loss_mask_1: 0.2756  loss_dice_1: 0.8082  loss_bbox_1: 0.3232  loss_giou_1: 0.7998  loss_ce_dn_1: 0.2537  loss_mask_dn_1: 0.3412  loss_dice_dn_1: 0.8107  loss_bbox_dn_1: 0.4049  loss_giou_dn_1: 0.7395  loss_ce_2: 1.144  loss_mask_2: 0.2736  loss_dice_2: 0.7768  loss_bbox_2: 0.2657  loss_giou_2: 0.7486  loss_ce_dn_2: 0.1976  loss_mask_dn_2: 0.3195  loss_dice_dn_2: 0.7775  loss_bbox_dn_2: 0.3217  loss_giou_dn_2: 0.6904  loss_ce_3: 0.8622  loss_mask_3: 0.2633  loss_dice_3: 0.7705  loss_bbox_3: 0.2615  loss_giou_3: 0.7296  loss_ce_dn_3: 0.1632  loss_mask_dn_3: 0.3015  loss_dice_dn_3: 0.7371  loss_bbox_dn_3: 0.3154  loss_giou_dn_3: 0.6657  loss_ce_4: 0.8179  loss_mask_4: 0.2828  loss_dice_4: 0.7203  loss_bbox_4: 0.2589  loss_giou_4: 0.7245  loss_ce_dn_4: 0.1513  loss_mask_dn_4: 0.2899  loss_dice_dn_4: 0.7472  loss_bbox_dn_4: 0.2868  loss_giou_dn_4: 0.6474  loss_ce_5: 0.6764  loss_mask_5: 0.2857  loss_dice_5: 0.7683  loss_bbox_5: 0.2675  loss_giou_5: 0.7261  loss_ce_dn_5: 0.1449  loss_mask_dn_5: 0.2838  loss_dice_dn_5: 0.7487  loss_bbox_dn_5: 0.2823  loss_giou_dn_5: 0.6431  loss_ce_6: 0.6353  loss_mask_6: 0.2826  loss_dice_6: 0.7904  loss_bbox_6: 0.272  loss_giou_6: 0.7184  loss_ce_dn_6: 0.1371  loss_mask_dn_6: 0.2767  loss_dice_dn_6: 0.7664  loss_bbox_dn_6: 0.2741  loss_giou_dn_6: 0.6337  loss_ce_7: 0.6315  loss_mask_7: 0.2909  loss_dice_7: 0.7501  loss_bbox_7: 0.2662  loss_giou_7: 0.7157  loss_ce_dn_7: 0.132  loss_mask_dn_7: 0.2794  loss_dice_dn_7: 0.7724  loss_bbox_dn_7: 0.2694  loss_giou_dn_7: 0.6286  loss_ce_8: 0.6106  loss_mask_8: 0.2892  loss_dice_8: 0.7743  loss_bbox_8: 0.2642  loss_giou_8: 0.7117  loss_ce_dn_8: 0.1243  loss_mask_dn_8: 0.2764  loss_dice_dn_8: 0.7394  loss_bbox_dn_8: 0.2682  loss_giou_dn_8: 0.626  loss_ce_interm: 1.533  loss_mask_interm: 0.2743  loss_dice_interm: 0.7551  loss_bbox_interm: 0.4424  loss_giou_interm: 0.8722    time: 0.8721  last_time: 0.8848  data_time: 0.0118  last_data_time: 0.0133   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:41:11 d2.utils.events]: \u001b[0m eta: 3 days, 16:56:53  iter: 819  total_loss: 66.59  loss_ce: 0.8214  loss_mask: 0.295  loss_dice: 0.7676  loss_bbox: 0.3079  loss_giou: 0.7326  loss_ce_dn: 0.1282  loss_mask_dn: 0.2875  loss_dice_dn: 0.7809  loss_bbox_dn: 0.2767  loss_giou_dn: 0.5602  loss_ce_0: 1.57  loss_mask_0: 0.3158  loss_dice_0: 0.8146  loss_bbox_0: 0.3961  loss_giou_0: 0.9172  loss_ce_dn_0: 0.7843  loss_mask_dn_0: 1.003  loss_dice_dn_0: 2.963  loss_bbox_dn_0: 0.7123  loss_giou_dn_0: 0.926  loss_ce_1: 1.483  loss_mask_1: 0.3026  loss_dice_1: 0.7943  loss_bbox_1: 0.2994  loss_giou_1: 0.8166  loss_ce_dn_1: 0.2768  loss_mask_dn_1: 0.3585  loss_dice_dn_1: 0.9016  loss_bbox_dn_1: 0.3938  loss_giou_dn_1: 0.6884  loss_ce_2: 1.269  loss_mask_2: 0.3064  loss_dice_2: 0.8352  loss_bbox_2: 0.3061  loss_giou_2: 0.7851  loss_ce_dn_2: 0.217  loss_mask_dn_2: 0.3262  loss_dice_dn_2: 0.8291  loss_bbox_dn_2: 0.3102  loss_giou_dn_2: 0.621  loss_ce_3: 1.176  loss_mask_3: 0.2995  loss_dice_3: 0.7708  loss_bbox_3: 0.3016  loss_giou_3: 0.7543  loss_ce_dn_3: 0.1875  loss_mask_dn_3: 0.2998  loss_dice_dn_3: 0.8293  loss_bbox_dn_3: 0.3002  loss_giou_dn_3: 0.5907  loss_ce_4: 1.059  loss_mask_4: 0.3032  loss_dice_4: 0.8027  loss_bbox_4: 0.2777  loss_giou_4: 0.7468  loss_ce_dn_4: 0.171  loss_mask_dn_4: 0.2909  loss_dice_dn_4: 0.7998  loss_bbox_dn_4: 0.2859  loss_giou_dn_4: 0.5828  loss_ce_5: 0.9854  loss_mask_5: 0.2975  loss_dice_5: 0.7554  loss_bbox_5: 0.294  loss_giou_5: 0.7435  loss_ce_dn_5: 0.1588  loss_mask_dn_5: 0.2883  loss_dice_dn_5: 0.8042  loss_bbox_dn_5: 0.2875  loss_giou_dn_5: 0.5741  loss_ce_6: 0.9202  loss_mask_6: 0.2978  loss_dice_6: 0.7287  loss_bbox_6: 0.2956  loss_giou_6: 0.766  loss_ce_dn_6: 0.1499  loss_mask_dn_6: 0.2845  loss_dice_dn_6: 0.8075  loss_bbox_dn_6: 0.2844  loss_giou_dn_6: 0.565  loss_ce_7: 0.9343  loss_mask_7: 0.2893  loss_dice_7: 0.7879  loss_bbox_7: 0.2932  loss_giou_7: 0.7421  loss_ce_dn_7: 0.1436  loss_mask_dn_7: 0.2834  loss_dice_dn_7: 0.7974  loss_bbox_dn_7: 0.2806  loss_giou_dn_7: 0.5639  loss_ce_8: 0.9023  loss_mask_8: 0.2993  loss_dice_8: 0.7609  loss_bbox_8: 0.2997  loss_giou_8: 0.7355  loss_ce_dn_8: 0.136  loss_mask_dn_8: 0.286  loss_dice_dn_8: 0.7726  loss_bbox_dn_8: 0.2773  loss_giou_dn_8: 0.5609  loss_ce_interm: 1.526  loss_mask_interm: 0.3084  loss_dice_interm: 0.7992  loss_bbox_interm: 0.3961  loss_giou_interm: 0.9112    time: 0.8723  last_time: 0.8748  data_time: 0.0131  last_data_time: 0.0151   lr: 1.25e-05  max_mem: 11889M\n",
            "\u001b[32m[02/20 07:41:29 d2.utils.events]: \u001b[0m eta: 3 days, 16:56:17  iter: 839  total_loss: 61.25  loss_ce: 0.7945  loss_mask: 0.1961  loss_dice: 0.742  loss_bbox: 0.2559  loss_giou: 0.7982  loss_ce_dn: 0.1563  loss_mask_dn: 0.1748  loss_dice_dn: 0.7555  loss_bbox_dn: 0.2388  loss_giou_dn: 0.6047  loss_ce_0: 1.468  loss_mask_0: 0.1732  loss_dice_0: 0.7933  loss_bbox_0: 0.3881  loss_giou_0: 1  loss_ce_dn_0: 0.7794  loss_mask_dn_0: 0.8232  loss_dice_dn_0: 3.156  loss_bbox_dn_0: 0.5369  loss_giou_dn_0: 0.9768  loss_ce_1: 1.334  loss_mask_1: 0.1743  loss_dice_1: 0.8063  loss_bbox_1: 0.2887  loss_giou_1: 0.912  loss_ce_dn_1: 0.2793  loss_mask_dn_1: 0.2461  loss_dice_dn_1: 0.8726  loss_bbox_dn_1: 0.3422  loss_giou_dn_1: 0.7512  loss_ce_2: 1.178  loss_mask_2: 0.1785  loss_dice_2: 0.7756  loss_bbox_2: 0.2613  loss_giou_2: 0.835  loss_ce_dn_2: 0.2409  loss_mask_dn_2: 0.1887  loss_dice_dn_2: 0.8197  loss_bbox_dn_2: 0.2663  loss_giou_dn_2: 0.6865  loss_ce_3: 1.037  loss_mask_3: 0.1602  loss_dice_3: 0.7807  loss_bbox_3: 0.2569  loss_giou_3: 0.8111  loss_ce_dn_3: 0.1959  loss_mask_dn_3: 0.1813  loss_dice_dn_3: 0.8052  loss_bbox_dn_3: 0.2596  loss_giou_dn_3: 0.6559  loss_ce_4: 0.9794  loss_mask_4: 0.1808  loss_dice_4: 0.7979  loss_bbox_4: 0.2626  loss_giou_4: 0.8049  loss_ce_dn_4: 0.1853  loss_mask_dn_4: 0.1853  loss_dice_dn_4: 0.781  loss_bbox_dn_4: 0.2533  loss_giou_dn_4: 0.625  loss_ce_5: 0.8875  loss_mask_5: 0.1833  loss_dice_5: 0.7452  loss_bbox_5: 0.2744  loss_giou_5: 0.7942  loss_ce_dn_5: 0.1762  loss_mask_dn_5: 0.1754  loss_dice_dn_5: 0.7627  loss_bbox_dn_5: 0.2508  loss_giou_dn_5: 0.6199  loss_ce_6: 0.8524  loss_mask_6: 0.1863  loss_dice_6: 0.7856  loss_bbox_6: 0.2534  loss_giou_6: 0.7931  loss_ce_dn_6: 0.1712  loss_mask_dn_6: 0.1707  loss_dice_dn_6: 0.7519  loss_bbox_dn_6: 0.2443  loss_giou_dn_6: 0.6115  loss_ce_7: 0.8306  loss_mask_7: 0.1932  loss_dice_7: 0.7673  loss_bbox_7: 0.2515  loss_giou_7: 0.8159  loss_ce_dn_7: 0.1656  loss_mask_dn_7: 0.1708  loss_dice_dn_7: 0.7553  loss_bbox_dn_7: 0.241  loss_giou_dn_7: 0.608  loss_ce_8: 0.8144  loss_mask_8: 0.1933  loss_dice_8: 0.7923  loss_bbox_8: 0.2533  loss_giou_8: 0.7844  loss_ce_dn_8: 0.1641  loss_mask_dn_8: 0.1698  loss_dice_dn_8: 0.7489  loss_bbox_dn_8: 0.2389  loss_giou_dn_8: 0.604  loss_ce_interm: 1.458  loss_mask_interm: 0.1652  loss_dice_interm: 0.75  loss_bbox_interm: 0.3909  loss_giou_interm: 1.008    time: 0.8723  last_time: 0.8770  data_time: 0.0120  last_data_time: 0.0137   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:41:46 d2.utils.events]: \u001b[0m eta: 3 days, 16:57:19  iter: 859  total_loss: 57.52  loss_ce: 0.7267  loss_mask: 0.1999  loss_dice: 0.6925  loss_bbox: 0.2365  loss_giou: 0.7351  loss_ce_dn: 0.1099  loss_mask_dn: 0.2066  loss_dice_dn: 0.7498  loss_bbox_dn: 0.2168  loss_giou_dn: 0.581  loss_ce_0: 1.341  loss_mask_0: 0.2185  loss_dice_0: 0.7391  loss_bbox_0: 0.3445  loss_giou_0: 0.8818  loss_ce_dn_0: 0.668  loss_mask_dn_0: 0.7416  loss_dice_dn_0: 2.689  loss_bbox_dn_0: 0.5879  loss_giou_dn_0: 0.9461  loss_ce_1: 1.341  loss_mask_1: 0.2159  loss_dice_1: 0.6901  loss_bbox_1: 0.255  loss_giou_1: 0.7795  loss_ce_dn_1: 0.2375  loss_mask_dn_1: 0.2618  loss_dice_dn_1: 0.856  loss_bbox_dn_1: 0.3225  loss_giou_dn_1: 0.7083  loss_ce_2: 1.172  loss_mask_2: 0.2016  loss_dice_2: 0.6616  loss_bbox_2: 0.2241  loss_giou_2: 0.7363  loss_ce_dn_2: 0.1859  loss_mask_dn_2: 0.2244  loss_dice_dn_2: 0.7577  loss_bbox_dn_2: 0.2731  loss_giou_dn_2: 0.642  loss_ce_3: 0.9567  loss_mask_3: 0.1946  loss_dice_3: 0.6828  loss_bbox_3: 0.2393  loss_giou_3: 0.7506  loss_ce_dn_3: 0.1599  loss_mask_dn_3: 0.2039  loss_dice_dn_3: 0.7245  loss_bbox_dn_3: 0.2529  loss_giou_dn_3: 0.6259  loss_ce_4: 0.8853  loss_mask_4: 0.1838  loss_dice_4: 0.6599  loss_bbox_4: 0.2408  loss_giou_4: 0.7527  loss_ce_dn_4: 0.1476  loss_mask_dn_4: 0.2062  loss_dice_dn_4: 0.7215  loss_bbox_dn_4: 0.2321  loss_giou_dn_4: 0.6044  loss_ce_5: 0.8169  loss_mask_5: 0.1952  loss_dice_5: 0.6991  loss_bbox_5: 0.2317  loss_giou_5: 0.7668  loss_ce_dn_5: 0.1403  loss_mask_dn_5: 0.2017  loss_dice_dn_5: 0.7493  loss_bbox_dn_5: 0.2294  loss_giou_dn_5: 0.6027  loss_ce_6: 0.7385  loss_mask_6: 0.1965  loss_dice_6: 0.7025  loss_bbox_6: 0.2501  loss_giou_6: 0.7497  loss_ce_dn_6: 0.1245  loss_mask_dn_6: 0.2064  loss_dice_dn_6: 0.7501  loss_bbox_dn_6: 0.2223  loss_giou_dn_6: 0.5922  loss_ce_7: 0.7287  loss_mask_7: 0.195  loss_dice_7: 0.7083  loss_bbox_7: 0.2359  loss_giou_7: 0.7391  loss_ce_dn_7: 0.1124  loss_mask_dn_7: 0.1994  loss_dice_dn_7: 0.7401  loss_bbox_dn_7: 0.2175  loss_giou_dn_7: 0.5876  loss_ce_8: 0.7248  loss_mask_8: 0.1953  loss_dice_8: 0.7447  loss_bbox_8: 0.2399  loss_giou_8: 0.7487  loss_ce_dn_8: 0.1083  loss_mask_dn_8: 0.2067  loss_dice_dn_8: 0.7486  loss_bbox_dn_8: 0.2175  loss_giou_dn_8: 0.5842  loss_ce_interm: 1.353  loss_mask_interm: 0.2144  loss_dice_interm: 0.7104  loss_bbox_interm: 0.349  loss_giou_interm: 0.8931    time: 0.8724  last_time: 0.8715  data_time: 0.0129  last_data_time: 0.0138   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:42:04 d2.utils.events]: \u001b[0m eta: 3 days, 16:57:26  iter: 879  total_loss: 57  loss_ce: 0.6981  loss_mask: 0.2341  loss_dice: 0.9284  loss_bbox: 0.2009  loss_giou: 0.6105  loss_ce_dn: 0.1116  loss_mask_dn: 0.2459  loss_dice_dn: 0.962  loss_bbox_dn: 0.2051  loss_giou_dn: 0.5103  loss_ce_0: 1.352  loss_mask_0: 0.2772  loss_dice_0: 1.032  loss_bbox_0: 0.4001  loss_giou_0: 0.8137  loss_ce_dn_0: 0.7388  loss_mask_dn_0: 0.8266  loss_dice_dn_0: 2.93  loss_bbox_dn_0: 0.6548  loss_giou_dn_0: 0.8974  loss_ce_1: 1.421  loss_mask_1: 0.2715  loss_dice_1: 1.042  loss_bbox_1: 0.2836  loss_giou_1: 0.6963  loss_ce_dn_1: 0.2749  loss_mask_dn_1: 0.3056  loss_dice_dn_1: 1.095  loss_bbox_dn_1: 0.3531  loss_giou_dn_1: 0.6541  loss_ce_2: 1.136  loss_mask_2: 0.2757  loss_dice_2: 0.9627  loss_bbox_2: 0.2704  loss_giou_2: 0.6744  loss_ce_dn_2: 0.2081  loss_mask_dn_2: 0.2548  loss_dice_dn_2: 1.017  loss_bbox_dn_2: 0.2895  loss_giou_dn_2: 0.6004  loss_ce_3: 0.9082  loss_mask_3: 0.2263  loss_dice_3: 0.9565  loss_bbox_3: 0.2361  loss_giou_3: 0.6473  loss_ce_dn_3: 0.1706  loss_mask_dn_3: 0.2445  loss_dice_dn_3: 0.9881  loss_bbox_dn_3: 0.2529  loss_giou_dn_3: 0.5782  loss_ce_4: 0.8891  loss_mask_4: 0.2316  loss_dice_4: 0.9143  loss_bbox_4: 0.2096  loss_giou_4: 0.611  loss_ce_dn_4: 0.1483  loss_mask_dn_4: 0.2556  loss_dice_dn_4: 0.9781  loss_bbox_dn_4: 0.2155  loss_giou_dn_4: 0.5383  loss_ce_5: 0.7412  loss_mask_5: 0.2318  loss_dice_5: 0.9564  loss_bbox_5: 0.2154  loss_giou_5: 0.6053  loss_ce_dn_5: 0.1328  loss_mask_dn_5: 0.2621  loss_dice_dn_5: 0.9621  loss_bbox_dn_5: 0.2114  loss_giou_dn_5: 0.5335  loss_ce_6: 0.7452  loss_mask_6: 0.2298  loss_dice_6: 0.9344  loss_bbox_6: 0.2159  loss_giou_6: 0.6035  loss_ce_dn_6: 0.1296  loss_mask_dn_6: 0.255  loss_dice_dn_6: 0.9746  loss_bbox_dn_6: 0.2095  loss_giou_dn_6: 0.5191  loss_ce_7: 0.7178  loss_mask_7: 0.2291  loss_dice_7: 0.9305  loss_bbox_7: 0.2084  loss_giou_7: 0.6133  loss_ce_dn_7: 0.1204  loss_mask_dn_7: 0.25  loss_dice_dn_7: 0.9664  loss_bbox_dn_7: 0.2066  loss_giou_dn_7: 0.5177  loss_ce_8: 0.7219  loss_mask_8: 0.2301  loss_dice_8: 0.9642  loss_bbox_8: 0.203  loss_giou_8: 0.6087  loss_ce_dn_8: 0.1136  loss_mask_dn_8: 0.2436  loss_dice_dn_8: 0.9508  loss_bbox_dn_8: 0.2051  loss_giou_dn_8: 0.5122  loss_ce_interm: 1.347  loss_mask_interm: 0.2702  loss_dice_interm: 1.023  loss_bbox_interm: 0.3998  loss_giou_interm: 0.8164    time: 0.8725  last_time: 0.8756  data_time: 0.0117  last_data_time: 0.0124   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:42:22 d2.utils.events]: \u001b[0m eta: 3 days, 16:58:33  iter: 899  total_loss: 57.2  loss_ce: 0.8016  loss_mask: 0.1978  loss_dice: 0.7329  loss_bbox: 0.2553  loss_giou: 0.6625  loss_ce_dn: 0.1223  loss_mask_dn: 0.1876  loss_dice_dn: 0.7693  loss_bbox_dn: 0.2249  loss_giou_dn: 0.6313  loss_ce_0: 1.48  loss_mask_0: 0.1952  loss_dice_0: 0.768  loss_bbox_0: 0.493  loss_giou_0: 0.9228  loss_ce_dn_0: 0.7544  loss_mask_dn_0: 0.7962  loss_dice_dn_0: 2.874  loss_bbox_dn_0: 0.5494  loss_giou_dn_0: 1.027  loss_ce_1: 1.398  loss_mask_1: 0.1911  loss_dice_1: 0.7559  loss_bbox_1: 0.299  loss_giou_1: 0.7435  loss_ce_dn_1: 0.2628  loss_mask_dn_1: 0.2418  loss_dice_dn_1: 0.884  loss_bbox_dn_1: 0.3098  loss_giou_dn_1: 0.7729  loss_ce_2: 1.205  loss_mask_2: 0.2016  loss_dice_2: 0.7827  loss_bbox_2: 0.2627  loss_giou_2: 0.7153  loss_ce_dn_2: 0.2143  loss_mask_dn_2: 0.2074  loss_dice_dn_2: 0.8188  loss_bbox_dn_2: 0.2646  loss_giou_dn_2: 0.7067  loss_ce_3: 1.097  loss_mask_3: 0.177  loss_dice_3: 0.752  loss_bbox_3: 0.2503  loss_giou_3: 0.6889  loss_ce_dn_3: 0.1778  loss_mask_dn_3: 0.2109  loss_dice_dn_3: 0.7736  loss_bbox_dn_3: 0.2422  loss_giou_dn_3: 0.677  loss_ce_4: 0.9637  loss_mask_4: 0.1889  loss_dice_4: 0.7626  loss_bbox_4: 0.25  loss_giou_4: 0.6983  loss_ce_dn_4: 0.1636  loss_mask_dn_4: 0.2022  loss_dice_dn_4: 0.7592  loss_bbox_dn_4: 0.2298  loss_giou_dn_4: 0.6561  loss_ce_5: 0.8782  loss_mask_5: 0.2203  loss_dice_5: 0.7618  loss_bbox_5: 0.251  loss_giou_5: 0.6892  loss_ce_dn_5: 0.1557  loss_mask_dn_5: 0.2031  loss_dice_dn_5: 0.7478  loss_bbox_dn_5: 0.2262  loss_giou_dn_5: 0.6463  loss_ce_6: 0.8486  loss_mask_6: 0.216  loss_dice_6: 0.7489  loss_bbox_6: 0.2597  loss_giou_6: 0.6792  loss_ce_dn_6: 0.1384  loss_mask_dn_6: 0.1843  loss_dice_dn_6: 0.7463  loss_bbox_dn_6: 0.2292  loss_giou_dn_6: 0.6396  loss_ce_7: 0.8012  loss_mask_7: 0.2075  loss_dice_7: 0.7112  loss_bbox_7: 0.2501  loss_giou_7: 0.6846  loss_ce_dn_7: 0.1288  loss_mask_dn_7: 0.1929  loss_dice_dn_7: 0.733  loss_bbox_dn_7: 0.229  loss_giou_dn_7: 0.6365  loss_ce_8: 0.7623  loss_mask_8: 0.2066  loss_dice_8: 0.7383  loss_bbox_8: 0.2545  loss_giou_8: 0.6813  loss_ce_dn_8: 0.1198  loss_mask_dn_8: 0.1919  loss_dice_dn_8: 0.7584  loss_bbox_dn_8: 0.2248  loss_giou_dn_8: 0.6316  loss_ce_interm: 1.468  loss_mask_interm: 0.194  loss_dice_interm: 0.7512  loss_bbox_interm: 0.4872  loss_giou_interm: 0.9195    time: 0.8726  last_time: 0.8701  data_time: 0.0128  last_data_time: 0.0104   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:42:39 d2.utils.events]: \u001b[0m eta: 3 days, 16:59:54  iter: 919  total_loss: 58.03  loss_ce: 0.6623  loss_mask: 0.2206  loss_dice: 0.6919  loss_bbox: 0.2394  loss_giou: 0.6911  loss_ce_dn: 0.1148  loss_mask_dn: 0.2324  loss_dice_dn: 0.748  loss_bbox_dn: 0.2278  loss_giou_dn: 0.5913  loss_ce_0: 1.367  loss_mask_0: 0.2273  loss_dice_0: 0.7805  loss_bbox_0: 0.3901  loss_giou_0: 0.8882  loss_ce_dn_0: 0.7577  loss_mask_dn_0: 1.051  loss_dice_dn_0: 2.906  loss_bbox_dn_0: 0.6821  loss_giou_dn_0: 0.986  loss_ce_1: 1.237  loss_mask_1: 0.2184  loss_dice_1: 0.7359  loss_bbox_1: 0.2972  loss_giou_1: 0.7746  loss_ce_dn_1: 0.2918  loss_mask_dn_1: 0.2882  loss_dice_dn_1: 0.8672  loss_bbox_dn_1: 0.3835  loss_giou_dn_1: 0.7295  loss_ce_2: 1.127  loss_mask_2: 0.2093  loss_dice_2: 0.6844  loss_bbox_2: 0.2559  loss_giou_2: 0.7385  loss_ce_dn_2: 0.1997  loss_mask_dn_2: 0.2713  loss_dice_dn_2: 0.7998  loss_bbox_dn_2: 0.3047  loss_giou_dn_2: 0.667  loss_ce_3: 0.9485  loss_mask_3: 0.2152  loss_dice_3: 0.7232  loss_bbox_3: 0.2578  loss_giou_3: 0.7084  loss_ce_dn_3: 0.1692  loss_mask_dn_3: 0.239  loss_dice_dn_3: 0.7561  loss_bbox_dn_3: 0.2781  loss_giou_dn_3: 0.6379  loss_ce_4: 0.8234  loss_mask_4: 0.2153  loss_dice_4: 0.8174  loss_bbox_4: 0.2393  loss_giou_4: 0.6984  loss_ce_dn_4: 0.1583  loss_mask_dn_4: 0.2285  loss_dice_dn_4: 0.7596  loss_bbox_dn_4: 0.2456  loss_giou_dn_4: 0.6053  loss_ce_5: 0.7135  loss_mask_5: 0.2076  loss_dice_5: 0.742  loss_bbox_5: 0.2313  loss_giou_5: 0.6974  loss_ce_dn_5: 0.138  loss_mask_dn_5: 0.2354  loss_dice_dn_5: 0.7395  loss_bbox_dn_5: 0.2371  loss_giou_dn_5: 0.5983  loss_ce_6: 0.7554  loss_mask_6: 0.2088  loss_dice_6: 0.7343  loss_bbox_6: 0.253  loss_giou_6: 0.705  loss_ce_dn_6: 0.1258  loss_mask_dn_6: 0.2334  loss_dice_dn_6: 0.745  loss_bbox_dn_6: 0.2284  loss_giou_dn_6: 0.5902  loss_ce_7: 0.7122  loss_mask_7: 0.2109  loss_dice_7: 0.7938  loss_bbox_7: 0.2311  loss_giou_7: 0.703  loss_ce_dn_7: 0.1205  loss_mask_dn_7: 0.2275  loss_dice_dn_7: 0.7438  loss_bbox_dn_7: 0.2275  loss_giou_dn_7: 0.5937  loss_ce_8: 0.6677  loss_mask_8: 0.2132  loss_dice_8: 0.6992  loss_bbox_8: 0.2228  loss_giou_8: 0.6947  loss_ce_dn_8: 0.116  loss_mask_dn_8: 0.231  loss_dice_dn_8: 0.7469  loss_bbox_dn_8: 0.2307  loss_giou_dn_8: 0.592  loss_ce_interm: 1.376  loss_mask_interm: 0.2282  loss_dice_interm: 0.7109  loss_bbox_interm: 0.4152  loss_giou_interm: 0.8849    time: 0.8726  last_time: 0.8851  data_time: 0.0118  last_data_time: 0.0127   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:42:57 d2.utils.events]: \u001b[0m eta: 3 days, 17:00:26  iter: 939  total_loss: 66.27  loss_ce: 0.7948  loss_mask: 0.229  loss_dice: 0.6754  loss_bbox: 0.3188  loss_giou: 0.9872  loss_ce_dn: 0.1545  loss_mask_dn: 0.237  loss_dice_dn: 0.5764  loss_bbox_dn: 0.214  loss_giou_dn: 0.7888  loss_ce_0: 1.395  loss_mask_0: 0.2343  loss_dice_0: 0.6363  loss_bbox_0: 0.4097  loss_giou_0: 1.107  loss_ce_dn_0: 0.7378  loss_mask_dn_0: 0.8839  loss_dice_dn_0: 3.395  loss_bbox_dn_0: 0.511  loss_giou_dn_0: 1.114  loss_ce_1: 1.292  loss_mask_1: 0.2414  loss_dice_1: 0.6825  loss_bbox_1: 0.3056  loss_giou_1: 1.04  loss_ce_dn_1: 0.2743  loss_mask_dn_1: 0.3011  loss_dice_dn_1: 0.7958  loss_bbox_dn_1: 0.3099  loss_giou_dn_1: 0.9007  loss_ce_2: 1.146  loss_mask_2: 0.233  loss_dice_2: 0.7284  loss_bbox_2: 0.301  loss_giou_2: 1.009  loss_ce_dn_2: 0.2107  loss_mask_dn_2: 0.2549  loss_dice_dn_2: 0.6627  loss_bbox_dn_2: 0.2673  loss_giou_dn_2: 0.852  loss_ce_3: 1.034  loss_mask_3: 0.2417  loss_dice_3: 0.6444  loss_bbox_3: 0.2747  loss_giou_3: 0.9968  loss_ce_dn_3: 0.1842  loss_mask_dn_3: 0.2534  loss_dice_dn_3: 0.6314  loss_bbox_dn_3: 0.2313  loss_giou_dn_3: 0.8215  loss_ce_4: 0.976  loss_mask_4: 0.2259  loss_dice_4: 0.6697  loss_bbox_4: 0.2815  loss_giou_4: 0.9884  loss_ce_dn_4: 0.1734  loss_mask_dn_4: 0.2449  loss_dice_dn_4: 0.5965  loss_bbox_dn_4: 0.2184  loss_giou_dn_4: 0.7985  loss_ce_5: 0.8945  loss_mask_5: 0.2292  loss_dice_5: 0.6618  loss_bbox_5: 0.2777  loss_giou_5: 0.9944  loss_ce_dn_5: 0.1641  loss_mask_dn_5: 0.2429  loss_dice_dn_5: 0.587  loss_bbox_dn_5: 0.2159  loss_giou_dn_5: 0.7964  loss_ce_6: 0.8319  loss_mask_6: 0.2316  loss_dice_6: 0.6804  loss_bbox_6: 0.2926  loss_giou_6: 0.9987  loss_ce_dn_6: 0.1615  loss_mask_dn_6: 0.2449  loss_dice_dn_6: 0.569  loss_bbox_dn_6: 0.2118  loss_giou_dn_6: 0.7916  loss_ce_7: 0.8211  loss_mask_7: 0.2228  loss_dice_7: 0.6473  loss_bbox_7: 0.3008  loss_giou_7: 0.9925  loss_ce_dn_7: 0.1566  loss_mask_dn_7: 0.2379  loss_dice_dn_7: 0.5661  loss_bbox_dn_7: 0.2136  loss_giou_dn_7: 0.7898  loss_ce_8: 0.7848  loss_mask_8: 0.2277  loss_dice_8: 0.7209  loss_bbox_8: 0.298  loss_giou_8: 0.9834  loss_ce_dn_8: 0.1548  loss_mask_dn_8: 0.2378  loss_dice_dn_8: 0.5615  loss_bbox_dn_8: 0.2146  loss_giou_dn_8: 0.7874  loss_ce_interm: 1.39  loss_mask_interm: 0.2374  loss_dice_interm: 0.6392  loss_bbox_interm: 0.4097  loss_giou_interm: 1.1    time: 0.8727  last_time: 0.8990  data_time: 0.0133  last_data_time: 0.0097   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:43:14 d2.utils.events]: \u001b[0m eta: 3 days, 17:00:14  iter: 959  total_loss: 55.12  loss_ce: 0.6191  loss_mask: 0.2102  loss_dice: 0.4986  loss_bbox: 0.2349  loss_giou: 0.6869  loss_ce_dn: 0.1142  loss_mask_dn: 0.2222  loss_dice_dn: 0.5658  loss_bbox_dn: 0.2119  loss_giou_dn: 0.615  loss_ce_0: 1.38  loss_mask_0: 0.2274  loss_dice_0: 0.5034  loss_bbox_0: 0.3897  loss_giou_0: 0.9071  loss_ce_dn_0: 0.7638  loss_mask_dn_0: 0.9425  loss_dice_dn_0: 2.681  loss_bbox_dn_0: 0.6905  loss_giou_dn_0: 0.9975  loss_ce_1: 1.265  loss_mask_1: 0.214  loss_dice_1: 0.4808  loss_bbox_1: 0.2976  loss_giou_1: 0.7704  loss_ce_dn_1: 0.257  loss_mask_dn_1: 0.2831  loss_dice_dn_1: 0.6882  loss_bbox_dn_1: 0.3429  loss_giou_dn_1: 0.7568  loss_ce_2: 1.08  loss_mask_2: 0.2203  loss_dice_2: 0.5659  loss_bbox_2: 0.265  loss_giou_2: 0.7408  loss_ce_dn_2: 0.2048  loss_mask_dn_2: 0.233  loss_dice_dn_2: 0.6218  loss_bbox_dn_2: 0.2674  loss_giou_dn_2: 0.6838  loss_ce_3: 0.8943  loss_mask_3: 0.2033  loss_dice_3: 0.5529  loss_bbox_3: 0.2595  loss_giou_3: 0.7428  loss_ce_dn_3: 0.1695  loss_mask_dn_3: 0.2302  loss_dice_dn_3: 0.5949  loss_bbox_dn_3: 0.2474  loss_giou_dn_3: 0.6556  loss_ce_4: 0.8775  loss_mask_4: 0.204  loss_dice_4: 0.5339  loss_bbox_4: 0.2541  loss_giou_4: 0.6658  loss_ce_dn_4: 0.1541  loss_mask_dn_4: 0.2216  loss_dice_dn_4: 0.581  loss_bbox_dn_4: 0.2276  loss_giou_dn_4: 0.6364  loss_ce_5: 0.7458  loss_mask_5: 0.2103  loss_dice_5: 0.5484  loss_bbox_5: 0.2515  loss_giou_5: 0.7029  loss_ce_dn_5: 0.137  loss_mask_dn_5: 0.2162  loss_dice_dn_5: 0.5407  loss_bbox_dn_5: 0.2212  loss_giou_dn_5: 0.6291  loss_ce_6: 0.6665  loss_mask_6: 0.2012  loss_dice_6: 0.5227  loss_bbox_6: 0.2498  loss_giou_6: 0.7138  loss_ce_dn_6: 0.1304  loss_mask_dn_6: 0.2118  loss_dice_dn_6: 0.5812  loss_bbox_dn_6: 0.2151  loss_giou_dn_6: 0.6223  loss_ce_7: 0.6171  loss_mask_7: 0.203  loss_dice_7: 0.5707  loss_bbox_7: 0.2436  loss_giou_7: 0.7207  loss_ce_dn_7: 0.1217  loss_mask_dn_7: 0.2142  loss_dice_dn_7: 0.5593  loss_bbox_dn_7: 0.2136  loss_giou_dn_7: 0.6177  loss_ce_8: 0.6436  loss_mask_8: 0.2063  loss_dice_8: 0.5577  loss_bbox_8: 0.2302  loss_giou_8: 0.6851  loss_ce_dn_8: 0.1149  loss_mask_dn_8: 0.2182  loss_dice_dn_8: 0.5676  loss_bbox_dn_8: 0.2116  loss_giou_dn_8: 0.6134  loss_ce_interm: 1.376  loss_mask_interm: 0.2278  loss_dice_interm: 0.4957  loss_bbox_interm: 0.3969  loss_giou_interm: 0.8827    time: 0.8727  last_time: 0.8456  data_time: 0.0126  last_data_time: 0.0088   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:43:32 d2.utils.events]: \u001b[0m eta: 3 days, 17:00:45  iter: 979  total_loss: 63.24  loss_ce: 0.8893  loss_mask: 0.244  loss_dice: 0.9556  loss_bbox: 0.2372  loss_giou: 0.7392  loss_ce_dn: 0.1374  loss_mask_dn: 0.2346  loss_dice_dn: 0.9634  loss_bbox_dn: 0.2126  loss_giou_dn: 0.5715  loss_ce_0: 1.519  loss_mask_0: 0.2658  loss_dice_0: 0.9711  loss_bbox_0: 0.3936  loss_giou_0: 0.9402  loss_ce_dn_0: 0.7154  loss_mask_dn_0: 0.797  loss_dice_dn_0: 3.077  loss_bbox_dn_0: 0.6459  loss_giou_dn_0: 0.9232  loss_ce_1: 1.408  loss_mask_1: 0.2765  loss_dice_1: 0.9676  loss_bbox_1: 0.3366  loss_giou_1: 0.842  loss_ce_dn_1: 0.2578  loss_mask_dn_1: 0.2946  loss_dice_dn_1: 1.004  loss_bbox_dn_1: 0.3354  loss_giou_dn_1: 0.696  loss_ce_2: 1.251  loss_mask_2: 0.2512  loss_dice_2: 0.9616  loss_bbox_2: 0.2917  loss_giou_2: 0.7858  loss_ce_dn_2: 0.2028  loss_mask_dn_2: 0.2755  loss_dice_dn_2: 0.9897  loss_bbox_dn_2: 0.2911  loss_giou_dn_2: 0.6549  loss_ce_3: 1.096  loss_mask_3: 0.2482  loss_dice_3: 0.9821  loss_bbox_3: 0.2668  loss_giou_3: 0.7954  loss_ce_dn_3: 0.1693  loss_mask_dn_3: 0.2574  loss_dice_dn_3: 0.9559  loss_bbox_dn_3: 0.2646  loss_giou_dn_3: 0.6236  loss_ce_4: 1.029  loss_mask_4: 0.2524  loss_dice_4: 0.976  loss_bbox_4: 0.2592  loss_giou_4: 0.7719  loss_ce_dn_4: 0.1633  loss_mask_dn_4: 0.2527  loss_dice_dn_4: 0.9547  loss_bbox_dn_4: 0.2307  loss_giou_dn_4: 0.6026  loss_ce_5: 0.9184  loss_mask_5: 0.2495  loss_dice_5: 0.9839  loss_bbox_5: 0.2574  loss_giou_5: 0.7331  loss_ce_dn_5: 0.1539  loss_mask_dn_5: 0.241  loss_dice_dn_5: 0.9546  loss_bbox_dn_5: 0.2248  loss_giou_dn_5: 0.5939  loss_ce_6: 0.9265  loss_mask_6: 0.2509  loss_dice_6: 0.9675  loss_bbox_6: 0.2457  loss_giou_6: 0.7459  loss_ce_dn_6: 0.1527  loss_mask_dn_6: 0.2402  loss_dice_dn_6: 0.9387  loss_bbox_dn_6: 0.2131  loss_giou_dn_6: 0.5834  loss_ce_7: 0.8931  loss_mask_7: 0.2419  loss_dice_7: 0.926  loss_bbox_7: 0.2445  loss_giou_7: 0.7263  loss_ce_dn_7: 0.1479  loss_mask_dn_7: 0.2355  loss_dice_dn_7: 0.9396  loss_bbox_dn_7: 0.2142  loss_giou_dn_7: 0.5793  loss_ce_8: 0.8945  loss_mask_8: 0.2429  loss_dice_8: 0.9309  loss_bbox_8: 0.2404  loss_giou_8: 0.751  loss_ce_dn_8: 0.1381  loss_mask_dn_8: 0.232  loss_dice_dn_8: 0.957  loss_bbox_dn_8: 0.2136  loss_giou_dn_8: 0.572  loss_ce_interm: 1.507  loss_mask_interm: 0.2657  loss_dice_interm: 0.9959  loss_bbox_interm: 0.3936  loss_giou_interm: 0.9695    time: 0.8727  last_time: 0.8818  data_time: 0.0130  last_data_time: 0.0125   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:43:49 d2.utils.events]: \u001b[0m eta: 3 days, 17:01:42  iter: 999  total_loss: 53.47  loss_ce: 0.6778  loss_mask: 0.2592  loss_dice: 0.6242  loss_bbox: 0.2769  loss_giou: 0.5401  loss_ce_dn: 0.1004  loss_mask_dn: 0.2853  loss_dice_dn: 0.6378  loss_bbox_dn: 0.2396  loss_giou_dn: 0.4071  loss_ce_0: 1.43  loss_mask_0: 0.2761  loss_dice_0: 0.6932  loss_bbox_0: 0.4341  loss_giou_0: 0.7842  loss_ce_dn_0: 0.7375  loss_mask_dn_0: 0.9398  loss_dice_dn_0: 2.957  loss_bbox_dn_0: 0.7107  loss_giou_dn_0: 0.8747  loss_ce_1: 1.361  loss_mask_1: 0.2651  loss_dice_1: 0.6781  loss_bbox_1: 0.3345  loss_giou_1: 0.647  loss_ce_dn_1: 0.2315  loss_mask_dn_1: 0.3513  loss_dice_dn_1: 0.781  loss_bbox_dn_1: 0.3582  loss_giou_dn_1: 0.5862  loss_ce_2: 1.166  loss_mask_2: 0.2696  loss_dice_2: 0.6892  loss_bbox_2: 0.3082  loss_giou_2: 0.5881  loss_ce_dn_2: 0.1755  loss_mask_dn_2: 0.3244  loss_dice_dn_2: 0.6943  loss_bbox_dn_2: 0.2892  loss_giou_dn_2: 0.4891  loss_ce_3: 0.9266  loss_mask_3: 0.2607  loss_dice_3: 0.6906  loss_bbox_3: 0.3061  loss_giou_3: 0.5815  loss_ce_dn_3: 0.1443  loss_mask_dn_3: 0.3132  loss_dice_dn_3: 0.6442  loss_bbox_dn_3: 0.2605  loss_giou_dn_3: 0.4515  loss_ce_4: 0.8723  loss_mask_4: 0.2651  loss_dice_4: 0.665  loss_bbox_4: 0.2943  loss_giou_4: 0.5525  loss_ce_dn_4: 0.1285  loss_mask_dn_4: 0.3114  loss_dice_dn_4: 0.6257  loss_bbox_dn_4: 0.2482  loss_giou_dn_4: 0.4317  loss_ce_5: 0.7471  loss_mask_5: 0.2628  loss_dice_5: 0.6475  loss_bbox_5: 0.2926  loss_giou_5: 0.5516  loss_ce_dn_5: 0.1245  loss_mask_dn_5: 0.3015  loss_dice_dn_5: 0.6076  loss_bbox_dn_5: 0.2451  loss_giou_dn_5: 0.4261  loss_ce_6: 0.7091  loss_mask_6: 0.2628  loss_dice_6: 0.6324  loss_bbox_6: 0.2746  loss_giou_6: 0.5387  loss_ce_dn_6: 0.116  loss_mask_dn_6: 0.3073  loss_dice_dn_6: 0.6248  loss_bbox_dn_6: 0.2412  loss_giou_dn_6: 0.4125  loss_ce_7: 0.7012  loss_mask_7: 0.2616  loss_dice_7: 0.6155  loss_bbox_7: 0.2785  loss_giou_7: 0.543  loss_ce_dn_7: 0.1032  loss_mask_dn_7: 0.2845  loss_dice_dn_7: 0.6264  loss_bbox_dn_7: 0.2407  loss_giou_dn_7: 0.4137  loss_ce_8: 0.6669  loss_mask_8: 0.2578  loss_dice_8: 0.6294  loss_bbox_8: 0.2698  loss_giou_8: 0.5466  loss_ce_dn_8: 0.1005  loss_mask_dn_8: 0.2863  loss_dice_dn_8: 0.6233  loss_bbox_dn_8: 0.24  loss_giou_dn_8: 0.4079  loss_ce_interm: 1.444  loss_mask_interm: 0.2712  loss_dice_interm: 0.6962  loss_bbox_interm: 0.4362  loss_giou_interm: 0.7804    time: 0.8728  last_time: 0.8927  data_time: 0.0139  last_data_time: 0.0175   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:44:07 d2.utils.events]: \u001b[0m eta: 3 days, 17:02:01  iter: 1019  total_loss: 61.19  loss_ce: 0.8293  loss_mask: 0.2179  loss_dice: 0.6125  loss_bbox: 0.2464  loss_giou: 0.8575  loss_ce_dn: 0.1339  loss_mask_dn: 0.201  loss_dice_dn: 0.6066  loss_bbox_dn: 0.2112  loss_giou_dn: 0.7854  loss_ce_0: 1.499  loss_mask_0: 0.2303  loss_dice_0: 0.6821  loss_bbox_0: 0.3984  loss_giou_0: 1.003  loss_ce_dn_0: 0.704  loss_mask_dn_0: 0.8968  loss_dice_dn_0: 3.025  loss_bbox_dn_0: 0.5669  loss_giou_dn_0: 1.12  loss_ce_1: 1.391  loss_mask_1: 0.2426  loss_dice_1: 0.6087  loss_bbox_1: 0.2994  loss_giou_1: 0.9175  loss_ce_dn_1: 0.2916  loss_mask_dn_1: 0.2395  loss_dice_dn_1: 0.7375  loss_bbox_dn_1: 0.3292  loss_giou_dn_1: 0.9002  loss_ce_2: 1.232  loss_mask_2: 0.2371  loss_dice_2: 0.615  loss_bbox_2: 0.2729  loss_giou_2: 0.8926  loss_ce_dn_2: 0.2314  loss_mask_dn_2: 0.2251  loss_dice_dn_2: 0.6543  loss_bbox_dn_2: 0.2659  loss_giou_dn_2: 0.8368  loss_ce_3: 1.022  loss_mask_3: 0.2243  loss_dice_3: 0.6375  loss_bbox_3: 0.2924  loss_giou_3: 0.8794  loss_ce_dn_3: 0.1793  loss_mask_dn_3: 0.2149  loss_dice_dn_3: 0.6191  loss_bbox_dn_3: 0.2397  loss_giou_dn_3: 0.8115  loss_ce_4: 0.9863  loss_mask_4: 0.2299  loss_dice_4: 0.6326  loss_bbox_4: 0.2987  loss_giou_4: 0.8702  loss_ce_dn_4: 0.1518  loss_mask_dn_4: 0.2114  loss_dice_dn_4: 0.6004  loss_bbox_dn_4: 0.2274  loss_giou_dn_4: 0.7994  loss_ce_5: 0.9068  loss_mask_5: 0.2246  loss_dice_5: 0.6337  loss_bbox_5: 0.2944  loss_giou_5: 0.8673  loss_ce_dn_5: 0.1353  loss_mask_dn_5: 0.2038  loss_dice_dn_5: 0.6075  loss_bbox_dn_5: 0.2225  loss_giou_dn_5: 0.796  loss_ce_6: 0.8574  loss_mask_6: 0.2302  loss_dice_6: 0.607  loss_bbox_6: 0.2586  loss_giou_6: 0.8474  loss_ce_dn_6: 0.1374  loss_mask_dn_6: 0.2039  loss_dice_dn_6: 0.6068  loss_bbox_dn_6: 0.2139  loss_giou_dn_6: 0.789  loss_ce_7: 0.8324  loss_mask_7: 0.23  loss_dice_7: 0.6412  loss_bbox_7: 0.2493  loss_giou_7: 0.8568  loss_ce_dn_7: 0.133  loss_mask_dn_7: 0.2016  loss_dice_dn_7: 0.6041  loss_bbox_dn_7: 0.2121  loss_giou_dn_7: 0.7876  loss_ce_8: 0.8133  loss_mask_8: 0.2304  loss_dice_8: 0.634  loss_bbox_8: 0.2421  loss_giou_8: 0.8594  loss_ce_dn_8: 0.1343  loss_mask_dn_8: 0.1993  loss_dice_dn_8: 0.6077  loss_bbox_dn_8: 0.2096  loss_giou_dn_8: 0.7844  loss_ce_interm: 1.507  loss_mask_interm: 0.2286  loss_dice_interm: 0.6707  loss_bbox_interm: 0.3748  loss_giou_interm: 1.007    time: 0.8729  last_time: 0.9091  data_time: 0.0129  last_data_time: 0.0141   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:44:25 d2.utils.events]: \u001b[0m eta: 3 days, 17:02:59  iter: 1039  total_loss: 63.58  loss_ce: 0.847  loss_mask: 0.185  loss_dice: 0.8823  loss_bbox: 0.2024  loss_giou: 0.7375  loss_ce_dn: 0.1544  loss_mask_dn: 0.1826  loss_dice_dn: 0.8436  loss_bbox_dn: 0.1583  loss_giou_dn: 0.6573  loss_ce_0: 1.472  loss_mask_0: 0.1953  loss_dice_0: 0.8912  loss_bbox_0: 0.2856  loss_giou_0: 0.9809  loss_ce_dn_0: 0.7213  loss_mask_dn_0: 0.6417  loss_dice_dn_0: 3.018  loss_bbox_dn_0: 0.43  loss_giou_dn_0: 1.008  loss_ce_1: 1.419  loss_mask_1: 0.1791  loss_dice_1: 0.8408  loss_bbox_1: 0.2407  loss_giou_1: 0.8736  loss_ce_dn_1: 0.3042  loss_mask_dn_1: 0.2328  loss_dice_dn_1: 0.9717  loss_bbox_dn_1: 0.2466  loss_giou_dn_1: 0.7872  loss_ce_2: 1.263  loss_mask_2: 0.1776  loss_dice_2: 0.8983  loss_bbox_2: 0.2043  loss_giou_2: 0.8317  loss_ce_dn_2: 0.2503  loss_mask_dn_2: 0.1954  loss_dice_dn_2: 0.8908  loss_bbox_dn_2: 0.1982  loss_giou_dn_2: 0.7103  loss_ce_3: 1.092  loss_mask_3: 0.1932  loss_dice_3: 0.7654  loss_bbox_3: 0.2088  loss_giou_3: 0.7681  loss_ce_dn_3: 0.2127  loss_mask_dn_3: 0.1906  loss_dice_dn_3: 0.8528  loss_bbox_dn_3: 0.1766  loss_giou_dn_3: 0.6816  loss_ce_4: 1.03  loss_mask_4: 0.1816  loss_dice_4: 0.8128  loss_bbox_4: 0.1924  loss_giou_4: 0.7944  loss_ce_dn_4: 0.1917  loss_mask_dn_4: 0.1846  loss_dice_dn_4: 0.8872  loss_bbox_dn_4: 0.1633  loss_giou_dn_4: 0.6671  loss_ce_5: 0.9545  loss_mask_5: 0.1868  loss_dice_5: 0.8233  loss_bbox_5: 0.2153  loss_giou_5: 0.7509  loss_ce_dn_5: 0.1846  loss_mask_dn_5: 0.1866  loss_dice_dn_5: 0.8616  loss_bbox_dn_5: 0.161  loss_giou_dn_5: 0.661  loss_ce_6: 0.9046  loss_mask_6: 0.1883  loss_dice_6: 0.8342  loss_bbox_6: 0.2227  loss_giou_6: 0.7467  loss_ce_dn_6: 0.1649  loss_mask_dn_6: 0.1836  loss_dice_dn_6: 0.8553  loss_bbox_dn_6: 0.1575  loss_giou_dn_6: 0.6565  loss_ce_7: 0.871  loss_mask_7: 0.182  loss_dice_7: 0.882  loss_bbox_7: 0.2158  loss_giou_7: 0.7761  loss_ce_dn_7: 0.167  loss_mask_dn_7: 0.1805  loss_dice_dn_7: 0.88  loss_bbox_dn_7: 0.1586  loss_giou_dn_7: 0.6587  loss_ce_8: 0.8403  loss_mask_8: 0.1935  loss_dice_8: 0.8971  loss_bbox_8: 0.2232  loss_giou_8: 0.7369  loss_ce_dn_8: 0.1623  loss_mask_dn_8: 0.1821  loss_dice_dn_8: 0.8288  loss_bbox_dn_8: 0.1586  loss_giou_dn_8: 0.6559  loss_ce_interm: 1.564  loss_mask_interm: 0.1942  loss_dice_interm: 0.8475  loss_bbox_interm: 0.2856  loss_giou_interm: 0.9837    time: 0.8730  last_time: 0.8864  data_time: 0.0129  last_data_time: 0.0256   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:44:42 d2.utils.events]: \u001b[0m eta: 3 days, 17:02:01  iter: 1059  total_loss: 57.48  loss_ce: 0.8133  loss_mask: 0.2053  loss_dice: 0.664  loss_bbox: 0.2261  loss_giou: 0.6883  loss_ce_dn: 0.1554  loss_mask_dn: 0.2339  loss_dice_dn: 0.6985  loss_bbox_dn: 0.2123  loss_giou_dn: 0.5461  loss_ce_0: 1.394  loss_mask_0: 0.2148  loss_dice_0: 0.7237  loss_bbox_0: 0.3583  loss_giou_0: 0.8949  loss_ce_dn_0: 0.7737  loss_mask_dn_0: 0.7696  loss_dice_dn_0: 2.886  loss_bbox_dn_0: 0.6236  loss_giou_dn_0: 0.9343  loss_ce_1: 1.269  loss_mask_1: 0.2127  loss_dice_1: 0.6566  loss_bbox_1: 0.2839  loss_giou_1: 0.7696  loss_ce_dn_1: 0.2611  loss_mask_dn_1: 0.2986  loss_dice_dn_1: 0.9119  loss_bbox_dn_1: 0.331  loss_giou_dn_1: 0.672  loss_ce_2: 1.137  loss_mask_2: 0.2037  loss_dice_2: 0.6173  loss_bbox_2: 0.2617  loss_giou_2: 0.7347  loss_ce_dn_2: 0.2175  loss_mask_dn_2: 0.2584  loss_dice_dn_2: 0.7543  loss_bbox_dn_2: 0.2452  loss_giou_dn_2: 0.609  loss_ce_3: 0.9675  loss_mask_3: 0.2098  loss_dice_3: 0.6413  loss_bbox_3: 0.2347  loss_giou_3: 0.7199  loss_ce_dn_3: 0.1798  loss_mask_dn_3: 0.2576  loss_dice_dn_3: 0.7436  loss_bbox_dn_3: 0.2233  loss_giou_dn_3: 0.5898  loss_ce_4: 0.9159  loss_mask_4: 0.2079  loss_dice_4: 0.641  loss_bbox_4: 0.2344  loss_giou_4: 0.6906  loss_ce_dn_4: 0.1812  loss_mask_dn_4: 0.2451  loss_dice_dn_4: 0.7192  loss_bbox_dn_4: 0.2163  loss_giou_dn_4: 0.5708  loss_ce_5: 0.8551  loss_mask_5: 0.2047  loss_dice_5: 0.5923  loss_bbox_5: 0.2427  loss_giou_5: 0.6769  loss_ce_dn_5: 0.1658  loss_mask_dn_5: 0.2463  loss_dice_dn_5: 0.7026  loss_bbox_dn_5: 0.2128  loss_giou_dn_5: 0.5624  loss_ce_6: 0.8947  loss_mask_6: 0.2003  loss_dice_6: 0.617  loss_bbox_6: 0.2295  loss_giou_6: 0.7263  loss_ce_dn_6: 0.1647  loss_mask_dn_6: 0.2437  loss_dice_dn_6: 0.7029  loss_bbox_dn_6: 0.216  loss_giou_dn_6: 0.5583  loss_ce_7: 0.817  loss_mask_7: 0.205  loss_dice_7: 0.6433  loss_bbox_7: 0.2285  loss_giou_7: 0.691  loss_ce_dn_7: 0.1567  loss_mask_dn_7: 0.2457  loss_dice_dn_7: 0.7  loss_bbox_dn_7: 0.2118  loss_giou_dn_7: 0.5556  loss_ce_8: 0.8104  loss_mask_8: 0.1988  loss_dice_8: 0.6148  loss_bbox_8: 0.226  loss_giou_8: 0.6815  loss_ce_dn_8: 0.1554  loss_mask_dn_8: 0.2368  loss_dice_dn_8: 0.7099  loss_bbox_dn_8: 0.2118  loss_giou_dn_8: 0.5462  loss_ce_interm: 1.371  loss_mask_interm: 0.2122  loss_dice_interm: 0.649  loss_bbox_interm: 0.3736  loss_giou_interm: 0.8889    time: 0.8729  last_time: 0.8466  data_time: 0.0123  last_data_time: 0.0123   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:44:59 d2.utils.events]: \u001b[0m eta: 3 days, 17:01:22  iter: 1079  total_loss: 69.89  loss_ce: 0.9222  loss_mask: 0.207  loss_dice: 0.8991  loss_bbox: 0.2577  loss_giou: 0.7979  loss_ce_dn: 0.1488  loss_mask_dn: 0.1941  loss_dice_dn: 0.7598  loss_bbox_dn: 0.2086  loss_giou_dn: 0.6185  loss_ce_0: 1.582  loss_mask_0: 0.2329  loss_dice_0: 0.9193  loss_bbox_0: 0.3711  loss_giou_0: 0.9352  loss_ce_dn_0: 0.7759  loss_mask_dn_0: 0.8558  loss_dice_dn_0: 3.206  loss_bbox_dn_0: 0.5862  loss_giou_dn_0: 0.9591  loss_ce_1: 1.473  loss_mask_1: 0.244  loss_dice_1: 0.8692  loss_bbox_1: 0.2506  loss_giou_1: 0.8712  loss_ce_dn_1: 0.2865  loss_mask_dn_1: 0.2693  loss_dice_dn_1: 0.9734  loss_bbox_dn_1: 0.3217  loss_giou_dn_1: 0.7164  loss_ce_2: 1.329  loss_mask_2: 0.2429  loss_dice_2: 0.8212  loss_bbox_2: 0.247  loss_giou_2: 0.8274  loss_ce_dn_2: 0.2273  loss_mask_dn_2: 0.2139  loss_dice_dn_2: 0.8915  loss_bbox_dn_2: 0.2557  loss_giou_dn_2: 0.6653  loss_ce_3: 1.169  loss_mask_3: 0.2135  loss_dice_3: 0.8538  loss_bbox_3: 0.2709  loss_giou_3: 0.8414  loss_ce_dn_3: 0.1986  loss_mask_dn_3: 0.2043  loss_dice_dn_3: 0.8262  loss_bbox_dn_3: 0.2292  loss_giou_dn_3: 0.6477  loss_ce_4: 1.055  loss_mask_4: 0.2054  loss_dice_4: 0.8286  loss_bbox_4: 0.2515  loss_giou_4: 0.8147  loss_ce_dn_4: 0.1853  loss_mask_dn_4: 0.1992  loss_dice_dn_4: 0.8161  loss_bbox_dn_4: 0.22  loss_giou_dn_4: 0.6353  loss_ce_5: 1.043  loss_mask_5: 0.2034  loss_dice_5: 0.8944  loss_bbox_5: 0.2611  loss_giou_5: 0.8216  loss_ce_dn_5: 0.1679  loss_mask_dn_5: 0.1951  loss_dice_dn_5: 0.7823  loss_bbox_dn_5: 0.2165  loss_giou_dn_5: 0.6323  loss_ce_6: 0.9692  loss_mask_6: 0.2057  loss_dice_6: 0.8174  loss_bbox_6: 0.2659  loss_giou_6: 0.7879  loss_ce_dn_6: 0.1514  loss_mask_dn_6: 0.1949  loss_dice_dn_6: 0.786  loss_bbox_dn_6: 0.211  loss_giou_dn_6: 0.6263  loss_ce_7: 0.9964  loss_mask_7: 0.2006  loss_dice_7: 0.7746  loss_bbox_7: 0.2595  loss_giou_7: 0.8015  loss_ce_dn_7: 0.1465  loss_mask_dn_7: 0.1949  loss_dice_dn_7: 0.7893  loss_bbox_dn_7: 0.2077  loss_giou_dn_7: 0.6231  loss_ce_8: 0.9113  loss_mask_8: 0.2083  loss_dice_8: 0.7848  loss_bbox_8: 0.2604  loss_giou_8: 0.8138  loss_ce_dn_8: 0.1457  loss_mask_dn_8: 0.1936  loss_dice_dn_8: 0.7873  loss_bbox_dn_8: 0.2088  loss_giou_dn_8: 0.6209  loss_ce_interm: 1.622  loss_mask_interm: 0.236  loss_dice_interm: 0.912  loss_bbox_interm: 0.3711  loss_giou_interm: 0.9635    time: 0.8727  last_time: 0.8784  data_time: 0.0111  last_data_time: 0.0072   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:45:17 d2.utils.events]: \u001b[0m eta: 3 days, 17:01:15  iter: 1099  total_loss: 61.79  loss_ce: 0.7153  loss_mask: 0.2491  loss_dice: 0.8528  loss_bbox: 0.2848  loss_giou: 0.7618  loss_ce_dn: 0.146  loss_mask_dn: 0.2794  loss_dice_dn: 0.8474  loss_bbox_dn: 0.2508  loss_giou_dn: 0.6134  loss_ce_0: 1.512  loss_mask_0: 0.2609  loss_dice_0: 0.8721  loss_bbox_0: 0.4376  loss_giou_0: 0.8755  loss_ce_dn_0: 0.7139  loss_mask_dn_0: 0.9216  loss_dice_dn_0: 3.227  loss_bbox_dn_0: 0.6598  loss_giou_dn_0: 0.9669  loss_ce_1: 1.367  loss_mask_1: 0.2557  loss_dice_1: 0.8617  loss_bbox_1: 0.3265  loss_giou_1: 0.8177  loss_ce_dn_1: 0.2559  loss_mask_dn_1: 0.3425  loss_dice_dn_1: 0.9846  loss_bbox_dn_1: 0.3905  loss_giou_dn_1: 0.7294  loss_ce_2: 1.177  loss_mask_2: 0.2202  loss_dice_2: 0.8823  loss_bbox_2: 0.285  loss_giou_2: 0.7691  loss_ce_dn_2: 0.1834  loss_mask_dn_2: 0.2909  loss_dice_dn_2: 0.8939  loss_bbox_dn_2: 0.3127  loss_giou_dn_2: 0.6742  loss_ce_3: 1.016  loss_mask_3: 0.2338  loss_dice_3: 0.8856  loss_bbox_3: 0.2851  loss_giou_3: 0.7892  loss_ce_dn_3: 0.1634  loss_mask_dn_3: 0.2825  loss_dice_dn_3: 0.8886  loss_bbox_dn_3: 0.2885  loss_giou_dn_3: 0.643  loss_ce_4: 0.8855  loss_mask_4: 0.2331  loss_dice_4: 0.8225  loss_bbox_4: 0.2757  loss_giou_4: 0.7821  loss_ce_dn_4: 0.1571  loss_mask_dn_4: 0.2815  loss_dice_dn_4: 0.8482  loss_bbox_dn_4: 0.2784  loss_giou_dn_4: 0.6297  loss_ce_5: 0.8108  loss_mask_5: 0.2477  loss_dice_5: 0.8086  loss_bbox_5: 0.284  loss_giou_5: 0.7724  loss_ce_dn_5: 0.1544  loss_mask_dn_5: 0.2823  loss_dice_dn_5: 0.8447  loss_bbox_dn_5: 0.2782  loss_giou_dn_5: 0.6227  loss_ce_6: 0.7712  loss_mask_6: 0.2472  loss_dice_6: 0.8254  loss_bbox_6: 0.2792  loss_giou_6: 0.7535  loss_ce_dn_6: 0.1499  loss_mask_dn_6: 0.2806  loss_dice_dn_6: 0.8394  loss_bbox_dn_6: 0.2673  loss_giou_dn_6: 0.6162  loss_ce_7: 0.7205  loss_mask_7: 0.246  loss_dice_7: 0.8321  loss_bbox_7: 0.2748  loss_giou_7: 0.7572  loss_ce_dn_7: 0.1466  loss_mask_dn_7: 0.2834  loss_dice_dn_7: 0.8572  loss_bbox_dn_7: 0.2585  loss_giou_dn_7: 0.6142  loss_ce_8: 0.7117  loss_mask_8: 0.2418  loss_dice_8: 0.8642  loss_bbox_8: 0.2876  loss_giou_8: 0.7781  loss_ce_dn_8: 0.1473  loss_mask_dn_8: 0.2812  loss_dice_dn_8: 0.8372  loss_bbox_dn_8: 0.2513  loss_giou_dn_8: 0.6126  loss_ce_interm: 1.516  loss_mask_interm: 0.2623  loss_dice_interm: 0.861  loss_bbox_interm: 0.4381  loss_giou_interm: 0.8687    time: 0.8727  last_time: 0.8576  data_time: 0.0131  last_data_time: 0.0148   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:45:34 d2.utils.events]: \u001b[0m eta: 3 days, 17:01:28  iter: 1119  total_loss: 62.47  loss_ce: 0.8221  loss_mask: 0.2576  loss_dice: 0.744  loss_bbox: 0.2503  loss_giou: 0.7632  loss_ce_dn: 0.1621  loss_mask_dn: 0.2871  loss_dice_dn: 0.6914  loss_bbox_dn: 0.2207  loss_giou_dn: 0.6454  loss_ce_0: 1.511  loss_mask_0: 0.2778  loss_dice_0: 0.6991  loss_bbox_0: 0.3857  loss_giou_0: 0.9627  loss_ce_dn_0: 0.7374  loss_mask_dn_0: 0.9424  loss_dice_dn_0: 3.102  loss_bbox_dn_0: 0.6608  loss_giou_dn_0: 1.021  loss_ce_1: 1.453  loss_mask_1: 0.2741  loss_dice_1: 0.7444  loss_bbox_1: 0.2754  loss_giou_1: 0.9036  loss_ce_dn_1: 0.281  loss_mask_dn_1: 0.3891  loss_dice_dn_1: 0.8189  loss_bbox_dn_1: 0.3663  loss_giou_dn_1: 0.7781  loss_ce_2: 1.269  loss_mask_2: 0.2652  loss_dice_2: 0.7612  loss_bbox_2: 0.2785  loss_giou_2: 0.8612  loss_ce_dn_2: 0.2199  loss_mask_dn_2: 0.3332  loss_dice_dn_2: 0.7255  loss_bbox_dn_2: 0.2742  loss_giou_dn_2: 0.7178  loss_ce_3: 1.026  loss_mask_3: 0.2683  loss_dice_3: 0.7031  loss_bbox_3: 0.2882  loss_giou_3: 0.8307  loss_ce_dn_3: 0.2047  loss_mask_dn_3: 0.3228  loss_dice_dn_3: 0.6846  loss_bbox_dn_3: 0.2349  loss_giou_dn_3: 0.6861  loss_ce_4: 0.9643  loss_mask_4: 0.2618  loss_dice_4: 0.7386  loss_bbox_4: 0.241  loss_giou_4: 0.7896  loss_ce_dn_4: 0.1998  loss_mask_dn_4: 0.3019  loss_dice_dn_4: 0.6643  loss_bbox_dn_4: 0.2281  loss_giou_dn_4: 0.6698  loss_ce_5: 0.8741  loss_mask_5: 0.2612  loss_dice_5: 0.7567  loss_bbox_5: 0.2299  loss_giou_5: 0.7871  loss_ce_dn_5: 0.1967  loss_mask_dn_5: 0.2894  loss_dice_dn_5: 0.6818  loss_bbox_dn_5: 0.2287  loss_giou_dn_5: 0.6568  loss_ce_6: 0.8816  loss_mask_6: 0.2582  loss_dice_6: 0.7581  loss_bbox_6: 0.2442  loss_giou_6: 0.759  loss_ce_dn_6: 0.1884  loss_mask_dn_6: 0.2884  loss_dice_dn_6: 0.7067  loss_bbox_dn_6: 0.2235  loss_giou_dn_6: 0.6499  loss_ce_7: 0.8295  loss_mask_7: 0.2664  loss_dice_7: 0.7689  loss_bbox_7: 0.2482  loss_giou_7: 0.7647  loss_ce_dn_7: 0.1808  loss_mask_dn_7: 0.2842  loss_dice_dn_7: 0.6777  loss_bbox_dn_7: 0.2224  loss_giou_dn_7: 0.6492  loss_ce_8: 0.859  loss_mask_8: 0.2605  loss_dice_8: 0.7537  loss_bbox_8: 0.2376  loss_giou_8: 0.7577  loss_ce_dn_8: 0.1678  loss_mask_dn_8: 0.2885  loss_dice_dn_8: 0.6985  loss_bbox_dn_8: 0.2221  loss_giou_dn_8: 0.6459  loss_ce_interm: 1.51  loss_mask_interm: 0.2727  loss_dice_interm: 0.7213  loss_bbox_interm: 0.3857  loss_giou_interm: 0.9687    time: 0.8727  last_time: 0.8774  data_time: 0.0115  last_data_time: 0.0104   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:45:52 d2.utils.events]: \u001b[0m eta: 3 days, 17:02:12  iter: 1139  total_loss: 63.93  loss_ce: 0.8922  loss_mask: 0.306  loss_dice: 0.7807  loss_bbox: 0.2873  loss_giou: 0.6797  loss_ce_dn: 0.1978  loss_mask_dn: 0.3421  loss_dice_dn: 0.7712  loss_bbox_dn: 0.2436  loss_giou_dn: 0.569  loss_ce_0: 1.525  loss_mask_0: 0.3703  loss_dice_0: 0.8145  loss_bbox_0: 0.4105  loss_giou_0: 0.8799  loss_ce_dn_0: 0.7964  loss_mask_dn_0: 0.9875  loss_dice_dn_0: 2.901  loss_bbox_dn_0: 0.6338  loss_giou_dn_0: 0.9703  loss_ce_1: 1.414  loss_mask_1: 0.3326  loss_dice_1: 0.7593  loss_bbox_1: 0.3097  loss_giou_1: 0.6757  loss_ce_dn_1: 0.3544  loss_mask_dn_1: 0.4174  loss_dice_dn_1: 0.8898  loss_bbox_dn_1: 0.4049  loss_giou_dn_1: 0.6921  loss_ce_2: 1.251  loss_mask_2: 0.3241  loss_dice_2: 0.7931  loss_bbox_2: 0.2905  loss_giou_2: 0.6666  loss_ce_dn_2: 0.3026  loss_mask_dn_2: 0.378  loss_dice_dn_2: 0.8364  loss_bbox_dn_2: 0.3537  loss_giou_dn_2: 0.6237  loss_ce_3: 1.096  loss_mask_3: 0.319  loss_dice_3: 0.7422  loss_bbox_3: 0.3012  loss_giou_3: 0.6953  loss_ce_dn_3: 0.2748  loss_mask_dn_3: 0.3653  loss_dice_dn_3: 0.7739  loss_bbox_dn_3: 0.3109  loss_giou_dn_3: 0.6004  loss_ce_4: 1.022  loss_mask_4: 0.3322  loss_dice_4: 0.7516  loss_bbox_4: 0.3064  loss_giou_4: 0.7536  loss_ce_dn_4: 0.2349  loss_mask_dn_4: 0.3561  loss_dice_dn_4: 0.7722  loss_bbox_dn_4: 0.2825  loss_giou_dn_4: 0.583  loss_ce_5: 0.9774  loss_mask_5: 0.3476  loss_dice_5: 0.7751  loss_bbox_5: 0.2931  loss_giou_5: 0.6925  loss_ce_dn_5: 0.2156  loss_mask_dn_5: 0.3514  loss_dice_dn_5: 0.752  loss_bbox_dn_5: 0.28  loss_giou_dn_5: 0.5839  loss_ce_6: 0.9114  loss_mask_6: 0.3189  loss_dice_6: 0.7664  loss_bbox_6: 0.2969  loss_giou_6: 0.6962  loss_ce_dn_6: 0.2012  loss_mask_dn_6: 0.3482  loss_dice_dn_6: 0.767  loss_bbox_dn_6: 0.2566  loss_giou_dn_6: 0.575  loss_ce_7: 0.9168  loss_mask_7: 0.3133  loss_dice_7: 0.7495  loss_bbox_7: 0.289  loss_giou_7: 0.6848  loss_ce_dn_7: 0.194  loss_mask_dn_7: 0.3467  loss_dice_dn_7: 0.7721  loss_bbox_dn_7: 0.2507  loss_giou_dn_7: 0.5739  loss_ce_8: 0.8998  loss_mask_8: 0.3086  loss_dice_8: 0.7597  loss_bbox_8: 0.2857  loss_giou_8: 0.7057  loss_ce_dn_8: 0.1932  loss_mask_dn_8: 0.3471  loss_dice_dn_8: 0.7594  loss_bbox_dn_8: 0.2442  loss_giou_dn_8: 0.5704  loss_ce_interm: 1.542  loss_mask_interm: 0.3823  loss_dice_interm: 0.8226  loss_bbox_interm: 0.4072  loss_giou_interm: 0.8331    time: 0.8727  last_time: 0.8983  data_time: 0.0117  last_data_time: 0.0108   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:46:09 d2.utils.events]: \u001b[0m eta: 3 days, 17:01:55  iter: 1159  total_loss: 68.89  loss_ce: 1  loss_mask: 0.319  loss_dice: 0.9222  loss_bbox: 0.2843  loss_giou: 0.6737  loss_ce_dn: 0.1226  loss_mask_dn: 0.3153  loss_dice_dn: 0.9225  loss_bbox_dn: 0.2309  loss_giou_dn: 0.6084  loss_ce_0: 1.581  loss_mask_0: 0.3164  loss_dice_0: 1.008  loss_bbox_0: 0.4353  loss_giou_0: 0.8645  loss_ce_dn_0: 0.7688  loss_mask_dn_0: 0.9013  loss_dice_dn_0: 3.056  loss_bbox_dn_0: 0.6306  loss_giou_dn_0: 0.9702  loss_ce_1: 1.471  loss_mask_1: 0.3247  loss_dice_1: 0.9404  loss_bbox_1: 0.3227  loss_giou_1: 0.7811  loss_ce_dn_1: 0.2575  loss_mask_dn_1: 0.3362  loss_dice_dn_1: 1.086  loss_bbox_dn_1: 0.4101  loss_giou_dn_1: 0.7741  loss_ce_2: 1.402  loss_mask_2: 0.3123  loss_dice_2: 0.8732  loss_bbox_2: 0.3104  loss_giou_2: 0.72  loss_ce_dn_2: 0.1947  loss_mask_dn_2: 0.3358  loss_dice_dn_2: 0.9842  loss_bbox_dn_2: 0.3533  loss_giou_dn_2: 0.6958  loss_ce_3: 1.229  loss_mask_3: 0.2902  loss_dice_3: 0.9526  loss_bbox_3: 0.3074  loss_giou_3: 0.6923  loss_ce_dn_3: 0.1753  loss_mask_dn_3: 0.3238  loss_dice_dn_3: 0.9558  loss_bbox_dn_3: 0.2925  loss_giou_dn_3: 0.6736  loss_ce_4: 1.11  loss_mask_4: 0.2962  loss_dice_4: 0.9192  loss_bbox_4: 0.3008  loss_giou_4: 0.7023  loss_ce_dn_4: 0.1642  loss_mask_dn_4: 0.3263  loss_dice_dn_4: 0.928  loss_bbox_dn_4: 0.2575  loss_giou_dn_4: 0.645  loss_ce_5: 1.066  loss_mask_5: 0.2997  loss_dice_5: 0.9141  loss_bbox_5: 0.3143  loss_giou_5: 0.6975  loss_ce_dn_5: 0.153  loss_mask_dn_5: 0.3224  loss_dice_dn_5: 0.9377  loss_bbox_dn_5: 0.2473  loss_giou_dn_5: 0.6386  loss_ce_6: 1.016  loss_mask_6: 0.3023  loss_dice_6: 0.9036  loss_bbox_6: 0.2989  loss_giou_6: 0.6915  loss_ce_dn_6: 0.136  loss_mask_dn_6: 0.3137  loss_dice_dn_6: 0.9226  loss_bbox_dn_6: 0.2373  loss_giou_dn_6: 0.6231  loss_ce_7: 1.06  loss_mask_7: 0.2958  loss_dice_7: 0.933  loss_bbox_7: 0.2951  loss_giou_7: 0.6865  loss_ce_dn_7: 0.1301  loss_mask_dn_7: 0.3168  loss_dice_dn_7: 0.9292  loss_bbox_dn_7: 0.2361  loss_giou_dn_7: 0.6167  loss_ce_8: 1.025  loss_mask_8: 0.2961  loss_dice_8: 0.9285  loss_bbox_8: 0.2882  loss_giou_8: 0.6914  loss_ce_dn_8: 0.1252  loss_mask_dn_8: 0.3166  loss_dice_dn_8: 0.9535  loss_bbox_dn_8: 0.2293  loss_giou_dn_8: 0.6099  loss_ce_interm: 1.567  loss_mask_interm: 0.3144  loss_dice_interm: 1.017  loss_bbox_interm: 0.4364  loss_giou_interm: 0.8746    time: 0.8727  last_time: 0.8434  data_time: 0.0118  last_data_time: 0.0089   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:46:27 d2.utils.events]: \u001b[0m eta: 3 days, 17:00:35  iter: 1179  total_loss: 63.09  loss_ce: 0.6521  loss_mask: 0.2469  loss_dice: 0.6617  loss_bbox: 0.3039  loss_giou: 0.6487  loss_ce_dn: 0.1295  loss_mask_dn: 0.2626  loss_dice_dn: 0.6524  loss_bbox_dn: 0.245  loss_giou_dn: 0.5012  loss_ce_0: 1.359  loss_mask_0: 0.2679  loss_dice_0: 0.7823  loss_bbox_0: 0.4338  loss_giou_0: 0.841  loss_ce_dn_0: 0.764  loss_mask_dn_0: 0.9164  loss_dice_dn_0: 3.125  loss_bbox_dn_0: 0.6141  loss_giou_dn_0: 0.9303  loss_ce_1: 1.278  loss_mask_1: 0.2732  loss_dice_1: 0.7444  loss_bbox_1: 0.3492  loss_giou_1: 0.7351  loss_ce_dn_1: 0.2615  loss_mask_dn_1: 0.3209  loss_dice_dn_1: 0.8201  loss_bbox_dn_1: 0.4058  loss_giou_dn_1: 0.6351  loss_ce_2: 1.147  loss_mask_2: 0.2727  loss_dice_2: 0.6751  loss_bbox_2: 0.3346  loss_giou_2: 0.6896  loss_ce_dn_2: 0.2025  loss_mask_dn_2: 0.2726  loss_dice_dn_2: 0.7354  loss_bbox_dn_2: 0.3225  loss_giou_dn_2: 0.5543  loss_ce_3: 1  loss_mask_3: 0.2599  loss_dice_3: 0.6899  loss_bbox_3: 0.3079  loss_giou_3: 0.6821  loss_ce_dn_3: 0.1653  loss_mask_dn_3: 0.269  loss_dice_dn_3: 0.6859  loss_bbox_dn_3: 0.2968  loss_giou_dn_3: 0.5378  loss_ce_4: 0.9208  loss_mask_4: 0.2568  loss_dice_4: 0.7596  loss_bbox_4: 0.29  loss_giou_4: 0.6628  loss_ce_dn_4: 0.1438  loss_mask_dn_4: 0.2698  loss_dice_dn_4: 0.6628  loss_bbox_dn_4: 0.2817  loss_giou_dn_4: 0.5218  loss_ce_5: 0.7663  loss_mask_5: 0.2495  loss_dice_5: 0.7432  loss_bbox_5: 0.3004  loss_giou_5: 0.7184  loss_ce_dn_5: 0.1418  loss_mask_dn_5: 0.2674  loss_dice_dn_5: 0.6404  loss_bbox_dn_5: 0.2702  loss_giou_dn_5: 0.5079  loss_ce_6: 0.7032  loss_mask_6: 0.2489  loss_dice_6: 0.7139  loss_bbox_6: 0.298  loss_giou_6: 0.6511  loss_ce_dn_6: 0.1399  loss_mask_dn_6: 0.2688  loss_dice_dn_6: 0.6588  loss_bbox_dn_6: 0.2583  loss_giou_dn_6: 0.5031  loss_ce_7: 0.6659  loss_mask_7: 0.2508  loss_dice_7: 0.7036  loss_bbox_7: 0.3018  loss_giou_7: 0.6757  loss_ce_dn_7: 0.1343  loss_mask_dn_7: 0.2676  loss_dice_dn_7: 0.639  loss_bbox_dn_7: 0.2509  loss_giou_dn_7: 0.5016  loss_ce_8: 0.6603  loss_mask_8: 0.2475  loss_dice_8: 0.721  loss_bbox_8: 0.3066  loss_giou_8: 0.6716  loss_ce_dn_8: 0.1281  loss_mask_dn_8: 0.266  loss_dice_dn_8: 0.6548  loss_bbox_dn_8: 0.2456  loss_giou_dn_8: 0.4998  loss_ce_interm: 1.356  loss_mask_interm: 0.2696  loss_dice_interm: 0.7265  loss_bbox_interm: 0.4336  loss_giou_interm: 0.8653    time: 0.8729  last_time: 0.8862  data_time: 0.0122  last_data_time: 0.0152   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:46:45 d2.utils.events]: \u001b[0m eta: 3 days, 17:01:20  iter: 1199  total_loss: 57.77  loss_ce: 0.7324  loss_mask: 0.2745  loss_dice: 0.6594  loss_bbox: 0.283  loss_giou: 0.6877  loss_ce_dn: 0.1192  loss_mask_dn: 0.2706  loss_dice_dn: 0.6588  loss_bbox_dn: 0.2143  loss_giou_dn: 0.5318  loss_ce_0: 1.449  loss_mask_0: 0.2754  loss_dice_0: 0.7247  loss_bbox_0: 0.3963  loss_giou_0: 0.8903  loss_ce_dn_0: 0.7774  loss_mask_dn_0: 0.9725  loss_dice_dn_0: 3.176  loss_bbox_dn_0: 0.6389  loss_giou_dn_0: 0.9514  loss_ce_1: 1.396  loss_mask_1: 0.2751  loss_dice_1: 0.7181  loss_bbox_1: 0.3581  loss_giou_1: 0.7813  loss_ce_dn_1: 0.2947  loss_mask_dn_1: 0.3578  loss_dice_dn_1: 0.7595  loss_bbox_dn_1: 0.3484  loss_giou_dn_1: 0.6652  loss_ce_2: 1.114  loss_mask_2: 0.2777  loss_dice_2: 0.7138  loss_bbox_2: 0.3212  loss_giou_2: 0.7413  loss_ce_dn_2: 0.2276  loss_mask_dn_2: 0.2981  loss_dice_dn_2: 0.7056  loss_bbox_dn_2: 0.2714  loss_giou_dn_2: 0.5815  loss_ce_3: 0.9341  loss_mask_3: 0.2826  loss_dice_3: 0.6969  loss_bbox_3: 0.3094  loss_giou_3: 0.7389  loss_ce_dn_3: 0.1958  loss_mask_dn_3: 0.2783  loss_dice_dn_3: 0.6625  loss_bbox_dn_3: 0.2449  loss_giou_dn_3: 0.5619  loss_ce_4: 0.8702  loss_mask_4: 0.2757  loss_dice_4: 0.6815  loss_bbox_4: 0.3038  loss_giou_4: 0.6969  loss_ce_dn_4: 0.1725  loss_mask_dn_4: 0.2773  loss_dice_dn_4: 0.6342  loss_bbox_dn_4: 0.2329  loss_giou_dn_4: 0.5415  loss_ce_5: 0.7941  loss_mask_5: 0.2592  loss_dice_5: 0.6635  loss_bbox_5: 0.3093  loss_giou_5: 0.7031  loss_ce_dn_5: 0.1603  loss_mask_dn_5: 0.2682  loss_dice_dn_5: 0.68  loss_bbox_dn_5: 0.227  loss_giou_dn_5: 0.5395  loss_ce_6: 0.7481  loss_mask_6: 0.2658  loss_dice_6: 0.6882  loss_bbox_6: 0.2963  loss_giou_6: 0.6805  loss_ce_dn_6: 0.1452  loss_mask_dn_6: 0.2711  loss_dice_dn_6: 0.6461  loss_bbox_dn_6: 0.2149  loss_giou_dn_6: 0.537  loss_ce_7: 0.7189  loss_mask_7: 0.2668  loss_dice_7: 0.6947  loss_bbox_7: 0.2976  loss_giou_7: 0.6932  loss_ce_dn_7: 0.124  loss_mask_dn_7: 0.2699  loss_dice_dn_7: 0.6593  loss_bbox_dn_7: 0.2157  loss_giou_dn_7: 0.5341  loss_ce_8: 0.7553  loss_mask_8: 0.2667  loss_dice_8: 0.6365  loss_bbox_8: 0.2874  loss_giou_8: 0.7114  loss_ce_dn_8: 0.1245  loss_mask_dn_8: 0.2654  loss_dice_dn_8: 0.6471  loss_bbox_dn_8: 0.2162  loss_giou_dn_8: 0.5327  loss_ce_interm: 1.455  loss_mask_interm: 0.2792  loss_dice_interm: 0.7329  loss_bbox_interm: 0.4033  loss_giou_interm: 0.8874    time: 0.8729  last_time: 0.8828  data_time: 0.0121  last_data_time: 0.0155   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:47:02 d2.utils.events]: \u001b[0m eta: 3 days, 17:03:45  iter: 1219  total_loss: 60.36  loss_ce: 0.7254  loss_mask: 0.2579  loss_dice: 0.7751  loss_bbox: 0.2627  loss_giou: 0.7228  loss_ce_dn: 0.1483  loss_mask_dn: 0.2244  loss_dice_dn: 0.7244  loss_bbox_dn: 0.2056  loss_giou_dn: 0.5821  loss_ce_0: 1.47  loss_mask_0: 0.2642  loss_dice_0: 0.7775  loss_bbox_0: 0.415  loss_giou_0: 1.004  loss_ce_dn_0: 0.7587  loss_mask_dn_0: 0.7909  loss_dice_dn_0: 2.91  loss_bbox_dn_0: 0.5734  loss_giou_dn_0: 0.9324  loss_ce_1: 1.342  loss_mask_1: 0.2619  loss_dice_1: 0.7861  loss_bbox_1: 0.2969  loss_giou_1: 0.8486  loss_ce_dn_1: 0.2837  loss_mask_dn_1: 0.3121  loss_dice_dn_1: 0.8752  loss_bbox_dn_1: 0.3242  loss_giou_dn_1: 0.7032  loss_ce_2: 1.131  loss_mask_2: 0.2705  loss_dice_2: 0.762  loss_bbox_2: 0.2606  loss_giou_2: 0.7817  loss_ce_dn_2: 0.2294  loss_mask_dn_2: 0.2486  loss_dice_dn_2: 0.7613  loss_bbox_dn_2: 0.2602  loss_giou_dn_2: 0.6508  loss_ce_3: 0.9756  loss_mask_3: 0.2461  loss_dice_3: 0.7496  loss_bbox_3: 0.2558  loss_giou_3: 0.7231  loss_ce_dn_3: 0.1982  loss_mask_dn_3: 0.2285  loss_dice_dn_3: 0.7735  loss_bbox_dn_3: 0.2434  loss_giou_dn_3: 0.6277  loss_ce_4: 0.9541  loss_mask_4: 0.2414  loss_dice_4: 0.7515  loss_bbox_4: 0.2398  loss_giou_4: 0.718  loss_ce_dn_4: 0.1825  loss_mask_dn_4: 0.2189  loss_dice_dn_4: 0.7382  loss_bbox_dn_4: 0.2273  loss_giou_dn_4: 0.6155  loss_ce_5: 0.8437  loss_mask_5: 0.2535  loss_dice_5: 0.8007  loss_bbox_5: 0.2644  loss_giou_5: 0.7541  loss_ce_dn_5: 0.1665  loss_mask_dn_5: 0.2138  loss_dice_dn_5: 0.7347  loss_bbox_dn_5: 0.2181  loss_giou_dn_5: 0.5995  loss_ce_6: 0.7888  loss_mask_6: 0.2553  loss_dice_6: 0.7771  loss_bbox_6: 0.26  loss_giou_6: 0.709  loss_ce_dn_6: 0.1593  loss_mask_dn_6: 0.2152  loss_dice_dn_6: 0.7249  loss_bbox_dn_6: 0.2094  loss_giou_dn_6: 0.5905  loss_ce_7: 0.7673  loss_mask_7: 0.2561  loss_dice_7: 0.7794  loss_bbox_7: 0.2875  loss_giou_7: 0.7467  loss_ce_dn_7: 0.1444  loss_mask_dn_7: 0.221  loss_dice_dn_7: 0.7177  loss_bbox_dn_7: 0.2065  loss_giou_dn_7: 0.5868  loss_ce_8: 0.7408  loss_mask_8: 0.2422  loss_dice_8: 0.7741  loss_bbox_8: 0.285  loss_giou_8: 0.7313  loss_ce_dn_8: 0.1451  loss_mask_dn_8: 0.2278  loss_dice_dn_8: 0.7179  loss_bbox_dn_8: 0.2071  loss_giou_dn_8: 0.5819  loss_ce_interm: 1.488  loss_mask_interm: 0.2628  loss_dice_interm: 0.7945  loss_bbox_interm: 0.4093  loss_giou_interm: 1.023    time: 0.8730  last_time: 0.8828  data_time: 0.0121  last_data_time: 0.0107   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:47:20 d2.utils.events]: \u001b[0m eta: 3 days, 17:03:12  iter: 1239  total_loss: 62.05  loss_ce: 0.7547  loss_mask: 0.2938  loss_dice: 0.7303  loss_bbox: 0.262  loss_giou: 0.6343  loss_ce_dn: 0.1219  loss_mask_dn: 0.2764  loss_dice_dn: 0.7379  loss_bbox_dn: 0.2636  loss_giou_dn: 0.5051  loss_ce_0: 1.375  loss_mask_0: 0.299  loss_dice_0: 0.8614  loss_bbox_0: 0.4195  loss_giou_0: 0.8481  loss_ce_dn_0: 0.7171  loss_mask_dn_0: 0.9563  loss_dice_dn_0: 2.782  loss_bbox_dn_0: 0.718  loss_giou_dn_0: 0.8875  loss_ce_1: 1.329  loss_mask_1: 0.2959  loss_dice_1: 0.7659  loss_bbox_1: 0.345  loss_giou_1: 0.7387  loss_ce_dn_1: 0.2409  loss_mask_dn_1: 0.3451  loss_dice_dn_1: 0.8703  loss_bbox_dn_1: 0.3755  loss_giou_dn_1: 0.6419  loss_ce_2: 1.13  loss_mask_2: 0.2744  loss_dice_2: 0.7564  loss_bbox_2: 0.2953  loss_giou_2: 0.7054  loss_ce_dn_2: 0.1849  loss_mask_dn_2: 0.3032  loss_dice_dn_2: 0.7896  loss_bbox_dn_2: 0.32  loss_giou_dn_2: 0.56  loss_ce_3: 0.9544  loss_mask_3: 0.2875  loss_dice_3: 0.7279  loss_bbox_3: 0.2991  loss_giou_3: 0.6806  loss_ce_dn_3: 0.1568  loss_mask_dn_3: 0.292  loss_dice_dn_3: 0.7723  loss_bbox_dn_3: 0.2869  loss_giou_dn_3: 0.5388  loss_ce_4: 0.8524  loss_mask_4: 0.2834  loss_dice_4: 0.6858  loss_bbox_4: 0.262  loss_giou_4: 0.653  loss_ce_dn_4: 0.1314  loss_mask_dn_4: 0.2752  loss_dice_dn_4: 0.7415  loss_bbox_dn_4: 0.2846  loss_giou_dn_4: 0.5192  loss_ce_5: 0.8032  loss_mask_5: 0.2799  loss_dice_5: 0.6918  loss_bbox_5: 0.2518  loss_giou_5: 0.6626  loss_ce_dn_5: 0.1201  loss_mask_dn_5: 0.2779  loss_dice_dn_5: 0.7317  loss_bbox_dn_5: 0.2744  loss_giou_dn_5: 0.5233  loss_ce_6: 0.7726  loss_mask_6: 0.2826  loss_dice_6: 0.694  loss_bbox_6: 0.2603  loss_giou_6: 0.631  loss_ce_dn_6: 0.1259  loss_mask_dn_6: 0.2782  loss_dice_dn_6: 0.7167  loss_bbox_dn_6: 0.268  loss_giou_dn_6: 0.5152  loss_ce_7: 0.7797  loss_mask_7: 0.2921  loss_dice_7: 0.6982  loss_bbox_7: 0.2486  loss_giou_7: 0.6093  loss_ce_dn_7: 0.1256  loss_mask_dn_7: 0.271  loss_dice_dn_7: 0.7156  loss_bbox_dn_7: 0.2689  loss_giou_dn_7: 0.511  loss_ce_8: 0.8225  loss_mask_8: 0.2914  loss_dice_8: 0.7113  loss_bbox_8: 0.2451  loss_giou_8: 0.6132  loss_ce_dn_8: 0.1215  loss_mask_dn_8: 0.2729  loss_dice_dn_8: 0.7093  loss_bbox_dn_8: 0.2622  loss_giou_dn_8: 0.5054  loss_ce_interm: 1.355  loss_mask_interm: 0.307  loss_dice_interm: 0.8246  loss_bbox_interm: 0.4214  loss_giou_interm: 0.847    time: 0.8729  last_time: 0.8968  data_time: 0.0108  last_data_time: 0.0137   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:47:37 d2.utils.events]: \u001b[0m eta: 3 days, 17:03:49  iter: 1259  total_loss: 58.89  loss_ce: 0.7318  loss_mask: 0.1914  loss_dice: 0.8962  loss_bbox: 0.2661  loss_giou: 0.564  loss_ce_dn: 0.1074  loss_mask_dn: 0.1703  loss_dice_dn: 0.7755  loss_bbox_dn: 0.1857  loss_giou_dn: 0.4551  loss_ce_0: 1.4  loss_mask_0: 0.2203  loss_dice_0: 0.9327  loss_bbox_0: 0.4273  loss_giou_0: 0.8089  loss_ce_dn_0: 0.7856  loss_mask_dn_0: 0.7807  loss_dice_dn_0: 2.915  loss_bbox_dn_0: 0.5205  loss_giou_dn_0: 0.8593  loss_ce_1: 1.348  loss_mask_1: 0.1804  loss_dice_1: 0.8868  loss_bbox_1: 0.3137  loss_giou_1: 0.6441  loss_ce_dn_1: 0.247  loss_mask_dn_1: 0.2041  loss_dice_dn_1: 0.8969  loss_bbox_dn_1: 0.3133  loss_giou_dn_1: 0.6082  loss_ce_2: 1.159  loss_mask_2: 0.2145  loss_dice_2: 0.8649  loss_bbox_2: 0.2689  loss_giou_2: 0.6184  loss_ce_dn_2: 0.1801  loss_mask_dn_2: 0.1856  loss_dice_dn_2: 0.851  loss_bbox_dn_2: 0.2262  loss_giou_dn_2: 0.532  loss_ce_3: 0.9489  loss_mask_3: 0.2055  loss_dice_3: 0.8494  loss_bbox_3: 0.2557  loss_giou_3: 0.641  loss_ce_dn_3: 0.1576  loss_mask_dn_3: 0.1865  loss_dice_dn_3: 0.8237  loss_bbox_dn_3: 0.2161  loss_giou_dn_3: 0.4997  loss_ce_4: 0.8891  loss_mask_4: 0.199  loss_dice_4: 0.7953  loss_bbox_4: 0.2632  loss_giou_4: 0.6232  loss_ce_dn_4: 0.1452  loss_mask_dn_4: 0.1846  loss_dice_dn_4: 0.7908  loss_bbox_dn_4: 0.2094  loss_giou_dn_4: 0.4776  loss_ce_5: 0.8125  loss_mask_5: 0.1988  loss_dice_5: 0.8803  loss_bbox_5: 0.2703  loss_giou_5: 0.6135  loss_ce_dn_5: 0.1278  loss_mask_dn_5: 0.1816  loss_dice_dn_5: 0.7913  loss_bbox_dn_5: 0.2058  loss_giou_dn_5: 0.469  loss_ce_6: 0.7884  loss_mask_6: 0.2044  loss_dice_6: 0.8309  loss_bbox_6: 0.264  loss_giou_6: 0.6098  loss_ce_dn_6: 0.1204  loss_mask_dn_6: 0.1739  loss_dice_dn_6: 0.7847  loss_bbox_dn_6: 0.2  loss_giou_dn_6: 0.4596  loss_ce_7: 0.7707  loss_mask_7: 0.1849  loss_dice_7: 0.9038  loss_bbox_7: 0.2584  loss_giou_7: 0.5929  loss_ce_dn_7: 0.1209  loss_mask_dn_7: 0.1716  loss_dice_dn_7: 0.7845  loss_bbox_dn_7: 0.1895  loss_giou_dn_7: 0.4552  loss_ce_8: 0.7715  loss_mask_8: 0.1807  loss_dice_8: 0.8729  loss_bbox_8: 0.2594  loss_giou_8: 0.606  loss_ce_dn_8: 0.1114  loss_mask_dn_8: 0.1718  loss_dice_dn_8: 0.7769  loss_bbox_dn_8: 0.1861  loss_giou_dn_8: 0.4536  loss_ce_interm: 1.429  loss_mask_interm: 0.2199  loss_dice_interm: 0.9494  loss_bbox_interm: 0.4408  loss_giou_interm: 0.8151    time: 0.8731  last_time: 0.8535  data_time: 0.0131  last_data_time: 0.0127   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:47:55 d2.utils.events]: \u001b[0m eta: 3 days, 17:03:43  iter: 1279  total_loss: 63.82  loss_ce: 0.8068  loss_mask: 0.2751  loss_dice: 0.7173  loss_bbox: 0.2548  loss_giou: 0.7364  loss_ce_dn: 0.1547  loss_mask_dn: 0.2625  loss_dice_dn: 0.6587  loss_bbox_dn: 0.2449  loss_giou_dn: 0.5791  loss_ce_0: 1.51  loss_mask_0: 0.2933  loss_dice_0: 0.7437  loss_bbox_0: 0.3856  loss_giou_0: 0.8881  loss_ce_dn_0: 0.7657  loss_mask_dn_0: 1.069  loss_dice_dn_0: 2.767  loss_bbox_dn_0: 0.6674  loss_giou_dn_0: 0.9132  loss_ce_1: 1.394  loss_mask_1: 0.277  loss_dice_1: 0.7381  loss_bbox_1: 0.309  loss_giou_1: 0.7957  loss_ce_dn_1: 0.2759  loss_mask_dn_1: 0.2979  loss_dice_dn_1: 0.784  loss_bbox_dn_1: 0.3672  loss_giou_dn_1: 0.6847  loss_ce_2: 1.2  loss_mask_2: 0.2525  loss_dice_2: 0.7466  loss_bbox_2: 0.295  loss_giou_2: 0.7111  loss_ce_dn_2: 0.2207  loss_mask_dn_2: 0.2663  loss_dice_dn_2: 0.722  loss_bbox_dn_2: 0.3008  loss_giou_dn_2: 0.634  loss_ce_3: 1.025  loss_mask_3: 0.2697  loss_dice_3: 0.6797  loss_bbox_3: 0.2973  loss_giou_3: 0.7317  loss_ce_dn_3: 0.1788  loss_mask_dn_3: 0.2599  loss_dice_dn_3: 0.6764  loss_bbox_dn_3: 0.284  loss_giou_dn_3: 0.6165  loss_ce_4: 0.9082  loss_mask_4: 0.2848  loss_dice_4: 0.7363  loss_bbox_4: 0.2542  loss_giou_4: 0.7115  loss_ce_dn_4: 0.1639  loss_mask_dn_4: 0.2544  loss_dice_dn_4: 0.6769  loss_bbox_dn_4: 0.2686  loss_giou_dn_4: 0.5965  loss_ce_5: 0.861  loss_mask_5: 0.2737  loss_dice_5: 0.7032  loss_bbox_5: 0.272  loss_giou_5: 0.7095  loss_ce_dn_5: 0.1597  loss_mask_dn_5: 0.2539  loss_dice_dn_5: 0.6776  loss_bbox_dn_5: 0.2555  loss_giou_dn_5: 0.6001  loss_ce_6: 0.8421  loss_mask_6: 0.2718  loss_dice_6: 0.6897  loss_bbox_6: 0.2678  loss_giou_6: 0.6841  loss_ce_dn_6: 0.1526  loss_mask_dn_6: 0.2525  loss_dice_dn_6: 0.6648  loss_bbox_dn_6: 0.25  loss_giou_dn_6: 0.5784  loss_ce_7: 0.8221  loss_mask_7: 0.2796  loss_dice_7: 0.6787  loss_bbox_7: 0.2557  loss_giou_7: 0.6828  loss_ce_dn_7: 0.1471  loss_mask_dn_7: 0.2615  loss_dice_dn_7: 0.6565  loss_bbox_dn_7: 0.2506  loss_giou_dn_7: 0.5741  loss_ce_8: 0.847  loss_mask_8: 0.2671  loss_dice_8: 0.688  loss_bbox_8: 0.2545  loss_giou_8: 0.6705  loss_ce_dn_8: 0.1498  loss_mask_dn_8: 0.2596  loss_dice_dn_8: 0.6516  loss_bbox_dn_8: 0.2447  loss_giou_dn_8: 0.5784  loss_ce_interm: 1.51  loss_mask_interm: 0.2842  loss_dice_interm: 0.7192  loss_bbox_interm: 0.3856  loss_giou_interm: 0.8825    time: 0.8730  last_time: 0.8694  data_time: 0.0120  last_data_time: 0.0088   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:48:13 d2.utils.events]: \u001b[0m eta: 3 days, 17:04:22  iter: 1299  total_loss: 66.92  loss_ce: 0.9345  loss_mask: 0.2962  loss_dice: 0.7588  loss_bbox: 0.2536  loss_giou: 0.8287  loss_ce_dn: 0.1358  loss_mask_dn: 0.2611  loss_dice_dn: 0.8311  loss_bbox_dn: 0.2466  loss_giou_dn: 0.6603  loss_ce_0: 1.445  loss_mask_0: 0.3111  loss_dice_0: 0.8207  loss_bbox_0: 0.4139  loss_giou_0: 1.047  loss_ce_dn_0: 0.7487  loss_mask_dn_0: 0.78  loss_dice_dn_0: 3.165  loss_bbox_dn_0: 0.55  loss_giou_dn_0: 1.019  loss_ce_1: 1.401  loss_mask_1: 0.3141  loss_dice_1: 0.801  loss_bbox_1: 0.3258  loss_giou_1: 0.9267  loss_ce_dn_1: 0.2734  loss_mask_dn_1: 0.3266  loss_dice_dn_1: 0.9061  loss_bbox_dn_1: 0.3341  loss_giou_dn_1: 0.7695  loss_ce_2: 1.205  loss_mask_2: 0.3153  loss_dice_2: 0.7928  loss_bbox_2: 0.3257  loss_giou_2: 0.8949  loss_ce_dn_2: 0.2113  loss_mask_dn_2: 0.3073  loss_dice_dn_2: 0.8358  loss_bbox_dn_2: 0.2842  loss_giou_dn_2: 0.7144  loss_ce_3: 1.105  loss_mask_3: 0.3088  loss_dice_3: 0.7923  loss_bbox_3: 0.3047  loss_giou_3: 0.8833  loss_ce_dn_3: 0.1924  loss_mask_dn_3: 0.287  loss_dice_dn_3: 0.814  loss_bbox_dn_3: 0.2782  loss_giou_dn_3: 0.6948  loss_ce_4: 1.005  loss_mask_4: 0.3037  loss_dice_4: 0.8386  loss_bbox_4: 0.3029  loss_giou_4: 0.8681  loss_ce_dn_4: 0.1691  loss_mask_dn_4: 0.2783  loss_dice_dn_4: 0.8242  loss_bbox_dn_4: 0.2633  loss_giou_dn_4: 0.6785  loss_ce_5: 0.9299  loss_mask_5: 0.2992  loss_dice_5: 0.8413  loss_bbox_5: 0.299  loss_giou_5: 0.8645  loss_ce_dn_5: 0.1565  loss_mask_dn_5: 0.2657  loss_dice_dn_5: 0.7945  loss_bbox_dn_5: 0.2576  loss_giou_dn_5: 0.6772  loss_ce_6: 0.9178  loss_mask_6: 0.2897  loss_dice_6: 0.8271  loss_bbox_6: 0.2788  loss_giou_6: 0.8717  loss_ce_dn_6: 0.1469  loss_mask_dn_6: 0.2636  loss_dice_dn_6: 0.7964  loss_bbox_dn_6: 0.252  loss_giou_dn_6: 0.6701  loss_ce_7: 0.9478  loss_mask_7: 0.2977  loss_dice_7: 0.8144  loss_bbox_7: 0.2876  loss_giou_7: 0.8545  loss_ce_dn_7: 0.1436  loss_mask_dn_7: 0.2595  loss_dice_dn_7: 0.8112  loss_bbox_dn_7: 0.2499  loss_giou_dn_7: 0.6673  loss_ce_8: 0.9436  loss_mask_8: 0.2868  loss_dice_8: 0.7983  loss_bbox_8: 0.285  loss_giou_8: 0.8295  loss_ce_dn_8: 0.1405  loss_mask_dn_8: 0.2596  loss_dice_dn_8: 0.7932  loss_bbox_dn_8: 0.2456  loss_giou_dn_8: 0.661  loss_ce_interm: 1.464  loss_mask_interm: 0.3171  loss_dice_interm: 0.87  loss_bbox_interm: 0.4189  loss_giou_interm: 1.038    time: 0.8731  last_time: 0.9176  data_time: 0.0127  last_data_time: 0.0194   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:48:30 d2.utils.events]: \u001b[0m eta: 3 days, 17:02:45  iter: 1319  total_loss: 56.65  loss_ce: 0.7962  loss_mask: 0.234  loss_dice: 0.6728  loss_bbox: 0.2492  loss_giou: 0.6344  loss_ce_dn: 0.08334  loss_mask_dn: 0.224  loss_dice_dn: 0.6005  loss_bbox_dn: 0.2409  loss_giou_dn: 0.6028  loss_ce_0: 1.517  loss_mask_0: 0.2425  loss_dice_0: 0.7128  loss_bbox_0: 0.3845  loss_giou_0: 0.8993  loss_ce_dn_0: 0.7519  loss_mask_dn_0: 0.8482  loss_dice_dn_0: 2.971  loss_bbox_dn_0: 0.6151  loss_giou_dn_0: 0.9595  loss_ce_1: 1.416  loss_mask_1: 0.2523  loss_dice_1: 0.6851  loss_bbox_1: 0.3072  loss_giou_1: 0.6914  loss_ce_dn_1: 0.2479  loss_mask_dn_1: 0.3192  loss_dice_dn_1: 0.7062  loss_bbox_dn_1: 0.3448  loss_giou_dn_1: 0.7348  loss_ce_2: 1.262  loss_mask_2: 0.2393  loss_dice_2: 0.6672  loss_bbox_2: 0.2698  loss_giou_2: 0.6978  loss_ce_dn_2: 0.1903  loss_mask_dn_2: 0.2699  loss_dice_dn_2: 0.6468  loss_bbox_dn_2: 0.2788  loss_giou_dn_2: 0.6672  loss_ce_3: 1.038  loss_mask_3: 0.2463  loss_dice_3: 0.6649  loss_bbox_3: 0.2704  loss_giou_3: 0.669  loss_ce_dn_3: 0.1449  loss_mask_dn_3: 0.2612  loss_dice_dn_3: 0.6083  loss_bbox_dn_3: 0.2624  loss_giou_dn_3: 0.6445  loss_ce_4: 0.9958  loss_mask_4: 0.235  loss_dice_4: 0.6438  loss_bbox_4: 0.2621  loss_giou_4: 0.6603  loss_ce_dn_4: 0.1255  loss_mask_dn_4: 0.2394  loss_dice_dn_4: 0.6204  loss_bbox_dn_4: 0.2554  loss_giou_dn_4: 0.6251  loss_ce_5: 0.9112  loss_mask_5: 0.2381  loss_dice_5: 0.6717  loss_bbox_5: 0.2493  loss_giou_5: 0.6463  loss_ce_dn_5: 0.112  loss_mask_dn_5: 0.2361  loss_dice_dn_5: 0.5997  loss_bbox_dn_5: 0.2469  loss_giou_dn_5: 0.6189  loss_ce_6: 0.8694  loss_mask_6: 0.2312  loss_dice_6: 0.6952  loss_bbox_6: 0.2394  loss_giou_6: 0.6401  loss_ce_dn_6: 0.1056  loss_mask_dn_6: 0.2319  loss_dice_dn_6: 0.6139  loss_bbox_dn_6: 0.2425  loss_giou_dn_6: 0.6112  loss_ce_7: 0.8155  loss_mask_7: 0.2402  loss_dice_7: 0.6546  loss_bbox_7: 0.2502  loss_giou_7: 0.6517  loss_ce_dn_7: 0.0942  loss_mask_dn_7: 0.2257  loss_dice_dn_7: 0.5947  loss_bbox_dn_7: 0.2449  loss_giou_dn_7: 0.6061  loss_ce_8: 0.7624  loss_mask_8: 0.2413  loss_dice_8: 0.6547  loss_bbox_8: 0.2492  loss_giou_8: 0.6339  loss_ce_dn_8: 0.08673  loss_mask_dn_8: 0.2308  loss_dice_dn_8: 0.623  loss_bbox_dn_8: 0.2418  loss_giou_dn_8: 0.601  loss_ce_interm: 1.518  loss_mask_interm: 0.2428  loss_dice_interm: 0.7311  loss_bbox_interm: 0.3799  loss_giou_interm: 0.8923    time: 0.8731  last_time: 0.8518  data_time: 0.0126  last_data_time: 0.0083   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:48:48 d2.utils.events]: \u001b[0m eta: 3 days, 17:02:52  iter: 1339  total_loss: 63.7  loss_ce: 0.909  loss_mask: 0.2396  loss_dice: 0.6742  loss_bbox: 0.2529  loss_giou: 0.7229  loss_ce_dn: 0.1554  loss_mask_dn: 0.2479  loss_dice_dn: 0.7449  loss_bbox_dn: 0.2058  loss_giou_dn: 0.6  loss_ce_0: 1.607  loss_mask_0: 0.263  loss_dice_0: 0.7818  loss_bbox_0: 0.3914  loss_giou_0: 0.9309  loss_ce_dn_0: 0.7133  loss_mask_dn_0: 0.746  loss_dice_dn_0: 3.254  loss_bbox_dn_0: 0.5568  loss_giou_dn_0: 0.9985  loss_ce_1: 1.482  loss_mask_1: 0.2543  loss_dice_1: 0.8193  loss_bbox_1: 0.2752  loss_giou_1: 0.8218  loss_ce_dn_1: 0.2851  loss_mask_dn_1: 0.2896  loss_dice_dn_1: 0.8612  loss_bbox_dn_1: 0.307  loss_giou_dn_1: 0.748  loss_ce_2: 1.279  loss_mask_2: 0.2461  loss_dice_2: 0.7303  loss_bbox_2: 0.2526  loss_giou_2: 0.7906  loss_ce_dn_2: 0.2354  loss_mask_dn_2: 0.2858  loss_dice_dn_2: 0.8167  loss_bbox_dn_2: 0.26  loss_giou_dn_2: 0.6803  loss_ce_3: 1.12  loss_mask_3: 0.2368  loss_dice_3: 0.7415  loss_bbox_3: 0.2762  loss_giou_3: 0.7631  loss_ce_dn_3: 0.2071  loss_mask_dn_3: 0.2748  loss_dice_dn_3: 0.7778  loss_bbox_dn_3: 0.2387  loss_giou_dn_3: 0.6432  loss_ce_4: 1.041  loss_mask_4: 0.2296  loss_dice_4: 0.7179  loss_bbox_4: 0.2794  loss_giou_4: 0.7553  loss_ce_dn_4: 0.1854  loss_mask_dn_4: 0.2623  loss_dice_dn_4: 0.7779  loss_bbox_dn_4: 0.2238  loss_giou_dn_4: 0.6237  loss_ce_5: 0.9652  loss_mask_5: 0.2299  loss_dice_5: 0.7541  loss_bbox_5: 0.2729  loss_giou_5: 0.7236  loss_ce_dn_5: 0.1731  loss_mask_dn_5: 0.2601  loss_dice_dn_5: 0.7804  loss_bbox_dn_5: 0.2191  loss_giou_dn_5: 0.6126  loss_ce_6: 0.9256  loss_mask_6: 0.2296  loss_dice_6: 0.7452  loss_bbox_6: 0.2643  loss_giou_6: 0.7212  loss_ce_dn_6: 0.1677  loss_mask_dn_6: 0.2482  loss_dice_dn_6: 0.7575  loss_bbox_dn_6: 0.2135  loss_giou_dn_6: 0.6013  loss_ce_7: 0.9481  loss_mask_7: 0.241  loss_dice_7: 0.7655  loss_bbox_7: 0.2512  loss_giou_7: 0.7207  loss_ce_dn_7: 0.1609  loss_mask_dn_7: 0.2539  loss_dice_dn_7: 0.7548  loss_bbox_dn_7: 0.21  loss_giou_dn_7: 0.5959  loss_ce_8: 0.9066  loss_mask_8: 0.235  loss_dice_8: 0.758  loss_bbox_8: 0.2613  loss_giou_8: 0.7165  loss_ce_dn_8: 0.1569  loss_mask_dn_8: 0.2519  loss_dice_dn_8: 0.7405  loss_bbox_dn_8: 0.2069  loss_giou_dn_8: 0.5939  loss_ce_interm: 1.609  loss_mask_interm: 0.2656  loss_dice_interm: 0.7704  loss_bbox_interm: 0.374  loss_giou_interm: 0.9486    time: 0.8731  last_time: 0.8926  data_time: 0.0122  last_data_time: 0.0208   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:49:05 d2.utils.events]: \u001b[0m eta: 3 days, 17:03:29  iter: 1359  total_loss: 69.2  loss_ce: 0.8477  loss_mask: 0.3863  loss_dice: 0.7806  loss_bbox: 0.2942  loss_giou: 0.7797  loss_ce_dn: 0.1314  loss_mask_dn: 0.3053  loss_dice_dn: 0.7474  loss_bbox_dn: 0.2819  loss_giou_dn: 0.6226  loss_ce_0: 1.537  loss_mask_0: 0.3761  loss_dice_0: 0.8472  loss_bbox_0: 0.4143  loss_giou_0: 0.9435  loss_ce_dn_0: 0.7003  loss_mask_dn_0: 0.9653  loss_dice_dn_0: 3.197  loss_bbox_dn_0: 0.7364  loss_giou_dn_0: 0.9512  loss_ce_1: 1.423  loss_mask_1: 0.3846  loss_dice_1: 0.7791  loss_bbox_1: 0.3567  loss_giou_1: 0.8231  loss_ce_dn_1: 0.2356  loss_mask_dn_1: 0.3642  loss_dice_dn_1: 0.9115  loss_bbox_dn_1: 0.3828  loss_giou_dn_1: 0.73  loss_ce_2: 1.31  loss_mask_2: 0.4054  loss_dice_2: 0.7868  loss_bbox_2: 0.3047  loss_giou_2: 0.7917  loss_ce_dn_2: 0.1948  loss_mask_dn_2: 0.3219  loss_dice_dn_2: 0.8364  loss_bbox_dn_2: 0.3258  loss_giou_dn_2: 0.6636  loss_ce_3: 1.156  loss_mask_3: 0.383  loss_dice_3: 0.7797  loss_bbox_3: 0.297  loss_giou_3: 0.7848  loss_ce_dn_3: 0.1621  loss_mask_dn_3: 0.3103  loss_dice_dn_3: 0.776  loss_bbox_dn_3: 0.3117  loss_giou_dn_3: 0.638  loss_ce_4: 1.014  loss_mask_4: 0.3637  loss_dice_4: 0.7392  loss_bbox_4: 0.3086  loss_giou_4: 0.7793  loss_ce_dn_4: 0.1498  loss_mask_dn_4: 0.3099  loss_dice_dn_4: 0.7788  loss_bbox_dn_4: 0.3076  loss_giou_dn_4: 0.625  loss_ce_5: 0.9053  loss_mask_5: 0.3689  loss_dice_5: 0.7214  loss_bbox_5: 0.3026  loss_giou_5: 0.7902  loss_ce_dn_5: 0.1437  loss_mask_dn_5: 0.3155  loss_dice_dn_5: 0.7548  loss_bbox_dn_5: 0.3009  loss_giou_dn_5: 0.6243  loss_ce_6: 0.8624  loss_mask_6: 0.3857  loss_dice_6: 0.7622  loss_bbox_6: 0.2983  loss_giou_6: 0.7727  loss_ce_dn_6: 0.1364  loss_mask_dn_6: 0.332  loss_dice_dn_6: 0.7367  loss_bbox_dn_6: 0.2893  loss_giou_dn_6: 0.6181  loss_ce_7: 0.8663  loss_mask_7: 0.4068  loss_dice_7: 0.8064  loss_bbox_7: 0.2951  loss_giou_7: 0.7791  loss_ce_dn_7: 0.1368  loss_mask_dn_7: 0.3189  loss_dice_dn_7: 0.7338  loss_bbox_dn_7: 0.287  loss_giou_dn_7: 0.6181  loss_ce_8: 0.8744  loss_mask_8: 0.4067  loss_dice_8: 0.7802  loss_bbox_8: 0.2925  loss_giou_8: 0.7678  loss_ce_dn_8: 0.1358  loss_mask_dn_8: 0.3194  loss_dice_dn_8: 0.7508  loss_bbox_dn_8: 0.2833  loss_giou_dn_8: 0.6207  loss_ce_interm: 1.531  loss_mask_interm: 0.3822  loss_dice_interm: 0.8115  loss_bbox_interm: 0.4143  loss_giou_interm: 0.9382    time: 0.8732  last_time: 0.8478  data_time: 0.0130  last_data_time: 0.0130   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:49:23 d2.utils.events]: \u001b[0m eta: 3 days, 17:03:42  iter: 1379  total_loss: 59.12  loss_ce: 0.7566  loss_mask: 0.3044  loss_dice: 0.6411  loss_bbox: 0.2514  loss_giou: 0.6947  loss_ce_dn: 0.115  loss_mask_dn: 0.2808  loss_dice_dn: 0.6359  loss_bbox_dn: 0.2704  loss_giou_dn: 0.535  loss_ce_0: 1.505  loss_mask_0: 0.3186  loss_dice_0: 0.6745  loss_bbox_0: 0.4481  loss_giou_0: 0.8702  loss_ce_dn_0: 0.7384  loss_mask_dn_0: 0.792  loss_dice_dn_0: 2.554  loss_bbox_dn_0: 0.7144  loss_giou_dn_0: 0.983  loss_ce_1: 1.347  loss_mask_1: 0.3184  loss_dice_1: 0.7071  loss_bbox_1: 0.3151  loss_giou_1: 0.7608  loss_ce_dn_1: 0.2539  loss_mask_dn_1: 0.3603  loss_dice_dn_1: 0.7137  loss_bbox_dn_1: 0.4299  loss_giou_dn_1: 0.6717  loss_ce_2: 1.146  loss_mask_2: 0.2933  loss_dice_2: 0.6335  loss_bbox_2: 0.2678  loss_giou_2: 0.7077  loss_ce_dn_2: 0.2091  loss_mask_dn_2: 0.3187  loss_dice_dn_2: 0.6757  loss_bbox_dn_2: 0.3612  loss_giou_dn_2: 0.5874  loss_ce_3: 1.05  loss_mask_3: 0.2932  loss_dice_3: 0.6435  loss_bbox_3: 0.2695  loss_giou_3: 0.7241  loss_ce_dn_3: 0.172  loss_mask_dn_3: 0.2926  loss_dice_dn_3: 0.6389  loss_bbox_dn_3: 0.3212  loss_giou_dn_3: 0.5661  loss_ce_4: 0.9234  loss_mask_4: 0.2866  loss_dice_4: 0.6155  loss_bbox_4: 0.2853  loss_giou_4: 0.731  loss_ce_dn_4: 0.1484  loss_mask_dn_4: 0.287  loss_dice_dn_4: 0.6251  loss_bbox_dn_4: 0.3029  loss_giou_dn_4: 0.5481  loss_ce_5: 0.8194  loss_mask_5: 0.2927  loss_dice_5: 0.6174  loss_bbox_5: 0.2775  loss_giou_5: 0.7029  loss_ce_dn_5: 0.1329  loss_mask_dn_5: 0.291  loss_dice_dn_5: 0.6224  loss_bbox_dn_5: 0.2952  loss_giou_dn_5: 0.5464  loss_ce_6: 0.7808  loss_mask_6: 0.3022  loss_dice_6: 0.614  loss_bbox_6: 0.2614  loss_giou_6: 0.6898  loss_ce_dn_6: 0.1257  loss_mask_dn_6: 0.288  loss_dice_dn_6: 0.6281  loss_bbox_dn_6: 0.2856  loss_giou_dn_6: 0.5386  loss_ce_7: 0.7714  loss_mask_7: 0.3037  loss_dice_7: 0.5993  loss_bbox_7: 0.2546  loss_giou_7: 0.7355  loss_ce_dn_7: 0.1224  loss_mask_dn_7: 0.2812  loss_dice_dn_7: 0.628  loss_bbox_dn_7: 0.2774  loss_giou_dn_7: 0.5343  loss_ce_8: 0.7587  loss_mask_8: 0.3061  loss_dice_8: 0.5992  loss_bbox_8: 0.2608  loss_giou_8: 0.6945  loss_ce_dn_8: 0.1254  loss_mask_dn_8: 0.2801  loss_dice_dn_8: 0.629  loss_bbox_dn_8: 0.271  loss_giou_dn_8: 0.533  loss_ce_interm: 1.512  loss_mask_interm: 0.3153  loss_dice_interm: 0.658  loss_bbox_interm: 0.4481  loss_giou_interm: 0.8648    time: 0.8732  last_time: 0.8776  data_time: 0.0118  last_data_time: 0.0189   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:49:40 d2.utils.events]: \u001b[0m eta: 3 days, 17:01:47  iter: 1399  total_loss: 63.36  loss_ce: 0.6492  loss_mask: 0.3744  loss_dice: 0.7855  loss_bbox: 0.3556  loss_giou: 0.6119  loss_ce_dn: 0.1217  loss_mask_dn: 0.3179  loss_dice_dn: 0.768  loss_bbox_dn: 0.2957  loss_giou_dn: 0.5039  loss_ce_0: 1.443  loss_mask_0: 0.406  loss_dice_0: 0.8606  loss_bbox_0: 0.5336  loss_giou_0: 0.8496  loss_ce_dn_0: 0.7403  loss_mask_dn_0: 1.066  loss_dice_dn_0: 3.035  loss_bbox_dn_0: 0.7528  loss_giou_dn_0: 0.9071  loss_ce_1: 1.291  loss_mask_1: 0.335  loss_dice_1: 0.8601  loss_bbox_1: 0.3848  loss_giou_1: 0.711  loss_ce_dn_1: 0.2149  loss_mask_dn_1: 0.3804  loss_dice_dn_1: 0.9289  loss_bbox_dn_1: 0.4388  loss_giou_dn_1: 0.6465  loss_ce_2: 1.055  loss_mask_2: 0.378  loss_dice_2: 0.8342  loss_bbox_2: 0.4046  loss_giou_2: 0.6735  loss_ce_dn_2: 0.1747  loss_mask_dn_2: 0.3349  loss_dice_dn_2: 0.8406  loss_bbox_dn_2: 0.3619  loss_giou_dn_2: 0.5745  loss_ce_3: 0.8823  loss_mask_3: 0.3499  loss_dice_3: 0.7214  loss_bbox_3: 0.4206  loss_giou_3: 0.6582  loss_ce_dn_3: 0.1541  loss_mask_dn_3: 0.3337  loss_dice_dn_3: 0.7953  loss_bbox_dn_3: 0.3323  loss_giou_dn_3: 0.5566  loss_ce_4: 0.8124  loss_mask_4: 0.3508  loss_dice_4: 0.7653  loss_bbox_4: 0.3997  loss_giou_4: 0.6511  loss_ce_dn_4: 0.1422  loss_mask_dn_4: 0.316  loss_dice_dn_4: 0.7892  loss_bbox_dn_4: 0.3147  loss_giou_dn_4: 0.5251  loss_ce_5: 0.7119  loss_mask_5: 0.3698  loss_dice_5: 0.7854  loss_bbox_5: 0.3753  loss_giou_5: 0.6381  loss_ce_dn_5: 0.1361  loss_mask_dn_5: 0.3235  loss_dice_dn_5: 0.7754  loss_bbox_dn_5: 0.3038  loss_giou_dn_5: 0.5188  loss_ce_6: 0.6995  loss_mask_6: 0.3657  loss_dice_6: 0.8024  loss_bbox_6: 0.3513  loss_giou_6: 0.6326  loss_ce_dn_6: 0.133  loss_mask_dn_6: 0.3225  loss_dice_dn_6: 0.7855  loss_bbox_dn_6: 0.296  loss_giou_dn_6: 0.5141  loss_ce_7: 0.6413  loss_mask_7: 0.3744  loss_dice_7: 0.8067  loss_bbox_7: 0.3558  loss_giou_7: 0.6213  loss_ce_dn_7: 0.1239  loss_mask_dn_7: 0.3187  loss_dice_dn_7: 0.7888  loss_bbox_dn_7: 0.291  loss_giou_dn_7: 0.5081  loss_ce_8: 0.6454  loss_mask_8: 0.3588  loss_dice_8: 0.8151  loss_bbox_8: 0.3559  loss_giou_8: 0.62  loss_ce_dn_8: 0.1223  loss_mask_dn_8: 0.3167  loss_dice_dn_8: 0.7964  loss_bbox_dn_8: 0.2957  loss_giou_dn_8: 0.5056  loss_ce_interm: 1.443  loss_mask_interm: 0.3982  loss_dice_interm: 0.827  loss_bbox_interm: 0.5222  loss_giou_interm: 0.8445    time: 0.8731  last_time: 0.8763  data_time: 0.0118  last_data_time: 0.0142   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:49:58 d2.utils.events]: \u001b[0m eta: 3 days, 17:01:17  iter: 1419  total_loss: 60.73  loss_ce: 0.8058  loss_mask: 0.1938  loss_dice: 0.7914  loss_bbox: 0.2214  loss_giou: 0.7104  loss_ce_dn: 0.1284  loss_mask_dn: 0.176  loss_dice_dn: 0.7554  loss_bbox_dn: 0.2004  loss_giou_dn: 0.6013  loss_ce_0: 1.419  loss_mask_0: 0.2066  loss_dice_0: 0.8105  loss_bbox_0: 0.3809  loss_giou_0: 0.8942  loss_ce_dn_0: 0.7516  loss_mask_dn_0: 0.7923  loss_dice_dn_0: 3.2  loss_bbox_dn_0: 0.5732  loss_giou_dn_0: 0.9752  loss_ce_1: 1.334  loss_mask_1: 0.199  loss_dice_1: 0.7836  loss_bbox_1: 0.2736  loss_giou_1: 0.7842  loss_ce_dn_1: 0.2807  loss_mask_dn_1: 0.2411  loss_dice_dn_1: 0.908  loss_bbox_dn_1: 0.3025  loss_giou_dn_1: 0.7263  loss_ce_2: 1.189  loss_mask_2: 0.1913  loss_dice_2: 0.8486  loss_bbox_2: 0.2325  loss_giou_2: 0.72  loss_ce_dn_2: 0.2172  loss_mask_dn_2: 0.193  loss_dice_dn_2: 0.8071  loss_bbox_dn_2: 0.2429  loss_giou_dn_2: 0.6647  loss_ce_3: 1.067  loss_mask_3: 0.1753  loss_dice_3: 0.8315  loss_bbox_3: 0.2378  loss_giou_3: 0.7332  loss_ce_dn_3: 0.1778  loss_mask_dn_3: 0.195  loss_dice_dn_3: 0.7816  loss_bbox_dn_3: 0.218  loss_giou_dn_3: 0.644  loss_ce_4: 0.9781  loss_mask_4: 0.17  loss_dice_4: 0.8462  loss_bbox_4: 0.219  loss_giou_4: 0.7105  loss_ce_dn_4: 0.1612  loss_mask_dn_4: 0.1982  loss_dice_dn_4: 0.7786  loss_bbox_dn_4: 0.2098  loss_giou_dn_4: 0.6255  loss_ce_5: 0.9385  loss_mask_5: 0.1661  loss_dice_5: 0.8443  loss_bbox_5: 0.2116  loss_giou_5: 0.7229  loss_ce_dn_5: 0.1514  loss_mask_dn_5: 0.1914  loss_dice_dn_5: 0.757  loss_bbox_dn_5: 0.2071  loss_giou_dn_5: 0.6272  loss_ce_6: 0.8768  loss_mask_6: 0.1662  loss_dice_6: 0.8141  loss_bbox_6: 0.231  loss_giou_6: 0.6918  loss_ce_dn_6: 0.1368  loss_mask_dn_6: 0.1949  loss_dice_dn_6: 0.7557  loss_bbox_dn_6: 0.2036  loss_giou_dn_6: 0.6144  loss_ce_7: 0.8183  loss_mask_7: 0.1948  loss_dice_7: 0.8167  loss_bbox_7: 0.224  loss_giou_7: 0.7145  loss_ce_dn_7: 0.1313  loss_mask_dn_7: 0.1868  loss_dice_dn_7: 0.7591  loss_bbox_dn_7: 0.2005  loss_giou_dn_7: 0.6096  loss_ce_8: 0.8097  loss_mask_8: 0.1874  loss_dice_8: 0.8111  loss_bbox_8: 0.2194  loss_giou_8: 0.7072  loss_ce_dn_8: 0.1257  loss_mask_dn_8: 0.1772  loss_dice_dn_8: 0.7721  loss_bbox_dn_8: 0.2002  loss_giou_dn_8: 0.6017  loss_ce_interm: 1.404  loss_mask_interm: 0.2043  loss_dice_interm: 0.8028  loss_bbox_interm: 0.3841  loss_giou_interm: 0.9122    time: 0.8732  last_time: 0.9078  data_time: 0.0144  last_data_time: 0.0230   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:50:15 d2.utils.events]: \u001b[0m eta: 3 days, 17:01:12  iter: 1439  total_loss: 60.35  loss_ce: 0.6866  loss_mask: 0.2319  loss_dice: 0.8363  loss_bbox: 0.2261  loss_giou: 0.7671  loss_ce_dn: 0.1367  loss_mask_dn: 0.2418  loss_dice_dn: 0.7859  loss_bbox_dn: 0.1958  loss_giou_dn: 0.5434  loss_ce_0: 1.509  loss_mask_0: 0.2573  loss_dice_0: 0.8333  loss_bbox_0: 0.4046  loss_giou_0: 0.9259  loss_ce_dn_0: 0.7126  loss_mask_dn_0: 0.8513  loss_dice_dn_0: 3.159  loss_bbox_dn_0: 0.5349  loss_giou_dn_0: 0.8993  loss_ce_1: 1.411  loss_mask_1: 0.2575  loss_dice_1: 0.9348  loss_bbox_1: 0.2646  loss_giou_1: 0.8104  loss_ce_dn_1: 0.273  loss_mask_dn_1: 0.3405  loss_dice_dn_1: 0.9481  loss_bbox_dn_1: 0.3211  loss_giou_dn_1: 0.6662  loss_ce_2: 1.227  loss_mask_2: 0.2421  loss_dice_2: 0.8804  loss_bbox_2: 0.2342  loss_giou_2: 0.7676  loss_ce_dn_2: 0.2135  loss_mask_dn_2: 0.2768  loss_dice_dn_2: 0.8665  loss_bbox_dn_2: 0.2766  loss_giou_dn_2: 0.608  loss_ce_3: 1.044  loss_mask_3: 0.2496  loss_dice_3: 0.8676  loss_bbox_3: 0.237  loss_giou_3: 0.7646  loss_ce_dn_3: 0.1785  loss_mask_dn_3: 0.2572  loss_dice_dn_3: 0.8209  loss_bbox_dn_3: 0.239  loss_giou_dn_3: 0.5922  loss_ce_4: 0.9293  loss_mask_4: 0.2493  loss_dice_4: 0.89  loss_bbox_4: 0.2266  loss_giou_4: 0.7395  loss_ce_dn_4: 0.1596  loss_mask_dn_4: 0.2548  loss_dice_dn_4: 0.8018  loss_bbox_dn_4: 0.2307  loss_giou_dn_4: 0.571  loss_ce_5: 0.8495  loss_mask_5: 0.2343  loss_dice_5: 0.8801  loss_bbox_5: 0.2269  loss_giou_5: 0.7472  loss_ce_dn_5: 0.1538  loss_mask_dn_5: 0.246  loss_dice_dn_5: 0.7838  loss_bbox_dn_5: 0.2175  loss_giou_dn_5: 0.5693  loss_ce_6: 0.7889  loss_mask_6: 0.234  loss_dice_6: 0.8456  loss_bbox_6: 0.2242  loss_giou_6: 0.7517  loss_ce_dn_6: 0.1374  loss_mask_dn_6: 0.2385  loss_dice_dn_6: 0.7793  loss_bbox_dn_6: 0.2051  loss_giou_dn_6: 0.5516  loss_ce_7: 0.7407  loss_mask_7: 0.2291  loss_dice_7: 0.8949  loss_bbox_7: 0.2159  loss_giou_7: 0.7533  loss_ce_dn_7: 0.1381  loss_mask_dn_7: 0.2479  loss_dice_dn_7: 0.7796  loss_bbox_dn_7: 0.201  loss_giou_dn_7: 0.5503  loss_ce_8: 0.7246  loss_mask_8: 0.2378  loss_dice_8: 0.8859  loss_bbox_8: 0.2297  loss_giou_8: 0.7557  loss_ce_dn_8: 0.1365  loss_mask_dn_8: 0.2433  loss_dice_dn_8: 0.7759  loss_bbox_dn_8: 0.1979  loss_giou_dn_8: 0.5427  loss_ce_interm: 1.523  loss_mask_interm: 0.257  loss_dice_interm: 0.8549  loss_bbox_interm: 0.4043  loss_giou_interm: 0.9305    time: 0.8732  last_time: 0.8732  data_time: 0.0117  last_data_time: 0.0083   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:50:33 d2.utils.events]: \u001b[0m eta: 3 days, 17:00:25  iter: 1459  total_loss: 65.53  loss_ce: 0.9166  loss_mask: 0.252  loss_dice: 0.8299  loss_bbox: 0.2251  loss_giou: 0.7332  loss_ce_dn: 0.1331  loss_mask_dn: 0.2625  loss_dice_dn: 0.7872  loss_bbox_dn: 0.2337  loss_giou_dn: 0.6125  loss_ce_0: 1.502  loss_mask_0: 0.2977  loss_dice_0: 0.8774  loss_bbox_0: 0.3568  loss_giou_0: 0.9058  loss_ce_dn_0: 0.7197  loss_mask_dn_0: 0.8302  loss_dice_dn_0: 3.068  loss_bbox_dn_0: 0.5109  loss_giou_dn_0: 0.9799  loss_ce_1: 1.401  loss_mask_1: 0.2954  loss_dice_1: 0.7811  loss_bbox_1: 0.2523  loss_giou_1: 0.7794  loss_ce_dn_1: 0.2462  loss_mask_dn_1: 0.2874  loss_dice_dn_1: 0.8851  loss_bbox_dn_1: 0.3083  loss_giou_dn_1: 0.719  loss_ce_2: 1.211  loss_mask_2: 0.3136  loss_dice_2: 0.8637  loss_bbox_2: 0.2634  loss_giou_2: 0.7638  loss_ce_dn_2: 0.1915  loss_mask_dn_2: 0.2876  loss_dice_dn_2: 0.8212  loss_bbox_dn_2: 0.2574  loss_giou_dn_2: 0.6574  loss_ce_3: 1.106  loss_mask_3: 0.2909  loss_dice_3: 0.8759  loss_bbox_3: 0.2387  loss_giou_3: 0.7285  loss_ce_dn_3: 0.1628  loss_mask_dn_3: 0.2866  loss_dice_dn_3: 0.8346  loss_bbox_dn_3: 0.2528  loss_giou_dn_3: 0.6264  loss_ce_4: 1.039  loss_mask_4: 0.2732  loss_dice_4: 0.8842  loss_bbox_4: 0.2245  loss_giou_4: 0.7348  loss_ce_dn_4: 0.156  loss_mask_dn_4: 0.2796  loss_dice_dn_4: 0.7832  loss_bbox_dn_4: 0.2396  loss_giou_dn_4: 0.617  loss_ce_5: 0.9354  loss_mask_5: 0.2795  loss_dice_5: 0.8512  loss_bbox_5: 0.23  loss_giou_5: 0.7326  loss_ce_dn_5: 0.1509  loss_mask_dn_5: 0.2706  loss_dice_dn_5: 0.7908  loss_bbox_dn_5: 0.2318  loss_giou_dn_5: 0.6057  loss_ce_6: 0.8882  loss_mask_6: 0.2804  loss_dice_6: 0.8677  loss_bbox_6: 0.231  loss_giou_6: 0.7276  loss_ce_dn_6: 0.1483  loss_mask_dn_6: 0.2712  loss_dice_dn_6: 0.7974  loss_bbox_dn_6: 0.2337  loss_giou_dn_6: 0.6062  loss_ce_7: 0.8956  loss_mask_7: 0.2644  loss_dice_7: 0.854  loss_bbox_7: 0.2277  loss_giou_7: 0.7333  loss_ce_dn_7: 0.1419  loss_mask_dn_7: 0.2684  loss_dice_dn_7: 0.8039  loss_bbox_dn_7: 0.2338  loss_giou_dn_7: 0.6094  loss_ce_8: 0.8613  loss_mask_8: 0.2612  loss_dice_8: 0.8782  loss_bbox_8: 0.2292  loss_giou_8: 0.7444  loss_ce_dn_8: 0.1365  loss_mask_dn_8: 0.2717  loss_dice_dn_8: 0.7917  loss_bbox_dn_8: 0.2349  loss_giou_dn_8: 0.6081  loss_ce_interm: 1.498  loss_mask_interm: 0.2969  loss_dice_interm: 0.892  loss_bbox_interm: 0.3573  loss_giou_interm: 0.9053    time: 0.8732  last_time: 0.8759  data_time: 0.0121  last_data_time: 0.0098   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:50:50 d2.utils.events]: \u001b[0m eta: 3 days, 17:00:11  iter: 1479  total_loss: 68.75  loss_ce: 0.8392  loss_mask: 0.3087  loss_dice: 0.88  loss_bbox: 0.2643  loss_giou: 0.7677  loss_ce_dn: 0.1146  loss_mask_dn: 0.3165  loss_dice_dn: 0.7972  loss_bbox_dn: 0.2127  loss_giou_dn: 0.6615  loss_ce_0: 1.498  loss_mask_0: 0.3475  loss_dice_0: 0.8495  loss_bbox_0: 0.4131  loss_giou_0: 0.9991  loss_ce_dn_0: 0.7403  loss_mask_dn_0: 0.8794  loss_dice_dn_0: 3.145  loss_bbox_dn_0: 0.6511  loss_giou_dn_0: 1.007  loss_ce_1: 1.333  loss_mask_1: 0.3414  loss_dice_1: 0.8923  loss_bbox_1: 0.307  loss_giou_1: 0.8632  loss_ce_dn_1: 0.2346  loss_mask_dn_1: 0.3611  loss_dice_dn_1: 0.9034  loss_bbox_dn_1: 0.3459  loss_giou_dn_1: 0.7796  loss_ce_2: 1.141  loss_mask_2: 0.3498  loss_dice_2: 0.8874  loss_bbox_2: 0.2583  loss_giou_2: 0.8129  loss_ce_dn_2: 0.174  loss_mask_dn_2: 0.3397  loss_dice_dn_2: 0.8441  loss_bbox_dn_2: 0.2937  loss_giou_dn_2: 0.7252  loss_ce_3: 0.9961  loss_mask_3: 0.3413  loss_dice_3: 0.8753  loss_bbox_3: 0.2514  loss_giou_3: 0.807  loss_ce_dn_3: 0.1493  loss_mask_dn_3: 0.3215  loss_dice_dn_3: 0.82  loss_bbox_dn_3: 0.2684  loss_giou_dn_3: 0.6915  loss_ce_4: 0.927  loss_mask_4: 0.3177  loss_dice_4: 0.8405  loss_bbox_4: 0.2757  loss_giou_4: 0.7822  loss_ce_dn_4: 0.1436  loss_mask_dn_4: 0.3092  loss_dice_dn_4: 0.8219  loss_bbox_dn_4: 0.2533  loss_giou_dn_4: 0.677  loss_ce_5: 0.8726  loss_mask_5: 0.3093  loss_dice_5: 0.836  loss_bbox_5: 0.2965  loss_giou_5: 0.8139  loss_ce_dn_5: 0.1346  loss_mask_dn_5: 0.323  loss_dice_dn_5: 0.8007  loss_bbox_dn_5: 0.2451  loss_giou_dn_5: 0.6728  loss_ce_6: 0.842  loss_mask_6: 0.3074  loss_dice_6: 0.8319  loss_bbox_6: 0.2798  loss_giou_6: 0.7834  loss_ce_dn_6: 0.1188  loss_mask_dn_6: 0.3235  loss_dice_dn_6: 0.8005  loss_bbox_dn_6: 0.2335  loss_giou_dn_6: 0.6655  loss_ce_7: 0.8044  loss_mask_7: 0.3101  loss_dice_7: 0.8808  loss_bbox_7: 0.2659  loss_giou_7: 0.7842  loss_ce_dn_7: 0.1156  loss_mask_dn_7: 0.3231  loss_dice_dn_7: 0.7954  loss_bbox_dn_7: 0.2243  loss_giou_dn_7: 0.6649  loss_ce_8: 0.8285  loss_mask_8: 0.3071  loss_dice_8: 0.8313  loss_bbox_8: 0.248  loss_giou_8: 0.7748  loss_ce_dn_8: 0.1131  loss_mask_dn_8: 0.3203  loss_dice_dn_8: 0.7989  loss_bbox_dn_8: 0.2128  loss_giou_dn_8: 0.6626  loss_ce_interm: 1.501  loss_mask_interm: 0.3365  loss_dice_interm: 0.8652  loss_bbox_interm: 0.4172  loss_giou_interm: 0.9942    time: 0.8732  last_time: 0.8643  data_time: 0.0143  last_data_time: 0.0167   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:51:08 d2.utils.events]: \u001b[0m eta: 3 days, 16:59:54  iter: 1499  total_loss: 61.55  loss_ce: 0.792  loss_mask: 0.245  loss_dice: 0.662  loss_bbox: 0.2273  loss_giou: 0.7087  loss_ce_dn: 0.1212  loss_mask_dn: 0.2342  loss_dice_dn: 0.771  loss_bbox_dn: 0.2239  loss_giou_dn: 0.5329  loss_ce_0: 1.538  loss_mask_0: 0.2411  loss_dice_0: 0.7457  loss_bbox_0: 0.3972  loss_giou_0: 0.9356  loss_ce_dn_0: 0.7556  loss_mask_dn_0: 0.7838  loss_dice_dn_0: 3.013  loss_bbox_dn_0: 0.648  loss_giou_dn_0: 0.8713  loss_ce_1: 1.41  loss_mask_1: 0.2364  loss_dice_1: 0.6952  loss_bbox_1: 0.311  loss_giou_1: 0.7814  loss_ce_dn_1: 0.2772  loss_mask_dn_1: 0.3113  loss_dice_dn_1: 0.8975  loss_bbox_dn_1: 0.36  loss_giou_dn_1: 0.6714  loss_ce_2: 1.246  loss_mask_2: 0.2486  loss_dice_2: 0.691  loss_bbox_2: 0.28  loss_giou_2: 0.7313  loss_ce_dn_2: 0.2197  loss_mask_dn_2: 0.2695  loss_dice_dn_2: 0.7812  loss_bbox_dn_2: 0.2962  loss_giou_dn_2: 0.6137  loss_ce_3: 1.031  loss_mask_3: 0.2247  loss_dice_3: 0.6592  loss_bbox_3: 0.2532  loss_giou_3: 0.7022  loss_ce_dn_3: 0.1863  loss_mask_dn_3: 0.2589  loss_dice_dn_3: 0.7514  loss_bbox_dn_3: 0.2672  loss_giou_dn_3: 0.5818  loss_ce_4: 0.9207  loss_mask_4: 0.2326  loss_dice_4: 0.6965  loss_bbox_4: 0.2414  loss_giou_4: 0.6967  loss_ce_dn_4: 0.1624  loss_mask_dn_4: 0.2609  loss_dice_dn_4: 0.7526  loss_bbox_dn_4: 0.2515  loss_giou_dn_4: 0.5612  loss_ce_5: 0.8386  loss_mask_5: 0.2405  loss_dice_5: 0.6869  loss_bbox_5: 0.2325  loss_giou_5: 0.6882  loss_ce_dn_5: 0.1465  loss_mask_dn_5: 0.2369  loss_dice_dn_5: 0.7413  loss_bbox_dn_5: 0.2359  loss_giou_dn_5: 0.5552  loss_ce_6: 0.8085  loss_mask_6: 0.2485  loss_dice_6: 0.7279  loss_bbox_6: 0.2278  loss_giou_6: 0.6983  loss_ce_dn_6: 0.1403  loss_mask_dn_6: 0.2421  loss_dice_dn_6: 0.7554  loss_bbox_dn_6: 0.2293  loss_giou_dn_6: 0.5415  loss_ce_7: 0.8133  loss_mask_7: 0.2469  loss_dice_7: 0.6637  loss_bbox_7: 0.2362  loss_giou_7: 0.6911  loss_ce_dn_7: 0.1303  loss_mask_dn_7: 0.2383  loss_dice_dn_7: 0.7637  loss_bbox_dn_7: 0.2254  loss_giou_dn_7: 0.5373  loss_ce_8: 0.8154  loss_mask_8: 0.2478  loss_dice_8: 0.6871  loss_bbox_8: 0.2239  loss_giou_8: 0.7015  loss_ce_dn_8: 0.1249  loss_mask_dn_8: 0.2346  loss_dice_dn_8: 0.7617  loss_bbox_dn_8: 0.2228  loss_giou_dn_8: 0.5324  loss_ce_interm: 1.518  loss_mask_interm: 0.2469  loss_dice_interm: 0.7278  loss_bbox_interm: 0.3951  loss_giou_interm: 0.9356    time: 0.8733  last_time: 0.8496  data_time: 0.0118  last_data_time: 0.0102   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:51:25 d2.utils.events]: \u001b[0m eta: 3 days, 16:59:36  iter: 1519  total_loss: 68.01  loss_ce: 0.9552  loss_mask: 0.2765  loss_dice: 0.7601  loss_bbox: 0.3255  loss_giou: 0.7412  loss_ce_dn: 0.166  loss_mask_dn: 0.2656  loss_dice_dn: 0.7588  loss_bbox_dn: 0.267  loss_giou_dn: 0.6299  loss_ce_0: 1.609  loss_mask_0: 0.2561  loss_dice_0: 0.7789  loss_bbox_0: 0.5108  loss_giou_0: 0.9308  loss_ce_dn_0: 0.7344  loss_mask_dn_0: 0.8929  loss_dice_dn_0: 3.135  loss_bbox_dn_0: 0.7049  loss_giou_dn_0: 0.9824  loss_ce_1: 1.557  loss_mask_1: 0.2706  loss_dice_1: 0.7046  loss_bbox_1: 0.3933  loss_giou_1: 0.8222  loss_ce_dn_1: 0.2724  loss_mask_dn_1: 0.3179  loss_dice_dn_1: 0.9225  loss_bbox_dn_1: 0.406  loss_giou_dn_1: 0.7704  loss_ce_2: 1.345  loss_mask_2: 0.2555  loss_dice_2: 0.8072  loss_bbox_2: 0.358  loss_giou_2: 0.7981  loss_ce_dn_2: 0.2258  loss_mask_dn_2: 0.2738  loss_dice_dn_2: 0.8059  loss_bbox_dn_2: 0.3358  loss_giou_dn_2: 0.7044  loss_ce_3: 1.223  loss_mask_3: 0.2704  loss_dice_3: 0.7406  loss_bbox_3: 0.3189  loss_giou_3: 0.7751  loss_ce_dn_3: 0.2039  loss_mask_dn_3: 0.2612  loss_dice_dn_3: 0.8063  loss_bbox_dn_3: 0.2993  loss_giou_dn_3: 0.6805  loss_ce_4: 1.085  loss_mask_4: 0.2587  loss_dice_4: 0.7676  loss_bbox_4: 0.296  loss_giou_4: 0.7681  loss_ce_dn_4: 0.1861  loss_mask_dn_4: 0.2731  loss_dice_dn_4: 0.8057  loss_bbox_dn_4: 0.2784  loss_giou_dn_4: 0.658  loss_ce_5: 1.005  loss_mask_5: 0.2676  loss_dice_5: 0.7238  loss_bbox_5: 0.3207  loss_giou_5: 0.7423  loss_ce_dn_5: 0.1776  loss_mask_dn_5: 0.2727  loss_dice_dn_5: 0.7981  loss_bbox_dn_5: 0.2684  loss_giou_dn_5: 0.6501  loss_ce_6: 0.9542  loss_mask_6: 0.2679  loss_dice_6: 0.8113  loss_bbox_6: 0.3257  loss_giou_6: 0.7654  loss_ce_dn_6: 0.1729  loss_mask_dn_6: 0.2654  loss_dice_dn_6: 0.8537  loss_bbox_dn_6: 0.2678  loss_giou_dn_6: 0.6337  loss_ce_7: 0.9186  loss_mask_7: 0.2682  loss_dice_7: 0.736  loss_bbox_7: 0.3275  loss_giou_7: 0.7364  loss_ce_dn_7: 0.1612  loss_mask_dn_7: 0.2644  loss_dice_dn_7: 0.7611  loss_bbox_dn_7: 0.2684  loss_giou_dn_7: 0.6356  loss_ce_8: 0.8909  loss_mask_8: 0.2726  loss_dice_8: 0.6858  loss_bbox_8: 0.3293  loss_giou_8: 0.7553  loss_ce_dn_8: 0.1683  loss_mask_dn_8: 0.2726  loss_dice_dn_8: 0.76  loss_bbox_dn_8: 0.2676  loss_giou_dn_8: 0.6301  loss_ce_interm: 1.585  loss_mask_interm: 0.2642  loss_dice_interm: 0.8254  loss_bbox_interm: 0.5104  loss_giou_interm: 0.9042    time: 0.8733  last_time: 0.8638  data_time: 0.0126  last_data_time: 0.0133   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:51:43 d2.utils.events]: \u001b[0m eta: 3 days, 16:59:19  iter: 1539  total_loss: 66.41  loss_ce: 0.9199  loss_mask: 0.2578  loss_dice: 0.7972  loss_bbox: 0.2501  loss_giou: 0.8148  loss_ce_dn: 0.212  loss_mask_dn: 0.2841  loss_dice_dn: 0.7091  loss_bbox_dn: 0.2306  loss_giou_dn: 0.6555  loss_ce_0: 1.516  loss_mask_0: 0.3198  loss_dice_0: 0.7442  loss_bbox_0: 0.3713  loss_giou_0: 0.9867  loss_ce_dn_0: 0.7461  loss_mask_dn_0: 0.8066  loss_dice_dn_0: 2.896  loss_bbox_dn_0: 0.5285  loss_giou_dn_0: 1.021  loss_ce_1: 1.466  loss_mask_1: 0.301  loss_dice_1: 0.7312  loss_bbox_1: 0.2769  loss_giou_1: 0.8577  loss_ce_dn_1: 0.3193  loss_mask_dn_1: 0.3611  loss_dice_dn_1: 0.8674  loss_bbox_dn_1: 0.3053  loss_giou_dn_1: 0.772  loss_ce_2: 1.313  loss_mask_2: 0.2825  loss_dice_2: 0.753  loss_bbox_2: 0.2738  loss_giou_2: 0.8417  loss_ce_dn_2: 0.2619  loss_mask_dn_2: 0.3216  loss_dice_dn_2: 0.7846  loss_bbox_dn_2: 0.2551  loss_giou_dn_2: 0.7136  loss_ce_3: 1.09  loss_mask_3: 0.2831  loss_dice_3: 0.7602  loss_bbox_3: 0.2586  loss_giou_3: 0.8337  loss_ce_dn_3: 0.244  loss_mask_dn_3: 0.3045  loss_dice_dn_3: 0.746  loss_bbox_dn_3: 0.2453  loss_giou_dn_3: 0.692  loss_ce_4: 1.07  loss_mask_4: 0.2763  loss_dice_4: 0.7644  loss_bbox_4: 0.2671  loss_giou_4: 0.8044  loss_ce_dn_4: 0.235  loss_mask_dn_4: 0.2889  loss_dice_dn_4: 0.7452  loss_bbox_dn_4: 0.2374  loss_giou_dn_4: 0.6732  loss_ce_5: 1.006  loss_mask_5: 0.2655  loss_dice_5: 0.7654  loss_bbox_5: 0.2643  loss_giou_5: 0.8142  loss_ce_dn_5: 0.2289  loss_mask_dn_5: 0.2897  loss_dice_dn_5: 0.7323  loss_bbox_dn_5: 0.2325  loss_giou_dn_5: 0.6634  loss_ce_6: 0.9539  loss_mask_6: 0.268  loss_dice_6: 0.7716  loss_bbox_6: 0.2512  loss_giou_6: 0.8062  loss_ce_dn_6: 0.2297  loss_mask_dn_6: 0.2865  loss_dice_dn_6: 0.7329  loss_bbox_dn_6: 0.2332  loss_giou_dn_6: 0.6514  loss_ce_7: 0.946  loss_mask_7: 0.2765  loss_dice_7: 0.771  loss_bbox_7: 0.2417  loss_giou_7: 0.8106  loss_ce_dn_7: 0.2186  loss_mask_dn_7: 0.2838  loss_dice_dn_7: 0.7243  loss_bbox_dn_7: 0.2318  loss_giou_dn_7: 0.6501  loss_ce_8: 0.9215  loss_mask_8: 0.2727  loss_dice_8: 0.7335  loss_bbox_8: 0.263  loss_giou_8: 0.8124  loss_ce_dn_8: 0.2111  loss_mask_dn_8: 0.2899  loss_dice_dn_8: 0.7068  loss_bbox_dn_8: 0.2307  loss_giou_dn_8: 0.6538  loss_ce_interm: 1.506  loss_mask_interm: 0.3233  loss_dice_interm: 0.7301  loss_bbox_interm: 0.3711  loss_giou_interm: 0.9937    time: 0.8733  last_time: 0.8564  data_time: 0.0113  last_data_time: 0.0072   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:52:01 d2.utils.events]: \u001b[0m eta: 3 days, 16:59:14  iter: 1559  total_loss: 57.34  loss_ce: 0.6261  loss_mask: 0.2421  loss_dice: 0.832  loss_bbox: 0.281  loss_giou: 0.5917  loss_ce_dn: 0.09093  loss_mask_dn: 0.2476  loss_dice_dn: 0.7716  loss_bbox_dn: 0.2584  loss_giou_dn: 0.4577  loss_ce_0: 1.319  loss_mask_0: 0.2506  loss_dice_0: 0.7759  loss_bbox_0: 0.4124  loss_giou_0: 0.7945  loss_ce_dn_0: 0.7194  loss_mask_dn_0: 0.7837  loss_dice_dn_0: 2.759  loss_bbox_dn_0: 0.6351  loss_giou_dn_0: 0.8634  loss_ce_1: 1.272  loss_mask_1: 0.2461  loss_dice_1: 0.7535  loss_bbox_1: 0.339  loss_giou_1: 0.6964  loss_ce_dn_1: 0.227  loss_mask_dn_1: 0.3121  loss_dice_dn_1: 0.9566  loss_bbox_dn_1: 0.3468  loss_giou_dn_1: 0.5934  loss_ce_2: 1.11  loss_mask_2: 0.2462  loss_dice_2: 0.7693  loss_bbox_2: 0.3369  loss_giou_2: 0.6359  loss_ce_dn_2: 0.176  loss_mask_dn_2: 0.2759  loss_dice_dn_2: 0.8247  loss_bbox_dn_2: 0.2892  loss_giou_dn_2: 0.5263  loss_ce_3: 0.8828  loss_mask_3: 0.2435  loss_dice_3: 0.772  loss_bbox_3: 0.316  loss_giou_3: 0.6001  loss_ce_dn_3: 0.1318  loss_mask_dn_3: 0.2579  loss_dice_dn_3: 0.8007  loss_bbox_dn_3: 0.278  loss_giou_dn_3: 0.5021  loss_ce_4: 0.7623  loss_mask_4: 0.2437  loss_dice_4: 0.7636  loss_bbox_4: 0.2721  loss_giou_4: 0.5688  loss_ce_dn_4: 0.1212  loss_mask_dn_4: 0.2511  loss_dice_dn_4: 0.7857  loss_bbox_dn_4: 0.2661  loss_giou_dn_4: 0.4798  loss_ce_5: 0.7106  loss_mask_5: 0.2401  loss_dice_5: 0.732  loss_bbox_5: 0.287  loss_giou_5: 0.5891  loss_ce_dn_5: 0.1089  loss_mask_dn_5: 0.2473  loss_dice_dn_5: 0.7823  loss_bbox_dn_5: 0.2647  loss_giou_dn_5: 0.4692  loss_ce_6: 0.6361  loss_mask_6: 0.2476  loss_dice_6: 0.7829  loss_bbox_6: 0.2938  loss_giou_6: 0.5752  loss_ce_dn_6: 0.1046  loss_mask_dn_6: 0.2491  loss_dice_dn_6: 0.7714  loss_bbox_dn_6: 0.2606  loss_giou_dn_6: 0.4579  loss_ce_7: 0.5931  loss_mask_7: 0.2449  loss_dice_7: 0.7979  loss_bbox_7: 0.2779  loss_giou_7: 0.6104  loss_ce_dn_7: 0.1003  loss_mask_dn_7: 0.2482  loss_dice_dn_7: 0.7772  loss_bbox_dn_7: 0.2588  loss_giou_dn_7: 0.4587  loss_ce_8: 0.6066  loss_mask_8: 0.2498  loss_dice_8: 0.7814  loss_bbox_8: 0.2784  loss_giou_8: 0.6222  loss_ce_dn_8: 0.09695  loss_mask_dn_8: 0.2479  loss_dice_dn_8: 0.779  loss_bbox_dn_8: 0.2588  loss_giou_dn_8: 0.4551  loss_ce_interm: 1.331  loss_mask_interm: 0.2516  loss_dice_interm: 0.7843  loss_bbox_interm: 0.4212  loss_giou_interm: 0.8238    time: 0.8733  last_time: 0.8593  data_time: 0.0116  last_data_time: 0.0093   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:52:18 d2.utils.events]: \u001b[0m eta: 3 days, 16:59:16  iter: 1579  total_loss: 61.18  loss_ce: 0.6531  loss_mask: 0.2501  loss_dice: 0.6427  loss_bbox: 0.2895  loss_giou: 0.7439  loss_ce_dn: 0.1121  loss_mask_dn: 0.2452  loss_dice_dn: 0.6456  loss_bbox_dn: 0.2119  loss_giou_dn: 0.5934  loss_ce_0: 1.436  loss_mask_0: 0.2689  loss_dice_0: 0.7112  loss_bbox_0: 0.4175  loss_giou_0: 0.948  loss_ce_dn_0: 0.7214  loss_mask_dn_0: 0.8212  loss_dice_dn_0: 3.187  loss_bbox_dn_0: 0.5584  loss_giou_dn_0: 0.9685  loss_ce_1: 1.283  loss_mask_1: 0.2869  loss_dice_1: 0.6583  loss_bbox_1: 0.3153  loss_giou_1: 0.8301  loss_ce_dn_1: 0.2625  loss_mask_dn_1: 0.2903  loss_dice_dn_1: 0.8411  loss_bbox_dn_1: 0.3121  loss_giou_dn_1: 0.7384  loss_ce_2: 1.083  loss_mask_2: 0.2682  loss_dice_2: 0.6655  loss_bbox_2: 0.3068  loss_giou_2: 0.809  loss_ce_dn_2: 0.2132  loss_mask_dn_2: 0.2653  loss_dice_dn_2: 0.6894  loss_bbox_dn_2: 0.2631  loss_giou_dn_2: 0.6796  loss_ce_3: 0.9263  loss_mask_3: 0.2686  loss_dice_3: 0.6562  loss_bbox_3: 0.3049  loss_giou_3: 0.7951  loss_ce_dn_3: 0.1841  loss_mask_dn_3: 0.25  loss_dice_dn_3: 0.6394  loss_bbox_dn_3: 0.2377  loss_giou_dn_3: 0.6434  loss_ce_4: 0.8471  loss_mask_4: 0.2679  loss_dice_4: 0.6441  loss_bbox_4: 0.2829  loss_giou_4: 0.7533  loss_ce_dn_4: 0.1574  loss_mask_dn_4: 0.253  loss_dice_dn_4: 0.6329  loss_bbox_dn_4: 0.2212  loss_giou_dn_4: 0.6259  loss_ce_5: 0.7559  loss_mask_5: 0.2571  loss_dice_5: 0.6427  loss_bbox_5: 0.2887  loss_giou_5: 0.7524  loss_ce_dn_5: 0.1399  loss_mask_dn_5: 0.2545  loss_dice_dn_5: 0.6449  loss_bbox_dn_5: 0.2166  loss_giou_dn_5: 0.6133  loss_ce_6: 0.7112  loss_mask_6: 0.2508  loss_dice_6: 0.651  loss_bbox_6: 0.2849  loss_giou_6: 0.7425  loss_ce_dn_6: 0.1246  loss_mask_dn_6: 0.2504  loss_dice_dn_6: 0.6238  loss_bbox_dn_6: 0.2131  loss_giou_dn_6: 0.6017  loss_ce_7: 0.6705  loss_mask_7: 0.2494  loss_dice_7: 0.671  loss_bbox_7: 0.2904  loss_giou_7: 0.7382  loss_ce_dn_7: 0.1177  loss_mask_dn_7: 0.245  loss_dice_dn_7: 0.6501  loss_bbox_dn_7: 0.2123  loss_giou_dn_7: 0.5958  loss_ce_8: 0.666  loss_mask_8: 0.2475  loss_dice_8: 0.6702  loss_bbox_8: 0.2923  loss_giou_8: 0.7419  loss_ce_dn_8: 0.1127  loss_mask_dn_8: 0.2488  loss_dice_dn_8: 0.6395  loss_bbox_dn_8: 0.2125  loss_giou_dn_8: 0.5927  loss_ce_interm: 1.442  loss_mask_interm: 0.2716  loss_dice_interm: 0.6805  loss_bbox_interm: 0.4208  loss_giou_interm: 0.9498    time: 0.8733  last_time: 0.8636  data_time: 0.0132  last_data_time: 0.0102   lr: 1.25e-05  max_mem: 11917M\n",
            "\u001b[32m[02/20 07:52:36 d2.utils.events]: \u001b[0m eta: 3 days, 17:02:17  iter: 1599  total_loss: 67.05  loss_ce: 0.8423  loss_mask: 0.278  loss_dice: 0.8985  loss_bbox: 0.2282  loss_giou: 0.6853  loss_ce_dn: 0.1329  loss_mask_dn: 0.286  loss_dice_dn: 0.9169  loss_bbox_dn: 0.2634  loss_giou_dn: 0.6122  loss_ce_0: 1.497  loss_mask_0: 0.2811  loss_dice_0: 0.9929  loss_bbox_0: 0.4922  loss_giou_0: 0.9351  loss_ce_dn_0: 0.7339  loss_mask_dn_0: 0.9237  loss_dice_dn_0: 3.103  loss_bbox_dn_0: 0.6377  loss_giou_dn_0: 0.9194  loss_ce_1: 1.444  loss_mask_1: 0.2869  loss_dice_1: 0.9408  loss_bbox_1: 0.3403  loss_giou_1: 0.8073  loss_ce_dn_1: 0.2835  loss_mask_dn_1: 0.3641  loss_dice_dn_1: 1.128  loss_bbox_dn_1: 0.3994  loss_giou_dn_1: 0.7139  loss_ce_2: 1.26  loss_mask_2: 0.3076  loss_dice_2: 0.9918  loss_bbox_2: 0.3086  loss_giou_2: 0.7412  loss_ce_dn_2: 0.2079  loss_mask_dn_2: 0.3035  loss_dice_dn_2: 1.045  loss_bbox_dn_2: 0.3166  loss_giou_dn_2: 0.6771  loss_ce_3: 1.082  loss_mask_3: 0.282  loss_dice_3: 0.8982  loss_bbox_3: 0.2799  loss_giou_3: 0.7263  loss_ce_dn_3: 0.175  loss_mask_dn_3: 0.2862  loss_dice_dn_3: 0.9782  loss_bbox_dn_3: 0.2798  loss_giou_dn_3: 0.65  loss_ce_4: 0.9937  loss_mask_4: 0.3081  loss_dice_4: 0.9263  loss_bbox_4: 0.2618  loss_giou_4: 0.6901  loss_ce_dn_4: 0.1616  loss_mask_dn_4: 0.2856  loss_dice_dn_4: 0.934  loss_bbox_dn_4: 0.2676  loss_giou_dn_4: 0.6331  loss_ce_5: 0.9256  loss_mask_5: 0.2826  loss_dice_5: 0.9546  loss_bbox_5: 0.2603  loss_giou_5: 0.6857  loss_ce_dn_5: 0.1483  loss_mask_dn_5: 0.2786  loss_dice_dn_5: 0.936  loss_bbox_dn_5: 0.2647  loss_giou_dn_5: 0.6236  loss_ce_6: 0.887  loss_mask_6: 0.2745  loss_dice_6: 0.8941  loss_bbox_6: 0.2809  loss_giou_6: 0.6992  loss_ce_dn_6: 0.1409  loss_mask_dn_6: 0.2828  loss_dice_dn_6: 0.9431  loss_bbox_dn_6: 0.2633  loss_giou_dn_6: 0.6181  loss_ce_7: 0.8471  loss_mask_7: 0.294  loss_dice_7: 0.9465  loss_bbox_7: 0.2383  loss_giou_7: 0.6927  loss_ce_dn_7: 0.1408  loss_mask_dn_7: 0.2813  loss_dice_dn_7: 0.9217  loss_bbox_dn_7: 0.265  loss_giou_dn_7: 0.6158  loss_ce_8: 0.8516  loss_mask_8: 0.2868  loss_dice_8: 0.9074  loss_bbox_8: 0.2289  loss_giou_8: 0.6831  loss_ce_dn_8: 0.1387  loss_mask_dn_8: 0.2899  loss_dice_dn_8: 0.9234  loss_bbox_dn_8: 0.264  loss_giou_dn_8: 0.6137  loss_ce_interm: 1.5  loss_mask_interm: 0.2832  loss_dice_interm: 1.027  loss_bbox_interm: 0.4746  loss_giou_interm: 0.9042    time: 0.8734  last_time: 0.8776  data_time: 0.0123  last_data_time: 0.0092   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 07:52:53 d2.utils.events]: \u001b[0m eta: 3 days, 17:02:21  iter: 1619  total_loss: 62.04  loss_ce: 0.9077  loss_mask: 0.2757  loss_dice: 0.6984  loss_bbox: 0.246  loss_giou: 0.619  loss_ce_dn: 0.1459  loss_mask_dn: 0.2665  loss_dice_dn: 0.7104  loss_bbox_dn: 0.2321  loss_giou_dn: 0.5291  loss_ce_0: 1.456  loss_mask_0: 0.251  loss_dice_0: 0.742  loss_bbox_0: 0.3832  loss_giou_0: 0.8535  loss_ce_dn_0: 0.744  loss_mask_dn_0: 0.8276  loss_dice_dn_0: 3.188  loss_bbox_dn_0: 0.6389  loss_giou_dn_0: 0.8984  loss_ce_1: 1.372  loss_mask_1: 0.2666  loss_dice_1: 0.7304  loss_bbox_1: 0.2791  loss_giou_1: 0.7465  loss_ce_dn_1: 0.2359  loss_mask_dn_1: 0.3439  loss_dice_dn_1: 0.8743  loss_bbox_dn_1: 0.337  loss_giou_dn_1: 0.6834  loss_ce_2: 1.162  loss_mask_2: 0.2632  loss_dice_2: 0.7043  loss_bbox_2: 0.2947  loss_giou_2: 0.7102  loss_ce_dn_2: 0.2026  loss_mask_dn_2: 0.2992  loss_dice_dn_2: 0.7474  loss_bbox_dn_2: 0.2662  loss_giou_dn_2: 0.6196  loss_ce_3: 1.026  loss_mask_3: 0.2711  loss_dice_3: 0.7376  loss_bbox_3: 0.2461  loss_giou_3: 0.6618  loss_ce_dn_3: 0.1724  loss_mask_dn_3: 0.2856  loss_dice_dn_3: 0.7462  loss_bbox_dn_3: 0.2499  loss_giou_dn_3: 0.5755  loss_ce_4: 0.9851  loss_mask_4: 0.2622  loss_dice_4: 0.6721  loss_bbox_4: 0.2441  loss_giou_4: 0.6621  loss_ce_dn_4: 0.1571  loss_mask_dn_4: 0.2725  loss_dice_dn_4: 0.6991  loss_bbox_dn_4: 0.2361  loss_giou_dn_4: 0.5562  loss_ce_5: 1.004  loss_mask_5: 0.2718  loss_dice_5: 0.7237  loss_bbox_5: 0.2378  loss_giou_5: 0.6131  loss_ce_dn_5: 0.1548  loss_mask_dn_5: 0.2779  loss_dice_dn_5: 0.7003  loss_bbox_dn_5: 0.2368  loss_giou_dn_5: 0.5457  loss_ce_6: 0.967  loss_mask_6: 0.2691  loss_dice_6: 0.7234  loss_bbox_6: 0.2324  loss_giou_6: 0.6683  loss_ce_dn_6: 0.1504  loss_mask_dn_6: 0.2753  loss_dice_dn_6: 0.7085  loss_bbox_dn_6: 0.2317  loss_giou_dn_6: 0.5356  loss_ce_7: 0.9387  loss_mask_7: 0.2617  loss_dice_7: 0.6963  loss_bbox_7: 0.2269  loss_giou_7: 0.6095  loss_ce_dn_7: 0.1421  loss_mask_dn_7: 0.2674  loss_dice_dn_7: 0.7026  loss_bbox_dn_7: 0.2303  loss_giou_dn_7: 0.5336  loss_ce_8: 0.9292  loss_mask_8: 0.2664  loss_dice_8: 0.7434  loss_bbox_8: 0.2174  loss_giou_8: 0.6193  loss_ce_dn_8: 0.1479  loss_mask_dn_8: 0.2673  loss_dice_dn_8: 0.711  loss_bbox_dn_8: 0.2326  loss_giou_dn_8: 0.53  loss_ce_interm: 1.459  loss_mask_interm: 0.2502  loss_dice_interm: 0.7406  loss_bbox_interm: 0.3881  loss_giou_interm: 0.8419    time: 0.8734  last_time: 0.8857  data_time: 0.0129  last_data_time: 0.0098   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 07:53:11 d2.utils.events]: \u001b[0m eta: 3 days, 17:01:42  iter: 1639  total_loss: 61.2  loss_ce: 0.7146  loss_mask: 0.2459  loss_dice: 0.8627  loss_bbox: 0.2379  loss_giou: 0.7767  loss_ce_dn: 0.1067  loss_mask_dn: 0.2362  loss_dice_dn: 0.8642  loss_bbox_dn: 0.2318  loss_giou_dn: 0.5182  loss_ce_0: 1.438  loss_mask_0: 0.2078  loss_dice_0: 0.9076  loss_bbox_0: 0.3884  loss_giou_0: 0.9216  loss_ce_dn_0: 0.7555  loss_mask_dn_0: 0.7531  loss_dice_dn_0: 3.02  loss_bbox_dn_0: 0.6261  loss_giou_dn_0: 0.8754  loss_ce_1: 1.316  loss_mask_1: 0.2245  loss_dice_1: 0.9635  loss_bbox_1: 0.28  loss_giou_1: 0.8475  loss_ce_dn_1: 0.2499  loss_mask_dn_1: 0.2833  loss_dice_dn_1: 0.9779  loss_bbox_dn_1: 0.3428  loss_giou_dn_1: 0.6485  loss_ce_2: 1.189  loss_mask_2: 0.2011  loss_dice_2: 0.9005  loss_bbox_2: 0.2506  loss_giou_2: 0.7817  loss_ce_dn_2: 0.2034  loss_mask_dn_2: 0.2616  loss_dice_dn_2: 0.9073  loss_bbox_dn_2: 0.2965  loss_giou_dn_2: 0.5821  loss_ce_3: 0.9765  loss_mask_3: 0.2117  loss_dice_3: 0.8726  loss_bbox_3: 0.2482  loss_giou_3: 0.7537  loss_ce_dn_3: 0.1771  loss_mask_dn_3: 0.2467  loss_dice_dn_3: 0.8758  loss_bbox_dn_3: 0.2722  loss_giou_dn_3: 0.5586  loss_ce_4: 0.8882  loss_mask_4: 0.1961  loss_dice_4: 0.7927  loss_bbox_4: 0.2326  loss_giou_4: 0.7667  loss_ce_dn_4: 0.1535  loss_mask_dn_4: 0.2394  loss_dice_dn_4: 0.8497  loss_bbox_dn_4: 0.2538  loss_giou_dn_4: 0.5357  loss_ce_5: 0.8063  loss_mask_5: 0.2013  loss_dice_5: 0.8152  loss_bbox_5: 0.237  loss_giou_5: 0.7732  loss_ce_dn_5: 0.1308  loss_mask_dn_5: 0.2344  loss_dice_dn_5: 0.8593  loss_bbox_dn_5: 0.2443  loss_giou_dn_5: 0.5323  loss_ce_6: 0.7453  loss_mask_6: 0.2341  loss_dice_6: 0.8206  loss_bbox_6: 0.2358  loss_giou_6: 0.7804  loss_ce_dn_6: 0.1257  loss_mask_dn_6: 0.2361  loss_dice_dn_6: 0.8859  loss_bbox_dn_6: 0.2365  loss_giou_dn_6: 0.5226  loss_ce_7: 0.714  loss_mask_7: 0.2361  loss_dice_7: 0.877  loss_bbox_7: 0.234  loss_giou_7: 0.7842  loss_ce_dn_7: 0.1147  loss_mask_dn_7: 0.2357  loss_dice_dn_7: 0.8811  loss_bbox_dn_7: 0.234  loss_giou_dn_7: 0.5246  loss_ce_8: 0.7301  loss_mask_8: 0.2381  loss_dice_8: 0.8667  loss_bbox_8: 0.2351  loss_giou_8: 0.7709  loss_ce_dn_8: 0.1086  loss_mask_dn_8: 0.2337  loss_dice_dn_8: 0.8843  loss_bbox_dn_8: 0.232  loss_giou_dn_8: 0.5177  loss_ce_interm: 1.428  loss_mask_interm: 0.2151  loss_dice_interm: 0.9305  loss_bbox_interm: 0.3898  loss_giou_interm: 0.9454    time: 0.8734  last_time: 0.8791  data_time: 0.0120  last_data_time: 0.0090   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 07:53:29 d2.utils.events]: \u001b[0m eta: 3 days, 17:00:42  iter: 1659  total_loss: 58.8  loss_ce: 0.6514  loss_mask: 0.2686  loss_dice: 0.6479  loss_bbox: 0.2908  loss_giou: 0.6773  loss_ce_dn: 0.09748  loss_mask_dn: 0.3064  loss_dice_dn: 0.6848  loss_bbox_dn: 0.2259  loss_giou_dn: 0.5686  loss_ce_0: 1.449  loss_mask_0: 0.2603  loss_dice_0: 0.6132  loss_bbox_0: 0.4475  loss_giou_0: 0.8582  loss_ce_dn_0: 0.7161  loss_mask_dn_0: 0.8049  loss_dice_dn_0: 2.97  loss_bbox_dn_0: 0.6326  loss_giou_dn_0: 0.9088  loss_ce_1: 1.349  loss_mask_1: 0.2767  loss_dice_1: 0.6112  loss_bbox_1: 0.3287  loss_giou_1: 0.7311  loss_ce_dn_1: 0.1951  loss_mask_dn_1: 0.3933  loss_dice_dn_1: 0.8078  loss_bbox_dn_1: 0.35  loss_giou_dn_1: 0.6638  loss_ce_2: 1.103  loss_mask_2: 0.2914  loss_dice_2: 0.6622  loss_bbox_2: 0.3177  loss_giou_2: 0.7159  loss_ce_dn_2: 0.1611  loss_mask_dn_2: 0.3324  loss_dice_dn_2: 0.7121  loss_bbox_dn_2: 0.2767  loss_giou_dn_2: 0.6209  loss_ce_3: 0.9379  loss_mask_3: 0.2852  loss_dice_3: 0.6173  loss_bbox_3: 0.3055  loss_giou_3: 0.7337  loss_ce_dn_3: 0.1428  loss_mask_dn_3: 0.333  loss_dice_dn_3: 0.6739  loss_bbox_dn_3: 0.2459  loss_giou_dn_3: 0.6056  loss_ce_4: 0.8407  loss_mask_4: 0.2841  loss_dice_4: 0.6891  loss_bbox_4: 0.2868  loss_giou_4: 0.7344  loss_ce_dn_4: 0.1348  loss_mask_dn_4: 0.3227  loss_dice_dn_4: 0.6838  loss_bbox_dn_4: 0.2313  loss_giou_dn_4: 0.5976  loss_ce_5: 0.7289  loss_mask_5: 0.2809  loss_dice_5: 0.6891  loss_bbox_5: 0.2861  loss_giou_5: 0.7242  loss_ce_dn_5: 0.1197  loss_mask_dn_5: 0.3198  loss_dice_dn_5: 0.6849  loss_bbox_dn_5: 0.2313  loss_giou_dn_5: 0.5921  loss_ce_6: 0.7123  loss_mask_6: 0.2775  loss_dice_6: 0.7042  loss_bbox_6: 0.2877  loss_giou_6: 0.7094  loss_ce_dn_6: 0.1111  loss_mask_dn_6: 0.302  loss_dice_dn_6: 0.6761  loss_bbox_dn_6: 0.2257  loss_giou_dn_6: 0.58  loss_ce_7: 0.7286  loss_mask_7: 0.2711  loss_dice_7: 0.6796  loss_bbox_7: 0.285  loss_giou_7: 0.7013  loss_ce_dn_7: 0.1085  loss_mask_dn_7: 0.3075  loss_dice_dn_7: 0.6783  loss_bbox_dn_7: 0.225  loss_giou_dn_7: 0.5795  loss_ce_8: 0.6589  loss_mask_8: 0.27  loss_dice_8: 0.6655  loss_bbox_8: 0.289  loss_giou_8: 0.7007  loss_ce_dn_8: 0.1007  loss_mask_dn_8: 0.3015  loss_dice_dn_8: 0.6822  loss_bbox_dn_8: 0.2257  loss_giou_dn_8: 0.5711  loss_ce_interm: 1.395  loss_mask_interm: 0.2684  loss_dice_interm: 0.6742  loss_bbox_interm: 0.4475  loss_giou_interm: 0.8532    time: 0.8735  last_time: 0.9091  data_time: 0.0154  last_data_time: 0.0277   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 07:53:46 d2.utils.events]: \u001b[0m eta: 3 days, 17:01:26  iter: 1679  total_loss: 67.45  loss_ce: 0.7612  loss_mask: 0.2487  loss_dice: 0.9123  loss_bbox: 0.3012  loss_giou: 0.7273  loss_ce_dn: 0.1688  loss_mask_dn: 0.2374  loss_dice_dn: 0.8596  loss_bbox_dn: 0.221  loss_giou_dn: 0.6089  loss_ce_0: 1.468  loss_mask_0: 0.2653  loss_dice_0: 0.9511  loss_bbox_0: 0.4241  loss_giou_0: 0.9588  loss_ce_dn_0: 0.7781  loss_mask_dn_0: 0.7569  loss_dice_dn_0: 3.092  loss_bbox_dn_0: 0.5569  loss_giou_dn_0: 0.9612  loss_ce_1: 1.399  loss_mask_1: 0.2653  loss_dice_1: 0.902  loss_bbox_1: 0.358  loss_giou_1: 0.8277  loss_ce_dn_1: 0.2943  loss_mask_dn_1: 0.2916  loss_dice_dn_1: 0.9606  loss_bbox_dn_1: 0.3201  loss_giou_dn_1: 0.7197  loss_ce_2: 1.234  loss_mask_2: 0.2664  loss_dice_2: 0.9025  loss_bbox_2: 0.3115  loss_giou_2: 0.7785  loss_ce_dn_2: 0.2349  loss_mask_dn_2: 0.2641  loss_dice_dn_2: 0.8993  loss_bbox_dn_2: 0.2609  loss_giou_dn_2: 0.6702  loss_ce_3: 1.042  loss_mask_3: 0.2591  loss_dice_3: 0.9006  loss_bbox_3: 0.3095  loss_giou_3: 0.756  loss_ce_dn_3: 0.2201  loss_mask_dn_3: 0.2586  loss_dice_dn_3: 0.878  loss_bbox_dn_3: 0.2367  loss_giou_dn_3: 0.6417  loss_ce_4: 0.9271  loss_mask_4: 0.2621  loss_dice_4: 0.8644  loss_bbox_4: 0.2988  loss_giou_4: 0.7606  loss_ce_dn_4: 0.2005  loss_mask_dn_4: 0.248  loss_dice_dn_4: 0.8558  loss_bbox_dn_4: 0.2293  loss_giou_dn_4: 0.6242  loss_ce_5: 0.8731  loss_mask_5: 0.2595  loss_dice_5: 0.8687  loss_bbox_5: 0.3119  loss_giou_5: 0.7504  loss_ce_dn_5: 0.1794  loss_mask_dn_5: 0.2443  loss_dice_dn_5: 0.8569  loss_bbox_dn_5: 0.2269  loss_giou_dn_5: 0.6203  loss_ce_6: 0.7878  loss_mask_6: 0.2483  loss_dice_6: 0.8949  loss_bbox_6: 0.3052  loss_giou_6: 0.745  loss_ce_dn_6: 0.175  loss_mask_dn_6: 0.2383  loss_dice_dn_6: 0.8562  loss_bbox_dn_6: 0.2223  loss_giou_dn_6: 0.6141  loss_ce_7: 0.7527  loss_mask_7: 0.2518  loss_dice_7: 0.9025  loss_bbox_7: 0.29  loss_giou_7: 0.7405  loss_ce_dn_7: 0.1704  loss_mask_dn_7: 0.2406  loss_dice_dn_7: 0.8654  loss_bbox_dn_7: 0.2189  loss_giou_dn_7: 0.6132  loss_ce_8: 0.758  loss_mask_8: 0.2471  loss_dice_8: 0.9331  loss_bbox_8: 0.3021  loss_giou_8: 0.7456  loss_ce_dn_8: 0.1652  loss_mask_dn_8: 0.2398  loss_dice_dn_8: 0.8674  loss_bbox_dn_8: 0.221  loss_giou_dn_8: 0.6099  loss_ce_interm: 1.48  loss_mask_interm: 0.2667  loss_dice_interm: 0.9479  loss_bbox_interm: 0.4198  loss_giou_interm: 0.9418    time: 0.8737  last_time: 0.8813  data_time: 0.0172  last_data_time: 0.0159   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 07:54:04 d2.utils.events]: \u001b[0m eta: 3 days, 17:01:16  iter: 1699  total_loss: 59.4  loss_ce: 0.7067  loss_mask: 0.2661  loss_dice: 0.6452  loss_bbox: 0.2266  loss_giou: 0.6857  loss_ce_dn: 0.138  loss_mask_dn: 0.281  loss_dice_dn: 0.6213  loss_bbox_dn: 0.2314  loss_giou_dn: 0.6492  loss_ce_0: 1.528  loss_mask_0: 0.2861  loss_dice_0: 0.7055  loss_bbox_0: 0.3798  loss_giou_0: 0.9464  loss_ce_dn_0: 0.7252  loss_mask_dn_0: 0.9265  loss_dice_dn_0: 3.021  loss_bbox_dn_0: 0.6762  loss_giou_dn_0: 1.019  loss_ce_1: 1.38  loss_mask_1: 0.2721  loss_dice_1: 0.6562  loss_bbox_1: 0.2619  loss_giou_1: 0.7592  loss_ce_dn_1: 0.244  loss_mask_dn_1: 0.3456  loss_dice_dn_1: 0.7514  loss_bbox_dn_1: 0.3644  loss_giou_dn_1: 0.7738  loss_ce_2: 1.149  loss_mask_2: 0.2926  loss_dice_2: 0.6586  loss_bbox_2: 0.2419  loss_giou_2: 0.7402  loss_ce_dn_2: 0.1951  loss_mask_dn_2: 0.3042  loss_dice_dn_2: 0.653  loss_bbox_dn_2: 0.3084  loss_giou_dn_2: 0.7037  loss_ce_3: 0.9943  loss_mask_3: 0.2924  loss_dice_3: 0.6329  loss_bbox_3: 0.2205  loss_giou_3: 0.6964  loss_ce_dn_3: 0.1831  loss_mask_dn_3: 0.2868  loss_dice_dn_3: 0.6479  loss_bbox_dn_3: 0.2704  loss_giou_dn_3: 0.6743  loss_ce_4: 0.8713  loss_mask_4: 0.2836  loss_dice_4: 0.6288  loss_bbox_4: 0.2401  loss_giou_4: 0.7151  loss_ce_dn_4: 0.1612  loss_mask_dn_4: 0.2855  loss_dice_dn_4: 0.6337  loss_bbox_dn_4: 0.2569  loss_giou_dn_4: 0.6614  loss_ce_5: 0.8623  loss_mask_5: 0.2703  loss_dice_5: 0.6026  loss_bbox_5: 0.2441  loss_giou_5: 0.6951  loss_ce_dn_5: 0.1464  loss_mask_dn_5: 0.2849  loss_dice_dn_5: 0.6341  loss_bbox_dn_5: 0.2531  loss_giou_dn_5: 0.6585  loss_ce_6: 0.7849  loss_mask_6: 0.2753  loss_dice_6: 0.6114  loss_bbox_6: 0.2388  loss_giou_6: 0.7024  loss_ce_dn_6: 0.1401  loss_mask_dn_6: 0.2887  loss_dice_dn_6: 0.6222  loss_bbox_dn_6: 0.242  loss_giou_dn_6: 0.6524  loss_ce_7: 0.7433  loss_mask_7: 0.2679  loss_dice_7: 0.6325  loss_bbox_7: 0.2169  loss_giou_7: 0.689  loss_ce_dn_7: 0.1423  loss_mask_dn_7: 0.2934  loss_dice_dn_7: 0.6217  loss_bbox_dn_7: 0.237  loss_giou_dn_7: 0.6531  loss_ce_8: 0.7368  loss_mask_8: 0.2644  loss_dice_8: 0.6156  loss_bbox_8: 0.2199  loss_giou_8: 0.6861  loss_ce_dn_8: 0.139  loss_mask_dn_8: 0.2858  loss_dice_dn_8: 0.6006  loss_bbox_dn_8: 0.2317  loss_giou_dn_8: 0.6484  loss_ce_interm: 1.537  loss_mask_interm: 0.2813  loss_dice_interm: 0.7073  loss_bbox_interm: 0.3783  loss_giou_interm: 0.959    time: 0.8736  last_time: 0.8752  data_time: 0.0115  last_data_time: 0.0133   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 07:54:21 d2.utils.events]: \u001b[0m eta: 3 days, 16:59:12  iter: 1719  total_loss: 60.08  loss_ce: 0.7609  loss_mask: 0.1819  loss_dice: 0.9837  loss_bbox: 0.2242  loss_giou: 0.67  loss_ce_dn: 0.08729  loss_mask_dn: 0.1639  loss_dice_dn: 0.845  loss_bbox_dn: 0.1904  loss_giou_dn: 0.5171  loss_ce_0: 1.464  loss_mask_0: 0.1888  loss_dice_0: 0.9815  loss_bbox_0: 0.3631  loss_giou_0: 0.8598  loss_ce_dn_0: 0.7075  loss_mask_dn_0: 0.6619  loss_dice_dn_0: 2.809  loss_bbox_dn_0: 0.4798  loss_giou_dn_0: 0.9204  loss_ce_1: 1.329  loss_mask_1: 0.1889  loss_dice_1: 0.9269  loss_bbox_1: 0.2694  loss_giou_1: 0.7155  loss_ce_dn_1: 0.2396  loss_mask_dn_1: 0.2003  loss_dice_dn_1: 0.9959  loss_bbox_dn_1: 0.2617  loss_giou_dn_1: 0.6668  loss_ce_2: 1.204  loss_mask_2: 0.1844  loss_dice_2: 0.9275  loss_bbox_2: 0.2471  loss_giou_2: 0.6744  loss_ce_dn_2: 0.1913  loss_mask_dn_2: 0.1782  loss_dice_dn_2: 0.9174  loss_bbox_dn_2: 0.2226  loss_giou_dn_2: 0.5927  loss_ce_3: 1.044  loss_mask_3: 0.1914  loss_dice_3: 0.9032  loss_bbox_3: 0.2443  loss_giou_3: 0.6734  loss_ce_dn_3: 0.1533  loss_mask_dn_3: 0.1729  loss_dice_dn_3: 0.8951  loss_bbox_dn_3: 0.2118  loss_giou_dn_3: 0.5644  loss_ce_4: 0.9695  loss_mask_4: 0.1789  loss_dice_4: 0.8997  loss_bbox_4: 0.2354  loss_giou_4: 0.642  loss_ce_dn_4: 0.1349  loss_mask_dn_4: 0.17  loss_dice_dn_4: 0.8519  loss_bbox_dn_4: 0.2051  loss_giou_dn_4: 0.538  loss_ce_5: 0.7982  loss_mask_5: 0.1864  loss_dice_5: 0.9031  loss_bbox_5: 0.2366  loss_giou_5: 0.6625  loss_ce_dn_5: 0.1152  loss_mask_dn_5: 0.172  loss_dice_dn_5: 0.8508  loss_bbox_dn_5: 0.2041  loss_giou_dn_5: 0.5367  loss_ce_6: 0.7506  loss_mask_6: 0.1854  loss_dice_6: 0.9268  loss_bbox_6: 0.2274  loss_giou_6: 0.662  loss_ce_dn_6: 0.1026  loss_mask_dn_6: 0.1711  loss_dice_dn_6: 0.8552  loss_bbox_dn_6: 0.1986  loss_giou_dn_6: 0.5239  loss_ce_7: 0.7701  loss_mask_7: 0.1764  loss_dice_7: 0.9133  loss_bbox_7: 0.2291  loss_giou_7: 0.6644  loss_ce_dn_7: 0.09628  loss_mask_dn_7: 0.1703  loss_dice_dn_7: 0.8196  loss_bbox_dn_7: 0.1939  loss_giou_dn_7: 0.5255  loss_ce_8: 0.7452  loss_mask_8: 0.1896  loss_dice_8: 1.011  loss_bbox_8: 0.223  loss_giou_8: 0.6708  loss_ce_dn_8: 0.0901  loss_mask_dn_8: 0.1653  loss_dice_dn_8: 0.8347  loss_bbox_dn_8: 0.1912  loss_giou_dn_8: 0.5183  loss_ce_interm: 1.453  loss_mask_interm: 0.1857  loss_dice_interm: 0.9393  loss_bbox_interm: 0.3685  loss_giou_interm: 0.8591    time: 0.8737  last_time: 0.8737  data_time: 0.0116  last_data_time: 0.0127   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 07:54:39 d2.utils.events]: \u001b[0m eta: 3 days, 17:00:57  iter: 1739  total_loss: 64.86  loss_ce: 0.9144  loss_mask: 0.2672  loss_dice: 0.9616  loss_bbox: 0.2747  loss_giou: 0.7425  loss_ce_dn: 0.1192  loss_mask_dn: 0.2967  loss_dice_dn: 0.9675  loss_bbox_dn: 0.2522  loss_giou_dn: 0.5767  loss_ce_0: 1.523  loss_mask_0: 0.2927  loss_dice_0: 1.009  loss_bbox_0: 0.4462  loss_giou_0: 0.9548  loss_ce_dn_0: 0.7328  loss_mask_dn_0: 0.8084  loss_dice_dn_0: 3.068  loss_bbox_dn_0: 0.6295  loss_giou_dn_0: 0.9553  loss_ce_1: 1.391  loss_mask_1: 0.3046  loss_dice_1: 1.068  loss_bbox_1: 0.3478  loss_giou_1: 0.7941  loss_ce_dn_1: 0.2533  loss_mask_dn_1: 0.3581  loss_dice_dn_1: 1.153  loss_bbox_dn_1: 0.3905  loss_giou_dn_1: 0.7005  loss_ce_2: 1.26  loss_mask_2: 0.2867  loss_dice_2: 0.9972  loss_bbox_2: 0.3148  loss_giou_2: 0.7704  loss_ce_dn_2: 0.1989  loss_mask_dn_2: 0.3094  loss_dice_dn_2: 1.073  loss_bbox_dn_2: 0.3043  loss_giou_dn_2: 0.6437  loss_ce_3: 1.141  loss_mask_3: 0.2733  loss_dice_3: 0.945  loss_bbox_3: 0.2947  loss_giou_3: 0.7526  loss_ce_dn_3: 0.1725  loss_mask_dn_3: 0.3075  loss_dice_dn_3: 1.038  loss_bbox_dn_3: 0.2743  loss_giou_dn_3: 0.6171  loss_ce_4: 1.086  loss_mask_4: 0.2796  loss_dice_4: 0.942  loss_bbox_4: 0.2655  loss_giou_4: 0.7496  loss_ce_dn_4: 0.1622  loss_mask_dn_4: 0.2983  loss_dice_dn_4: 1.024  loss_bbox_dn_4: 0.266  loss_giou_dn_4: 0.5967  loss_ce_5: 0.9701  loss_mask_5: 0.2667  loss_dice_5: 1.011  loss_bbox_5: 0.2622  loss_giou_5: 0.7596  loss_ce_dn_5: 0.1389  loss_mask_dn_5: 0.2992  loss_dice_dn_5: 1.017  loss_bbox_dn_5: 0.2593  loss_giou_dn_5: 0.5897  loss_ce_6: 0.9739  loss_mask_6: 0.2586  loss_dice_6: 0.9766  loss_bbox_6: 0.2595  loss_giou_6: 0.743  loss_ce_dn_6: 0.1356  loss_mask_dn_6: 0.2968  loss_dice_dn_6: 1.001  loss_bbox_dn_6: 0.2596  loss_giou_dn_6: 0.579  loss_ce_7: 0.9253  loss_mask_7: 0.2695  loss_dice_7: 0.9689  loss_bbox_7: 0.2708  loss_giou_7: 0.7393  loss_ce_dn_7: 0.1251  loss_mask_dn_7: 0.297  loss_dice_dn_7: 0.9982  loss_bbox_dn_7: 0.2546  loss_giou_dn_7: 0.5778  loss_ce_8: 0.9125  loss_mask_8: 0.2606  loss_dice_8: 0.9618  loss_bbox_8: 0.2728  loss_giou_8: 0.7488  loss_ce_dn_8: 0.1206  loss_mask_dn_8: 0.2957  loss_dice_dn_8: 0.978  loss_bbox_dn_8: 0.2525  loss_giou_dn_8: 0.5768  loss_ce_interm: 1.456  loss_mask_interm: 0.2988  loss_dice_interm: 1.065  loss_bbox_interm: 0.4367  loss_giou_interm: 0.9566    time: 0.8738  last_time: 0.8728  data_time: 0.0154  last_data_time: 0.0089   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 07:54:57 d2.utils.events]: \u001b[0m eta: 3 days, 17:00:16  iter: 1759  total_loss: 62.74  loss_ce: 0.8832  loss_mask: 0.2994  loss_dice: 0.5971  loss_bbox: 0.2331  loss_giou: 0.6601  loss_ce_dn: 0.1357  loss_mask_dn: 0.259  loss_dice_dn: 0.5971  loss_bbox_dn: 0.2434  loss_giou_dn: 0.5136  loss_ce_0: 1.537  loss_mask_0: 0.2842  loss_dice_0: 0.6476  loss_bbox_0: 0.4599  loss_giou_0: 0.8523  loss_ce_dn_0: 0.7472  loss_mask_dn_0: 1.275  loss_dice_dn_0: 2.816  loss_bbox_dn_0: 0.6995  loss_giou_dn_0: 0.9195  loss_ce_1: 1.424  loss_mask_1: 0.332  loss_dice_1: 0.5849  loss_bbox_1: 0.3124  loss_giou_1: 0.7193  loss_ce_dn_1: 0.2592  loss_mask_dn_1: 0.371  loss_dice_dn_1: 0.8209  loss_bbox_dn_1: 0.3727  loss_giou_dn_1: 0.679  loss_ce_2: 1.266  loss_mask_2: 0.3149  loss_dice_2: 0.5409  loss_bbox_2: 0.2629  loss_giou_2: 0.6777  loss_ce_dn_2: 0.2121  loss_mask_dn_2: 0.2974  loss_dice_dn_2: 0.6282  loss_bbox_dn_2: 0.3026  loss_giou_dn_2: 0.627  loss_ce_3: 1.078  loss_mask_3: 0.2998  loss_dice_3: 0.602  loss_bbox_3: 0.2506  loss_giou_3: 0.6551  loss_ce_dn_3: 0.1862  loss_mask_dn_3: 0.2704  loss_dice_dn_3: 0.6045  loss_bbox_dn_3: 0.2746  loss_giou_dn_3: 0.5912  loss_ce_4: 1.045  loss_mask_4: 0.299  loss_dice_4: 0.6251  loss_bbox_4: 0.2436  loss_giou_4: 0.6342  loss_ce_dn_4: 0.1733  loss_mask_dn_4: 0.2645  loss_dice_dn_4: 0.6027  loss_bbox_dn_4: 0.2653  loss_giou_dn_4: 0.5658  loss_ce_5: 0.935  loss_mask_5: 0.282  loss_dice_5: 0.534  loss_bbox_5: 0.236  loss_giou_5: 0.6645  loss_ce_dn_5: 0.1592  loss_mask_dn_5: 0.2635  loss_dice_dn_5: 0.6144  loss_bbox_dn_5: 0.263  loss_giou_dn_5: 0.5503  loss_ce_6: 0.8662  loss_mask_6: 0.2811  loss_dice_6: 0.5475  loss_bbox_6: 0.2379  loss_giou_6: 0.6569  loss_ce_dn_6: 0.1465  loss_mask_dn_6: 0.2676  loss_dice_dn_6: 0.6334  loss_bbox_dn_6: 0.2519  loss_giou_dn_6: 0.549  loss_ce_7: 0.8232  loss_mask_7: 0.29  loss_dice_7: 0.5081  loss_bbox_7: 0.2328  loss_giou_7: 0.6535  loss_ce_dn_7: 0.1396  loss_mask_dn_7: 0.2549  loss_dice_dn_7: 0.6142  loss_bbox_dn_7: 0.2466  loss_giou_dn_7: 0.5264  loss_ce_8: 0.8907  loss_mask_8: 0.2992  loss_dice_8: 0.5282  loss_bbox_8: 0.2318  loss_giou_8: 0.6278  loss_ce_dn_8: 0.1363  loss_mask_dn_8: 0.2591  loss_dice_dn_8: 0.5997  loss_bbox_dn_8: 0.2451  loss_giou_dn_8: 0.521  loss_ce_interm: 1.524  loss_mask_interm: 0.2929  loss_dice_interm: 0.6231  loss_bbox_interm: 0.4597  loss_giou_interm: 0.8519    time: 0.8738  last_time: 0.9042  data_time: 0.0119  last_data_time: 0.0083   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 07:55:14 d2.utils.events]: \u001b[0m eta: 3 days, 17:03:30  iter: 1779  total_loss: 65.14  loss_ce: 0.8629  loss_mask: 0.2916  loss_dice: 0.9805  loss_bbox: 0.2753  loss_giou: 0.6571  loss_ce_dn: 0.1521  loss_mask_dn: 0.2497  loss_dice_dn: 0.8821  loss_bbox_dn: 0.2235  loss_giou_dn: 0.4845  loss_ce_0: 1.502  loss_mask_0: 0.3126  loss_dice_0: 0.9251  loss_bbox_0: 0.4183  loss_giou_0: 0.8853  loss_ce_dn_0: 0.673  loss_mask_dn_0: 0.7463  loss_dice_dn_0: 3.033  loss_bbox_dn_0: 0.5258  loss_giou_dn_0: 0.871  loss_ce_1: 1.412  loss_mask_1: 0.3369  loss_dice_1: 0.9362  loss_bbox_1: 0.3626  loss_giou_1: 0.7389  loss_ce_dn_1: 0.2759  loss_mask_dn_1: 0.3123  loss_dice_dn_1: 1.037  loss_bbox_dn_1: 0.3068  loss_giou_dn_1: 0.6008  loss_ce_2: 1.23  loss_mask_2: 0.3237  loss_dice_2: 1.09  loss_bbox_2: 0.3041  loss_giou_2: 0.7065  loss_ce_dn_2: 0.2461  loss_mask_dn_2: 0.2704  loss_dice_dn_2: 0.9823  loss_bbox_dn_2: 0.2725  loss_giou_dn_2: 0.5375  loss_ce_3: 1.076  loss_mask_3: 0.3136  loss_dice_3: 0.9378  loss_bbox_3: 0.303  loss_giou_3: 0.6781  loss_ce_dn_3: 0.2069  loss_mask_dn_3: 0.2603  loss_dice_dn_3: 0.9349  loss_bbox_dn_3: 0.2579  loss_giou_dn_3: 0.5174  loss_ce_4: 1.021  loss_mask_4: 0.2965  loss_dice_4: 0.9804  loss_bbox_4: 0.3014  loss_giou_4: 0.6663  loss_ce_dn_4: 0.1943  loss_mask_dn_4: 0.2561  loss_dice_dn_4: 0.9294  loss_bbox_dn_4: 0.2455  loss_giou_dn_4: 0.5092  loss_ce_5: 0.9449  loss_mask_5: 0.2979  loss_dice_5: 0.9815  loss_bbox_5: 0.2858  loss_giou_5: 0.6923  loss_ce_dn_5: 0.1921  loss_mask_dn_5: 0.2496  loss_dice_dn_5: 0.8921  loss_bbox_dn_5: 0.2419  loss_giou_dn_5: 0.5042  loss_ce_6: 0.9509  loss_mask_6: 0.2913  loss_dice_6: 0.9509  loss_bbox_6: 0.2806  loss_giou_6: 0.6672  loss_ce_dn_6: 0.1733  loss_mask_dn_6: 0.2556  loss_dice_dn_6: 0.8989  loss_bbox_dn_6: 0.235  loss_giou_dn_6: 0.4939  loss_ce_7: 0.8939  loss_mask_7: 0.3089  loss_dice_7: 0.9693  loss_bbox_7: 0.2766  loss_giou_7: 0.6594  loss_ce_dn_7: 0.164  loss_mask_dn_7: 0.2476  loss_dice_dn_7: 0.9323  loss_bbox_dn_7: 0.2254  loss_giou_dn_7: 0.4888  loss_ce_8: 0.8691  loss_mask_8: 0.2974  loss_dice_8: 1.015  loss_bbox_8: 0.2756  loss_giou_8: 0.6596  loss_ce_dn_8: 0.1532  loss_mask_dn_8: 0.2474  loss_dice_dn_8: 0.8952  loss_bbox_dn_8: 0.2214  loss_giou_dn_8: 0.4856  loss_ce_interm: 1.525  loss_mask_interm: 0.3095  loss_dice_interm: 0.9513  loss_bbox_interm: 0.4321  loss_giou_interm: 0.8866    time: 0.8739  last_time: 0.8710  data_time: 0.0153  last_data_time: 0.0127   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 07:55:32 d2.utils.events]: \u001b[0m eta: 3 days, 17:03:02  iter: 1799  total_loss: 53.17  loss_ce: 0.6058  loss_mask: 0.2141  loss_dice: 0.5667  loss_bbox: 0.2586  loss_giou: 0.4826  loss_ce_dn: 0.07694  loss_mask_dn: 0.2164  loss_dice_dn: 0.5706  loss_bbox_dn: 0.2163  loss_giou_dn: 0.4873  loss_ce_0: 1.329  loss_mask_0: 0.2227  loss_dice_0: 0.5851  loss_bbox_0: 0.4252  loss_giou_0: 0.7431  loss_ce_dn_0: 0.759  loss_mask_dn_0: 1.071  loss_dice_dn_0: 2.742  loss_bbox_dn_0: 0.6909  loss_giou_dn_0: 0.8887  loss_ce_1: 1.277  loss_mask_1: 0.2204  loss_dice_1: 0.5448  loss_bbox_1: 0.2762  loss_giou_1: 0.614  loss_ce_dn_1: 0.2197  loss_mask_dn_1: 0.273  loss_dice_dn_1: 0.6894  loss_bbox_dn_1: 0.3547  loss_giou_dn_1: 0.6155  loss_ce_2: 1.084  loss_mask_2: 0.222  loss_dice_2: 0.56  loss_bbox_2: 0.3105  loss_giou_2: 0.5385  loss_ce_dn_2: 0.1609  loss_mask_dn_2: 0.235  loss_dice_dn_2: 0.6171  loss_bbox_dn_2: 0.2948  loss_giou_dn_2: 0.549  loss_ce_3: 0.8373  loss_mask_3: 0.2102  loss_dice_3: 0.5822  loss_bbox_3: 0.2751  loss_giou_3: 0.5434  loss_ce_dn_3: 0.1233  loss_mask_dn_3: 0.2213  loss_dice_dn_3: 0.6241  loss_bbox_dn_3: 0.2535  loss_giou_dn_3: 0.5389  loss_ce_4: 0.7572  loss_mask_4: 0.2171  loss_dice_4: 0.58  loss_bbox_4: 0.2697  loss_giou_4: 0.5289  loss_ce_dn_4: 0.1174  loss_mask_dn_4: 0.2167  loss_dice_dn_4: 0.6076  loss_bbox_dn_4: 0.2304  loss_giou_dn_4: 0.5099  loss_ce_5: 0.717  loss_mask_5: 0.2114  loss_dice_5: 0.5578  loss_bbox_5: 0.2615  loss_giou_5: 0.5161  loss_ce_dn_5: 0.0989  loss_mask_dn_5: 0.2147  loss_dice_dn_5: 0.5773  loss_bbox_dn_5: 0.2296  loss_giou_dn_5: 0.5108  loss_ce_6: 0.7049  loss_mask_6: 0.2101  loss_dice_6: 0.544  loss_bbox_6: 0.2624  loss_giou_6: 0.51  loss_ce_dn_6: 0.09041  loss_mask_dn_6: 0.2182  loss_dice_dn_6: 0.5653  loss_bbox_dn_6: 0.2191  loss_giou_dn_6: 0.4956  loss_ce_7: 0.6644  loss_mask_7: 0.2112  loss_dice_7: 0.5528  loss_bbox_7: 0.2626  loss_giou_7: 0.4845  loss_ce_dn_7: 0.08046  loss_mask_dn_7: 0.2197  loss_dice_dn_7: 0.5597  loss_bbox_dn_7: 0.2194  loss_giou_dn_7: 0.4892  loss_ce_8: 0.6155  loss_mask_8: 0.2139  loss_dice_8: 0.5581  loss_bbox_8: 0.2585  loss_giou_8: 0.4846  loss_ce_dn_8: 0.07984  loss_mask_dn_8: 0.2182  loss_dice_dn_8: 0.56  loss_bbox_dn_8: 0.2158  loss_giou_dn_8: 0.4837  loss_ce_interm: 1.32  loss_mask_interm: 0.2264  loss_dice_interm: 0.5576  loss_bbox_interm: 0.4285  loss_giou_interm: 0.7418    time: 0.8738  last_time: 0.8560  data_time: 0.0104  last_data_time: 0.0151   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 07:55:49 d2.utils.events]: \u001b[0m eta: 3 days, 17:02:50  iter: 1819  total_loss: 66.73  loss_ce: 0.8616  loss_mask: 0.296  loss_dice: 0.8426  loss_bbox: 0.2617  loss_giou: 0.6512  loss_ce_dn: 0.1361  loss_mask_dn: 0.3183  loss_dice_dn: 0.8866  loss_bbox_dn: 0.2383  loss_giou_dn: 0.5241  loss_ce_0: 1.499  loss_mask_0: 0.2835  loss_dice_0: 0.8911  loss_bbox_0: 0.4217  loss_giou_0: 0.8761  loss_ce_dn_0: 0.6782  loss_mask_dn_0: 0.9806  loss_dice_dn_0: 2.663  loss_bbox_dn_0: 0.6256  loss_giou_dn_0: 0.9422  loss_ce_1: 1.371  loss_mask_1: 0.2727  loss_dice_1: 0.8791  loss_bbox_1: 0.3301  loss_giou_1: 0.7484  loss_ce_dn_1: 0.2712  loss_mask_dn_1: 0.4534  loss_dice_dn_1: 1.006  loss_bbox_dn_1: 0.3348  loss_giou_dn_1: 0.6512  loss_ce_2: 1.263  loss_mask_2: 0.2685  loss_dice_2: 0.9014  loss_bbox_2: 0.3021  loss_giou_2: 0.7052  loss_ce_dn_2: 0.2217  loss_mask_dn_2: 0.39  loss_dice_dn_2: 0.9694  loss_bbox_dn_2: 0.2855  loss_giou_dn_2: 0.582  loss_ce_3: 1.081  loss_mask_3: 0.3046  loss_dice_3: 0.846  loss_bbox_3: 0.2819  loss_giou_3: 0.6422  loss_ce_dn_3: 0.1877  loss_mask_dn_3: 0.3434  loss_dice_dn_3: 0.952  loss_bbox_dn_3: 0.2616  loss_giou_dn_3: 0.5639  loss_ce_4: 1.019  loss_mask_4: 0.2927  loss_dice_4: 0.9281  loss_bbox_4: 0.2683  loss_giou_4: 0.672  loss_ce_dn_4: 0.1789  loss_mask_dn_4: 0.3301  loss_dice_dn_4: 0.8999  loss_bbox_dn_4: 0.2442  loss_giou_dn_4: 0.5433  loss_ce_5: 0.9567  loss_mask_5: 0.3068  loss_dice_5: 0.8421  loss_bbox_5: 0.2608  loss_giou_5: 0.6756  loss_ce_dn_5: 0.1649  loss_mask_dn_5: 0.3286  loss_dice_dn_5: 0.8977  loss_bbox_dn_5: 0.2451  loss_giou_dn_5: 0.5463  loss_ce_6: 0.8985  loss_mask_6: 0.3155  loss_dice_6: 0.8555  loss_bbox_6: 0.2596  loss_giou_6: 0.6499  loss_ce_dn_6: 0.1532  loss_mask_dn_6: 0.3264  loss_dice_dn_6: 0.8999  loss_bbox_dn_6: 0.2442  loss_giou_dn_6: 0.5319  loss_ce_7: 0.8545  loss_mask_7: 0.3093  loss_dice_7: 0.8574  loss_bbox_7: 0.2658  loss_giou_7: 0.6787  loss_ce_dn_7: 0.1459  loss_mask_dn_7: 0.3202  loss_dice_dn_7: 0.8729  loss_bbox_dn_7: 0.2405  loss_giou_dn_7: 0.5231  loss_ce_8: 0.8349  loss_mask_8: 0.3019  loss_dice_8: 0.8519  loss_bbox_8: 0.2705  loss_giou_8: 0.6389  loss_ce_dn_8: 0.1379  loss_mask_dn_8: 0.3218  loss_dice_dn_8: 0.8898  loss_bbox_dn_8: 0.2386  loss_giou_dn_8: 0.5214  loss_ce_interm: 1.522  loss_mask_interm: 0.2775  loss_dice_interm: 0.975  loss_bbox_interm: 0.4529  loss_giou_interm: 0.8968    time: 0.8738  last_time: 0.8809  data_time: 0.0118  last_data_time: 0.0117   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 07:56:07 d2.utils.events]: \u001b[0m eta: 3 days, 17:02:46  iter: 1839  total_loss: 63.58  loss_ce: 0.7838  loss_mask: 0.3583  loss_dice: 0.8477  loss_bbox: 0.3217  loss_giou: 0.7064  loss_ce_dn: 0.123  loss_mask_dn: 0.3443  loss_dice_dn: 0.757  loss_bbox_dn: 0.2824  loss_giou_dn: 0.6201  loss_ce_0: 1.466  loss_mask_0: 0.3206  loss_dice_0: 0.875  loss_bbox_0: 0.4944  loss_giou_0: 0.9233  loss_ce_dn_0: 0.6903  loss_mask_dn_0: 0.995  loss_dice_dn_0: 3.132  loss_bbox_dn_0: 0.6827  loss_giou_dn_0: 0.9749  loss_ce_1: 1.349  loss_mask_1: 0.3101  loss_dice_1: 0.8832  loss_bbox_1: 0.3916  loss_giou_1: 0.7879  loss_ce_dn_1: 0.2383  loss_mask_dn_1: 0.3982  loss_dice_dn_1: 0.8654  loss_bbox_dn_1: 0.3872  loss_giou_dn_1: 0.7507  loss_ce_2: 1.144  loss_mask_2: 0.3248  loss_dice_2: 0.8989  loss_bbox_2: 0.3654  loss_giou_2: 0.7778  loss_ce_dn_2: 0.1888  loss_mask_dn_2: 0.381  loss_dice_dn_2: 0.8528  loss_bbox_dn_2: 0.3364  loss_giou_dn_2: 0.678  loss_ce_3: 1.06  loss_mask_3: 0.3553  loss_dice_3: 0.8738  loss_bbox_3: 0.3578  loss_giou_3: 0.7635  loss_ce_dn_3: 0.1728  loss_mask_dn_3: 0.3659  loss_dice_dn_3: 0.7851  loss_bbox_dn_3: 0.3038  loss_giou_dn_3: 0.6495  loss_ce_4: 0.9398  loss_mask_4: 0.3533  loss_dice_4: 0.9089  loss_bbox_4: 0.3524  loss_giou_4: 0.7429  loss_ce_dn_4: 0.15  loss_mask_dn_4: 0.3452  loss_dice_dn_4: 0.7812  loss_bbox_dn_4: 0.2941  loss_giou_dn_4: 0.6312  loss_ce_5: 0.8688  loss_mask_5: 0.3522  loss_dice_5: 0.8367  loss_bbox_5: 0.3381  loss_giou_5: 0.7532  loss_ce_dn_5: 0.1375  loss_mask_dn_5: 0.3489  loss_dice_dn_5: 0.7833  loss_bbox_dn_5: 0.2915  loss_giou_dn_5: 0.6241  loss_ce_6: 0.8364  loss_mask_6: 0.3346  loss_dice_6: 0.8648  loss_bbox_6: 0.3372  loss_giou_6: 0.7423  loss_ce_dn_6: 0.127  loss_mask_dn_6: 0.3437  loss_dice_dn_6: 0.7561  loss_bbox_dn_6: 0.285  loss_giou_dn_6: 0.6184  loss_ce_7: 0.8143  loss_mask_7: 0.3361  loss_dice_7: 0.8654  loss_bbox_7: 0.3281  loss_giou_7: 0.7364  loss_ce_dn_7: 0.124  loss_mask_dn_7: 0.3465  loss_dice_dn_7: 0.7529  loss_bbox_dn_7: 0.2828  loss_giou_dn_7: 0.6205  loss_ce_8: 0.818  loss_mask_8: 0.3598  loss_dice_8: 0.8737  loss_bbox_8: 0.3351  loss_giou_8: 0.7129  loss_ce_dn_8: 0.1227  loss_mask_dn_8: 0.3446  loss_dice_dn_8: 0.7495  loss_bbox_dn_8: 0.2825  loss_giou_dn_8: 0.6192  loss_ce_interm: 1.467  loss_mask_interm: 0.3138  loss_dice_interm: 0.8951  loss_bbox_interm: 0.4801  loss_giou_interm: 0.9349    time: 0.8739  last_time: 0.8834  data_time: 0.0119  last_data_time: 0.0120   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 07:56:24 d2.utils.events]: \u001b[0m eta: 3 days, 16:59:50  iter: 1859  total_loss: 61.82  loss_ce: 0.7018  loss_mask: 0.2954  loss_dice: 0.9098  loss_bbox: 0.305  loss_giou: 0.6606  loss_ce_dn: 0.1033  loss_mask_dn: 0.2916  loss_dice_dn: 0.8597  loss_bbox_dn: 0.2264  loss_giou_dn: 0.5614  loss_ce_0: 1.428  loss_mask_0: 0.2997  loss_dice_0: 0.9763  loss_bbox_0: 0.378  loss_giou_0: 0.8634  loss_ce_dn_0: 0.6849  loss_mask_dn_0: 0.9273  loss_dice_dn_0: 2.815  loss_bbox_dn_0: 0.6589  loss_giou_dn_0: 0.9136  loss_ce_1: 1.353  loss_mask_1: 0.2836  loss_dice_1: 0.9268  loss_bbox_1: 0.32  loss_giou_1: 0.7613  loss_ce_dn_1: 0.2213  loss_mask_dn_1: 0.3178  loss_dice_dn_1: 0.9563  loss_bbox_dn_1: 0.3967  loss_giou_dn_1: 0.6865  loss_ce_2: 1.153  loss_mask_2: 0.3043  loss_dice_2: 0.9101  loss_bbox_2: 0.3186  loss_giou_2: 0.7291  loss_ce_dn_2: 0.1723  loss_mask_dn_2: 0.3037  loss_dice_dn_2: 0.9206  loss_bbox_dn_2: 0.3091  loss_giou_dn_2: 0.6255  loss_ce_3: 1.034  loss_mask_3: 0.3028  loss_dice_3: 0.9198  loss_bbox_3: 0.3046  loss_giou_3: 0.6828  loss_ce_dn_3: 0.1435  loss_mask_dn_3: 0.307  loss_dice_dn_3: 0.8974  loss_bbox_dn_3: 0.2626  loss_giou_dn_3: 0.605  loss_ce_4: 0.9429  loss_mask_4: 0.3172  loss_dice_4: 0.8808  loss_bbox_4: 0.2835  loss_giou_4: 0.6762  loss_ce_dn_4: 0.1346  loss_mask_dn_4: 0.2998  loss_dice_dn_4: 0.8785  loss_bbox_dn_4: 0.2483  loss_giou_dn_4: 0.5882  loss_ce_5: 0.7808  loss_mask_5: 0.2924  loss_dice_5: 0.8192  loss_bbox_5: 0.2965  loss_giou_5: 0.6552  loss_ce_dn_5: 0.1221  loss_mask_dn_5: 0.297  loss_dice_dn_5: 0.8819  loss_bbox_dn_5: 0.2385  loss_giou_dn_5: 0.5794  loss_ce_6: 0.7667  loss_mask_6: 0.2863  loss_dice_6: 0.894  loss_bbox_6: 0.2739  loss_giou_6: 0.6337  loss_ce_dn_6: 0.1193  loss_mask_dn_6: 0.2984  loss_dice_dn_6: 0.8478  loss_bbox_dn_6: 0.2277  loss_giou_dn_6: 0.5716  loss_ce_7: 0.7407  loss_mask_7: 0.2835  loss_dice_7: 0.8983  loss_bbox_7: 0.2782  loss_giou_7: 0.6308  loss_ce_dn_7: 0.1148  loss_mask_dn_7: 0.2933  loss_dice_dn_7: 0.869  loss_bbox_dn_7: 0.229  loss_giou_dn_7: 0.5683  loss_ce_8: 0.7215  loss_mask_8: 0.2977  loss_dice_8: 0.9279  loss_bbox_8: 0.3018  loss_giou_8: 0.6403  loss_ce_dn_8: 0.1084  loss_mask_dn_8: 0.2938  loss_dice_dn_8: 0.8563  loss_bbox_dn_8: 0.225  loss_giou_dn_8: 0.5613  loss_ce_interm: 1.434  loss_mask_interm: 0.3029  loss_dice_interm: 0.959  loss_bbox_interm: 0.381  loss_giou_interm: 0.8588    time: 0.8738  last_time: 0.8668  data_time: 0.0110  last_data_time: 0.0109   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 07:56:42 d2.utils.events]: \u001b[0m eta: 3 days, 16:59:17  iter: 1879  total_loss: 70.15  loss_ce: 0.813  loss_mask: 0.3441  loss_dice: 0.7223  loss_bbox: 0.2607  loss_giou: 0.8179  loss_ce_dn: 0.1294  loss_mask_dn: 0.3706  loss_dice_dn: 0.7616  loss_bbox_dn: 0.2589  loss_giou_dn: 0.6199  loss_ce_0: 1.557  loss_mask_0: 0.4046  loss_dice_0: 0.78  loss_bbox_0: 0.4833  loss_giou_0: 0.99  loss_ce_dn_0: 0.6638  loss_mask_dn_0: 1.116  loss_dice_dn_0: 3.202  loss_bbox_dn_0: 0.6508  loss_giou_dn_0: 1.008  loss_ce_1: 1.47  loss_mask_1: 0.353  loss_dice_1: 0.7618  loss_bbox_1: 0.2827  loss_giou_1: 0.8609  loss_ce_dn_1: 0.2736  loss_mask_dn_1: 0.461  loss_dice_dn_1: 0.8887  loss_bbox_dn_1: 0.4037  loss_giou_dn_1: 0.7257  loss_ce_2: 1.292  loss_mask_2: 0.366  loss_dice_2: 0.8113  loss_bbox_2: 0.2551  loss_giou_2: 0.8071  loss_ce_dn_2: 0.2264  loss_mask_dn_2: 0.3863  loss_dice_dn_2: 0.8039  loss_bbox_dn_2: 0.3364  loss_giou_dn_2: 0.6646  loss_ce_3: 1.086  loss_mask_3: 0.3367  loss_dice_3: 0.8183  loss_bbox_3: 0.2583  loss_giou_3: 0.8142  loss_ce_dn_3: 0.1969  loss_mask_dn_3: 0.3648  loss_dice_dn_3: 0.787  loss_bbox_dn_3: 0.3138  loss_giou_dn_3: 0.6506  loss_ce_4: 1.012  loss_mask_4: 0.3387  loss_dice_4: 0.7665  loss_bbox_4: 0.2486  loss_giou_4: 0.8039  loss_ce_dn_4: 0.1788  loss_mask_dn_4: 0.3643  loss_dice_dn_4: 0.8064  loss_bbox_dn_4: 0.291  loss_giou_dn_4: 0.6323  loss_ce_5: 0.9086  loss_mask_5: 0.3659  loss_dice_5: 0.7866  loss_bbox_5: 0.266  loss_giou_5: 0.8145  loss_ce_dn_5: 0.1638  loss_mask_dn_5: 0.3851  loss_dice_dn_5: 0.7702  loss_bbox_dn_5: 0.2781  loss_giou_dn_5: 0.6312  loss_ce_6: 0.8415  loss_mask_6: 0.35  loss_dice_6: 0.7428  loss_bbox_6: 0.2606  loss_giou_6: 0.8128  loss_ce_dn_6: 0.1459  loss_mask_dn_6: 0.3667  loss_dice_dn_6: 0.77  loss_bbox_dn_6: 0.2703  loss_giou_dn_6: 0.6259  loss_ce_7: 0.8076  loss_mask_7: 0.3511  loss_dice_7: 0.6872  loss_bbox_7: 0.2612  loss_giou_7: 0.8144  loss_ce_dn_7: 0.139  loss_mask_dn_7: 0.3823  loss_dice_dn_7: 0.7539  loss_bbox_dn_7: 0.265  loss_giou_dn_7: 0.6229  loss_ce_8: 0.8171  loss_mask_8: 0.3491  loss_dice_8: 0.7196  loss_bbox_8: 0.2623  loss_giou_8: 0.8121  loss_ce_dn_8: 0.1342  loss_mask_dn_8: 0.3797  loss_dice_dn_8: 0.7544  loss_bbox_dn_8: 0.2609  loss_giou_dn_8: 0.6197  loss_ce_interm: 1.557  loss_mask_interm: 0.4024  loss_dice_interm: 0.78  loss_bbox_interm: 0.4789  loss_giou_interm: 0.9744    time: 0.8737  last_time: 0.8494  data_time: 0.0115  last_data_time: 0.0143   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 07:56:59 d2.utils.events]: \u001b[0m eta: 3 days, 16:58:32  iter: 1899  total_loss: 62.9  loss_ce: 0.7464  loss_mask: 0.2264  loss_dice: 0.8402  loss_bbox: 0.2802  loss_giou: 0.689  loss_ce_dn: 0.1129  loss_mask_dn: 0.2441  loss_dice_dn: 0.8343  loss_bbox_dn: 0.2545  loss_giou_dn: 0.5335  loss_ce_0: 1.393  loss_mask_0: 0.2554  loss_dice_0: 0.8931  loss_bbox_0: 0.3691  loss_giou_0: 0.8963  loss_ce_dn_0: 0.7004  loss_mask_dn_0: 0.7532  loss_dice_dn_0: 2.681  loss_bbox_dn_0: 0.5408  loss_giou_dn_0: 0.9129  loss_ce_1: 1.31  loss_mask_1: 0.2428  loss_dice_1: 0.8731  loss_bbox_1: 0.2706  loss_giou_1: 0.7673  loss_ce_dn_1: 0.2451  loss_mask_dn_1: 0.3128  loss_dice_dn_1: 0.929  loss_bbox_dn_1: 0.3486  loss_giou_dn_1: 0.6745  loss_ce_2: 1.145  loss_mask_2: 0.2472  loss_dice_2: 0.8649  loss_bbox_2: 0.264  loss_giou_2: 0.741  loss_ce_dn_2: 0.1866  loss_mask_dn_2: 0.2572  loss_dice_dn_2: 0.8552  loss_bbox_dn_2: 0.3143  loss_giou_dn_2: 0.6095  loss_ce_3: 0.9553  loss_mask_3: 0.238  loss_dice_3: 0.8185  loss_bbox_3: 0.245  loss_giou_3: 0.7078  loss_ce_dn_3: 0.1474  loss_mask_dn_3: 0.2512  loss_dice_dn_3: 0.8239  loss_bbox_dn_3: 0.2868  loss_giou_dn_3: 0.5783  loss_ce_4: 0.8616  loss_mask_4: 0.2394  loss_dice_4: 0.8282  loss_bbox_4: 0.2385  loss_giou_4: 0.7192  loss_ce_dn_4: 0.1376  loss_mask_dn_4: 0.2463  loss_dice_dn_4: 0.8297  loss_bbox_dn_4: 0.2756  loss_giou_dn_4: 0.5542  loss_ce_5: 0.8178  loss_mask_5: 0.2424  loss_dice_5: 0.7814  loss_bbox_5: 0.2193  loss_giou_5: 0.7024  loss_ce_dn_5: 0.13  loss_mask_dn_5: 0.2439  loss_dice_dn_5: 0.802  loss_bbox_dn_5: 0.2663  loss_giou_dn_5: 0.5491  loss_ce_6: 0.8041  loss_mask_6: 0.2371  loss_dice_6: 0.7993  loss_bbox_6: 0.2299  loss_giou_6: 0.7136  loss_ce_dn_6: 0.121  loss_mask_dn_6: 0.2451  loss_dice_dn_6: 0.8224  loss_bbox_dn_6: 0.262  loss_giou_dn_6: 0.5459  loss_ce_7: 0.76  loss_mask_7: 0.227  loss_dice_7: 0.7603  loss_bbox_7: 0.2575  loss_giou_7: 0.6983  loss_ce_dn_7: 0.1193  loss_mask_dn_7: 0.2433  loss_dice_dn_7: 0.8259  loss_bbox_dn_7: 0.2555  loss_giou_dn_7: 0.5388  loss_ce_8: 0.757  loss_mask_8: 0.2301  loss_dice_8: 0.7544  loss_bbox_8: 0.2769  loss_giou_8: 0.7508  loss_ce_dn_8: 0.1155  loss_mask_dn_8: 0.2428  loss_dice_dn_8: 0.8194  loss_bbox_dn_8: 0.2548  loss_giou_dn_8: 0.5329  loss_ce_interm: 1.417  loss_mask_interm: 0.249  loss_dice_interm: 0.9207  loss_bbox_interm: 0.3493  loss_giou_interm: 0.8951    time: 0.8738  last_time: 0.8667  data_time: 0.0125  last_data_time: 0.0129   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 07:57:17 d2.utils.events]: \u001b[0m eta: 3 days, 16:57:41  iter: 1919  total_loss: 54.65  loss_ce: 0.7314  loss_mask: 0.2295  loss_dice: 0.6712  loss_bbox: 0.2332  loss_giou: 0.6401  loss_ce_dn: 0.1044  loss_mask_dn: 0.2541  loss_dice_dn: 0.6129  loss_bbox_dn: 0.1877  loss_giou_dn: 0.523  loss_ce_0: 1.466  loss_mask_0: 0.2557  loss_dice_0: 0.7208  loss_bbox_0: 0.3631  loss_giou_0: 0.8311  loss_ce_dn_0: 0.7154  loss_mask_dn_0: 0.905  loss_dice_dn_0: 3.08  loss_bbox_dn_0: 0.5404  loss_giou_dn_0: 0.9634  loss_ce_1: 1.375  loss_mask_1: 0.2456  loss_dice_1: 0.7065  loss_bbox_1: 0.2614  loss_giou_1: 0.6752  loss_ce_dn_1: 0.2552  loss_mask_dn_1: 0.2933  loss_dice_dn_1: 0.7686  loss_bbox_dn_1: 0.3011  loss_giou_dn_1: 0.6746  loss_ce_2: 1.229  loss_mask_2: 0.2598  loss_dice_2: 0.6734  loss_bbox_2: 0.248  loss_giou_2: 0.6511  loss_ce_dn_2: 0.2053  loss_mask_dn_2: 0.2579  loss_dice_dn_2: 0.6688  loss_bbox_dn_2: 0.2469  loss_giou_dn_2: 0.5977  loss_ce_3: 1.082  loss_mask_3: 0.233  loss_dice_3: 0.672  loss_bbox_3: 0.2459  loss_giou_3: 0.6401  loss_ce_dn_3: 0.1815  loss_mask_dn_3: 0.2501  loss_dice_dn_3: 0.6435  loss_bbox_dn_3: 0.2272  loss_giou_dn_3: 0.5671  loss_ce_4: 0.9181  loss_mask_4: 0.2443  loss_dice_4: 0.6907  loss_bbox_4: 0.2364  loss_giou_4: 0.6245  loss_ce_dn_4: 0.1617  loss_mask_dn_4: 0.2484  loss_dice_dn_4: 0.6205  loss_bbox_dn_4: 0.2146  loss_giou_dn_4: 0.5507  loss_ce_5: 0.8298  loss_mask_5: 0.2286  loss_dice_5: 0.714  loss_bbox_5: 0.2326  loss_giou_5: 0.6681  loss_ce_dn_5: 0.1332  loss_mask_dn_5: 0.2438  loss_dice_dn_5: 0.615  loss_bbox_dn_5: 0.2059  loss_giou_dn_5: 0.542  loss_ce_6: 0.7631  loss_mask_6: 0.2284  loss_dice_6: 0.7123  loss_bbox_6: 0.2311  loss_giou_6: 0.6609  loss_ce_dn_6: 0.1178  loss_mask_dn_6: 0.252  loss_dice_dn_6: 0.6227  loss_bbox_dn_6: 0.1957  loss_giou_dn_6: 0.5322  loss_ce_7: 0.7482  loss_mask_7: 0.23  loss_dice_7: 0.7475  loss_bbox_7: 0.2333  loss_giou_7: 0.6333  loss_ce_dn_7: 0.1093  loss_mask_dn_7: 0.2512  loss_dice_dn_7: 0.5928  loss_bbox_dn_7: 0.191  loss_giou_dn_7: 0.5292  loss_ce_8: 0.7786  loss_mask_8: 0.2326  loss_dice_8: 0.7056  loss_bbox_8: 0.2338  loss_giou_8: 0.6342  loss_ce_dn_8: 0.1072  loss_mask_dn_8: 0.25  loss_dice_dn_8: 0.6105  loss_bbox_dn_8: 0.1877  loss_giou_dn_8: 0.523  loss_ce_interm: 1.458  loss_mask_interm: 0.2557  loss_dice_interm: 0.6916  loss_bbox_interm: 0.3631  loss_giou_interm: 0.8309    time: 0.8738  last_time: 0.8714  data_time: 0.0119  last_data_time: 0.0071   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 07:57:34 d2.utils.events]: \u001b[0m eta: 3 days, 16:53:21  iter: 1939  total_loss: 61.83  loss_ce: 0.8313  loss_mask: 0.3285  loss_dice: 0.6867  loss_bbox: 0.2967  loss_giou: 0.7059  loss_ce_dn: 0.1483  loss_mask_dn: 0.3109  loss_dice_dn: 0.6998  loss_bbox_dn: 0.2754  loss_giou_dn: 0.4962  loss_ce_0: 1.519  loss_mask_0: 0.3401  loss_dice_0: 0.696  loss_bbox_0: 0.4289  loss_giou_0: 0.9338  loss_ce_dn_0: 0.7637  loss_mask_dn_0: 1.053  loss_dice_dn_0: 3.373  loss_bbox_dn_0: 0.6895  loss_giou_dn_0: 0.9789  loss_ce_1: 1.5  loss_mask_1: 0.3198  loss_dice_1: 0.7373  loss_bbox_1: 0.342  loss_giou_1: 0.8011  loss_ce_dn_1: 0.3276  loss_mask_dn_1: 0.3807  loss_dice_dn_1: 0.7861  loss_bbox_dn_1: 0.4183  loss_giou_dn_1: 0.6406  loss_ce_2: 1.278  loss_mask_2: 0.3095  loss_dice_2: 0.7156  loss_bbox_2: 0.3413  loss_giou_2: 0.7161  loss_ce_dn_2: 0.2707  loss_mask_dn_2: 0.3494  loss_dice_dn_2: 0.7362  loss_bbox_dn_2: 0.3403  loss_giou_dn_2: 0.5456  loss_ce_3: 1.158  loss_mask_3: 0.3177  loss_dice_3: 0.7141  loss_bbox_3: 0.3184  loss_giou_3: 0.7267  loss_ce_dn_3: 0.2375  loss_mask_dn_3: 0.3346  loss_dice_dn_3: 0.7226  loss_bbox_dn_3: 0.3185  loss_giou_dn_3: 0.5155  loss_ce_4: 1.072  loss_mask_4: 0.3036  loss_dice_4: 0.7032  loss_bbox_4: 0.2878  loss_giou_4: 0.7195  loss_ce_dn_4: 0.2119  loss_mask_dn_4: 0.3228  loss_dice_dn_4: 0.7004  loss_bbox_dn_4: 0.2972  loss_giou_dn_4: 0.5073  loss_ce_5: 0.9692  loss_mask_5: 0.3226  loss_dice_5: 0.6891  loss_bbox_5: 0.3009  loss_giou_5: 0.702  loss_ce_dn_5: 0.1904  loss_mask_dn_5: 0.3131  loss_dice_dn_5: 0.7061  loss_bbox_dn_5: 0.2915  loss_giou_dn_5: 0.5028  loss_ce_6: 0.9149  loss_mask_6: 0.3411  loss_dice_6: 0.6923  loss_bbox_6: 0.3021  loss_giou_6: 0.7049  loss_ce_dn_6: 0.1815  loss_mask_dn_6: 0.3155  loss_dice_dn_6: 0.6945  loss_bbox_dn_6: 0.2826  loss_giou_dn_6: 0.4976  loss_ce_7: 0.8523  loss_mask_7: 0.3314  loss_dice_7: 0.6877  loss_bbox_7: 0.299  loss_giou_7: 0.7066  loss_ce_dn_7: 0.17  loss_mask_dn_7: 0.3107  loss_dice_dn_7: 0.6726  loss_bbox_dn_7: 0.2797  loss_giou_dn_7: 0.4965  loss_ce_8: 0.8524  loss_mask_8: 0.3286  loss_dice_8: 0.6928  loss_bbox_8: 0.2961  loss_giou_8: 0.7036  loss_ce_dn_8: 0.1599  loss_mask_dn_8: 0.3106  loss_dice_dn_8: 0.6928  loss_bbox_dn_8: 0.2756  loss_giou_dn_8: 0.4986  loss_ce_interm: 1.502  loss_mask_interm: 0.3435  loss_dice_interm: 0.7423  loss_bbox_interm: 0.421  loss_giou_interm: 0.9416    time: 0.8737  last_time: 0.8661  data_time: 0.0120  last_data_time: 0.0114   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 07:57:52 d2.utils.events]: \u001b[0m eta: 3 days, 16:52:55  iter: 1959  total_loss: 60.74  loss_ce: 0.8539  loss_mask: 0.2285  loss_dice: 0.8247  loss_bbox: 0.244  loss_giou: 0.7613  loss_ce_dn: 0.1227  loss_mask_dn: 0.2268  loss_dice_dn: 0.7811  loss_bbox_dn: 0.2253  loss_giou_dn: 0.5872  loss_ce_0: 1.392  loss_mask_0: 0.2424  loss_dice_0: 0.8717  loss_bbox_0: 0.3661  loss_giou_0: 0.9643  loss_ce_dn_0: 0.7259  loss_mask_dn_0: 0.7914  loss_dice_dn_0: 2.903  loss_bbox_dn_0: 0.5593  loss_giou_dn_0: 0.9689  loss_ce_1: 1.35  loss_mask_1: 0.2348  loss_dice_1: 0.8384  loss_bbox_1: 0.2881  loss_giou_1: 0.8425  loss_ce_dn_1: 0.2317  loss_mask_dn_1: 0.2745  loss_dice_dn_1: 0.9571  loss_bbox_dn_1: 0.3193  loss_giou_dn_1: 0.704  loss_ce_2: 1.188  loss_mask_2: 0.2293  loss_dice_2: 0.8552  loss_bbox_2: 0.2597  loss_giou_2: 0.7976  loss_ce_dn_2: 0.1953  loss_mask_dn_2: 0.2464  loss_dice_dn_2: 0.8963  loss_bbox_dn_2: 0.2716  loss_giou_dn_2: 0.6455  loss_ce_3: 0.9859  loss_mask_3: 0.2194  loss_dice_3: 0.9223  loss_bbox_3: 0.2625  loss_giou_3: 0.7876  loss_ce_dn_3: 0.1676  loss_mask_dn_3: 0.2447  loss_dice_dn_3: 0.8531  loss_bbox_dn_3: 0.2527  loss_giou_dn_3: 0.6227  loss_ce_4: 0.9118  loss_mask_4: 0.2377  loss_dice_4: 0.8371  loss_bbox_4: 0.2589  loss_giou_4: 0.7901  loss_ce_dn_4: 0.1558  loss_mask_dn_4: 0.232  loss_dice_dn_4: 0.8356  loss_bbox_dn_4: 0.2392  loss_giou_dn_4: 0.6074  loss_ce_5: 0.8547  loss_mask_5: 0.2289  loss_dice_5: 0.8513  loss_bbox_5: 0.2456  loss_giou_5: 0.779  loss_ce_dn_5: 0.1468  loss_mask_dn_5: 0.2296  loss_dice_dn_5: 0.8301  loss_bbox_dn_5: 0.2339  loss_giou_dn_5: 0.6016  loss_ce_6: 0.8508  loss_mask_6: 0.2343  loss_dice_6: 0.8902  loss_bbox_6: 0.2492  loss_giou_6: 0.7751  loss_ce_dn_6: 0.1337  loss_mask_dn_6: 0.2301  loss_dice_dn_6: 0.8113  loss_bbox_dn_6: 0.2267  loss_giou_dn_6: 0.5912  loss_ce_7: 0.8524  loss_mask_7: 0.2271  loss_dice_7: 0.8359  loss_bbox_7: 0.2426  loss_giou_7: 0.7695  loss_ce_dn_7: 0.128  loss_mask_dn_7: 0.2233  loss_dice_dn_7: 0.794  loss_bbox_dn_7: 0.2262  loss_giou_dn_7: 0.5913  loss_ce_8: 0.8942  loss_mask_8: 0.2295  loss_dice_8: 0.822  loss_bbox_8: 0.2417  loss_giou_8: 0.7673  loss_ce_dn_8: 0.1264  loss_mask_dn_8: 0.2287  loss_dice_dn_8: 0.7956  loss_bbox_dn_8: 0.224  loss_giou_dn_8: 0.5877  loss_ce_interm: 1.387  loss_mask_interm: 0.2391  loss_dice_interm: 0.8997  loss_bbox_interm: 0.3852  loss_giou_interm: 0.9642    time: 0.8737  last_time: 0.8822  data_time: 0.0125  last_data_time: 0.0147   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 07:58:09 d2.utils.events]: \u001b[0m eta: 3 days, 16:51:56  iter: 1979  total_loss: 57.27  loss_ce: 0.723  loss_mask: 0.305  loss_dice: 0.7457  loss_bbox: 0.2547  loss_giou: 0.5562  loss_ce_dn: 0.1085  loss_mask_dn: 0.3305  loss_dice_dn: 0.6791  loss_bbox_dn: 0.2872  loss_giou_dn: 0.5351  loss_ce_0: 1.453  loss_mask_0: 0.3137  loss_dice_0: 0.825  loss_bbox_0: 0.4262  loss_giou_0: 0.847  loss_ce_dn_0: 0.7266  loss_mask_dn_0: 0.9728  loss_dice_dn_0: 2.888  loss_bbox_dn_0: 0.7885  loss_giou_dn_0: 0.916  loss_ce_1: 1.42  loss_mask_1: 0.3192  loss_dice_1: 0.7425  loss_bbox_1: 0.3271  loss_giou_1: 0.6835  loss_ce_dn_1: 0.2365  loss_mask_dn_1: 0.4494  loss_dice_dn_1: 0.8246  loss_bbox_dn_1: 0.438  loss_giou_dn_1: 0.6648  loss_ce_2: 1.239  loss_mask_2: 0.301  loss_dice_2: 0.7229  loss_bbox_2: 0.269  loss_giou_2: 0.616  loss_ce_dn_2: 0.1783  loss_mask_dn_2: 0.3699  loss_dice_dn_2: 0.7139  loss_bbox_dn_2: 0.365  loss_giou_dn_2: 0.6068  loss_ce_3: 1.041  loss_mask_3: 0.3035  loss_dice_3: 0.7165  loss_bbox_3: 0.2624  loss_giou_3: 0.5952  loss_ce_dn_3: 0.1415  loss_mask_dn_3: 0.3452  loss_dice_dn_3: 0.709  loss_bbox_dn_3: 0.3181  loss_giou_dn_3: 0.5815  loss_ce_4: 0.979  loss_mask_4: 0.316  loss_dice_4: 0.6994  loss_bbox_4: 0.2431  loss_giou_4: 0.5587  loss_ce_dn_4: 0.1344  loss_mask_dn_4: 0.3305  loss_dice_dn_4: 0.7103  loss_bbox_dn_4: 0.298  loss_giou_dn_4: 0.5593  loss_ce_5: 0.8074  loss_mask_5: 0.3063  loss_dice_5: 0.7491  loss_bbox_5: 0.2592  loss_giou_5: 0.5711  loss_ce_dn_5: 0.1236  loss_mask_dn_5: 0.3268  loss_dice_dn_5: 0.7051  loss_bbox_dn_5: 0.2928  loss_giou_dn_5: 0.5532  loss_ce_6: 0.7954  loss_mask_6: 0.3053  loss_dice_6: 0.7412  loss_bbox_6: 0.2548  loss_giou_6: 0.5659  loss_ce_dn_6: 0.1165  loss_mask_dn_6: 0.3259  loss_dice_dn_6: 0.6931  loss_bbox_dn_6: 0.2905  loss_giou_dn_6: 0.5433  loss_ce_7: 0.7573  loss_mask_7: 0.2976  loss_dice_7: 0.7541  loss_bbox_7: 0.2591  loss_giou_7: 0.5813  loss_ce_dn_7: 0.1113  loss_mask_dn_7: 0.3283  loss_dice_dn_7: 0.6846  loss_bbox_dn_7: 0.2912  loss_giou_dn_7: 0.54  loss_ce_8: 0.7456  loss_mask_8: 0.3078  loss_dice_8: 0.7509  loss_bbox_8: 0.2549  loss_giou_8: 0.5565  loss_ce_dn_8: 0.1129  loss_mask_dn_8: 0.3275  loss_dice_dn_8: 0.6702  loss_bbox_dn_8: 0.2869  loss_giou_dn_8: 0.5368  loss_ce_interm: 1.432  loss_mask_interm: 0.3141  loss_dice_interm: 0.8143  loss_bbox_interm: 0.4159  loss_giou_interm: 0.8485    time: 0.8736  last_time: 0.8829  data_time: 0.0121  last_data_time: 0.0167   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 07:58:27 d2.utils.events]: \u001b[0m eta: 3 days, 16:50:43  iter: 1999  total_loss: 57.29  loss_ce: 0.7278  loss_mask: 0.2206  loss_dice: 0.7066  loss_bbox: 0.2376  loss_giou: 0.6089  loss_ce_dn: 0.1108  loss_mask_dn: 0.2317  loss_dice_dn: 0.7153  loss_bbox_dn: 0.1858  loss_giou_dn: 0.5238  loss_ce_0: 1.377  loss_mask_0: 0.2618  loss_dice_0: 0.8026  loss_bbox_0: 0.3733  loss_giou_0: 0.8522  loss_ce_dn_0: 0.6859  loss_mask_dn_0: 0.8143  loss_dice_dn_0: 2.906  loss_bbox_dn_0: 0.6389  loss_giou_dn_0: 0.9245  loss_ce_1: 1.326  loss_mask_1: 0.2491  loss_dice_1: 0.7649  loss_bbox_1: 0.2854  loss_giou_1: 0.7215  loss_ce_dn_1: 0.2258  loss_mask_dn_1: 0.2907  loss_dice_dn_1: 0.8377  loss_bbox_dn_1: 0.3306  loss_giou_dn_1: 0.656  loss_ce_2: 1.162  loss_mask_2: 0.2523  loss_dice_2: 0.7203  loss_bbox_2: 0.2373  loss_giou_2: 0.6714  loss_ce_dn_2: 0.1771  loss_mask_dn_2: 0.2373  loss_dice_dn_2: 0.7702  loss_bbox_dn_2: 0.2539  loss_giou_dn_2: 0.5846  loss_ce_3: 0.9658  loss_mask_3: 0.2309  loss_dice_3: 0.7114  loss_bbox_3: 0.2316  loss_giou_3: 0.6581  loss_ce_dn_3: 0.1423  loss_mask_dn_3: 0.235  loss_dice_dn_3: 0.7424  loss_bbox_dn_3: 0.2158  loss_giou_dn_3: 0.5704  loss_ce_4: 0.8534  loss_mask_4: 0.2274  loss_dice_4: 0.7543  loss_bbox_4: 0.2216  loss_giou_4: 0.6397  loss_ce_dn_4: 0.1239  loss_mask_dn_4: 0.2301  loss_dice_dn_4: 0.7254  loss_bbox_dn_4: 0.2066  loss_giou_dn_4: 0.5474  loss_ce_5: 0.8163  loss_mask_5: 0.2158  loss_dice_5: 0.7506  loss_bbox_5: 0.2076  loss_giou_5: 0.6127  loss_ce_dn_5: 0.1184  loss_mask_dn_5: 0.2307  loss_dice_dn_5: 0.7236  loss_bbox_dn_5: 0.2007  loss_giou_dn_5: 0.5403  loss_ce_6: 0.7667  loss_mask_6: 0.2281  loss_dice_6: 0.7019  loss_bbox_6: 0.2407  loss_giou_6: 0.6029  loss_ce_dn_6: 0.1133  loss_mask_dn_6: 0.2305  loss_dice_dn_6: 0.715  loss_bbox_dn_6: 0.1923  loss_giou_dn_6: 0.5326  loss_ce_7: 0.7185  loss_mask_7: 0.2254  loss_dice_7: 0.6998  loss_bbox_7: 0.2381  loss_giou_7: 0.6116  loss_ce_dn_7: 0.1116  loss_mask_dn_7: 0.2293  loss_dice_dn_7: 0.7175  loss_bbox_dn_7: 0.1896  loss_giou_dn_7: 0.5282  loss_ce_8: 0.7701  loss_mask_8: 0.2202  loss_dice_8: 0.6987  loss_bbox_8: 0.2383  loss_giou_8: 0.6085  loss_ce_dn_8: 0.1085  loss_mask_dn_8: 0.2283  loss_dice_dn_8: 0.716  loss_bbox_dn_8: 0.1857  loss_giou_dn_8: 0.5251  loss_ce_interm: 1.377  loss_mask_interm: 0.261  loss_dice_interm: 0.7557  loss_bbox_interm: 0.3887  loss_giou_interm: 0.8589    time: 0.8736  last_time: 0.8780  data_time: 0.0123  last_data_time: 0.0130   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 07:58:44 d2.utils.events]: \u001b[0m eta: 3 days, 16:49:10  iter: 2019  total_loss: 65.44  loss_ce: 0.859  loss_mask: 0.3104  loss_dice: 0.74  loss_bbox: 0.3081  loss_giou: 0.6865  loss_ce_dn: 0.1627  loss_mask_dn: 0.3529  loss_dice_dn: 0.7329  loss_bbox_dn: 0.2645  loss_giou_dn: 0.5209  loss_ce_0: 1.578  loss_mask_0: 0.3234  loss_dice_0: 0.7428  loss_bbox_0: 0.456  loss_giou_0: 0.903  loss_ce_dn_0: 0.8065  loss_mask_dn_0: 0.9226  loss_dice_dn_0: 3.093  loss_bbox_dn_0: 0.7123  loss_giou_dn_0: 0.9191  loss_ce_1: 1.414  loss_mask_1: 0.3163  loss_dice_1: 0.7452  loss_bbox_1: 0.3698  loss_giou_1: 0.7379  loss_ce_dn_1: 0.2582  loss_mask_dn_1: 0.421  loss_dice_dn_1: 0.9478  loss_bbox_dn_1: 0.4185  loss_giou_dn_1: 0.6748  loss_ce_2: 1.257  loss_mask_2: 0.3034  loss_dice_2: 0.7923  loss_bbox_2: 0.3315  loss_giou_2: 0.7218  loss_ce_dn_2: 0.2102  loss_mask_dn_2: 0.3915  loss_dice_dn_2: 0.852  loss_bbox_dn_2: 0.338  loss_giou_dn_2: 0.6048  loss_ce_3: 1.142  loss_mask_3: 0.3129  loss_dice_3: 0.8527  loss_bbox_3: 0.3445  loss_giou_3: 0.7143  loss_ce_dn_3: 0.2008  loss_mask_dn_3: 0.3693  loss_dice_dn_3: 0.8026  loss_bbox_dn_3: 0.3004  loss_giou_dn_3: 0.5674  loss_ce_4: 1.097  loss_mask_4: 0.3063  loss_dice_4: 0.801  loss_bbox_4: 0.3426  loss_giou_4: 0.7201  loss_ce_dn_4: 0.1801  loss_mask_dn_4: 0.3558  loss_dice_dn_4: 0.7747  loss_bbox_dn_4: 0.2757  loss_giou_dn_4: 0.5467  loss_ce_5: 0.9466  loss_mask_5: 0.3062  loss_dice_5: 0.7522  loss_bbox_5: 0.3429  loss_giou_5: 0.698  loss_ce_dn_5: 0.1654  loss_mask_dn_5: 0.3491  loss_dice_dn_5: 0.7578  loss_bbox_dn_5: 0.2707  loss_giou_dn_5: 0.5397  loss_ce_6: 0.9229  loss_mask_6: 0.3075  loss_dice_6: 0.7663  loss_bbox_6: 0.3337  loss_giou_6: 0.6967  loss_ce_dn_6: 0.1515  loss_mask_dn_6: 0.3527  loss_dice_dn_6: 0.7357  loss_bbox_dn_6: 0.2675  loss_giou_dn_6: 0.5302  loss_ce_7: 0.8295  loss_mask_7: 0.3083  loss_dice_7: 0.7884  loss_bbox_7: 0.3345  loss_giou_7: 0.6873  loss_ce_dn_7: 0.1539  loss_mask_dn_7: 0.3523  loss_dice_dn_7: 0.738  loss_bbox_dn_7: 0.2643  loss_giou_dn_7: 0.5256  loss_ce_8: 0.8604  loss_mask_8: 0.2931  loss_dice_8: 0.7845  loss_bbox_8: 0.3354  loss_giou_8: 0.6835  loss_ce_dn_8: 0.161  loss_mask_dn_8: 0.3524  loss_dice_dn_8: 0.7312  loss_bbox_dn_8: 0.2658  loss_giou_dn_8: 0.5209  loss_ce_interm: 1.575  loss_mask_interm: 0.3265  loss_dice_interm: 0.7443  loss_bbox_interm: 0.452  loss_giou_interm: 0.8866    time: 0.8735  last_time: 0.8690  data_time: 0.0114  last_data_time: 0.0129   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 07:59:02 d2.utils.events]: \u001b[0m eta: 3 days, 16:48:53  iter: 2039  total_loss: 63.06  loss_ce: 0.8669  loss_mask: 0.2322  loss_dice: 0.6193  loss_bbox: 0.2149  loss_giou: 0.7374  loss_ce_dn: 0.172  loss_mask_dn: 0.2549  loss_dice_dn: 0.6553  loss_bbox_dn: 0.2191  loss_giou_dn: 0.6701  loss_ce_0: 1.519  loss_mask_0: 0.2575  loss_dice_0: 0.6523  loss_bbox_0: 0.329  loss_giou_0: 1.012  loss_ce_dn_0: 0.7193  loss_mask_dn_0: 0.8846  loss_dice_dn_0: 3.168  loss_bbox_dn_0: 0.5298  loss_giou_dn_0: 1.045  loss_ce_1: 1.446  loss_mask_1: 0.2475  loss_dice_1: 0.6396  loss_bbox_1: 0.2426  loss_giou_1: 0.8806  loss_ce_dn_1: 0.2793  loss_mask_dn_1: 0.3221  loss_dice_dn_1: 0.7657  loss_bbox_dn_1: 0.326  loss_giou_dn_1: 0.8062  loss_ce_2: 1.285  loss_mask_2: 0.2427  loss_dice_2: 0.694  loss_bbox_2: 0.2169  loss_giou_2: 0.7794  loss_ce_dn_2: 0.2381  loss_mask_dn_2: 0.273  loss_dice_dn_2: 0.7157  loss_bbox_dn_2: 0.2681  loss_giou_dn_2: 0.7352  loss_ce_3: 1.143  loss_mask_3: 0.2514  loss_dice_3: 0.642  loss_bbox_3: 0.2171  loss_giou_3: 0.8032  loss_ce_dn_3: 0.2178  loss_mask_dn_3: 0.2713  loss_dice_dn_3: 0.6682  loss_bbox_dn_3: 0.2508  loss_giou_dn_3: 0.708  loss_ce_4: 1.05  loss_mask_4: 0.2474  loss_dice_4: 0.6412  loss_bbox_4: 0.2092  loss_giou_4: 0.7997  loss_ce_dn_4: 0.2047  loss_mask_dn_4: 0.2686  loss_dice_dn_4: 0.6465  loss_bbox_dn_4: 0.2411  loss_giou_dn_4: 0.6882  loss_ce_5: 0.9602  loss_mask_5: 0.2429  loss_dice_5: 0.6372  loss_bbox_5: 0.2169  loss_giou_5: 0.7793  loss_ce_dn_5: 0.1961  loss_mask_dn_5: 0.2644  loss_dice_dn_5: 0.6235  loss_bbox_dn_5: 0.2321  loss_giou_dn_5: 0.6816  loss_ce_6: 0.9089  loss_mask_6: 0.2435  loss_dice_6: 0.6365  loss_bbox_6: 0.2139  loss_giou_6: 0.7706  loss_ce_dn_6: 0.182  loss_mask_dn_6: 0.2574  loss_dice_dn_6: 0.6346  loss_bbox_dn_6: 0.2237  loss_giou_dn_6: 0.6741  loss_ce_7: 0.9087  loss_mask_7: 0.2395  loss_dice_7: 0.6337  loss_bbox_7: 0.2139  loss_giou_7: 0.7651  loss_ce_dn_7: 0.1754  loss_mask_dn_7: 0.2554  loss_dice_dn_7: 0.6265  loss_bbox_dn_7: 0.223  loss_giou_dn_7: 0.6683  loss_ce_8: 0.8913  loss_mask_8: 0.2365  loss_dice_8: 0.6415  loss_bbox_8: 0.2143  loss_giou_8: 0.7655  loss_ce_dn_8: 0.1691  loss_mask_dn_8: 0.2522  loss_dice_dn_8: 0.6445  loss_bbox_dn_8: 0.2195  loss_giou_dn_8: 0.6672  loss_ce_interm: 1.528  loss_mask_interm: 0.2562  loss_dice_interm: 0.6795  loss_bbox_interm: 0.329  loss_giou_interm: 1.009    time: 0.8737  last_time: 0.9205  data_time: 0.0138  last_data_time: 0.0117   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 07:59:20 d2.utils.events]: \u001b[0m eta: 3 days, 16:49:18  iter: 2059  total_loss: 63.13  loss_ce: 0.794  loss_mask: 0.2339  loss_dice: 0.7476  loss_bbox: 0.2437  loss_giou: 0.6729  loss_ce_dn: 0.1006  loss_mask_dn: 0.2281  loss_dice_dn: 0.7124  loss_bbox_dn: 0.2145  loss_giou_dn: 0.5403  loss_ce_0: 1.423  loss_mask_0: 0.2405  loss_dice_0: 0.8123  loss_bbox_0: 0.4049  loss_giou_0: 0.8879  loss_ce_dn_0: 0.7044  loss_mask_dn_0: 0.7209  loss_dice_dn_0: 2.897  loss_bbox_dn_0: 0.6321  loss_giou_dn_0: 0.9348  loss_ce_1: 1.309  loss_mask_1: 0.2345  loss_dice_1: 0.8004  loss_bbox_1: 0.2901  loss_giou_1: 0.7661  loss_ce_dn_1: 0.2231  loss_mask_dn_1: 0.2889  loss_dice_dn_1: 0.9108  loss_bbox_dn_1: 0.3654  loss_giou_dn_1: 0.7035  loss_ce_2: 1.109  loss_mask_2: 0.2316  loss_dice_2: 0.7825  loss_bbox_2: 0.3002  loss_giou_2: 0.7582  loss_ce_dn_2: 0.18  loss_mask_dn_2: 0.2533  loss_dice_dn_2: 0.7922  loss_bbox_dn_2: 0.2898  loss_giou_dn_2: 0.6203  loss_ce_3: 1.023  loss_mask_3: 0.2228  loss_dice_3: 0.7576  loss_bbox_3: 0.2716  loss_giou_3: 0.7101  loss_ce_dn_3: 0.1501  loss_mask_dn_3: 0.241  loss_dice_dn_3: 0.7764  loss_bbox_dn_3: 0.2625  loss_giou_dn_3: 0.5988  loss_ce_4: 0.938  loss_mask_4: 0.2215  loss_dice_4: 0.7647  loss_bbox_4: 0.2799  loss_giou_4: 0.696  loss_ce_dn_4: 0.1336  loss_mask_dn_4: 0.2457  loss_dice_dn_4: 0.7356  loss_bbox_dn_4: 0.2476  loss_giou_dn_4: 0.5704  loss_ce_5: 0.8686  loss_mask_5: 0.2257  loss_dice_5: 0.763  loss_bbox_5: 0.2716  loss_giou_5: 0.687  loss_ce_dn_5: 0.1147  loss_mask_dn_5: 0.2381  loss_dice_dn_5: 0.7185  loss_bbox_dn_5: 0.2375  loss_giou_dn_5: 0.5654  loss_ce_6: 0.8077  loss_mask_6: 0.2284  loss_dice_6: 0.7379  loss_bbox_6: 0.2534  loss_giou_6: 0.6785  loss_ce_dn_6: 0.1044  loss_mask_dn_6: 0.2291  loss_dice_dn_6: 0.7119  loss_bbox_dn_6: 0.2238  loss_giou_dn_6: 0.5562  loss_ce_7: 0.7856  loss_mask_7: 0.2269  loss_dice_7: 0.7651  loss_bbox_7: 0.248  loss_giou_7: 0.7014  loss_ce_dn_7: 0.1063  loss_mask_dn_7: 0.2277  loss_dice_dn_7: 0.725  loss_bbox_dn_7: 0.2196  loss_giou_dn_7: 0.5505  loss_ce_8: 0.7996  loss_mask_8: 0.227  loss_dice_8: 0.7368  loss_bbox_8: 0.2439  loss_giou_8: 0.6687  loss_ce_dn_8: 0.09763  loss_mask_dn_8: 0.2295  loss_dice_dn_8: 0.7209  loss_bbox_dn_8: 0.2158  loss_giou_dn_8: 0.5424  loss_ce_interm: 1.432  loss_mask_interm: 0.2434  loss_dice_interm: 0.8201  loss_bbox_interm: 0.3981  loss_giou_interm: 0.8844    time: 0.8738  last_time: 0.8645  data_time: 0.0130  last_data_time: 0.0163   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 07:59:37 d2.utils.events]: \u001b[0m eta: 3 days, 16:49:45  iter: 2079  total_loss: 61.08  loss_ce: 0.8734  loss_mask: 0.2119  loss_dice: 0.7328  loss_bbox: 0.2113  loss_giou: 0.6837  loss_ce_dn: 0.1461  loss_mask_dn: 0.2158  loss_dice_dn: 0.7456  loss_bbox_dn: 0.164  loss_giou_dn: 0.5659  loss_ce_0: 1.513  loss_mask_0: 0.2328  loss_dice_0: 0.7868  loss_bbox_0: 0.3443  loss_giou_0: 0.9358  loss_ce_dn_0: 0.7022  loss_mask_dn_0: 0.7579  loss_dice_dn_0: 2.97  loss_bbox_dn_0: 0.5873  loss_giou_dn_0: 0.901  loss_ce_1: 1.422  loss_mask_1: 0.2174  loss_dice_1: 0.8173  loss_bbox_1: 0.2292  loss_giou_1: 0.8004  loss_ce_dn_1: 0.2815  loss_mask_dn_1: 0.2732  loss_dice_dn_1: 0.9503  loss_bbox_dn_1: 0.3322  loss_giou_dn_1: 0.6678  loss_ce_2: 1.228  loss_mask_2: 0.2244  loss_dice_2: 0.7893  loss_bbox_2: 0.2147  loss_giou_2: 0.7474  loss_ce_dn_2: 0.2219  loss_mask_dn_2: 0.2436  loss_dice_dn_2: 0.8164  loss_bbox_dn_2: 0.2573  loss_giou_dn_2: 0.6106  loss_ce_3: 1.061  loss_mask_3: 0.2219  loss_dice_3: 0.7718  loss_bbox_3: 0.2048  loss_giou_3: 0.7473  loss_ce_dn_3: 0.202  loss_mask_dn_3: 0.2278  loss_dice_dn_3: 0.7936  loss_bbox_dn_3: 0.2279  loss_giou_dn_3: 0.6033  loss_ce_4: 1.015  loss_mask_4: 0.2006  loss_dice_4: 0.8381  loss_bbox_4: 0.2273  loss_giou_4: 0.7298  loss_ce_dn_4: 0.1821  loss_mask_dn_4: 0.2222  loss_dice_dn_4: 0.7905  loss_bbox_dn_4: 0.1964  loss_giou_dn_4: 0.5829  loss_ce_5: 0.8628  loss_mask_5: 0.2163  loss_dice_5: 0.7849  loss_bbox_5: 0.2172  loss_giou_5: 0.6941  loss_ce_dn_5: 0.161  loss_mask_dn_5: 0.2094  loss_dice_dn_5: 0.7621  loss_bbox_dn_5: 0.1891  loss_giou_dn_5: 0.5774  loss_ce_6: 0.837  loss_mask_6: 0.2138  loss_dice_6: 0.7968  loss_bbox_6: 0.2141  loss_giou_6: 0.6864  loss_ce_dn_6: 0.163  loss_mask_dn_6: 0.2118  loss_dice_dn_6: 0.7661  loss_bbox_dn_6: 0.1808  loss_giou_dn_6: 0.5688  loss_ce_7: 0.8381  loss_mask_7: 0.2168  loss_dice_7: 0.797  loss_bbox_7: 0.204  loss_giou_7: 0.6771  loss_ce_dn_7: 0.147  loss_mask_dn_7: 0.2147  loss_dice_dn_7: 0.7564  loss_bbox_dn_7: 0.1695  loss_giou_dn_7: 0.5693  loss_ce_8: 0.8475  loss_mask_8: 0.2149  loss_dice_8: 0.7916  loss_bbox_8: 0.2048  loss_giou_8: 0.6795  loss_ce_dn_8: 0.1474  loss_mask_dn_8: 0.2112  loss_dice_dn_8: 0.7488  loss_bbox_dn_8: 0.1643  loss_giou_dn_8: 0.5659  loss_ce_interm: 1.519  loss_mask_interm: 0.232  loss_dice_interm: 0.7968  loss_bbox_interm: 0.3443  loss_giou_interm: 0.9398    time: 0.8738  last_time: 0.8898  data_time: 0.0132  last_data_time: 0.0125   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 07:59:55 d2.utils.events]: \u001b[0m eta: 3 days, 16:49:35  iter: 2099  total_loss: 66.24  loss_ce: 0.8581  loss_mask: 0.2802  loss_dice: 0.6595  loss_bbox: 0.256  loss_giou: 0.7424  loss_ce_dn: 0.1392  loss_mask_dn: 0.3056  loss_dice_dn: 0.7081  loss_bbox_dn: 0.168  loss_giou_dn: 0.5914  loss_ce_0: 1.462  loss_mask_0: 0.2614  loss_dice_0: 0.7151  loss_bbox_0: 0.3988  loss_giou_0: 0.9666  loss_ce_dn_0: 0.7497  loss_mask_dn_0: 0.8772  loss_dice_dn_0: 3.162  loss_bbox_dn_0: 0.6832  loss_giou_dn_0: 0.9418  loss_ce_1: 1.361  loss_mask_1: 0.2929  loss_dice_1: 0.6798  loss_bbox_1: 0.2585  loss_giou_1: 0.8258  loss_ce_dn_1: 0.2447  loss_mask_dn_1: 0.3591  loss_dice_dn_1: 0.8488  loss_bbox_dn_1: 0.3343  loss_giou_dn_1: 0.7088  loss_ce_2: 1.188  loss_mask_2: 0.2901  loss_dice_2: 0.7437  loss_bbox_2: 0.2519  loss_giou_2: 0.8254  loss_ce_dn_2: 0.2079  loss_mask_dn_2: 0.3192  loss_dice_dn_2: 0.772  loss_bbox_dn_2: 0.2515  loss_giou_dn_2: 0.6478  loss_ce_3: 1.04  loss_mask_3: 0.285  loss_dice_3: 0.6804  loss_bbox_3: 0.2477  loss_giou_3: 0.8063  loss_ce_dn_3: 0.1923  loss_mask_dn_3: 0.2832  loss_dice_dn_3: 0.7484  loss_bbox_dn_3: 0.2256  loss_giou_dn_3: 0.6198  loss_ce_4: 0.9659  loss_mask_4: 0.2834  loss_dice_4: 0.65  loss_bbox_4: 0.231  loss_giou_4: 0.7496  loss_ce_dn_4: 0.1709  loss_mask_dn_4: 0.2779  loss_dice_dn_4: 0.7104  loss_bbox_dn_4: 0.2015  loss_giou_dn_4: 0.6037  loss_ce_5: 0.8987  loss_mask_5: 0.3046  loss_dice_5: 0.6933  loss_bbox_5: 0.2486  loss_giou_5: 0.7938  loss_ce_dn_5: 0.1565  loss_mask_dn_5: 0.3204  loss_dice_dn_5: 0.7367  loss_bbox_dn_5: 0.1899  loss_giou_dn_5: 0.5979  loss_ce_6: 0.8331  loss_mask_6: 0.3073  loss_dice_6: 0.6225  loss_bbox_6: 0.2327  loss_giou_6: 0.7483  loss_ce_dn_6: 0.1457  loss_mask_dn_6: 0.3163  loss_dice_dn_6: 0.7128  loss_bbox_dn_6: 0.1753  loss_giou_dn_6: 0.5956  loss_ce_7: 0.8129  loss_mask_7: 0.2885  loss_dice_7: 0.6697  loss_bbox_7: 0.245  loss_giou_7: 0.759  loss_ce_dn_7: 0.1456  loss_mask_dn_7: 0.3122  loss_dice_dn_7: 0.7068  loss_bbox_dn_7: 0.1704  loss_giou_dn_7: 0.5968  loss_ce_8: 0.8388  loss_mask_8: 0.3032  loss_dice_8: 0.6696  loss_bbox_8: 0.2605  loss_giou_8: 0.7781  loss_ce_dn_8: 0.1435  loss_mask_dn_8: 0.3086  loss_dice_dn_8: 0.751  loss_bbox_dn_8: 0.1696  loss_giou_dn_8: 0.5894  loss_ce_interm: 1.448  loss_mask_interm: 0.2628  loss_dice_interm: 0.7862  loss_bbox_interm: 0.3962  loss_giou_interm: 0.9789    time: 0.8738  last_time: 0.8633  data_time: 0.0119  last_data_time: 0.0111   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 08:00:12 d2.utils.events]: \u001b[0m eta: 3 days, 16:49:34  iter: 2119  total_loss: 60.11  loss_ce: 0.6847  loss_mask: 0.2774  loss_dice: 0.7607  loss_bbox: 0.2976  loss_giou: 0.8001  loss_ce_dn: 0.141  loss_mask_dn: 0.2948  loss_dice_dn: 0.7124  loss_bbox_dn: 0.2745  loss_giou_dn: 0.6636  loss_ce_0: 1.44  loss_mask_0: 0.2906  loss_dice_0: 0.7256  loss_bbox_0: 0.4655  loss_giou_0: 1.009  loss_ce_dn_0: 0.7436  loss_mask_dn_0: 0.9534  loss_dice_dn_0: 2.783  loss_bbox_dn_0: 0.6828  loss_giou_dn_0: 1.019  loss_ce_1: 1.272  loss_mask_1: 0.2713  loss_dice_1: 0.7286  loss_bbox_1: 0.3594  loss_giou_1: 0.8896  loss_ce_dn_1: 0.2466  loss_mask_dn_1: 0.3918  loss_dice_dn_1: 0.8316  loss_bbox_dn_1: 0.4047  loss_giou_dn_1: 0.8009  loss_ce_2: 1.192  loss_mask_2: 0.2821  loss_dice_2: 0.766  loss_bbox_2: 0.2996  loss_giou_2: 0.8351  loss_ce_dn_2: 0.2035  loss_mask_dn_2: 0.3613  loss_dice_dn_2: 0.7518  loss_bbox_dn_2: 0.3307  loss_giou_dn_2: 0.7334  loss_ce_3: 0.9629  loss_mask_3: 0.2757  loss_dice_3: 0.7201  loss_bbox_3: 0.3066  loss_giou_3: 0.817  loss_ce_dn_3: 0.1746  loss_mask_dn_3: 0.3375  loss_dice_dn_3: 0.7255  loss_bbox_dn_3: 0.3068  loss_giou_dn_3: 0.7087  loss_ce_4: 0.9204  loss_mask_4: 0.2708  loss_dice_4: 0.7333  loss_bbox_4: 0.3116  loss_giou_4: 0.7844  loss_ce_dn_4: 0.1679  loss_mask_dn_4: 0.3232  loss_dice_dn_4: 0.7123  loss_bbox_dn_4: 0.2935  loss_giou_dn_4: 0.6895  loss_ce_5: 0.8189  loss_mask_5: 0.2806  loss_dice_5: 0.7134  loss_bbox_5: 0.3417  loss_giou_5: 0.8206  loss_ce_dn_5: 0.1495  loss_mask_dn_5: 0.3079  loss_dice_dn_5: 0.6993  loss_bbox_dn_5: 0.2836  loss_giou_dn_5: 0.6783  loss_ce_6: 0.7634  loss_mask_6: 0.2776  loss_dice_6: 0.7686  loss_bbox_6: 0.3105  loss_giou_6: 0.7974  loss_ce_dn_6: 0.1421  loss_mask_dn_6: 0.3108  loss_dice_dn_6: 0.7107  loss_bbox_dn_6: 0.2749  loss_giou_dn_6: 0.6721  loss_ce_7: 0.6663  loss_mask_7: 0.2813  loss_dice_7: 0.7758  loss_bbox_7: 0.3075  loss_giou_7: 0.8329  loss_ce_dn_7: 0.135  loss_mask_dn_7: 0.3042  loss_dice_dn_7: 0.7193  loss_bbox_dn_7: 0.2766  loss_giou_dn_7: 0.668  loss_ce_8: 0.6527  loss_mask_8: 0.2721  loss_dice_8: 0.8118  loss_bbox_8: 0.298  loss_giou_8: 0.7967  loss_ce_dn_8: 0.1388  loss_mask_dn_8: 0.2968  loss_dice_dn_8: 0.7053  loss_bbox_dn_8: 0.2743  loss_giou_dn_8: 0.6635  loss_ce_interm: 1.404  loss_mask_interm: 0.2925  loss_dice_interm: 0.7442  loss_bbox_interm: 0.4702  loss_giou_interm: 1.006    time: 0.8738  last_time: 0.8620  data_time: 0.0117  last_data_time: 0.0092   lr: 1.25e-05  max_mem: 11920M\n",
            "\u001b[32m[02/20 08:00:30 d2.utils.events]: \u001b[0m eta: 3 days, 16:50:06  iter: 2139  total_loss: 57.6  loss_ce: 0.7438  loss_mask: 0.2311  loss_dice: 0.5981  loss_bbox: 0.2691  loss_giou: 0.6823  loss_ce_dn: 0.1289  loss_mask_dn: 0.237  loss_dice_dn: 0.6476  loss_bbox_dn: 0.2035  loss_giou_dn: 0.5461  loss_ce_0: 1.43  loss_mask_0: 0.2344  loss_dice_0: 0.7228  loss_bbox_0: 0.396  loss_giou_0: 0.8828  loss_ce_dn_0: 0.7221  loss_mask_dn_0: 0.9964  loss_dice_dn_0: 3.296  loss_bbox_dn_0: 0.6034  loss_giou_dn_0: 0.9272  loss_ce_1: 1.305  loss_mask_1: 0.2278  loss_dice_1: 0.704  loss_bbox_1: 0.2992  loss_giou_1: 0.7804  loss_ce_dn_1: 0.2796  loss_mask_dn_1: 0.2764  loss_dice_dn_1: 0.7646  loss_bbox_dn_1: 0.3205  loss_giou_dn_1: 0.6791  loss_ce_2: 1.108  loss_mask_2: 0.2232  loss_dice_2: 0.6698  loss_bbox_2: 0.2959  loss_giou_2: 0.728  loss_ce_dn_2: 0.2187  loss_mask_dn_2: 0.2597  loss_dice_dn_2: 0.6861  loss_bbox_dn_2: 0.2661  loss_giou_dn_2: 0.6066  loss_ce_3: 0.9605  loss_mask_3: 0.2364  loss_dice_3: 0.6494  loss_bbox_3: 0.2913  loss_giou_3: 0.7238  loss_ce_dn_3: 0.1888  loss_mask_dn_3: 0.2488  loss_dice_dn_3: 0.663  loss_bbox_dn_3: 0.2325  loss_giou_dn_3: 0.5885  loss_ce_4: 0.9072  loss_mask_4: 0.2369  loss_dice_4: 0.6574  loss_bbox_4: 0.2925  loss_giou_4: 0.7173  loss_ce_dn_4: 0.1741  loss_mask_dn_4: 0.2437  loss_dice_dn_4: 0.6456  loss_bbox_dn_4: 0.2201  loss_giou_dn_4: 0.5649  loss_ce_5: 0.809  loss_mask_5: 0.2251  loss_dice_5: 0.6289  loss_bbox_5: 0.2821  loss_giou_5: 0.7046  loss_ce_dn_5: 0.1573  loss_mask_dn_5: 0.2384  loss_dice_dn_5: 0.6472  loss_bbox_dn_5: 0.2145  loss_giou_dn_5: 0.56  loss_ce_6: 0.7896  loss_mask_6: 0.2337  loss_dice_6: 0.6506  loss_bbox_6: 0.2693  loss_giou_6: 0.6831  loss_ce_dn_6: 0.1401  loss_mask_dn_6: 0.2377  loss_dice_dn_6: 0.642  loss_bbox_dn_6: 0.2067  loss_giou_dn_6: 0.5505  loss_ce_7: 0.7626  loss_mask_7: 0.225  loss_dice_7: 0.6322  loss_bbox_7: 0.2711  loss_giou_7: 0.69  loss_ce_dn_7: 0.1343  loss_mask_dn_7: 0.2352  loss_dice_dn_7: 0.6362  loss_bbox_dn_7: 0.2046  loss_giou_dn_7: 0.5508  loss_ce_8: 0.7459  loss_mask_8: 0.2255  loss_dice_8: 0.6111  loss_bbox_8: 0.2581  loss_giou_8: 0.6847  loss_ce_dn_8: 0.1296  loss_mask_dn_8: 0.2373  loss_dice_dn_8: 0.6465  loss_bbox_dn_8: 0.2034  loss_giou_dn_8: 0.5454  loss_ce_interm: 1.402  loss_mask_interm: 0.2474  loss_dice_interm: 0.7165  loss_bbox_interm: 0.395  loss_giou_interm: 0.8753    time: 0.8738  last_time: 0.8642  data_time: 0.0120  last_data_time: 0.0137   lr: 1.25e-05  max_mem: 11978M\n",
            "\u001b[32m[02/20 08:00:47 d2.utils.events]: \u001b[0m eta: 3 days, 16:49:38  iter: 2159  total_loss: 62.59  loss_ce: 0.7406  loss_mask: 0.2275  loss_dice: 0.5355  loss_bbox: 0.2014  loss_giou: 0.7631  loss_ce_dn: 0.1528  loss_mask_dn: 0.1863  loss_dice_dn: 0.5116  loss_bbox_dn: 0.2266  loss_giou_dn: 0.6359  loss_ce_0: 1.418  loss_mask_0: 0.2336  loss_dice_0: 0.5792  loss_bbox_0: 0.3858  loss_giou_0: 0.9174  loss_ce_dn_0: 0.7572  loss_mask_dn_0: 0.9177  loss_dice_dn_0: 2.859  loss_bbox_dn_0: 0.5331  loss_giou_dn_0: 1.013  loss_ce_1: 1.267  loss_mask_1: 0.2587  loss_dice_1: 0.5566  loss_bbox_1: 0.2713  loss_giou_1: 0.7991  loss_ce_dn_1: 0.2907  loss_mask_dn_1: 0.224  loss_dice_dn_1: 0.59  loss_bbox_dn_1: 0.3171  loss_giou_dn_1: 0.7935  loss_ce_2: 1.147  loss_mask_2: 0.2482  loss_dice_2: 0.5243  loss_bbox_2: 0.266  loss_giou_2: 0.7606  loss_ce_dn_2: 0.222  loss_mask_dn_2: 0.2092  loss_dice_dn_2: 0.5949  loss_bbox_dn_2: 0.2649  loss_giou_dn_2: 0.717  loss_ce_3: 0.9382  loss_mask_3: 0.2367  loss_dice_3: 0.5566  loss_bbox_3: 0.2691  loss_giou_3: 0.745  loss_ce_dn_3: 0.1994  loss_mask_dn_3: 0.2029  loss_dice_dn_3: 0.5487  loss_bbox_dn_3: 0.2466  loss_giou_dn_3: 0.6815  loss_ce_4: 0.9228  loss_mask_4: 0.222  loss_dice_4: 0.5538  loss_bbox_4: 0.2163  loss_giou_4: 0.7461  loss_ce_dn_4: 0.1751  loss_mask_dn_4: 0.2032  loss_dice_dn_4: 0.5503  loss_bbox_dn_4: 0.2411  loss_giou_dn_4: 0.6577  loss_ce_5: 0.8773  loss_mask_5: 0.2319  loss_dice_5: 0.5412  loss_bbox_5: 0.2159  loss_giou_5: 0.7901  loss_ce_dn_5: 0.16  loss_mask_dn_5: 0.1958  loss_dice_dn_5: 0.5346  loss_bbox_dn_5: 0.2346  loss_giou_dn_5: 0.6437  loss_ce_6: 0.8081  loss_mask_6: 0.2283  loss_dice_6: 0.5346  loss_bbox_6: 0.2292  loss_giou_6: 0.7761  loss_ce_dn_6: 0.1587  loss_mask_dn_6: 0.1908  loss_dice_dn_6: 0.5281  loss_bbox_dn_6: 0.232  loss_giou_dn_6: 0.6382  loss_ce_7: 0.7858  loss_mask_7: 0.2295  loss_dice_7: 0.5191  loss_bbox_7: 0.206  loss_giou_7: 0.7695  loss_ce_dn_7: 0.1515  loss_mask_dn_7: 0.1876  loss_dice_dn_7: 0.5444  loss_bbox_dn_7: 0.2292  loss_giou_dn_7: 0.6382  loss_ce_8: 0.8014  loss_mask_8: 0.2255  loss_dice_8: 0.5326  loss_bbox_8: 0.2024  loss_giou_8: 0.7848  loss_ce_dn_8: 0.1481  loss_mask_dn_8: 0.1878  loss_dice_dn_8: 0.5327  loss_bbox_dn_8: 0.2282  loss_giou_dn_8: 0.636  loss_ce_interm: 1.405  loss_mask_interm: 0.2381  loss_dice_interm: 0.5635  loss_bbox_interm: 0.3872  loss_giou_interm: 0.9196    time: 0.8738  last_time: 0.8602  data_time: 0.0111  last_data_time: 0.0136   lr: 1.25e-05  max_mem: 11978M\n",
            "\u001b[32m[02/20 08:01:05 d2.utils.events]: \u001b[0m eta: 3 days, 16:49:31  iter: 2179  total_loss: 65.69  loss_ce: 0.9209  loss_mask: 0.284  loss_dice: 0.8308  loss_bbox: 0.3115  loss_giou: 0.6512  loss_ce_dn: 0.1372  loss_mask_dn: 0.2943  loss_dice_dn: 0.8332  loss_bbox_dn: 0.2361  loss_giou_dn: 0.5308  loss_ce_0: 1.565  loss_mask_0: 0.2919  loss_dice_0: 0.8579  loss_bbox_0: 0.431  loss_giou_0: 0.9448  loss_ce_dn_0: 0.7103  loss_mask_dn_0: 0.6563  loss_dice_dn_0: 3.092  loss_bbox_dn_0: 0.564  loss_giou_dn_0: 0.9384  loss_ce_1: 1.443  loss_mask_1: 0.3023  loss_dice_1: 0.7788  loss_bbox_1: 0.3059  loss_giou_1: 0.7993  loss_ce_dn_1: 0.2813  loss_mask_dn_1: 0.3354  loss_dice_dn_1: 1.023  loss_bbox_dn_1: 0.312  loss_giou_dn_1: 0.6861  loss_ce_2: 1.303  loss_mask_2: 0.2868  loss_dice_2: 0.9035  loss_bbox_2: 0.2864  loss_giou_2: 0.7424  loss_ce_dn_2: 0.2198  loss_mask_dn_2: 0.3201  loss_dice_dn_2: 0.8974  loss_bbox_dn_2: 0.2872  loss_giou_dn_2: 0.606  loss_ce_3: 1.12  loss_mask_3: 0.2653  loss_dice_3: 0.8143  loss_bbox_3: 0.3169  loss_giou_3: 0.7248  loss_ce_dn_3: 0.1894  loss_mask_dn_3: 0.3021  loss_dice_dn_3: 0.874  loss_bbox_dn_3: 0.2727  loss_giou_dn_3: 0.5789  loss_ce_4: 1.02  loss_mask_4: 0.2875  loss_dice_4: 0.8787  loss_bbox_4: 0.3097  loss_giou_4: 0.6922  loss_ce_dn_4: 0.1709  loss_mask_dn_4: 0.297  loss_dice_dn_4: 0.856  loss_bbox_dn_4: 0.2599  loss_giou_dn_4: 0.5634  loss_ce_5: 0.9405  loss_mask_5: 0.2889  loss_dice_5: 0.922  loss_bbox_5: 0.31  loss_giou_5: 0.6804  loss_ce_dn_5: 0.1546  loss_mask_dn_5: 0.2907  loss_dice_dn_5: 0.839  loss_bbox_dn_5: 0.2467  loss_giou_dn_5: 0.55  loss_ce_6: 0.8996  loss_mask_6: 0.2848  loss_dice_6: 0.8451  loss_bbox_6: 0.3097  loss_giou_6: 0.6764  loss_ce_dn_6: 0.1381  loss_mask_dn_6: 0.2958  loss_dice_dn_6: 0.8218  loss_bbox_dn_6: 0.2401  loss_giou_dn_6: 0.5384  loss_ce_7: 0.8966  loss_mask_7: 0.2953  loss_dice_7: 0.8014  loss_bbox_7: 0.3094  loss_giou_7: 0.6621  loss_ce_dn_7: 0.1347  loss_mask_dn_7: 0.2958  loss_dice_dn_7: 0.8207  loss_bbox_dn_7: 0.2385  loss_giou_dn_7: 0.5387  loss_ce_8: 0.9049  loss_mask_8: 0.29  loss_dice_8: 0.8732  loss_bbox_8: 0.3101  loss_giou_8: 0.6525  loss_ce_dn_8: 0.1367  loss_mask_dn_8: 0.2921  loss_dice_dn_8: 0.8234  loss_bbox_dn_8: 0.2363  loss_giou_dn_8: 0.5341  loss_ce_interm: 1.567  loss_mask_interm: 0.3002  loss_dice_interm: 0.8598  loss_bbox_interm: 0.4442  loss_giou_interm: 0.9545    time: 0.8738  last_time: 0.8649  data_time: 0.0133  last_data_time: 0.0104   lr: 1.25e-05  max_mem: 11978M\n",
            "\u001b[32m[02/20 08:01:22 d2.utils.events]: \u001b[0m eta: 3 days, 16:49:16  iter: 2199  total_loss: 65.58  loss_ce: 0.8615  loss_mask: 0.3748  loss_dice: 0.6687  loss_bbox: 0.2864  loss_giou: 0.7061  loss_ce_dn: 0.1276  loss_mask_dn: 0.3573  loss_dice_dn: 0.7085  loss_bbox_dn: 0.3159  loss_giou_dn: 0.5446  loss_ce_0: 1.475  loss_mask_0: 0.4146  loss_dice_0: 0.7068  loss_bbox_0: 0.4165  loss_giou_0: 0.8426  loss_ce_dn_0: 0.7896  loss_mask_dn_0: 0.8492  loss_dice_dn_0: 2.902  loss_bbox_dn_0: 0.6821  loss_giou_dn_0: 0.9538  loss_ce_1: 1.434  loss_mask_1: 0.3917  loss_dice_1: 0.6738  loss_bbox_1: 0.3195  loss_giou_1: 0.7688  loss_ce_dn_1: 0.2709  loss_mask_dn_1: 0.4248  loss_dice_dn_1: 0.8509  loss_bbox_dn_1: 0.4254  loss_giou_dn_1: 0.6676  loss_ce_2: 1.186  loss_mask_2: 0.3817  loss_dice_2: 0.6734  loss_bbox_2: 0.3185  loss_giou_2: 0.7121  loss_ce_dn_2: 0.2131  loss_mask_dn_2: 0.3951  loss_dice_dn_2: 0.752  loss_bbox_dn_2: 0.3772  loss_giou_dn_2: 0.6115  loss_ce_3: 1.115  loss_mask_3: 0.4138  loss_dice_3: 0.7344  loss_bbox_3: 0.3093  loss_giou_3: 0.7132  loss_ce_dn_3: 0.1689  loss_mask_dn_3: 0.3739  loss_dice_dn_3: 0.7016  loss_bbox_dn_3: 0.3652  loss_giou_dn_3: 0.5834  loss_ce_4: 1.022  loss_mask_4: 0.3841  loss_dice_4: 0.6679  loss_bbox_4: 0.3024  loss_giou_4: 0.7189  loss_ce_dn_4: 0.1548  loss_mask_dn_4: 0.3628  loss_dice_dn_4: 0.6951  loss_bbox_dn_4: 0.3373  loss_giou_dn_4: 0.5657  loss_ce_5: 0.9722  loss_mask_5: 0.3832  loss_dice_5: 0.6748  loss_bbox_5: 0.2968  loss_giou_5: 0.6998  loss_ce_dn_5: 0.1592  loss_mask_dn_5: 0.3561  loss_dice_dn_5: 0.702  loss_bbox_dn_5: 0.3278  loss_giou_dn_5: 0.5562  loss_ce_6: 0.8767  loss_mask_6: 0.377  loss_dice_6: 0.6765  loss_bbox_6: 0.3005  loss_giou_6: 0.7253  loss_ce_dn_6: 0.1417  loss_mask_dn_6: 0.3679  loss_dice_dn_6: 0.7184  loss_bbox_dn_6: 0.3197  loss_giou_dn_6: 0.5443  loss_ce_7: 0.8625  loss_mask_7: 0.3788  loss_dice_7: 0.672  loss_bbox_7: 0.2698  loss_giou_7: 0.6955  loss_ce_dn_7: 0.1294  loss_mask_dn_7: 0.3532  loss_dice_dn_7: 0.7006  loss_bbox_dn_7: 0.3169  loss_giou_dn_7: 0.5434  loss_ce_8: 0.8466  loss_mask_8: 0.3796  loss_dice_8: 0.6666  loss_bbox_8: 0.2989  loss_giou_8: 0.7325  loss_ce_dn_8: 0.127  loss_mask_dn_8: 0.3548  loss_dice_dn_8: 0.709  loss_bbox_dn_8: 0.3155  loss_giou_dn_8: 0.5424  loss_ce_interm: 1.481  loss_mask_interm: 0.3957  loss_dice_interm: 0.6811  loss_bbox_interm: 0.4097  loss_giou_interm: 0.846    time: 0.8738  last_time: 0.8499  data_time: 0.0111  last_data_time: 0.0107   lr: 1.25e-05  max_mem: 11978M\n",
            "\u001b[32m[02/20 08:01:40 d2.utils.events]: \u001b[0m eta: 3 days, 16:48:26  iter: 2219  total_loss: 58.55  loss_ce: 0.721  loss_mask: 0.2159  loss_dice: 0.7099  loss_bbox: 0.2311  loss_giou: 0.6755  loss_ce_dn: 0.1427  loss_mask_dn: 0.2286  loss_dice_dn: 0.6426  loss_bbox_dn: 0.2337  loss_giou_dn: 0.5076  loss_ce_0: 1.453  loss_mask_0: 0.2258  loss_dice_0: 0.6524  loss_bbox_0: 0.3829  loss_giou_0: 0.8591  loss_ce_dn_0: 0.732  loss_mask_dn_0: 0.7928  loss_dice_dn_0: 2.972  loss_bbox_dn_0: 0.6397  loss_giou_dn_0: 0.9357  loss_ce_1: 1.309  loss_mask_1: 0.232  loss_dice_1: 0.636  loss_bbox_1: 0.2741  loss_giou_1: 0.7108  loss_ce_dn_1: 0.2613  loss_mask_dn_1: 0.2588  loss_dice_dn_1: 0.7405  loss_bbox_dn_1: 0.3475  loss_giou_dn_1: 0.6304  loss_ce_2: 1.114  loss_mask_2: 0.2213  loss_dice_2: 0.6445  loss_bbox_2: 0.2418  loss_giou_2: 0.6798  loss_ce_dn_2: 0.2237  loss_mask_dn_2: 0.2414  loss_dice_dn_2: 0.6663  loss_bbox_dn_2: 0.2641  loss_giou_dn_2: 0.57  loss_ce_3: 0.998  loss_mask_3: 0.2166  loss_dice_3: 0.697  loss_bbox_3: 0.2472  loss_giou_3: 0.6965  loss_ce_dn_3: 0.2017  loss_mask_dn_3: 0.2374  loss_dice_dn_3: 0.6446  loss_bbox_dn_3: 0.2532  loss_giou_dn_3: 0.5449  loss_ce_4: 0.8512  loss_mask_4: 0.2159  loss_dice_4: 0.7245  loss_bbox_4: 0.2557  loss_giou_4: 0.6847  loss_ce_dn_4: 0.1805  loss_mask_dn_4: 0.227  loss_dice_dn_4: 0.644  loss_bbox_dn_4: 0.2447  loss_giou_dn_4: 0.5285  loss_ce_5: 0.8019  loss_mask_5: 0.2333  loss_dice_5: 0.6904  loss_bbox_5: 0.2556  loss_giou_5: 0.6916  loss_ce_dn_5: 0.1701  loss_mask_dn_5: 0.2245  loss_dice_dn_5: 0.6257  loss_bbox_dn_5: 0.2416  loss_giou_dn_5: 0.522  loss_ce_6: 0.7396  loss_mask_6: 0.2112  loss_dice_6: 0.6758  loss_bbox_6: 0.253  loss_giou_6: 0.6944  loss_ce_dn_6: 0.1573  loss_mask_dn_6: 0.2243  loss_dice_dn_6: 0.6177  loss_bbox_dn_6: 0.2381  loss_giou_dn_6: 0.5124  loss_ce_7: 0.7105  loss_mask_7: 0.2118  loss_dice_7: 0.6956  loss_bbox_7: 0.2584  loss_giou_7: 0.6846  loss_ce_dn_7: 0.1517  loss_mask_dn_7: 0.2296  loss_dice_dn_7: 0.6138  loss_bbox_dn_7: 0.2392  loss_giou_dn_7: 0.5119  loss_ce_8: 0.7193  loss_mask_8: 0.2183  loss_dice_8: 0.7079  loss_bbox_8: 0.2552  loss_giou_8: 0.666  loss_ce_dn_8: 0.1485  loss_mask_dn_8: 0.2294  loss_dice_dn_8: 0.6221  loss_bbox_dn_8: 0.2356  loss_giou_dn_8: 0.5064  loss_ce_interm: 1.453  loss_mask_interm: 0.2254  loss_dice_interm: 0.6612  loss_bbox_interm: 0.3853  loss_giou_interm: 0.8757    time: 0.8738  last_time: 0.8353  data_time: 0.0120  last_data_time: 0.0122   lr: 1.25e-05  max_mem: 11978M\n",
            "\u001b[32m[02/20 08:01:57 d2.utils.events]: \u001b[0m eta: 3 days, 16:48:01  iter: 2239  total_loss: 62.18  loss_ce: 0.7808  loss_mask: 0.2846  loss_dice: 0.7003  loss_bbox: 0.301  loss_giou: 0.6366  loss_ce_dn: 0.1273  loss_mask_dn: 0.3128  loss_dice_dn: 0.6908  loss_bbox_dn: 0.3019  loss_giou_dn: 0.5144  loss_ce_0: 1.527  loss_mask_0: 0.333  loss_dice_0: 0.6947  loss_bbox_0: 0.4824  loss_giou_0: 0.8568  loss_ce_dn_0: 0.7119  loss_mask_dn_0: 1.015  loss_dice_dn_0: 2.835  loss_bbox_dn_0: 0.7451  loss_giou_dn_0: 0.9002  loss_ce_1: 1.452  loss_mask_1: 0.321  loss_dice_1: 0.681  loss_bbox_1: 0.3577  loss_giou_1: 0.7468  loss_ce_dn_1: 0.2524  loss_mask_dn_1: 0.4211  loss_dice_dn_1: 0.8449  loss_bbox_dn_1: 0.4586  loss_giou_dn_1: 0.6319  loss_ce_2: 1.197  loss_mask_2: 0.3041  loss_dice_2: 0.7806  loss_bbox_2: 0.3676  loss_giou_2: 0.7066  loss_ce_dn_2: 0.1903  loss_mask_dn_2: 0.3641  loss_dice_dn_2: 0.7529  loss_bbox_dn_2: 0.4024  loss_giou_dn_2: 0.5706  loss_ce_3: 1.043  loss_mask_3: 0.3046  loss_dice_3: 0.6989  loss_bbox_3: 0.3598  loss_giou_3: 0.672  loss_ce_dn_3: 0.1775  loss_mask_dn_3: 0.3384  loss_dice_dn_3: 0.7351  loss_bbox_dn_3: 0.37  loss_giou_dn_3: 0.5476  loss_ce_4: 0.908  loss_mask_4: 0.3142  loss_dice_4: 0.7347  loss_bbox_4: 0.3328  loss_giou_4: 0.6312  loss_ce_dn_4: 0.1566  loss_mask_dn_4: 0.3385  loss_dice_dn_4: 0.7126  loss_bbox_dn_4: 0.339  loss_giou_dn_4: 0.5271  loss_ce_5: 0.8208  loss_mask_5: 0.2984  loss_dice_5: 0.7522  loss_bbox_5: 0.3288  loss_giou_5: 0.6307  loss_ce_dn_5: 0.1478  loss_mask_dn_5: 0.3267  loss_dice_dn_5: 0.7318  loss_bbox_dn_5: 0.327  loss_giou_dn_5: 0.5238  loss_ce_6: 0.8113  loss_mask_6: 0.3019  loss_dice_6: 0.7185  loss_bbox_6: 0.3097  loss_giou_6: 0.6383  loss_ce_dn_6: 0.1388  loss_mask_dn_6: 0.3241  loss_dice_dn_6: 0.6931  loss_bbox_dn_6: 0.3117  loss_giou_dn_6: 0.5188  loss_ce_7: 0.7781  loss_mask_7: 0.2875  loss_dice_7: 0.6797  loss_bbox_7: 0.3047  loss_giou_7: 0.6292  loss_ce_dn_7: 0.1327  loss_mask_dn_7: 0.319  loss_dice_dn_7: 0.6876  loss_bbox_dn_7: 0.3047  loss_giou_dn_7: 0.5164  loss_ce_8: 0.7493  loss_mask_8: 0.2908  loss_dice_8: 0.7205  loss_bbox_8: 0.311  loss_giou_8: 0.6163  loss_ce_dn_8: 0.1292  loss_mask_dn_8: 0.3169  loss_dice_dn_8: 0.6932  loss_bbox_dn_8: 0.3021  loss_giou_dn_8: 0.5123  loss_ce_interm: 1.548  loss_mask_interm: 0.3275  loss_dice_interm: 0.7162  loss_bbox_interm: 0.4898  loss_giou_interm: 0.8479    time: 0.8737  last_time: 0.8742  data_time: 0.0118  last_data_time: 0.0103   lr: 1.25e-05  max_mem: 11978M\n",
            "\u001b[32m[02/20 08:02:15 d2.utils.events]: \u001b[0m eta: 3 days, 16:48:11  iter: 2259  total_loss: 69.17  loss_ce: 0.8704  loss_mask: 0.2778  loss_dice: 0.8565  loss_bbox: 0.2885  loss_giou: 0.8254  loss_ce_dn: 0.1304  loss_mask_dn: 0.2764  loss_dice_dn: 0.8652  loss_bbox_dn: 0.2692  loss_giou_dn: 0.6429  loss_ce_0: 1.592  loss_mask_0: 0.2955  loss_dice_0: 0.936  loss_bbox_0: 0.4779  loss_giou_0: 0.9776  loss_ce_dn_0: 0.7021  loss_mask_dn_0: 0.7812  loss_dice_dn_0: 3.32  loss_bbox_dn_0: 0.6104  loss_giou_dn_0: 1.002  loss_ce_1: 1.504  loss_mask_1: 0.2896  loss_dice_1: 0.8856  loss_bbox_1: 0.3658  loss_giou_1: 0.899  loss_ce_dn_1: 0.253  loss_mask_dn_1: 0.3374  loss_dice_dn_1: 1.044  loss_bbox_dn_1: 0.3682  loss_giou_dn_1: 0.7586  loss_ce_2: 1.221  loss_mask_2: 0.2998  loss_dice_2: 0.9384  loss_bbox_2: 0.3395  loss_giou_2: 0.8639  loss_ce_dn_2: 0.2001  loss_mask_dn_2: 0.2985  loss_dice_dn_2: 0.9379  loss_bbox_dn_2: 0.3152  loss_giou_dn_2: 0.7124  loss_ce_3: 1.069  loss_mask_3: 0.2809  loss_dice_3: 0.8786  loss_bbox_3: 0.3182  loss_giou_3: 0.8471  loss_ce_dn_3: 0.1693  loss_mask_dn_3: 0.2827  loss_dice_dn_3: 0.8467  loss_bbox_dn_3: 0.3012  loss_giou_dn_3: 0.6862  loss_ce_4: 1.077  loss_mask_4: 0.2877  loss_dice_4: 0.8653  loss_bbox_4: 0.3056  loss_giou_4: 0.8155  loss_ce_dn_4: 0.1589  loss_mask_dn_4: 0.2777  loss_dice_dn_4: 0.8489  loss_bbox_dn_4: 0.2868  loss_giou_dn_4: 0.6699  loss_ce_5: 0.9617  loss_mask_5: 0.2753  loss_dice_5: 0.8235  loss_bbox_5: 0.301  loss_giou_5: 0.8418  loss_ce_dn_5: 0.1461  loss_mask_dn_5: 0.2692  loss_dice_dn_5: 0.8722  loss_bbox_dn_5: 0.2825  loss_giou_dn_5: 0.6591  loss_ce_6: 0.9329  loss_mask_6: 0.2822  loss_dice_6: 0.8387  loss_bbox_6: 0.3012  loss_giou_6: 0.8169  loss_ce_dn_6: 0.1402  loss_mask_dn_6: 0.2754  loss_dice_dn_6: 0.8265  loss_bbox_dn_6: 0.273  loss_giou_dn_6: 0.6523  loss_ce_7: 0.905  loss_mask_7: 0.2864  loss_dice_7: 0.8276  loss_bbox_7: 0.2997  loss_giou_7: 0.8309  loss_ce_dn_7: 0.1406  loss_mask_dn_7: 0.2801  loss_dice_dn_7: 0.8457  loss_bbox_dn_7: 0.2718  loss_giou_dn_7: 0.6437  loss_ce_8: 0.8732  loss_mask_8: 0.2793  loss_dice_8: 0.8689  loss_bbox_8: 0.2931  loss_giou_8: 0.8433  loss_ce_dn_8: 0.1364  loss_mask_dn_8: 0.2773  loss_dice_dn_8: 0.8629  loss_bbox_dn_8: 0.2681  loss_giou_dn_8: 0.6435  loss_ce_interm: 1.61  loss_mask_interm: 0.2993  loss_dice_interm: 0.9007  loss_bbox_interm: 0.4645  loss_giou_interm: 0.9948    time: 0.8738  last_time: 0.9030  data_time: 0.0126  last_data_time: 0.0108   lr: 1.25e-05  max_mem: 11978M\n",
            "\u001b[32m[02/20 08:02:32 d2.utils.events]: \u001b[0m eta: 3 days, 16:47:53  iter: 2279  total_loss: 64.66  loss_ce: 0.7695  loss_mask: 0.3483  loss_dice: 0.8345  loss_bbox: 0.3159  loss_giou: 0.6653  loss_ce_dn: 0.1078  loss_mask_dn: 0.352  loss_dice_dn: 0.8451  loss_bbox_dn: 0.2816  loss_giou_dn: 0.5469  loss_ce_0: 1.458  loss_mask_0: 0.3884  loss_dice_0: 0.8863  loss_bbox_0: 0.5133  loss_giou_0: 0.8971  loss_ce_dn_0: 0.7412  loss_mask_dn_0: 0.9115  loss_dice_dn_0: 3.06  loss_bbox_dn_0: 0.7026  loss_giou_dn_0: 0.9319  loss_ce_1: 1.4  loss_mask_1: 0.3941  loss_dice_1: 0.8673  loss_bbox_1: 0.392  loss_giou_1: 0.8308  loss_ce_dn_1: 0.2316  loss_mask_dn_1: 0.4319  loss_dice_dn_1: 0.9426  loss_bbox_dn_1: 0.4036  loss_giou_dn_1: 0.6971  loss_ce_2: 1.247  loss_mask_2: 0.391  loss_dice_2: 0.8554  loss_bbox_2: 0.3169  loss_giou_2: 0.7545  loss_ce_dn_2: 0.179  loss_mask_dn_2: 0.3827  loss_dice_dn_2: 0.8725  loss_bbox_dn_2: 0.3607  loss_giou_dn_2: 0.6205  loss_ce_3: 1.013  loss_mask_3: 0.3854  loss_dice_3: 0.8914  loss_bbox_3: 0.3374  loss_giou_3: 0.7363  loss_ce_dn_3: 0.1608  loss_mask_dn_3: 0.3684  loss_dice_dn_3: 0.8436  loss_bbox_dn_3: 0.3284  loss_giou_dn_3: 0.5979  loss_ce_4: 0.9764  loss_mask_4: 0.3758  loss_dice_4: 0.825  loss_bbox_4: 0.317  loss_giou_4: 0.7582  loss_ce_dn_4: 0.1448  loss_mask_dn_4: 0.3547  loss_dice_dn_4: 0.8501  loss_bbox_dn_4: 0.3046  loss_giou_dn_4: 0.5759  loss_ce_5: 0.886  loss_mask_5: 0.3558  loss_dice_5: 0.8259  loss_bbox_5: 0.3136  loss_giou_5: 0.7185  loss_ce_dn_5: 0.1274  loss_mask_dn_5: 0.3622  loss_dice_dn_5: 0.8383  loss_bbox_dn_5: 0.3039  loss_giou_dn_5: 0.5594  loss_ce_6: 0.8413  loss_mask_6: 0.3698  loss_dice_6: 0.8475  loss_bbox_6: 0.3182  loss_giou_6: 0.7027  loss_ce_dn_6: 0.1214  loss_mask_dn_6: 0.3527  loss_dice_dn_6: 0.8392  loss_bbox_dn_6: 0.2863  loss_giou_dn_6: 0.555  loss_ce_7: 0.7855  loss_mask_7: 0.3702  loss_dice_7: 0.8447  loss_bbox_7: 0.3163  loss_giou_7: 0.7227  loss_ce_dn_7: 0.1107  loss_mask_dn_7: 0.3607  loss_dice_dn_7: 0.8479  loss_bbox_dn_7: 0.2832  loss_giou_dn_7: 0.5514  loss_ce_8: 0.7825  loss_mask_8: 0.3666  loss_dice_8: 0.8162  loss_bbox_8: 0.3201  loss_giou_8: 0.7571  loss_ce_dn_8: 0.1098  loss_mask_dn_8: 0.3574  loss_dice_dn_8: 0.8449  loss_bbox_dn_8: 0.2822  loss_giou_dn_8: 0.5463  loss_ce_interm: 1.462  loss_mask_interm: 0.3855  loss_dice_interm: 0.8834  loss_bbox_interm: 0.5066  loss_giou_interm: 0.9064    time: 0.8738  last_time: 0.8733  data_time: 0.0129  last_data_time: 0.0153   lr: 1.25e-05  max_mem: 11978M\n",
            "\u001b[32m[02/20 08:02:50 d2.utils.events]: \u001b[0m eta: 3 days, 16:47:25  iter: 2299  total_loss: 63.5  loss_ce: 0.8375  loss_mask: 0.281  loss_dice: 0.8071  loss_bbox: 0.2813  loss_giou: 0.6397  loss_ce_dn: 0.09247  loss_mask_dn: 0.2802  loss_dice_dn: 0.7879  loss_bbox_dn: 0.2636  loss_giou_dn: 0.556  loss_ce_0: 1.501  loss_mask_0: 0.3164  loss_dice_0: 0.9355  loss_bbox_0: 0.4048  loss_giou_0: 0.8953  loss_ce_dn_0: 0.7209  loss_mask_dn_0: 0.7935  loss_dice_dn_0: 2.972  loss_bbox_dn_0: 0.5855  loss_giou_dn_0: 0.9163  loss_ce_1: 1.39  loss_mask_1: 0.3014  loss_dice_1: 0.9346  loss_bbox_1: 0.321  loss_giou_1: 0.7212  loss_ce_dn_1: 0.2174  loss_mask_dn_1: 0.3117  loss_dice_dn_1: 0.8949  loss_bbox_dn_1: 0.3606  loss_giou_dn_1: 0.6736  loss_ce_2: 1.251  loss_mask_2: 0.3012  loss_dice_2: 0.8484  loss_bbox_2: 0.299  loss_giou_2: 0.6873  loss_ce_dn_2: 0.17  loss_mask_dn_2: 0.2944  loss_dice_dn_2: 0.8566  loss_bbox_dn_2: 0.3169  loss_giou_dn_2: 0.6134  loss_ce_3: 1.114  loss_mask_3: 0.2912  loss_dice_3: 0.8603  loss_bbox_3: 0.283  loss_giou_3: 0.6582  loss_ce_dn_3: 0.1445  loss_mask_dn_3: 0.2871  loss_dice_dn_3: 0.8086  loss_bbox_dn_3: 0.2937  loss_giou_dn_3: 0.5903  loss_ce_4: 1.017  loss_mask_4: 0.2764  loss_dice_4: 0.8302  loss_bbox_4: 0.2714  loss_giou_4: 0.6347  loss_ce_dn_4: 0.1214  loss_mask_dn_4: 0.2792  loss_dice_dn_4: 0.8059  loss_bbox_dn_4: 0.2754  loss_giou_dn_4: 0.5639  loss_ce_5: 0.9047  loss_mask_5: 0.2856  loss_dice_5: 0.8573  loss_bbox_5: 0.2632  loss_giou_5: 0.638  loss_ce_dn_5: 0.1103  loss_mask_dn_5: 0.2866  loss_dice_dn_5: 0.7869  loss_bbox_dn_5: 0.2758  loss_giou_dn_5: 0.5663  loss_ce_6: 0.8508  loss_mask_6: 0.2817  loss_dice_6: 0.8288  loss_bbox_6: 0.289  loss_giou_6: 0.6422  loss_ce_dn_6: 0.09592  loss_mask_dn_6: 0.2833  loss_dice_dn_6: 0.8095  loss_bbox_dn_6: 0.2706  loss_giou_dn_6: 0.5557  loss_ce_7: 0.8365  loss_mask_7: 0.2809  loss_dice_7: 0.8635  loss_bbox_7: 0.2863  loss_giou_7: 0.6543  loss_ce_dn_7: 0.0901  loss_mask_dn_7: 0.2799  loss_dice_dn_7: 0.786  loss_bbox_dn_7: 0.2666  loss_giou_dn_7: 0.5524  loss_ce_8: 0.8305  loss_mask_8: 0.2775  loss_dice_8: 0.8234  loss_bbox_8: 0.3069  loss_giou_8: 0.6316  loss_ce_dn_8: 0.08792  loss_mask_dn_8: 0.2795  loss_dice_dn_8: 0.7813  loss_bbox_dn_8: 0.2644  loss_giou_dn_8: 0.5533  loss_ce_interm: 1.475  loss_mask_interm: 0.3222  loss_dice_interm: 0.8878  loss_bbox_interm: 0.412  loss_giou_interm: 0.9043    time: 0.8738  last_time: 0.8664  data_time: 0.0124  last_data_time: 0.0094   lr: 1.25e-05  max_mem: 11978M\n",
            "\u001b[32m[02/20 08:03:07 d2.utils.events]: \u001b[0m eta: 3 days, 16:47:08  iter: 2319  total_loss: 63.79  loss_ce: 0.6337  loss_mask: 0.2631  loss_dice: 0.8932  loss_bbox: 0.2476  loss_giou: 0.6589  loss_ce_dn: 0.09181  loss_mask_dn: 0.2614  loss_dice_dn: 0.8809  loss_bbox_dn: 0.2192  loss_giou_dn: 0.5414  loss_ce_0: 1.477  loss_mask_0: 0.3037  loss_dice_0: 0.9413  loss_bbox_0: 0.3806  loss_giou_0: 0.9095  loss_ce_dn_0: 0.7101  loss_mask_dn_0: 0.7872  loss_dice_dn_0: 2.834  loss_bbox_dn_0: 0.5989  loss_giou_dn_0: 0.8948  loss_ce_1: 1.361  loss_mask_1: 0.2946  loss_dice_1: 0.8731  loss_bbox_1: 0.2736  loss_giou_1: 0.7352  loss_ce_dn_1: 0.253  loss_mask_dn_1: 0.3356  loss_dice_dn_1: 1.123  loss_bbox_dn_1: 0.3253  loss_giou_dn_1: 0.6603  loss_ce_2: 1.187  loss_mask_2: 0.2818  loss_dice_2: 0.9075  loss_bbox_2: 0.2525  loss_giou_2: 0.7559  loss_ce_dn_2: 0.2098  loss_mask_dn_2: 0.3085  loss_dice_dn_2: 0.9818  loss_bbox_dn_2: 0.272  loss_giou_dn_2: 0.5999  loss_ce_3: 0.9878  loss_mask_3: 0.2564  loss_dice_3: 0.8849  loss_bbox_3: 0.2577  loss_giou_3: 0.713  loss_ce_dn_3: 0.1699  loss_mask_dn_3: 0.2642  loss_dice_dn_3: 0.9284  loss_bbox_dn_3: 0.2526  loss_giou_dn_3: 0.5808  loss_ce_4: 0.7891  loss_mask_4: 0.2815  loss_dice_4: 0.8847  loss_bbox_4: 0.2542  loss_giou_4: 0.6967  loss_ce_dn_4: 0.1464  loss_mask_dn_4: 0.2588  loss_dice_dn_4: 0.9047  loss_bbox_dn_4: 0.2375  loss_giou_dn_4: 0.5644  loss_ce_5: 0.7359  loss_mask_5: 0.2573  loss_dice_5: 0.8955  loss_bbox_5: 0.2585  loss_giou_5: 0.7  loss_ce_dn_5: 0.1159  loss_mask_dn_5: 0.258  loss_dice_dn_5: 0.8864  loss_bbox_dn_5: 0.2325  loss_giou_dn_5: 0.5589  loss_ce_6: 0.6962  loss_mask_6: 0.2603  loss_dice_6: 0.8656  loss_bbox_6: 0.2564  loss_giou_6: 0.6973  loss_ce_dn_6: 0.1103  loss_mask_dn_6: 0.2546  loss_dice_dn_6: 0.889  loss_bbox_dn_6: 0.2231  loss_giou_dn_6: 0.5511  loss_ce_7: 0.6716  loss_mask_7: 0.2554  loss_dice_7: 0.8795  loss_bbox_7: 0.2523  loss_giou_7: 0.6718  loss_ce_dn_7: 0.0988  loss_mask_dn_7: 0.2563  loss_dice_dn_7: 0.9007  loss_bbox_dn_7: 0.2194  loss_giou_dn_7: 0.546  loss_ce_8: 0.6375  loss_mask_8: 0.2471  loss_dice_8: 0.8579  loss_bbox_8: 0.2438  loss_giou_8: 0.7101  loss_ce_dn_8: 0.09655  loss_mask_dn_8: 0.2578  loss_dice_dn_8: 0.8945  loss_bbox_dn_8: 0.219  loss_giou_dn_8: 0.54  loss_ce_interm: 1.496  loss_mask_interm: 0.3074  loss_dice_interm: 0.9208  loss_bbox_interm: 0.3795  loss_giou_interm: 0.8936    time: 0.8738  last_time: 0.8558  data_time: 0.0118  last_data_time: 0.0101   lr: 1.25e-05  max_mem: 11978M\n",
            "\u001b[32m[02/20 08:03:25 d2.utils.events]: \u001b[0m eta: 3 days, 16:45:25  iter: 2339  total_loss: 61.85  loss_ce: 0.8777  loss_mask: 0.2633  loss_dice: 0.7061  loss_bbox: 0.2795  loss_giou: 0.7087  loss_ce_dn: 0.1341  loss_mask_dn: 0.2631  loss_dice_dn: 0.7517  loss_bbox_dn: 0.2167  loss_giou_dn: 0.6341  loss_ce_0: 1.516  loss_mask_0: 0.2857  loss_dice_0: 0.7898  loss_bbox_0: 0.3917  loss_giou_0: 0.9141  loss_ce_dn_0: 0.7131  loss_mask_dn_0: 0.8318  loss_dice_dn_0: 2.956  loss_bbox_dn_0: 0.6357  loss_giou_dn_0: 0.9565  loss_ce_1: 1.392  loss_mask_1: 0.2779  loss_dice_1: 0.7983  loss_bbox_1: 0.2936  loss_giou_1: 0.7888  loss_ce_dn_1: 0.2444  loss_mask_dn_1: 0.3533  loss_dice_dn_1: 0.9133  loss_bbox_dn_1: 0.3497  loss_giou_dn_1: 0.7556  loss_ce_2: 1.263  loss_mask_2: 0.2867  loss_dice_2: 0.7985  loss_bbox_2: 0.2623  loss_giou_2: 0.7498  loss_ce_dn_2: 0.201  loss_mask_dn_2: 0.288  loss_dice_dn_2: 0.8349  loss_bbox_dn_2: 0.2954  loss_giou_dn_2: 0.6939  loss_ce_3: 1.137  loss_mask_3: 0.2837  loss_dice_3: 0.7402  loss_bbox_3: 0.2659  loss_giou_3: 0.7313  loss_ce_dn_3: 0.1873  loss_mask_dn_3: 0.2801  loss_dice_dn_3: 0.7934  loss_bbox_dn_3: 0.2646  loss_giou_dn_3: 0.6653  loss_ce_4: 1.046  loss_mask_4: 0.2761  loss_dice_4: 0.696  loss_bbox_4: 0.2696  loss_giou_4: 0.7121  loss_ce_dn_4: 0.1757  loss_mask_dn_4: 0.2615  loss_dice_dn_4: 0.7834  loss_bbox_dn_4: 0.2375  loss_giou_dn_4: 0.6452  loss_ce_5: 0.9952  loss_mask_5: 0.27  loss_dice_5: 0.7254  loss_bbox_5: 0.2841  loss_giou_5: 0.7119  loss_ce_dn_5: 0.1628  loss_mask_dn_5: 0.2641  loss_dice_dn_5: 0.7879  loss_bbox_dn_5: 0.2316  loss_giou_dn_5: 0.6397  loss_ce_6: 0.9215  loss_mask_6: 0.2829  loss_dice_6: 0.7592  loss_bbox_6: 0.2831  loss_giou_6: 0.7121  loss_ce_dn_6: 0.1559  loss_mask_dn_6: 0.2617  loss_dice_dn_6: 0.7961  loss_bbox_dn_6: 0.2244  loss_giou_dn_6: 0.6372  loss_ce_7: 0.896  loss_mask_7: 0.27  loss_dice_7: 0.7282  loss_bbox_7: 0.3193  loss_giou_7: 0.7148  loss_ce_dn_7: 0.1441  loss_mask_dn_7: 0.2657  loss_dice_dn_7: 0.7778  loss_bbox_dn_7: 0.2201  loss_giou_dn_7: 0.6366  loss_ce_8: 0.9156  loss_mask_8: 0.2634  loss_dice_8: 0.6982  loss_bbox_8: 0.2779  loss_giou_8: 0.6918  loss_ce_dn_8: 0.1389  loss_mask_dn_8: 0.2668  loss_dice_dn_8: 0.786  loss_bbox_dn_8: 0.2166  loss_giou_dn_8: 0.6353  loss_ce_interm: 1.5  loss_mask_interm: 0.2849  loss_dice_interm: 0.7702  loss_bbox_interm: 0.3917  loss_giou_interm: 0.9138    time: 0.8737  last_time: 0.8864  data_time: 0.0115  last_data_time: 0.0104   lr: 1.25e-05  max_mem: 11978M\n",
            "\u001b[32m[02/20 08:03:42 d2.utils.events]: \u001b[0m eta: 3 days, 16:44:44  iter: 2359  total_loss: 62.25  loss_ce: 0.7531  loss_mask: 0.1855  loss_dice: 0.7667  loss_bbox: 0.2759  loss_giou: 0.7218  loss_ce_dn: 0.1297  loss_mask_dn: 0.1976  loss_dice_dn: 0.6823  loss_bbox_dn: 0.234  loss_giou_dn: 0.5914  loss_ce_0: 1.367  loss_mask_0: 0.2181  loss_dice_0: 0.85  loss_bbox_0: 0.3832  loss_giou_0: 0.9443  loss_ce_dn_0: 0.7513  loss_mask_dn_0: 0.6412  loss_dice_dn_0: 2.821  loss_bbox_dn_0: 0.6041  loss_giou_dn_0: 0.9772  loss_ce_1: 1.264  loss_mask_1: 0.2132  loss_dice_1: 0.7592  loss_bbox_1: 0.3131  loss_giou_1: 0.8157  loss_ce_dn_1: 0.279  loss_mask_dn_1: 0.2298  loss_dice_dn_1: 0.839  loss_bbox_dn_1: 0.3282  loss_giou_dn_1: 0.7541  loss_ce_2: 1.165  loss_mask_2: 0.1998  loss_dice_2: 0.8193  loss_bbox_2: 0.2935  loss_giou_2: 0.8034  loss_ce_dn_2: 0.2195  loss_mask_dn_2: 0.2068  loss_dice_dn_2: 0.7372  loss_bbox_dn_2: 0.2794  loss_giou_dn_2: 0.6838  loss_ce_3: 1.051  loss_mask_3: 0.1894  loss_dice_3: 0.7703  loss_bbox_3: 0.297  loss_giou_3: 0.789  loss_ce_dn_3: 0.19  loss_mask_dn_3: 0.2043  loss_dice_dn_3: 0.7166  loss_bbox_dn_3: 0.272  loss_giou_dn_3: 0.6402  loss_ce_4: 0.9735  loss_mask_4: 0.1968  loss_dice_4: 0.7897  loss_bbox_4: 0.2731  loss_giou_4: 0.7638  loss_ce_dn_4: 0.1673  loss_mask_dn_4: 0.2026  loss_dice_dn_4: 0.7147  loss_bbox_dn_4: 0.26  loss_giou_dn_4: 0.6282  loss_ce_5: 0.8121  loss_mask_5: 0.1868  loss_dice_5: 0.8032  loss_bbox_5: 0.2737  loss_giou_5: 0.7469  loss_ce_dn_5: 0.1496  loss_mask_dn_5: 0.2079  loss_dice_dn_5: 0.7098  loss_bbox_dn_5: 0.2521  loss_giou_dn_5: 0.6172  loss_ce_6: 0.82  loss_mask_6: 0.1895  loss_dice_6: 0.8727  loss_bbox_6: 0.2741  loss_giou_6: 0.7682  loss_ce_dn_6: 0.1425  loss_mask_dn_6: 0.1989  loss_dice_dn_6: 0.6861  loss_bbox_dn_6: 0.2431  loss_giou_dn_6: 0.6078  loss_ce_7: 0.7691  loss_mask_7: 0.1925  loss_dice_7: 0.7935  loss_bbox_7: 0.2843  loss_giou_7: 0.74  loss_ce_dn_7: 0.1324  loss_mask_dn_7: 0.2013  loss_dice_dn_7: 0.7094  loss_bbox_dn_7: 0.2381  loss_giou_dn_7: 0.5993  loss_ce_8: 0.7308  loss_mask_8: 0.1959  loss_dice_8: 0.7777  loss_bbox_8: 0.2684  loss_giou_8: 0.7627  loss_ce_dn_8: 0.1305  loss_mask_dn_8: 0.1983  loss_dice_dn_8: 0.6968  loss_bbox_dn_8: 0.2348  loss_giou_dn_8: 0.5928  loss_ce_interm: 1.399  loss_mask_interm: 0.2148  loss_dice_interm: 0.8476  loss_bbox_interm: 0.3832  loss_giou_interm: 0.9378    time: 0.8737  last_time: 0.8729  data_time: 0.0120  last_data_time: 0.0145   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:04:00 d2.utils.events]: \u001b[0m eta: 3 days, 16:43:35  iter: 2379  total_loss: 65.75  loss_ce: 0.7854  loss_mask: 0.2223  loss_dice: 0.6631  loss_bbox: 0.2352  loss_giou: 0.7812  loss_ce_dn: 0.1601  loss_mask_dn: 0.2635  loss_dice_dn: 0.6301  loss_bbox_dn: 0.2384  loss_giou_dn: 0.6807  loss_ce_0: 1.4  loss_mask_0: 0.239  loss_dice_0: 0.6835  loss_bbox_0: 0.4382  loss_giou_0: 1.013  loss_ce_dn_0: 0.7384  loss_mask_dn_0: 0.9408  loss_dice_dn_0: 3.38  loss_bbox_dn_0: 0.5897  loss_giou_dn_0: 1.071  loss_ce_1: 1.293  loss_mask_1: 0.2453  loss_dice_1: 0.6883  loss_bbox_1: 0.3338  loss_giou_1: 0.9047  loss_ce_dn_1: 0.2865  loss_mask_dn_1: 0.3255  loss_dice_dn_1: 0.7045  loss_bbox_dn_1: 0.3413  loss_giou_dn_1: 0.8306  loss_ce_2: 1.172  loss_mask_2: 0.227  loss_dice_2: 0.6824  loss_bbox_2: 0.2667  loss_giou_2: 0.8327  loss_ce_dn_2: 0.2403  loss_mask_dn_2: 0.2816  loss_dice_dn_2: 0.6748  loss_bbox_dn_2: 0.2959  loss_giou_dn_2: 0.7484  loss_ce_3: 1.004  loss_mask_3: 0.2441  loss_dice_3: 0.72  loss_bbox_3: 0.2859  loss_giou_3: 0.8232  loss_ce_dn_3: 0.1994  loss_mask_dn_3: 0.2702  loss_dice_dn_3: 0.6466  loss_bbox_dn_3: 0.2655  loss_giou_dn_3: 0.7219  loss_ce_4: 0.8559  loss_mask_4: 0.2347  loss_dice_4: 0.6627  loss_bbox_4: 0.2524  loss_giou_4: 0.8138  loss_ce_dn_4: 0.1903  loss_mask_dn_4: 0.267  loss_dice_dn_4: 0.6625  loss_bbox_dn_4: 0.2497  loss_giou_dn_4: 0.7062  loss_ce_5: 0.8405  loss_mask_5: 0.2381  loss_dice_5: 0.6801  loss_bbox_5: 0.245  loss_giou_5: 0.7978  loss_ce_dn_5: 0.171  loss_mask_dn_5: 0.284  loss_dice_dn_5: 0.6509  loss_bbox_dn_5: 0.2442  loss_giou_dn_5: 0.6936  loss_ce_6: 0.8413  loss_mask_6: 0.2308  loss_dice_6: 0.6453  loss_bbox_6: 0.2245  loss_giou_6: 0.7954  loss_ce_dn_6: 0.1587  loss_mask_dn_6: 0.2719  loss_dice_dn_6: 0.6301  loss_bbox_dn_6: 0.2414  loss_giou_dn_6: 0.6866  loss_ce_7: 0.8166  loss_mask_7: 0.2426  loss_dice_7: 0.5877  loss_bbox_7: 0.2304  loss_giou_7: 0.7925  loss_ce_dn_7: 0.1629  loss_mask_dn_7: 0.2617  loss_dice_dn_7: 0.6032  loss_bbox_dn_7: 0.2406  loss_giou_dn_7: 0.6848  loss_ce_8: 0.7973  loss_mask_8: 0.2356  loss_dice_8: 0.5803  loss_bbox_8: 0.2358  loss_giou_8: 0.782  loss_ce_dn_8: 0.1595  loss_mask_dn_8: 0.2675  loss_dice_dn_8: 0.6689  loss_bbox_dn_8: 0.2387  loss_giou_dn_8: 0.6818  loss_ce_interm: 1.397  loss_mask_interm: 0.2388  loss_dice_interm: 0.6625  loss_bbox_interm: 0.4433  loss_giou_interm: 1.017    time: 0.8736  last_time: 0.8807  data_time: 0.0119  last_data_time: 0.0106   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:04:17 d2.utils.events]: \u001b[0m eta: 3 days, 16:43:49  iter: 2399  total_loss: 56.62  loss_ce: 0.6834  loss_mask: 0.3042  loss_dice: 0.6423  loss_bbox: 0.205  loss_giou: 0.6106  loss_ce_dn: 0.1233  loss_mask_dn: 0.3456  loss_dice_dn: 0.6761  loss_bbox_dn: 0.2217  loss_giou_dn: 0.5337  loss_ce_0: 1.432  loss_mask_0: 0.3507  loss_dice_0: 0.705  loss_bbox_0: 0.4112  loss_giou_0: 0.8217  loss_ce_dn_0: 0.725  loss_mask_dn_0: 0.8942  loss_dice_dn_0: 2.939  loss_bbox_dn_0: 0.6841  loss_giou_dn_0: 0.9473  loss_ce_1: 1.412  loss_mask_1: 0.3402  loss_dice_1: 0.729  loss_bbox_1: 0.264  loss_giou_1: 0.6893  loss_ce_dn_1: 0.2538  loss_mask_dn_1: 0.3854  loss_dice_dn_1: 0.7993  loss_bbox_dn_1: 0.3839  loss_giou_dn_1: 0.6892  loss_ce_2: 1.176  loss_mask_2: 0.3355  loss_dice_2: 0.7776  loss_bbox_2: 0.244  loss_giou_2: 0.6481  loss_ce_dn_2: 0.2071  loss_mask_dn_2: 0.3727  loss_dice_dn_2: 0.7207  loss_bbox_dn_2: 0.3106  loss_giou_dn_2: 0.6009  loss_ce_3: 0.9815  loss_mask_3: 0.3135  loss_dice_3: 0.7583  loss_bbox_3: 0.232  loss_giou_3: 0.6511  loss_ce_dn_3: 0.1897  loss_mask_dn_3: 0.3605  loss_dice_dn_3: 0.7016  loss_bbox_dn_3: 0.2809  loss_giou_dn_3: 0.5785  loss_ce_4: 0.8739  loss_mask_4: 0.2953  loss_dice_4: 0.6606  loss_bbox_4: 0.2193  loss_giou_4: 0.64  loss_ce_dn_4: 0.1658  loss_mask_dn_4: 0.3414  loss_dice_dn_4: 0.6741  loss_bbox_dn_4: 0.2526  loss_giou_dn_4: 0.56  loss_ce_5: 0.7241  loss_mask_5: 0.2918  loss_dice_5: 0.6517  loss_bbox_5: 0.2042  loss_giou_5: 0.6228  loss_ce_dn_5: 0.144  loss_mask_dn_5: 0.3565  loss_dice_dn_5: 0.686  loss_bbox_dn_5: 0.2396  loss_giou_dn_5: 0.5548  loss_ce_6: 0.7549  loss_mask_6: 0.3077  loss_dice_6: 0.6715  loss_bbox_6: 0.1987  loss_giou_6: 0.6142  loss_ce_dn_6: 0.128  loss_mask_dn_6: 0.3525  loss_dice_dn_6: 0.6576  loss_bbox_dn_6: 0.2341  loss_giou_dn_6: 0.5432  loss_ce_7: 0.7177  loss_mask_7: 0.2949  loss_dice_7: 0.6553  loss_bbox_7: 0.1996  loss_giou_7: 0.6185  loss_ce_dn_7: 0.1282  loss_mask_dn_7: 0.3474  loss_dice_dn_7: 0.6923  loss_bbox_dn_7: 0.2312  loss_giou_dn_7: 0.5376  loss_ce_8: 0.705  loss_mask_8: 0.2985  loss_dice_8: 0.662  loss_bbox_8: 0.2055  loss_giou_8: 0.6117  loss_ce_dn_8: 0.125  loss_mask_dn_8: 0.3477  loss_dice_dn_8: 0.657  loss_bbox_dn_8: 0.2236  loss_giou_dn_8: 0.5324  loss_ce_interm: 1.45  loss_mask_interm: 0.341  loss_dice_interm: 0.7256  loss_bbox_interm: 0.4222  loss_giou_interm: 0.8249    time: 0.8736  last_time: 0.8550  data_time: 0.0120  last_data_time: 0.0089   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:04:35 d2.utils.events]: \u001b[0m eta: 3 days, 16:42:03  iter: 2419  total_loss: 62.7  loss_ce: 0.756  loss_mask: 0.2091  loss_dice: 0.914  loss_bbox: 0.2477  loss_giou: 0.735  loss_ce_dn: 0.1108  loss_mask_dn: 0.2231  loss_dice_dn: 0.9215  loss_bbox_dn: 0.2294  loss_giou_dn: 0.5341  loss_ce_0: 1.402  loss_mask_0: 0.1914  loss_dice_0: 0.9295  loss_bbox_0: 0.3846  loss_giou_0: 0.9659  loss_ce_dn_0: 0.7434  loss_mask_dn_0: 0.7684  loss_dice_dn_0: 3.173  loss_bbox_dn_0: 0.5741  loss_giou_dn_0: 0.9309  loss_ce_1: 1.323  loss_mask_1: 0.2381  loss_dice_1: 0.9156  loss_bbox_1: 0.2803  loss_giou_1: 0.822  loss_ce_dn_1: 0.2357  loss_mask_dn_1: 0.2699  loss_dice_dn_1: 1.055  loss_bbox_dn_1: 0.3218  loss_giou_dn_1: 0.6599  loss_ce_2: 1.145  loss_mask_2: 0.2328  loss_dice_2: 0.893  loss_bbox_2: 0.2503  loss_giou_2: 0.767  loss_ce_dn_2: 0.1794  loss_mask_dn_2: 0.2451  loss_dice_dn_2: 0.9986  loss_bbox_dn_2: 0.2728  loss_giou_dn_2: 0.5957  loss_ce_3: 0.9997  loss_mask_3: 0.2223  loss_dice_3: 0.989  loss_bbox_3: 0.2469  loss_giou_3: 0.7679  loss_ce_dn_3: 0.1491  loss_mask_dn_3: 0.2318  loss_dice_dn_3: 0.9784  loss_bbox_dn_3: 0.2516  loss_giou_dn_3: 0.5653  loss_ce_4: 0.8758  loss_mask_4: 0.213  loss_dice_4: 0.9316  loss_bbox_4: 0.2587  loss_giou_4: 0.7488  loss_ce_dn_4: 0.1305  loss_mask_dn_4: 0.2234  loss_dice_dn_4: 0.9567  loss_bbox_dn_4: 0.2424  loss_giou_dn_4: 0.5469  loss_ce_5: 0.795  loss_mask_5: 0.219  loss_dice_5: 0.9161  loss_bbox_5: 0.2652  loss_giou_5: 0.7452  loss_ce_dn_5: 0.1176  loss_mask_dn_5: 0.2329  loss_dice_dn_5: 0.9456  loss_bbox_dn_5: 0.2391  loss_giou_dn_5: 0.5358  loss_ce_6: 0.7602  loss_mask_6: 0.2148  loss_dice_6: 0.9792  loss_bbox_6: 0.262  loss_giou_6: 0.7306  loss_ce_dn_6: 0.1153  loss_mask_dn_6: 0.2305  loss_dice_dn_6: 0.92  loss_bbox_dn_6: 0.2351  loss_giou_dn_6: 0.5352  loss_ce_7: 0.7342  loss_mask_7: 0.2232  loss_dice_7: 0.9794  loss_bbox_7: 0.255  loss_giou_7: 0.7128  loss_ce_dn_7: 0.1156  loss_mask_dn_7: 0.2239  loss_dice_dn_7: 0.9306  loss_bbox_dn_7: 0.2313  loss_giou_dn_7: 0.5344  loss_ce_8: 0.7287  loss_mask_8: 0.214  loss_dice_8: 0.9312  loss_bbox_8: 0.2557  loss_giou_8: 0.738  loss_ce_dn_8: 0.1109  loss_mask_dn_8: 0.2191  loss_dice_dn_8: 0.9359  loss_bbox_dn_8: 0.2289  loss_giou_dn_8: 0.5328  loss_ce_interm: 1.395  loss_mask_interm: 0.1959  loss_dice_interm: 0.9402  loss_bbox_interm: 0.3861  loss_giou_interm: 0.9426    time: 0.8736  last_time: 0.8695  data_time: 0.0130  last_data_time: 0.0173   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:04:52 d2.utils.events]: \u001b[0m eta: 3 days, 16:40:45  iter: 2439  total_loss: 65.3  loss_ce: 0.8765  loss_mask: 0.2925  loss_dice: 0.6074  loss_bbox: 0.2339  loss_giou: 0.9435  loss_ce_dn: 0.147  loss_mask_dn: 0.2832  loss_dice_dn: 0.5637  loss_bbox_dn: 0.208  loss_giou_dn: 0.7314  loss_ce_0: 1.54  loss_mask_0: 0.2949  loss_dice_0: 0.6829  loss_bbox_0: 0.4254  loss_giou_0: 1.039  loss_ce_dn_0: 0.7413  loss_mask_dn_0: 0.7958  loss_dice_dn_0: 2.996  loss_bbox_dn_0: 0.6457  loss_giou_dn_0: 1.105  loss_ce_1: 1.473  loss_mask_1: 0.2835  loss_dice_1: 0.5824  loss_bbox_1: 0.2841  loss_giou_1: 1.009  loss_ce_dn_1: 0.2796  loss_mask_dn_1: 0.3615  loss_dice_dn_1: 0.6332  loss_bbox_dn_1: 0.3437  loss_giou_dn_1: 0.8564  loss_ce_2: 1.251  loss_mask_2: 0.2926  loss_dice_2: 0.605  loss_bbox_2: 0.293  loss_giou_2: 0.9454  loss_ce_dn_2: 0.2284  loss_mask_dn_2: 0.3175  loss_dice_dn_2: 0.6085  loss_bbox_dn_2: 0.3033  loss_giou_dn_2: 0.8012  loss_ce_3: 1.133  loss_mask_3: 0.2897  loss_dice_3: 0.5973  loss_bbox_3: 0.2568  loss_giou_3: 0.9673  loss_ce_dn_3: 0.2013  loss_mask_dn_3: 0.3023  loss_dice_dn_3: 0.5739  loss_bbox_dn_3: 0.2432  loss_giou_dn_3: 0.7704  loss_ce_4: 1.031  loss_mask_4: 0.2837  loss_dice_4: 0.5881  loss_bbox_4: 0.2476  loss_giou_4: 0.9454  loss_ce_dn_4: 0.1776  loss_mask_dn_4: 0.2943  loss_dice_dn_4: 0.5759  loss_bbox_dn_4: 0.23  loss_giou_dn_4: 0.7467  loss_ce_5: 0.9264  loss_mask_5: 0.2918  loss_dice_5: 0.622  loss_bbox_5: 0.2471  loss_giou_5: 0.959  loss_ce_dn_5: 0.169  loss_mask_dn_5: 0.286  loss_dice_dn_5: 0.5602  loss_bbox_dn_5: 0.2159  loss_giou_dn_5: 0.7464  loss_ce_6: 0.8867  loss_mask_6: 0.2754  loss_dice_6: 0.6167  loss_bbox_6: 0.2408  loss_giou_6: 0.9446  loss_ce_dn_6: 0.157  loss_mask_dn_6: 0.2835  loss_dice_dn_6: 0.5682  loss_bbox_dn_6: 0.2147  loss_giou_dn_6: 0.7359  loss_ce_7: 0.8604  loss_mask_7: 0.2776  loss_dice_7: 0.5851  loss_bbox_7: 0.2398  loss_giou_7: 0.9594  loss_ce_dn_7: 0.1537  loss_mask_dn_7: 0.2818  loss_dice_dn_7: 0.5564  loss_bbox_dn_7: 0.2102  loss_giou_dn_7: 0.7352  loss_ce_8: 0.828  loss_mask_8: 0.2806  loss_dice_8: 0.6313  loss_bbox_8: 0.2527  loss_giou_8: 0.9614  loss_ce_dn_8: 0.1514  loss_mask_dn_8: 0.2845  loss_dice_dn_8: 0.5636  loss_bbox_dn_8: 0.2091  loss_giou_dn_8: 0.7325  loss_ce_interm: 1.536  loss_mask_interm: 0.3005  loss_dice_interm: 0.674  loss_bbox_interm: 0.4269  loss_giou_interm: 1.036    time: 0.8736  last_time: 0.8690  data_time: 0.0120  last_data_time: 0.0157   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:05:10 d2.utils.events]: \u001b[0m eta: 3 days, 16:41:28  iter: 2459  total_loss: 64.11  loss_ce: 0.8115  loss_mask: 0.2935  loss_dice: 0.8771  loss_bbox: 0.2709  loss_giou: 0.7191  loss_ce_dn: 0.1379  loss_mask_dn: 0.279  loss_dice_dn: 0.79  loss_bbox_dn: 0.2245  loss_giou_dn: 0.5279  loss_ce_0: 1.621  loss_mask_0: 0.2988  loss_dice_0: 0.8802  loss_bbox_0: 0.3976  loss_giou_0: 0.8841  loss_ce_dn_0: 0.7538  loss_mask_dn_0: 0.9196  loss_dice_dn_0: 3.169  loss_bbox_dn_0: 0.6108  loss_giou_dn_0: 0.8757  loss_ce_1: 1.425  loss_mask_1: 0.3152  loss_dice_1: 0.8233  loss_bbox_1: 0.3046  loss_giou_1: 0.8258  loss_ce_dn_1: 0.2486  loss_mask_dn_1: 0.3499  loss_dice_dn_1: 0.9328  loss_bbox_dn_1: 0.3433  loss_giou_dn_1: 0.6397  loss_ce_2: 1.264  loss_mask_2: 0.3166  loss_dice_2: 0.9103  loss_bbox_2: 0.3035  loss_giou_2: 0.7554  loss_ce_dn_2: 0.202  loss_mask_dn_2: 0.3203  loss_dice_dn_2: 0.8099  loss_bbox_dn_2: 0.2773  loss_giou_dn_2: 0.5754  loss_ce_3: 1.046  loss_mask_3: 0.2856  loss_dice_3: 0.9079  loss_bbox_3: 0.2863  loss_giou_3: 0.7226  loss_ce_dn_3: 0.1697  loss_mask_dn_3: 0.2966  loss_dice_dn_3: 0.7836  loss_bbox_dn_3: 0.2545  loss_giou_dn_3: 0.5647  loss_ce_4: 0.9521  loss_mask_4: 0.3012  loss_dice_4: 0.8373  loss_bbox_4: 0.2793  loss_giou_4: 0.701  loss_ce_dn_4: 0.1593  loss_mask_dn_4: 0.2914  loss_dice_dn_4: 0.7616  loss_bbox_dn_4: 0.2424  loss_giou_dn_4: 0.543  loss_ce_5: 0.8694  loss_mask_5: 0.2957  loss_dice_5: 0.8444  loss_bbox_5: 0.2682  loss_giou_5: 0.7016  loss_ce_dn_5: 0.1564  loss_mask_dn_5: 0.2896  loss_dice_dn_5: 0.764  loss_bbox_dn_5: 0.2338  loss_giou_dn_5: 0.5384  loss_ce_6: 0.8589  loss_mask_6: 0.288  loss_dice_6: 0.8719  loss_bbox_6: 0.2672  loss_giou_6: 0.6939  loss_ce_dn_6: 0.1427  loss_mask_dn_6: 0.2832  loss_dice_dn_6: 0.7712  loss_bbox_dn_6: 0.2266  loss_giou_dn_6: 0.5326  loss_ce_7: 0.7999  loss_mask_7: 0.2899  loss_dice_7: 0.859  loss_bbox_7: 0.2912  loss_giou_7: 0.6878  loss_ce_dn_7: 0.142  loss_mask_dn_7: 0.2809  loss_dice_dn_7: 0.7948  loss_bbox_dn_7: 0.2237  loss_giou_dn_7: 0.5303  loss_ce_8: 0.7787  loss_mask_8: 0.2904  loss_dice_8: 0.8075  loss_bbox_8: 0.2924  loss_giou_8: 0.6938  loss_ce_dn_8: 0.1398  loss_mask_dn_8: 0.2838  loss_dice_dn_8: 0.7716  loss_bbox_dn_8: 0.2249  loss_giou_dn_8: 0.5272  loss_ce_interm: 1.592  loss_mask_interm: 0.2969  loss_dice_interm: 0.7995  loss_bbox_interm: 0.3959  loss_giou_interm: 0.8847    time: 0.8736  last_time: 0.8955  data_time: 0.0132  last_data_time: 0.0155   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:05:27 d2.utils.events]: \u001b[0m eta: 3 days, 16:40:24  iter: 2479  total_loss: 64.57  loss_ce: 0.826  loss_mask: 0.2095  loss_dice: 0.8348  loss_bbox: 0.2811  loss_giou: 0.7709  loss_ce_dn: 0.1319  loss_mask_dn: 0.2332  loss_dice_dn: 0.8695  loss_bbox_dn: 0.2425  loss_giou_dn: 0.6853  loss_ce_0: 1.357  loss_mask_0: 0.2201  loss_dice_0: 0.8789  loss_bbox_0: 0.3734  loss_giou_0: 0.9768  loss_ce_dn_0: 0.7848  loss_mask_dn_0: 0.8339  loss_dice_dn_0: 3.253  loss_bbox_dn_0: 0.5743  loss_giou_dn_0: 1.039  loss_ce_1: 1.3  loss_mask_1: 0.22  loss_dice_1: 0.8509  loss_bbox_1: 0.3351  loss_giou_1: 0.8357  loss_ce_dn_1: 0.2695  loss_mask_dn_1: 0.2788  loss_dice_dn_1: 0.9978  loss_bbox_dn_1: 0.3332  loss_giou_dn_1: 0.7951  loss_ce_2: 1.109  loss_mask_2: 0.2251  loss_dice_2: 0.8677  loss_bbox_2: 0.3104  loss_giou_2: 0.7957  loss_ce_dn_2: 0.213  loss_mask_dn_2: 0.2569  loss_dice_dn_2: 0.8999  loss_bbox_dn_2: 0.2839  loss_giou_dn_2: 0.7385  loss_ce_3: 0.9447  loss_mask_3: 0.2271  loss_dice_3: 0.8707  loss_bbox_3: 0.3033  loss_giou_3: 0.7948  loss_ce_dn_3: 0.1789  loss_mask_dn_3: 0.2404  loss_dice_dn_3: 0.863  loss_bbox_dn_3: 0.2573  loss_giou_dn_3: 0.7198  loss_ce_4: 0.8577  loss_mask_4: 0.2243  loss_dice_4: 0.8705  loss_bbox_4: 0.2773  loss_giou_4: 0.7853  loss_ce_dn_4: 0.1656  loss_mask_dn_4: 0.2325  loss_dice_dn_4: 0.905  loss_bbox_dn_4: 0.245  loss_giou_dn_4: 0.7001  loss_ce_5: 0.803  loss_mask_5: 0.2134  loss_dice_5: 0.8584  loss_bbox_5: 0.2837  loss_giou_5: 0.787  loss_ce_dn_5: 0.1542  loss_mask_dn_5: 0.2258  loss_dice_dn_5: 0.876  loss_bbox_dn_5: 0.2391  loss_giou_dn_5: 0.6969  loss_ce_6: 0.8093  loss_mask_6: 0.2234  loss_dice_6: 0.8564  loss_bbox_6: 0.2876  loss_giou_6: 0.7776  loss_ce_dn_6: 0.1473  loss_mask_dn_6: 0.2287  loss_dice_dn_6: 0.8634  loss_bbox_dn_6: 0.2391  loss_giou_dn_6: 0.693  loss_ce_7: 0.8434  loss_mask_7: 0.2136  loss_dice_7: 0.8458  loss_bbox_7: 0.287  loss_giou_7: 0.7703  loss_ce_dn_7: 0.1355  loss_mask_dn_7: 0.2316  loss_dice_dn_7: 0.8768  loss_bbox_dn_7: 0.2386  loss_giou_dn_7: 0.6926  loss_ce_8: 0.8419  loss_mask_8: 0.2142  loss_dice_8: 0.8661  loss_bbox_8: 0.2817  loss_giou_8: 0.7735  loss_ce_dn_8: 0.1315  loss_mask_dn_8: 0.2304  loss_dice_dn_8: 0.8857  loss_bbox_dn_8: 0.2423  loss_giou_dn_8: 0.6884  loss_ce_interm: 1.368  loss_mask_interm: 0.2212  loss_dice_interm: 0.9175  loss_bbox_interm: 0.3732  loss_giou_interm: 0.9747    time: 0.8736  last_time: 0.8747  data_time: 0.0140  last_data_time: 0.0172   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:05:45 d2.utils.events]: \u001b[0m eta: 3 days, 16:39:52  iter: 2499  total_loss: 61.32  loss_ce: 0.8646  loss_mask: 0.2437  loss_dice: 0.8203  loss_bbox: 0.2169  loss_giou: 0.8046  loss_ce_dn: 0.1327  loss_mask_dn: 0.2506  loss_dice_dn: 0.8499  loss_bbox_dn: 0.1768  loss_giou_dn: 0.5642  loss_ce_0: 1.477  loss_mask_0: 0.2479  loss_dice_0: 0.8461  loss_bbox_0: 0.4125  loss_giou_0: 1.014  loss_ce_dn_0: 0.7574  loss_mask_dn_0: 0.6727  loss_dice_dn_0: 3.132  loss_bbox_dn_0: 0.54  loss_giou_dn_0: 0.9193  loss_ce_1: 1.389  loss_mask_1: 0.243  loss_dice_1: 0.8574  loss_bbox_1: 0.2737  loss_giou_1: 0.87  loss_ce_dn_1: 0.2868  loss_mask_dn_1: 0.258  loss_dice_dn_1: 0.968  loss_bbox_dn_1: 0.3086  loss_giou_dn_1: 0.6866  loss_ce_2: 1.24  loss_mask_2: 0.2413  loss_dice_2: 0.8  loss_bbox_2: 0.2466  loss_giou_2: 0.8367  loss_ce_dn_2: 0.2428  loss_mask_dn_2: 0.2537  loss_dice_dn_2: 0.9342  loss_bbox_dn_2: 0.2272  loss_giou_dn_2: 0.6194  loss_ce_3: 1.039  loss_mask_3: 0.242  loss_dice_3: 0.8535  loss_bbox_3: 0.2447  loss_giou_3: 0.8213  loss_ce_dn_3: 0.2012  loss_mask_dn_3: 0.2433  loss_dice_dn_3: 0.8465  loss_bbox_dn_3: 0.2153  loss_giou_dn_3: 0.594  loss_ce_4: 0.9615  loss_mask_4: 0.238  loss_dice_4: 0.8398  loss_bbox_4: 0.2137  loss_giou_4: 0.8017  loss_ce_dn_4: 0.1861  loss_mask_dn_4: 0.2458  loss_dice_dn_4: 0.8573  loss_bbox_dn_4: 0.1887  loss_giou_dn_4: 0.5788  loss_ce_5: 0.9759  loss_mask_5: 0.2407  loss_dice_5: 0.824  loss_bbox_5: 0.2195  loss_giou_5: 0.8081  loss_ce_dn_5: 0.1647  loss_mask_dn_5: 0.2535  loss_dice_dn_5: 0.843  loss_bbox_dn_5: 0.1885  loss_giou_dn_5: 0.5757  loss_ce_6: 0.9508  loss_mask_6: 0.2374  loss_dice_6: 0.8119  loss_bbox_6: 0.2265  loss_giou_6: 0.8105  loss_ce_dn_6: 0.1538  loss_mask_dn_6: 0.2546  loss_dice_dn_6: 0.8595  loss_bbox_dn_6: 0.1847  loss_giou_dn_6: 0.5664  loss_ce_7: 0.9125  loss_mask_7: 0.236  loss_dice_7: 0.8554  loss_bbox_7: 0.2148  loss_giou_7: 0.8109  loss_ce_dn_7: 0.1495  loss_mask_dn_7: 0.2527  loss_dice_dn_7: 0.866  loss_bbox_dn_7: 0.1815  loss_giou_dn_7: 0.5666  loss_ce_8: 0.8679  loss_mask_8: 0.2448  loss_dice_8: 0.8329  loss_bbox_8: 0.2197  loss_giou_8: 0.8048  loss_ce_dn_8: 0.1435  loss_mask_dn_8: 0.2506  loss_dice_dn_8: 0.8766  loss_bbox_dn_8: 0.1785  loss_giou_dn_8: 0.5638  loss_ce_interm: 1.479  loss_mask_interm: 0.2531  loss_dice_interm: 0.8718  loss_bbox_interm: 0.4084  loss_giou_interm: 1.012    time: 0.8736  last_time: 0.8791  data_time: 0.0123  last_data_time: 0.0127   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:06:02 d2.utils.events]: \u001b[0m eta: 3 days, 16:40:00  iter: 2519  total_loss: 71.03  loss_ce: 0.8333  loss_mask: 0.3405  loss_dice: 0.8034  loss_bbox: 0.303  loss_giou: 0.7561  loss_ce_dn: 0.1466  loss_mask_dn: 0.3616  loss_dice_dn: 0.847  loss_bbox_dn: 0.317  loss_giou_dn: 0.6232  loss_ce_0: 1.494  loss_mask_0: 0.3709  loss_dice_0: 0.9158  loss_bbox_0: 0.5157  loss_giou_0: 0.8991  loss_ce_dn_0: 0.7256  loss_mask_dn_0: 1.028  loss_dice_dn_0: 2.968  loss_bbox_dn_0: 0.7493  loss_giou_dn_0: 0.9899  loss_ce_1: 1.402  loss_mask_1: 0.3574  loss_dice_1: 0.9477  loss_bbox_1: 0.3523  loss_giou_1: 0.7842  loss_ce_dn_1: 0.2728  loss_mask_dn_1: 0.4306  loss_dice_dn_1: 1.014  loss_bbox_dn_1: 0.4846  loss_giou_dn_1: 0.7533  loss_ce_2: 1.258  loss_mask_2: 0.3479  loss_dice_2: 0.8473  loss_bbox_2: 0.3634  loss_giou_2: 0.808  loss_ce_dn_2: 0.219  loss_mask_dn_2: 0.3969  loss_dice_dn_2: 0.9106  loss_bbox_dn_2: 0.3948  loss_giou_dn_2: 0.6887  loss_ce_3: 1.095  loss_mask_3: 0.3484  loss_dice_3: 0.8509  loss_bbox_3: 0.3653  loss_giou_3: 0.7535  loss_ce_dn_3: 0.1887  loss_mask_dn_3: 0.3785  loss_dice_dn_3: 0.8722  loss_bbox_dn_3: 0.3725  loss_giou_dn_3: 0.6581  loss_ce_4: 1.009  loss_mask_4: 0.3459  loss_dice_4: 0.8175  loss_bbox_4: 0.3511  loss_giou_4: 0.7596  loss_ce_dn_4: 0.1732  loss_mask_dn_4: 0.364  loss_dice_dn_4: 0.8647  loss_bbox_dn_4: 0.3488  loss_giou_dn_4: 0.6431  loss_ce_5: 0.8991  loss_mask_5: 0.3627  loss_dice_5: 0.8423  loss_bbox_5: 0.3405  loss_giou_5: 0.7297  loss_ce_dn_5: 0.1599  loss_mask_dn_5: 0.366  loss_dice_dn_5: 0.8574  loss_bbox_dn_5: 0.3315  loss_giou_dn_5: 0.6372  loss_ce_6: 0.8827  loss_mask_6: 0.3617  loss_dice_6: 0.8395  loss_bbox_6: 0.3296  loss_giou_6: 0.7164  loss_ce_dn_6: 0.1542  loss_mask_dn_6: 0.3709  loss_dice_dn_6: 0.8531  loss_bbox_dn_6: 0.3246  loss_giou_dn_6: 0.6319  loss_ce_7: 0.8771  loss_mask_7: 0.3425  loss_dice_7: 0.8511  loss_bbox_7: 0.3094  loss_giou_7: 0.7255  loss_ce_dn_7: 0.1454  loss_mask_dn_7: 0.3682  loss_dice_dn_7: 0.8443  loss_bbox_dn_7: 0.3228  loss_giou_dn_7: 0.6302  loss_ce_8: 0.839  loss_mask_8: 0.3506  loss_dice_8: 0.8362  loss_bbox_8: 0.3007  loss_giou_8: 0.7495  loss_ce_dn_8: 0.1477  loss_mask_dn_8: 0.3603  loss_dice_dn_8: 0.8518  loss_bbox_dn_8: 0.3179  loss_giou_dn_8: 0.6228  loss_ce_interm: 1.492  loss_mask_interm: 0.3721  loss_dice_interm: 0.9928  loss_bbox_interm: 0.5157  loss_giou_interm: 0.9045    time: 0.8737  last_time: 0.8826  data_time: 0.0117  last_data_time: 0.0109   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:06:20 d2.utils.events]: \u001b[0m eta: 3 days, 16:39:55  iter: 2539  total_loss: 70.62  loss_ce: 0.9548  loss_mask: 0.2303  loss_dice: 1.009  loss_bbox: 0.3131  loss_giou: 0.8723  loss_ce_dn: 0.1585  loss_mask_dn: 0.2048  loss_dice_dn: 1.01  loss_bbox_dn: 0.2012  loss_giou_dn: 0.5878  loss_ce_0: 1.458  loss_mask_0: 0.2419  loss_dice_0: 1.098  loss_bbox_0: 0.3869  loss_giou_0: 1.023  loss_ce_dn_0: 0.7316  loss_mask_dn_0: 0.8764  loss_dice_dn_0: 3.062  loss_bbox_dn_0: 0.4808  loss_giou_dn_0: 0.9148  loss_ce_1: 1.443  loss_mask_1: 0.2529  loss_dice_1: 0.9948  loss_bbox_1: 0.3018  loss_giou_1: 0.9412  loss_ce_dn_1: 0.2733  loss_mask_dn_1: 0.2467  loss_dice_dn_1: 1.18  loss_bbox_dn_1: 0.2849  loss_giou_dn_1: 0.7034  loss_ce_2: 1.33  loss_mask_2: 0.2502  loss_dice_2: 1.054  loss_bbox_2: 0.2765  loss_giou_2: 0.8979  loss_ce_dn_2: 0.2291  loss_mask_dn_2: 0.2225  loss_dice_dn_2: 1.077  loss_bbox_dn_2: 0.2456  loss_giou_dn_2: 0.6499  loss_ce_3: 1.133  loss_mask_3: 0.2421  loss_dice_3: 1.018  loss_bbox_3: 0.3123  loss_giou_3: 0.8939  loss_ce_dn_3: 0.1975  loss_mask_dn_3: 0.2124  loss_dice_dn_3: 0.9603  loss_bbox_dn_3: 0.2272  loss_giou_dn_3: 0.6223  loss_ce_4: 1.077  loss_mask_4: 0.2405  loss_dice_4: 1.007  loss_bbox_4: 0.3005  loss_giou_4: 0.8839  loss_ce_dn_4: 0.1759  loss_mask_dn_4: 0.1985  loss_dice_dn_4: 0.9286  loss_bbox_dn_4: 0.2147  loss_giou_dn_4: 0.5991  loss_ce_5: 1.034  loss_mask_5: 0.2337  loss_dice_5: 0.9958  loss_bbox_5: 0.3116  loss_giou_5: 0.9  loss_ce_dn_5: 0.1684  loss_mask_dn_5: 0.2007  loss_dice_dn_5: 1.032  loss_bbox_dn_5: 0.2102  loss_giou_dn_5: 0.5964  loss_ce_6: 0.9815  loss_mask_6: 0.2304  loss_dice_6: 1.02  loss_bbox_6: 0.3204  loss_giou_6: 0.8646  loss_ce_dn_6: 0.1703  loss_mask_dn_6: 0.1994  loss_dice_dn_6: 0.9727  loss_bbox_dn_6: 0.2054  loss_giou_dn_6: 0.5874  loss_ce_7: 0.9881  loss_mask_7: 0.2334  loss_dice_7: 0.9824  loss_bbox_7: 0.3102  loss_giou_7: 0.8509  loss_ce_dn_7: 0.1574  loss_mask_dn_7: 0.204  loss_dice_dn_7: 0.9793  loss_bbox_dn_7: 0.2036  loss_giou_dn_7: 0.588  loss_ce_8: 0.9997  loss_mask_8: 0.2284  loss_dice_8: 0.9549  loss_bbox_8: 0.306  loss_giou_8: 0.8517  loss_ce_dn_8: 0.1591  loss_mask_dn_8: 0.2051  loss_dice_dn_8: 0.9836  loss_bbox_dn_8: 0.2012  loss_giou_dn_8: 0.5863  loss_ce_interm: 1.509  loss_mask_interm: 0.2569  loss_dice_interm: 1.036  loss_bbox_interm: 0.391  loss_giou_interm: 1.005    time: 0.8736  last_time: 0.8745  data_time: 0.0124  last_data_time: 0.0095   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:06:37 d2.utils.events]: \u001b[0m eta: 3 days, 16:38:38  iter: 2559  total_loss: 60.57  loss_ce: 0.7546  loss_mask: 0.2556  loss_dice: 0.6989  loss_bbox: 0.2412  loss_giou: 0.6523  loss_ce_dn: 0.1311  loss_mask_dn: 0.2671  loss_dice_dn: 0.6817  loss_bbox_dn: 0.2621  loss_giou_dn: 0.5586  loss_ce_0: 1.424  loss_mask_0: 0.2897  loss_dice_0: 0.7974  loss_bbox_0: 0.3795  loss_giou_0: 0.8722  loss_ce_dn_0: 0.7584  loss_mask_dn_0: 0.8916  loss_dice_dn_0: 2.795  loss_bbox_dn_0: 0.668  loss_giou_dn_0: 0.9735  loss_ce_1: 1.373  loss_mask_1: 0.2742  loss_dice_1: 0.7376  loss_bbox_1: 0.2928  loss_giou_1: 0.7233  loss_ce_dn_1: 0.2462  loss_mask_dn_1: 0.3731  loss_dice_dn_1: 0.7799  loss_bbox_dn_1: 0.3775  loss_giou_dn_1: 0.6966  loss_ce_2: 1.168  loss_mask_2: 0.2663  loss_dice_2: 0.7633  loss_bbox_2: 0.2816  loss_giou_2: 0.6826  loss_ce_dn_2: 0.2079  loss_mask_dn_2: 0.3116  loss_dice_dn_2: 0.7298  loss_bbox_dn_2: 0.3209  loss_giou_dn_2: 0.6158  loss_ce_3: 1.011  loss_mask_3: 0.2593  loss_dice_3: 0.7232  loss_bbox_3: 0.2752  loss_giou_3: 0.6685  loss_ce_dn_3: 0.1748  loss_mask_dn_3: 0.3022  loss_dice_dn_3: 0.7306  loss_bbox_dn_3: 0.3048  loss_giou_dn_3: 0.6072  loss_ce_4: 0.9236  loss_mask_4: 0.2664  loss_dice_4: 0.7104  loss_bbox_4: 0.2671  loss_giou_4: 0.6633  loss_ce_dn_4: 0.1615  loss_mask_dn_4: 0.2944  loss_dice_dn_4: 0.7112  loss_bbox_dn_4: 0.2898  loss_giou_dn_4: 0.587  loss_ce_5: 0.857  loss_mask_5: 0.2692  loss_dice_5: 0.6595  loss_bbox_5: 0.2597  loss_giou_5: 0.6794  loss_ce_dn_5: 0.1473  loss_mask_dn_5: 0.2905  loss_dice_dn_5: 0.7083  loss_bbox_dn_5: 0.2778  loss_giou_dn_5: 0.58  loss_ce_6: 0.8379  loss_mask_6: 0.2585  loss_dice_6: 0.7397  loss_bbox_6: 0.2492  loss_giou_6: 0.6798  loss_ce_dn_6: 0.1425  loss_mask_dn_6: 0.2876  loss_dice_dn_6: 0.7061  loss_bbox_dn_6: 0.2679  loss_giou_dn_6: 0.57  loss_ce_7: 0.8004  loss_mask_7: 0.2594  loss_dice_7: 0.7225  loss_bbox_7: 0.2473  loss_giou_7: 0.6587  loss_ce_dn_7: 0.1372  loss_mask_dn_7: 0.2836  loss_dice_dn_7: 0.7057  loss_bbox_dn_7: 0.2662  loss_giou_dn_7: 0.5645  loss_ce_8: 0.725  loss_mask_8: 0.2564  loss_dice_8: 0.7283  loss_bbox_8: 0.2437  loss_giou_8: 0.6495  loss_ce_dn_8: 0.134  loss_mask_dn_8: 0.2747  loss_dice_dn_8: 0.6798  loss_bbox_dn_8: 0.2623  loss_giou_dn_8: 0.5589  loss_ce_interm: 1.369  loss_mask_interm: 0.2891  loss_dice_interm: 0.8006  loss_bbox_interm: 0.3886  loss_giou_interm: 0.8492    time: 0.8736  last_time: 0.8666  data_time: 0.0104  last_data_time: 0.0153   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:06:55 d2.utils.events]: \u001b[0m eta: 3 days, 16:38:07  iter: 2579  total_loss: 60.92  loss_ce: 0.6559  loss_mask: 0.2782  loss_dice: 0.8273  loss_bbox: 0.3374  loss_giou: 0.6283  loss_ce_dn: 0.08287  loss_mask_dn: 0.2852  loss_dice_dn: 0.8191  loss_bbox_dn: 0.2579  loss_giou_dn: 0.5051  loss_ce_0: 1.353  loss_mask_0: 0.2791  loss_dice_0: 0.8758  loss_bbox_0: 0.4305  loss_giou_0: 0.812  loss_ce_dn_0: 0.6755  loss_mask_dn_0: 0.9059  loss_dice_dn_0: 2.931  loss_bbox_dn_0: 0.642  loss_giou_dn_0: 0.8987  loss_ce_1: 1.305  loss_mask_1: 0.2861  loss_dice_1: 0.8371  loss_bbox_1: 0.325  loss_giou_1: 0.7005  loss_ce_dn_1: 0.2259  loss_mask_dn_1: 0.3485  loss_dice_dn_1: 0.909  loss_bbox_dn_1: 0.3657  loss_giou_dn_1: 0.6595  loss_ce_2: 1.086  loss_mask_2: 0.2711  loss_dice_2: 0.8297  loss_bbox_2: 0.3025  loss_giou_2: 0.6411  loss_ce_dn_2: 0.1727  loss_mask_dn_2: 0.3085  loss_dice_dn_2: 0.897  loss_bbox_dn_2: 0.2902  loss_giou_dn_2: 0.5692  loss_ce_3: 0.9533  loss_mask_3: 0.2769  loss_dice_3: 0.8776  loss_bbox_3: 0.3083  loss_giou_3: 0.6442  loss_ce_dn_3: 0.1462  loss_mask_dn_3: 0.308  loss_dice_dn_3: 0.862  loss_bbox_dn_3: 0.2841  loss_giou_dn_3: 0.5414  loss_ce_4: 0.8397  loss_mask_4: 0.2782  loss_dice_4: 0.8015  loss_bbox_4: 0.302  loss_giou_4: 0.6224  loss_ce_dn_4: 0.1225  loss_mask_dn_4: 0.2906  loss_dice_dn_4: 0.8473  loss_bbox_dn_4: 0.2741  loss_giou_dn_4: 0.5223  loss_ce_5: 0.7281  loss_mask_5: 0.2875  loss_dice_5: 0.818  loss_bbox_5: 0.3505  loss_giou_5: 0.6322  loss_ce_dn_5: 0.1089  loss_mask_dn_5: 0.2971  loss_dice_dn_5: 0.8366  loss_bbox_dn_5: 0.2679  loss_giou_dn_5: 0.5211  loss_ce_6: 0.7251  loss_mask_6: 0.2797  loss_dice_6: 0.8274  loss_bbox_6: 0.3299  loss_giou_6: 0.6472  loss_ce_dn_6: 0.1014  loss_mask_dn_6: 0.2893  loss_dice_dn_6: 0.8287  loss_bbox_dn_6: 0.265  loss_giou_dn_6: 0.5165  loss_ce_7: 0.7056  loss_mask_7: 0.2712  loss_dice_7: 0.8319  loss_bbox_7: 0.3412  loss_giou_7: 0.6279  loss_ce_dn_7: 0.08989  loss_mask_dn_7: 0.288  loss_dice_dn_7: 0.8351  loss_bbox_dn_7: 0.2616  loss_giou_dn_7: 0.5118  loss_ce_8: 0.6571  loss_mask_8: 0.2732  loss_dice_8: 0.8144  loss_bbox_8: 0.3355  loss_giou_8: 0.6223  loss_ce_dn_8: 0.08725  loss_mask_dn_8: 0.2902  loss_dice_dn_8: 0.8231  loss_bbox_dn_8: 0.2581  loss_giou_dn_8: 0.5074  loss_ce_interm: 1.353  loss_mask_interm: 0.2828  loss_dice_interm: 0.8998  loss_bbox_interm: 0.4238  loss_giou_interm: 0.7997    time: 0.8736  last_time: 0.8836  data_time: 0.0126  last_data_time: 0.0094   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:07:12 d2.utils.events]: \u001b[0m eta: 3 days, 16:37:36  iter: 2599  total_loss: 55.29  loss_ce: 0.6251  loss_mask: 0.2073  loss_dice: 0.632  loss_bbox: 0.1992  loss_giou: 0.6747  loss_ce_dn: 0.1058  loss_mask_dn: 0.2197  loss_dice_dn: 0.6383  loss_bbox_dn: 0.1961  loss_giou_dn: 0.5307  loss_ce_0: 1.428  loss_mask_0: 0.2868  loss_dice_0: 0.6969  loss_bbox_0: 0.3927  loss_giou_0: 0.8971  loss_ce_dn_0: 0.7607  loss_mask_dn_0: 0.9225  loss_dice_dn_0: 2.746  loss_bbox_dn_0: 0.6852  loss_giou_dn_0: 0.9368  loss_ce_1: 1.32  loss_mask_1: 0.2371  loss_dice_1: 0.7095  loss_bbox_1: 0.3176  loss_giou_1: 0.7518  loss_ce_dn_1: 0.2548  loss_mask_dn_1: 0.3029  loss_dice_dn_1: 0.8104  loss_bbox_dn_1: 0.3736  loss_giou_dn_1: 0.6796  loss_ce_2: 1.131  loss_mask_2: 0.2345  loss_dice_2: 0.6922  loss_bbox_2: 0.2637  loss_giou_2: 0.7589  loss_ce_dn_2: 0.1891  loss_mask_dn_2: 0.2582  loss_dice_dn_2: 0.7311  loss_bbox_dn_2: 0.2626  loss_giou_dn_2: 0.6127  loss_ce_3: 0.9029  loss_mask_3: 0.247  loss_dice_3: 0.7174  loss_bbox_3: 0.2233  loss_giou_3: 0.7185  loss_ce_dn_3: 0.1423  loss_mask_dn_3: 0.2331  loss_dice_dn_3: 0.6827  loss_bbox_dn_3: 0.2203  loss_giou_dn_3: 0.5722  loss_ce_4: 0.8293  loss_mask_4: 0.2303  loss_dice_4: 0.6637  loss_bbox_4: 0.2126  loss_giou_4: 0.6988  loss_ce_dn_4: 0.126  loss_mask_dn_4: 0.2332  loss_dice_dn_4: 0.62  loss_bbox_dn_4: 0.2019  loss_giou_dn_4: 0.5502  loss_ce_5: 0.7036  loss_mask_5: 0.2173  loss_dice_5: 0.6817  loss_bbox_5: 0.2059  loss_giou_5: 0.6974  loss_ce_dn_5: 0.1199  loss_mask_dn_5: 0.2247  loss_dice_dn_5: 0.6247  loss_bbox_dn_5: 0.1987  loss_giou_dn_5: 0.538  loss_ce_6: 0.6676  loss_mask_6: 0.2176  loss_dice_6: 0.6481  loss_bbox_6: 0.2078  loss_giou_6: 0.7098  loss_ce_dn_6: 0.114  loss_mask_dn_6: 0.223  loss_dice_dn_6: 0.6074  loss_bbox_dn_6: 0.1967  loss_giou_dn_6: 0.5365  loss_ce_7: 0.6451  loss_mask_7: 0.2097  loss_dice_7: 0.649  loss_bbox_7: 0.2024  loss_giou_7: 0.6906  loss_ce_dn_7: 0.1104  loss_mask_dn_7: 0.2254  loss_dice_dn_7: 0.6281  loss_bbox_dn_7: 0.1962  loss_giou_dn_7: 0.5362  loss_ce_8: 0.633  loss_mask_8: 0.2109  loss_dice_8: 0.6095  loss_bbox_8: 0.2001  loss_giou_8: 0.688  loss_ce_dn_8: 0.1043  loss_mask_dn_8: 0.2222  loss_dice_dn_8: 0.6323  loss_bbox_dn_8: 0.1954  loss_giou_dn_8: 0.5321  loss_ce_interm: 1.426  loss_mask_interm: 0.2934  loss_dice_interm: 0.6778  loss_bbox_interm: 0.3764  loss_giou_interm: 0.9024    time: 0.8736  last_time: 0.8687  data_time: 0.0110  last_data_time: 0.0094   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:07:30 d2.utils.events]: \u001b[0m eta: 3 days, 16:37:08  iter: 2619  total_loss: 67.99  loss_ce: 0.7404  loss_mask: 0.3437  loss_dice: 0.7803  loss_bbox: 0.2831  loss_giou: 0.6878  loss_ce_dn: 0.1246  loss_mask_dn: 0.3521  loss_dice_dn: 0.7701  loss_bbox_dn: 0.2643  loss_giou_dn: 0.5759  loss_ce_0: 1.455  loss_mask_0: 0.3796  loss_dice_0: 0.8328  loss_bbox_0: 0.4725  loss_giou_0: 0.9164  loss_ce_dn_0: 0.7543  loss_mask_dn_0: 1.022  loss_dice_dn_0: 3.015  loss_bbox_dn_0: 0.6874  loss_giou_dn_0: 0.9401  loss_ce_1: 1.375  loss_mask_1: 0.36  loss_dice_1: 0.7322  loss_bbox_1: 0.3337  loss_giou_1: 0.782  loss_ce_dn_1: 0.2546  loss_mask_dn_1: 0.4653  loss_dice_dn_1: 0.8895  loss_bbox_dn_1: 0.4163  loss_giou_dn_1: 0.6987  loss_ce_2: 1.168  loss_mask_2: 0.3548  loss_dice_2: 0.7507  loss_bbox_2: 0.3139  loss_giou_2: 0.7339  loss_ce_dn_2: 0.2063  loss_mask_dn_2: 0.4179  loss_dice_dn_2: 0.8418  loss_bbox_dn_2: 0.3202  loss_giou_dn_2: 0.6443  loss_ce_3: 0.9609  loss_mask_3: 0.3696  loss_dice_3: 0.8213  loss_bbox_3: 0.2911  loss_giou_3: 0.7297  loss_ce_dn_3: 0.1882  loss_mask_dn_3: 0.3988  loss_dice_dn_3: 0.8233  loss_bbox_dn_3: 0.2929  loss_giou_dn_3: 0.6258  loss_ce_4: 0.9163  loss_mask_4: 0.3787  loss_dice_4: 0.8134  loss_bbox_4: 0.2811  loss_giou_4: 0.712  loss_ce_dn_4: 0.1668  loss_mask_dn_4: 0.3774  loss_dice_dn_4: 0.7952  loss_bbox_dn_4: 0.2818  loss_giou_dn_4: 0.5943  loss_ce_5: 0.8805  loss_mask_5: 0.3418  loss_dice_5: 0.8051  loss_bbox_5: 0.2905  loss_giou_5: 0.7229  loss_ce_dn_5: 0.1532  loss_mask_dn_5: 0.3494  loss_dice_dn_5: 0.7747  loss_bbox_dn_5: 0.2788  loss_giou_dn_5: 0.5869  loss_ce_6: 0.8213  loss_mask_6: 0.3594  loss_dice_6: 0.8015  loss_bbox_6: 0.3011  loss_giou_6: 0.7039  loss_ce_dn_6: 0.144  loss_mask_dn_6: 0.3643  loss_dice_dn_6: 0.7652  loss_bbox_dn_6: 0.2724  loss_giou_dn_6: 0.5847  loss_ce_7: 0.7363  loss_mask_7: 0.3629  loss_dice_7: 0.837  loss_bbox_7: 0.3  loss_giou_7: 0.7095  loss_ce_dn_7: 0.1326  loss_mask_dn_7: 0.3547  loss_dice_dn_7: 0.7691  loss_bbox_dn_7: 0.2689  loss_giou_dn_7: 0.5828  loss_ce_8: 0.7031  loss_mask_8: 0.3498  loss_dice_8: 0.8054  loss_bbox_8: 0.321  loss_giou_8: 0.6847  loss_ce_dn_8: 0.1294  loss_mask_dn_8: 0.348  loss_dice_dn_8: 0.7707  loss_bbox_dn_8: 0.2644  loss_giou_dn_8: 0.5763  loss_ce_interm: 1.455  loss_mask_interm: 0.3887  loss_dice_interm: 0.9006  loss_bbox_interm: 0.4725  loss_giou_interm: 0.9174    time: 0.8736  last_time: 0.9074  data_time: 0.0117  last_data_time: 0.0091   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:07:47 d2.utils.events]: \u001b[0m eta: 3 days, 16:36:51  iter: 2639  total_loss: 69.77  loss_ce: 0.8655  loss_mask: 0.2556  loss_dice: 1.041  loss_bbox: 0.2286  loss_giou: 0.7038  loss_ce_dn: 0.1314  loss_mask_dn: 0.2522  loss_dice_dn: 1.129  loss_bbox_dn: 0.2262  loss_giou_dn: 0.5751  loss_ce_0: 1.524  loss_mask_0: 0.2404  loss_dice_0: 1.157  loss_bbox_0: 0.4401  loss_giou_0: 0.9999  loss_ce_dn_0: 0.7621  loss_mask_dn_0: 0.7439  loss_dice_dn_0: 3.095  loss_bbox_dn_0: 0.5634  loss_giou_dn_0: 0.9032  loss_ce_1: 1.438  loss_mask_1: 0.2336  loss_dice_1: 1.21  loss_bbox_1: 0.3334  loss_giou_1: 0.8693  loss_ce_dn_1: 0.2617  loss_mask_dn_1: 0.3315  loss_dice_dn_1: 1.355  loss_bbox_dn_1: 0.3535  loss_giou_dn_1: 0.6824  loss_ce_2: 1.288  loss_mask_2: 0.2304  loss_dice_2: 1.09  loss_bbox_2: 0.2764  loss_giou_2: 0.775  loss_ce_dn_2: 0.2127  loss_mask_dn_2: 0.2677  loss_dice_dn_2: 1.3  loss_bbox_dn_2: 0.3017  loss_giou_dn_2: 0.6389  loss_ce_3: 1.142  loss_mask_3: 0.2336  loss_dice_3: 1.105  loss_bbox_3: 0.3037  loss_giou_3: 0.733  loss_ce_dn_3: 0.178  loss_mask_dn_3: 0.2689  loss_dice_dn_3: 1.272  loss_bbox_dn_3: 0.277  loss_giou_dn_3: 0.6101  loss_ce_4: 1.032  loss_mask_4: 0.2422  loss_dice_4: 1.094  loss_bbox_4: 0.2448  loss_giou_4: 0.7242  loss_ce_dn_4: 0.164  loss_mask_dn_4: 0.2617  loss_dice_dn_4: 1.17  loss_bbox_dn_4: 0.2573  loss_giou_dn_4: 0.594  loss_ce_5: 0.9623  loss_mask_5: 0.2463  loss_dice_5: 1.17  loss_bbox_5: 0.2779  loss_giou_5: 0.7332  loss_ce_dn_5: 0.1528  loss_mask_dn_5: 0.2513  loss_dice_dn_5: 1.156  loss_bbox_dn_5: 0.2461  loss_giou_dn_5: 0.5901  loss_ce_6: 0.9085  loss_mask_6: 0.2365  loss_dice_6: 1.03  loss_bbox_6: 0.2552  loss_giou_6: 0.7148  loss_ce_dn_6: 0.1408  loss_mask_dn_6: 0.2491  loss_dice_dn_6: 1.176  loss_bbox_dn_6: 0.2375  loss_giou_dn_6: 0.5834  loss_ce_7: 0.9297  loss_mask_7: 0.245  loss_dice_7: 1.16  loss_bbox_7: 0.2456  loss_giou_7: 0.7281  loss_ce_dn_7: 0.136  loss_mask_dn_7: 0.2539  loss_dice_dn_7: 1.128  loss_bbox_dn_7: 0.2313  loss_giou_dn_7: 0.5792  loss_ce_8: 0.8742  loss_mask_8: 0.2507  loss_dice_8: 1.188  loss_bbox_8: 0.2333  loss_giou_8: 0.731  loss_ce_dn_8: 0.1283  loss_mask_dn_8: 0.2507  loss_dice_dn_8: 1.165  loss_bbox_dn_8: 0.2268  loss_giou_dn_8: 0.5759  loss_ce_interm: 1.527  loss_mask_interm: 0.2505  loss_dice_interm: 1.179  loss_bbox_interm: 0.4148  loss_giou_interm: 1.022    time: 0.8736  last_time: 0.8509  data_time: 0.0125  last_data_time: 0.0079   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:08:05 d2.utils.events]: \u001b[0m eta: 3 days, 16:35:42  iter: 2659  total_loss: 67.37  loss_ce: 0.8166  loss_mask: 0.2565  loss_dice: 0.8937  loss_bbox: 0.2904  loss_giou: 0.7208  loss_ce_dn: 0.1044  loss_mask_dn: 0.2641  loss_dice_dn: 0.9201  loss_bbox_dn: 0.2252  loss_giou_dn: 0.5257  loss_ce_0: 1.46  loss_mask_0: 0.2915  loss_dice_0: 0.9797  loss_bbox_0: 0.4196  loss_giou_0: 0.872  loss_ce_dn_0: 0.6576  loss_mask_dn_0: 0.8774  loss_dice_dn_0: 3.159  loss_bbox_dn_0: 0.6255  loss_giou_dn_0: 0.878  loss_ce_1: 1.378  loss_mask_1: 0.2911  loss_dice_1: 0.9451  loss_bbox_1: 0.3289  loss_giou_1: 0.798  loss_ce_dn_1: 0.2308  loss_mask_dn_1: 0.3094  loss_dice_dn_1: 1.08  loss_bbox_dn_1: 0.3477  loss_giou_dn_1: 0.6645  loss_ce_2: 1.263  loss_mask_2: 0.2945  loss_dice_2: 0.9221  loss_bbox_2: 0.3011  loss_giou_2: 0.7476  loss_ce_dn_2: 0.1695  loss_mask_dn_2: 0.3028  loss_dice_dn_2: 0.9977  loss_bbox_dn_2: 0.2827  loss_giou_dn_2: 0.6067  loss_ce_3: 1.079  loss_mask_3: 0.2921  loss_dice_3: 0.8828  loss_bbox_3: 0.306  loss_giou_3: 0.7343  loss_ce_dn_3: 0.1458  loss_mask_dn_3: 0.2708  loss_dice_dn_3: 0.9149  loss_bbox_dn_3: 0.2613  loss_giou_dn_3: 0.5761  loss_ce_4: 0.9849  loss_mask_4: 0.2957  loss_dice_4: 0.8861  loss_bbox_4: 0.3123  loss_giou_4: 0.6992  loss_ce_dn_4: 0.1405  loss_mask_dn_4: 0.268  loss_dice_dn_4: 0.9146  loss_bbox_dn_4: 0.2403  loss_giou_dn_4: 0.5474  loss_ce_5: 0.9344  loss_mask_5: 0.278  loss_dice_5: 0.8919  loss_bbox_5: 0.2937  loss_giou_5: 0.7169  loss_ce_dn_5: 0.1263  loss_mask_dn_5: 0.254  loss_dice_dn_5: 0.8901  loss_bbox_dn_5: 0.2362  loss_giou_dn_5: 0.5436  loss_ce_6: 0.88  loss_mask_6: 0.2774  loss_dice_6: 0.8869  loss_bbox_6: 0.294  loss_giou_6: 0.6777  loss_ce_dn_6: 0.1164  loss_mask_dn_6: 0.2607  loss_dice_dn_6: 0.9194  loss_bbox_dn_6: 0.2328  loss_giou_dn_6: 0.5337  loss_ce_7: 0.8331  loss_mask_7: 0.2652  loss_dice_7: 0.9272  loss_bbox_7: 0.3002  loss_giou_7: 0.6896  loss_ce_dn_7: 0.1094  loss_mask_dn_7: 0.2603  loss_dice_dn_7: 0.8877  loss_bbox_dn_7: 0.225  loss_giou_dn_7: 0.5316  loss_ce_8: 0.8214  loss_mask_8: 0.2616  loss_dice_8: 0.9021  loss_bbox_8: 0.3066  loss_giou_8: 0.6815  loss_ce_dn_8: 0.103  loss_mask_dn_8: 0.2634  loss_dice_dn_8: 0.9123  loss_bbox_dn_8: 0.2254  loss_giou_dn_8: 0.5262  loss_ce_interm: 1.47  loss_mask_interm: 0.2945  loss_dice_interm: 0.976  loss_bbox_interm: 0.4168  loss_giou_interm: 0.8793    time: 0.8735  last_time: 0.8770  data_time: 0.0114  last_data_time: 0.0147   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:08:22 d2.utils.events]: \u001b[0m eta: 3 days, 16:34:49  iter: 2679  total_loss: 61.56  loss_ce: 0.7525  loss_mask: 0.2981  loss_dice: 0.7837  loss_bbox: 0.2465  loss_giou: 0.5802  loss_ce_dn: 0.114  loss_mask_dn: 0.3124  loss_dice_dn: 0.8222  loss_bbox_dn: 0.235  loss_giou_dn: 0.491  loss_ce_0: 1.557  loss_mask_0: 0.3222  loss_dice_0: 0.929  loss_bbox_0: 0.4315  loss_giou_0: 0.7891  loss_ce_dn_0: 0.7708  loss_mask_dn_0: 0.8308  loss_dice_dn_0: 2.873  loss_bbox_dn_0: 0.7244  loss_giou_dn_0: 0.8902  loss_ce_1: 1.363  loss_mask_1: 0.3217  loss_dice_1: 0.8805  loss_bbox_1: 0.3311  loss_giou_1: 0.6695  loss_ce_dn_1: 0.2403  loss_mask_dn_1: 0.387  loss_dice_dn_1: 0.9493  loss_bbox_dn_1: 0.4196  loss_giou_dn_1: 0.6461  loss_ce_2: 1.181  loss_mask_2: 0.3278  loss_dice_2: 0.8834  loss_bbox_2: 0.2809  loss_giou_2: 0.6329  loss_ce_dn_2: 0.184  loss_mask_dn_2: 0.3418  loss_dice_dn_2: 0.8878  loss_bbox_dn_2: 0.3317  loss_giou_dn_2: 0.5691  loss_ce_3: 1.042  loss_mask_3: 0.3048  loss_dice_3: 0.8878  loss_bbox_3: 0.2892  loss_giou_3: 0.6044  loss_ce_dn_3: 0.1536  loss_mask_dn_3: 0.3098  loss_dice_dn_3: 0.8412  loss_bbox_dn_3: 0.2914  loss_giou_dn_3: 0.5441  loss_ce_4: 0.9478  loss_mask_4: 0.2999  loss_dice_4: 0.8129  loss_bbox_4: 0.2666  loss_giou_4: 0.6016  loss_ce_dn_4: 0.1358  loss_mask_dn_4: 0.2992  loss_dice_dn_4: 0.8293  loss_bbox_dn_4: 0.2647  loss_giou_dn_4: 0.5141  loss_ce_5: 0.8668  loss_mask_5: 0.2839  loss_dice_5: 0.8594  loss_bbox_5: 0.2645  loss_giou_5: 0.5932  loss_ce_dn_5: 0.1258  loss_mask_dn_5: 0.3181  loss_dice_dn_5: 0.8195  loss_bbox_dn_5: 0.2558  loss_giou_dn_5: 0.5036  loss_ce_6: 0.8266  loss_mask_6: 0.3046  loss_dice_6: 0.8482  loss_bbox_6: 0.2632  loss_giou_6: 0.6021  loss_ce_dn_6: 0.1206  loss_mask_dn_6: 0.3131  loss_dice_dn_6: 0.8139  loss_bbox_dn_6: 0.2431  loss_giou_dn_6: 0.4995  loss_ce_7: 0.7557  loss_mask_7: 0.2792  loss_dice_7: 0.8574  loss_bbox_7: 0.2552  loss_giou_7: 0.5977  loss_ce_dn_7: 0.1169  loss_mask_dn_7: 0.3052  loss_dice_dn_7: 0.8138  loss_bbox_dn_7: 0.238  loss_giou_dn_7: 0.4934  loss_ce_8: 0.759  loss_mask_8: 0.2924  loss_dice_8: 0.8686  loss_bbox_8: 0.2566  loss_giou_8: 0.5853  loss_ce_dn_8: 0.1116  loss_mask_dn_8: 0.3071  loss_dice_dn_8: 0.806  loss_bbox_dn_8: 0.2345  loss_giou_dn_8: 0.4916  loss_ce_interm: 1.557  loss_mask_interm: 0.3266  loss_dice_interm: 0.9159  loss_bbox_interm: 0.4327  loss_giou_interm: 0.797    time: 0.8736  last_time: 0.8666  data_time: 0.0124  last_data_time: 0.0108   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:08:40 d2.utils.events]: \u001b[0m eta: 3 days, 16:34:25  iter: 2699  total_loss: 58.92  loss_ce: 0.7558  loss_mask: 0.2704  loss_dice: 0.6694  loss_bbox: 0.2729  loss_giou: 0.7725  loss_ce_dn: 0.1375  loss_mask_dn: 0.3373  loss_dice_dn: 0.6392  loss_bbox_dn: 0.2594  loss_giou_dn: 0.5545  loss_ce_0: 1.47  loss_mask_0: 0.275  loss_dice_0: 0.6762  loss_bbox_0: 0.4594  loss_giou_0: 0.97  loss_ce_dn_0: 0.6907  loss_mask_dn_0: 0.9015  loss_dice_dn_0: 3.142  loss_bbox_dn_0: 0.7393  loss_giou_dn_0: 0.9177  loss_ce_1: 1.356  loss_mask_1: 0.3023  loss_dice_1: 0.7045  loss_bbox_1: 0.3662  loss_giou_1: 0.8711  loss_ce_dn_1: 0.2481  loss_mask_dn_1: 0.375  loss_dice_dn_1: 0.7697  loss_bbox_dn_1: 0.4177  loss_giou_dn_1: 0.675  loss_ce_2: 1.21  loss_mask_2: 0.2777  loss_dice_2: 0.6492  loss_bbox_2: 0.2961  loss_giou_2: 0.8248  loss_ce_dn_2: 0.1921  loss_mask_dn_2: 0.3532  loss_dice_dn_2: 0.7074  loss_bbox_dn_2: 0.3304  loss_giou_dn_2: 0.604  loss_ce_3: 1.04  loss_mask_3: 0.2732  loss_dice_3: 0.6916  loss_bbox_3: 0.2786  loss_giou_3: 0.8121  loss_ce_dn_3: 0.1738  loss_mask_dn_3: 0.3516  loss_dice_dn_3: 0.6871  loss_bbox_dn_3: 0.2961  loss_giou_dn_3: 0.584  loss_ce_4: 0.9194  loss_mask_4: 0.2786  loss_dice_4: 0.6792  loss_bbox_4: 0.2527  loss_giou_4: 0.7987  loss_ce_dn_4: 0.1639  loss_mask_dn_4: 0.3334  loss_dice_dn_4: 0.6936  loss_bbox_dn_4: 0.2757  loss_giou_dn_4: 0.5661  loss_ce_5: 0.8516  loss_mask_5: 0.2835  loss_dice_5: 0.6902  loss_bbox_5: 0.2614  loss_giou_5: 0.7961  loss_ce_dn_5: 0.1523  loss_mask_dn_5: 0.3489  loss_dice_dn_5: 0.6723  loss_bbox_dn_5: 0.2748  loss_giou_dn_5: 0.5638  loss_ce_6: 0.8139  loss_mask_6: 0.2664  loss_dice_6: 0.6732  loss_bbox_6: 0.2562  loss_giou_6: 0.7842  loss_ce_dn_6: 0.143  loss_mask_dn_6: 0.3432  loss_dice_dn_6: 0.6741  loss_bbox_dn_6: 0.2642  loss_giou_dn_6: 0.5611  loss_ce_7: 0.7972  loss_mask_7: 0.2669  loss_dice_7: 0.6926  loss_bbox_7: 0.2556  loss_giou_7: 0.7806  loss_ce_dn_7: 0.1422  loss_mask_dn_7: 0.337  loss_dice_dn_7: 0.6803  loss_bbox_dn_7: 0.2634  loss_giou_dn_7: 0.5592  loss_ce_8: 0.7558  loss_mask_8: 0.2634  loss_dice_8: 0.6486  loss_bbox_8: 0.2679  loss_giou_8: 0.7729  loss_ce_dn_8: 0.1392  loss_mask_dn_8: 0.3368  loss_dice_dn_8: 0.6524  loss_bbox_dn_8: 0.2618  loss_giou_dn_8: 0.5542  loss_ce_interm: 1.47  loss_mask_interm: 0.2708  loss_dice_interm: 0.7012  loss_bbox_interm: 0.4571  loss_giou_interm: 0.9638    time: 0.8736  last_time: 0.8951  data_time: 0.0125  last_data_time: 0.0089   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:08:58 d2.utils.events]: \u001b[0m eta: 3 days, 16:35:11  iter: 2719  total_loss: 65.74  loss_ce: 0.8386  loss_mask: 0.2037  loss_dice: 0.9126  loss_bbox: 0.2401  loss_giou: 0.7938  loss_ce_dn: 0.1655  loss_mask_dn: 0.2082  loss_dice_dn: 0.888  loss_bbox_dn: 0.2071  loss_giou_dn: 0.6163  loss_ce_0: 1.44  loss_mask_0: 0.2433  loss_dice_0: 0.8783  loss_bbox_0: 0.4771  loss_giou_0: 0.9695  loss_ce_dn_0: 0.6884  loss_mask_dn_0: 0.8616  loss_dice_dn_0: 3.036  loss_bbox_dn_0: 0.6057  loss_giou_dn_0: 0.9859  loss_ce_1: 1.437  loss_mask_1: 0.2373  loss_dice_1: 0.8952  loss_bbox_1: 0.2925  loss_giou_1: 0.8397  loss_ce_dn_1: 0.2849  loss_mask_dn_1: 0.2451  loss_dice_dn_1: 0.9998  loss_bbox_dn_1: 0.3309  loss_giou_dn_1: 0.7466  loss_ce_2: 1.246  loss_mask_2: 0.2278  loss_dice_2: 0.8933  loss_bbox_2: 0.2333  loss_giou_2: 0.7483  loss_ce_dn_2: 0.2293  loss_mask_dn_2: 0.2141  loss_dice_dn_2: 0.9363  loss_bbox_dn_2: 0.2737  loss_giou_dn_2: 0.6881  loss_ce_3: 1.075  loss_mask_3: 0.2296  loss_dice_3: 0.8883  loss_bbox_3: 0.2199  loss_giou_3: 0.8248  loss_ce_dn_3: 0.2111  loss_mask_dn_3: 0.2168  loss_dice_dn_3: 0.9184  loss_bbox_dn_3: 0.2339  loss_giou_dn_3: 0.659  loss_ce_4: 1.012  loss_mask_4: 0.2132  loss_dice_4: 0.8364  loss_bbox_4: 0.2254  loss_giou_4: 0.801  loss_ce_dn_4: 0.1952  loss_mask_dn_4: 0.2182  loss_dice_dn_4: 0.9084  loss_bbox_dn_4: 0.2189  loss_giou_dn_4: 0.6416  loss_ce_5: 0.9583  loss_mask_5: 0.1948  loss_dice_5: 0.8909  loss_bbox_5: 0.239  loss_giou_5: 0.8236  loss_ce_dn_5: 0.1869  loss_mask_dn_5: 0.2144  loss_dice_dn_5: 0.9001  loss_bbox_dn_5: 0.2165  loss_giou_dn_5: 0.6249  loss_ce_6: 0.8805  loss_mask_6: 0.1965  loss_dice_6: 0.8771  loss_bbox_6: 0.2228  loss_giou_6: 0.8095  loss_ce_dn_6: 0.1748  loss_mask_dn_6: 0.2086  loss_dice_dn_6: 0.9212  loss_bbox_dn_6: 0.2088  loss_giou_dn_6: 0.6223  loss_ce_7: 0.8677  loss_mask_7: 0.205  loss_dice_7: 0.8972  loss_bbox_7: 0.2561  loss_giou_7: 0.7931  loss_ce_dn_7: 0.1706  loss_mask_dn_7: 0.2068  loss_dice_dn_7: 0.9148  loss_bbox_dn_7: 0.2095  loss_giou_dn_7: 0.6192  loss_ce_8: 0.8838  loss_mask_8: 0.1995  loss_dice_8: 0.8538  loss_bbox_8: 0.2563  loss_giou_8: 0.7939  loss_ce_dn_8: 0.1675  loss_mask_dn_8: 0.2055  loss_dice_dn_8: 0.8946  loss_bbox_dn_8: 0.2076  loss_giou_dn_8: 0.6162  loss_ce_interm: 1.48  loss_mask_interm: 0.2398  loss_dice_interm: 0.95  loss_bbox_interm: 0.4504  loss_giou_interm: 0.9606    time: 0.8736  last_time: 0.8806  data_time: 0.0114  last_data_time: 0.0131   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:09:15 d2.utils.events]: \u001b[0m eta: 3 days, 16:33:51  iter: 2739  total_loss: 62.57  loss_ce: 0.7849  loss_mask: 0.297  loss_dice: 0.6586  loss_bbox: 0.2222  loss_giou: 0.7156  loss_ce_dn: 0.1309  loss_mask_dn: 0.2729  loss_dice_dn: 0.6845  loss_bbox_dn: 0.2347  loss_giou_dn: 0.5646  loss_ce_0: 1.51  loss_mask_0: 0.3054  loss_dice_0: 0.7772  loss_bbox_0: 0.3772  loss_giou_0: 0.9697  loss_ce_dn_0: 0.7592  loss_mask_dn_0: 0.8237  loss_dice_dn_0: 3.018  loss_bbox_dn_0: 0.6415  loss_giou_dn_0: 0.9328  loss_ce_1: 1.368  loss_mask_1: 0.3037  loss_dice_1: 0.6959  loss_bbox_1: 0.2595  loss_giou_1: 0.8385  loss_ce_dn_1: 0.2734  loss_mask_dn_1: 0.3756  loss_dice_dn_1: 0.8465  loss_bbox_dn_1: 0.3787  loss_giou_dn_1: 0.675  loss_ce_2: 1.177  loss_mask_2: 0.295  loss_dice_2: 0.6953  loss_bbox_2: 0.2644  loss_giou_2: 0.7691  loss_ce_dn_2: 0.2229  loss_mask_dn_2: 0.331  loss_dice_dn_2: 0.7789  loss_bbox_dn_2: 0.2827  loss_giou_dn_2: 0.6152  loss_ce_3: 1.054  loss_mask_3: 0.2805  loss_dice_3: 0.7018  loss_bbox_3: 0.2249  loss_giou_3: 0.7686  loss_ce_dn_3: 0.1912  loss_mask_dn_3: 0.2883  loss_dice_dn_3: 0.7413  loss_bbox_dn_3: 0.2506  loss_giou_dn_3: 0.5865  loss_ce_4: 0.9859  loss_mask_4: 0.2908  loss_dice_4: 0.6981  loss_bbox_4: 0.2192  loss_giou_4: 0.7326  loss_ce_dn_4: 0.1764  loss_mask_dn_4: 0.2871  loss_dice_dn_4: 0.7328  loss_bbox_dn_4: 0.2444  loss_giou_dn_4: 0.5778  loss_ce_5: 0.8493  loss_mask_5: 0.2984  loss_dice_5: 0.7034  loss_bbox_5: 0.2301  loss_giou_5: 0.7174  loss_ce_dn_5: 0.1569  loss_mask_dn_5: 0.275  loss_dice_dn_5: 0.7294  loss_bbox_dn_5: 0.2407  loss_giou_dn_5: 0.5696  loss_ce_6: 0.8321  loss_mask_6: 0.2909  loss_dice_6: 0.6785  loss_bbox_6: 0.2404  loss_giou_6: 0.7769  loss_ce_dn_6: 0.1467  loss_mask_dn_6: 0.2701  loss_dice_dn_6: 0.7251  loss_bbox_dn_6: 0.2354  loss_giou_dn_6: 0.5658  loss_ce_7: 0.789  loss_mask_7: 0.2922  loss_dice_7: 0.6822  loss_bbox_7: 0.2252  loss_giou_7: 0.7091  loss_ce_dn_7: 0.1424  loss_mask_dn_7: 0.2715  loss_dice_dn_7: 0.7316  loss_bbox_dn_7: 0.2318  loss_giou_dn_7: 0.5634  loss_ce_8: 0.7847  loss_mask_8: 0.2988  loss_dice_8: 0.6663  loss_bbox_8: 0.2284  loss_giou_8: 0.7696  loss_ce_dn_8: 0.1367  loss_mask_dn_8: 0.2742  loss_dice_dn_8: 0.7002  loss_bbox_dn_8: 0.2341  loss_giou_dn_8: 0.5645  loss_ce_interm: 1.51  loss_mask_interm: 0.3133  loss_dice_interm: 0.7707  loss_bbox_interm: 0.3912  loss_giou_interm: 0.9681    time: 0.8736  last_time: 0.8746  data_time: 0.0118  last_data_time: 0.0114   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:09:33 d2.utils.events]: \u001b[0m eta: 3 days, 16:34:15  iter: 2759  total_loss: 66.59  loss_ce: 0.9082  loss_mask: 0.3288  loss_dice: 0.8486  loss_bbox: 0.3162  loss_giou: 0.7479  loss_ce_dn: 0.1365  loss_mask_dn: 0.3277  loss_dice_dn: 0.8184  loss_bbox_dn: 0.2541  loss_giou_dn: 0.5741  loss_ce_0: 1.515  loss_mask_0: 0.3359  loss_dice_0: 0.7799  loss_bbox_0: 0.4859  loss_giou_0: 0.9213  loss_ce_dn_0: 0.7615  loss_mask_dn_0: 0.9231  loss_dice_dn_0: 3.112  loss_bbox_dn_0: 0.6117  loss_giou_dn_0: 0.9638  loss_ce_1: 1.401  loss_mask_1: 0.3836  loss_dice_1: 0.8873  loss_bbox_1: 0.3628  loss_giou_1: 0.8384  loss_ce_dn_1: 0.2535  loss_mask_dn_1: 0.3474  loss_dice_dn_1: 0.9482  loss_bbox_dn_1: 0.37  loss_giou_dn_1: 0.7342  loss_ce_2: 1.218  loss_mask_2: 0.3431  loss_dice_2: 0.8085  loss_bbox_2: 0.3316  loss_giou_2: 0.8192  loss_ce_dn_2: 0.2001  loss_mask_dn_2: 0.336  loss_dice_dn_2: 0.8954  loss_bbox_dn_2: 0.3129  loss_giou_dn_2: 0.6553  loss_ce_3: 1.051  loss_mask_3: 0.3189  loss_dice_3: 0.8969  loss_bbox_3: 0.3411  loss_giou_3: 0.7976  loss_ce_dn_3: 0.171  loss_mask_dn_3: 0.3339  loss_dice_dn_3: 0.8484  loss_bbox_dn_3: 0.2855  loss_giou_dn_3: 0.6282  loss_ce_4: 1.009  loss_mask_4: 0.2894  loss_dice_4: 0.8718  loss_bbox_4: 0.315  loss_giou_4: 0.7626  loss_ce_dn_4: 0.1641  loss_mask_dn_4: 0.3304  loss_dice_dn_4: 0.8425  loss_bbox_dn_4: 0.2704  loss_giou_dn_4: 0.6092  loss_ce_5: 0.9453  loss_mask_5: 0.3162  loss_dice_5: 0.8794  loss_bbox_5: 0.3046  loss_giou_5: 0.7584  loss_ce_dn_5: 0.1462  loss_mask_dn_5: 0.3351  loss_dice_dn_5: 0.8239  loss_bbox_dn_5: 0.2648  loss_giou_dn_5: 0.5975  loss_ce_6: 0.9334  loss_mask_6: 0.3294  loss_dice_6: 0.7805  loss_bbox_6: 0.3157  loss_giou_6: 0.743  loss_ce_dn_6: 0.146  loss_mask_dn_6: 0.3381  loss_dice_dn_6: 0.8145  loss_bbox_dn_6: 0.2567  loss_giou_dn_6: 0.5851  loss_ce_7: 0.899  loss_mask_7: 0.3177  loss_dice_7: 0.8066  loss_bbox_7: 0.3072  loss_giou_7: 0.7814  loss_ce_dn_7: 0.137  loss_mask_dn_7: 0.3322  loss_dice_dn_7: 0.8311  loss_bbox_dn_7: 0.2557  loss_giou_dn_7: 0.5841  loss_ce_8: 0.9169  loss_mask_8: 0.3196  loss_dice_8: 0.8034  loss_bbox_8: 0.3204  loss_giou_8: 0.7484  loss_ce_dn_8: 0.1351  loss_mask_dn_8: 0.3211  loss_dice_dn_8: 0.8284  loss_bbox_dn_8: 0.2541  loss_giou_dn_8: 0.5766  loss_ce_interm: 1.511  loss_mask_interm: 0.3356  loss_dice_interm: 0.7974  loss_bbox_interm: 0.4859  loss_giou_interm: 0.9328    time: 0.8736  last_time: 0.8894  data_time: 0.0122  last_data_time: 0.0127   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:09:50 d2.utils.events]: \u001b[0m eta: 3 days, 16:32:32  iter: 2779  total_loss: 61.95  loss_ce: 0.7166  loss_mask: 0.2708  loss_dice: 0.7133  loss_bbox: 0.2466  loss_giou: 0.6694  loss_ce_dn: 0.08948  loss_mask_dn: 0.2724  loss_dice_dn: 0.6839  loss_bbox_dn: 0.1692  loss_giou_dn: 0.5636  loss_ce_0: 1.396  loss_mask_0: 0.3157  loss_dice_0: 0.7471  loss_bbox_0: 0.3983  loss_giou_0: 0.9175  loss_ce_dn_0: 0.6986  loss_mask_dn_0: 0.8184  loss_dice_dn_0: 2.956  loss_bbox_dn_0: 0.5215  loss_giou_dn_0: 0.9646  loss_ce_1: 1.314  loss_mask_1: 0.3033  loss_dice_1: 0.7212  loss_bbox_1: 0.315  loss_giou_1: 0.7561  loss_ce_dn_1: 0.2432  loss_mask_dn_1: 0.3897  loss_dice_dn_1: 0.8904  loss_bbox_dn_1: 0.2794  loss_giou_dn_1: 0.7105  loss_ce_2: 1.101  loss_mask_2: 0.2758  loss_dice_2: 0.8021  loss_bbox_2: 0.2659  loss_giou_2: 0.7228  loss_ce_dn_2: 0.1685  loss_mask_dn_2: 0.3044  loss_dice_dn_2: 0.7999  loss_bbox_dn_2: 0.2253  loss_giou_dn_2: 0.6372  loss_ce_3: 1.001  loss_mask_3: 0.2785  loss_dice_3: 0.7765  loss_bbox_3: 0.2449  loss_giou_3: 0.7208  loss_ce_dn_3: 0.1277  loss_mask_dn_3: 0.2839  loss_dice_dn_3: 0.7645  loss_bbox_dn_3: 0.21  loss_giou_dn_3: 0.6121  loss_ce_4: 0.9041  loss_mask_4: 0.2732  loss_dice_4: 0.7295  loss_bbox_4: 0.237  loss_giou_4: 0.6869  loss_ce_dn_4: 0.1162  loss_mask_dn_4: 0.2797  loss_dice_dn_4: 0.7451  loss_bbox_dn_4: 0.2002  loss_giou_dn_4: 0.5817  loss_ce_5: 0.8375  loss_mask_5: 0.2595  loss_dice_5: 0.6405  loss_bbox_5: 0.2461  loss_giou_5: 0.6815  loss_ce_dn_5: 0.1065  loss_mask_dn_5: 0.2786  loss_dice_dn_5: 0.7386  loss_bbox_dn_5: 0.1908  loss_giou_dn_5: 0.5786  loss_ce_6: 0.7919  loss_mask_6: 0.271  loss_dice_6: 0.6849  loss_bbox_6: 0.2538  loss_giou_6: 0.6723  loss_ce_dn_6: 0.1003  loss_mask_dn_6: 0.2728  loss_dice_dn_6: 0.7085  loss_bbox_dn_6: 0.1821  loss_giou_dn_6: 0.572  loss_ce_7: 0.7151  loss_mask_7: 0.2702  loss_dice_7: 0.6881  loss_bbox_7: 0.2583  loss_giou_7: 0.686  loss_ce_dn_7: 0.09464  loss_mask_dn_7: 0.2657  loss_dice_dn_7: 0.7142  loss_bbox_dn_7: 0.1766  loss_giou_dn_7: 0.5681  loss_ce_8: 0.7367  loss_mask_8: 0.2665  loss_dice_8: 0.6847  loss_bbox_8: 0.259  loss_giou_8: 0.6826  loss_ce_dn_8: 0.09043  loss_mask_dn_8: 0.267  loss_dice_dn_8: 0.6748  loss_bbox_dn_8: 0.1708  loss_giou_dn_8: 0.5634  loss_ce_interm: 1.366  loss_mask_interm: 0.315  loss_dice_interm: 0.7536  loss_bbox_interm: 0.4046  loss_giou_interm: 0.9122    time: 0.8735  last_time: 0.8610  data_time: 0.0118  last_data_time: 0.0179   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:10:08 d2.utils.events]: \u001b[0m eta: 3 days, 16:32:40  iter: 2799  total_loss: 58.61  loss_ce: 0.7167  loss_mask: 0.2905  loss_dice: 0.832  loss_bbox: 0.2849  loss_giou: 0.5805  loss_ce_dn: 0.1271  loss_mask_dn: 0.2918  loss_dice_dn: 0.79  loss_bbox_dn: 0.2205  loss_giou_dn: 0.4992  loss_ce_0: 1.486  loss_mask_0: 0.3194  loss_dice_0: 0.8599  loss_bbox_0: 0.4262  loss_giou_0: 0.8011  loss_ce_dn_0: 0.7778  loss_mask_dn_0: 0.9529  loss_dice_dn_0: 2.725  loss_bbox_dn_0: 0.6444  loss_giou_dn_0: 0.8714  loss_ce_1: 1.421  loss_mask_1: 0.3244  loss_dice_1: 0.8756  loss_bbox_1: 0.2987  loss_giou_1: 0.6774  loss_ce_dn_1: 0.2373  loss_mask_dn_1: 0.3114  loss_dice_dn_1: 0.941  loss_bbox_dn_1: 0.3612  loss_giou_dn_1: 0.6345  loss_ce_2: 1.206  loss_mask_2: 0.2975  loss_dice_2: 0.8539  loss_bbox_2: 0.2652  loss_giou_2: 0.6224  loss_ce_dn_2: 0.1822  loss_mask_dn_2: 0.2914  loss_dice_dn_2: 0.8312  loss_bbox_dn_2: 0.2748  loss_giou_dn_2: 0.5727  loss_ce_3: 1.009  loss_mask_3: 0.2917  loss_dice_3: 0.8009  loss_bbox_3: 0.2668  loss_giou_3: 0.6291  loss_ce_dn_3: 0.1532  loss_mask_dn_3: 0.2917  loss_dice_dn_3: 0.817  loss_bbox_dn_3: 0.257  loss_giou_dn_3: 0.5591  loss_ce_4: 1.007  loss_mask_4: 0.2921  loss_dice_4: 0.8088  loss_bbox_4: 0.266  loss_giou_4: 0.598  loss_ce_dn_4: 0.1413  loss_mask_dn_4: 0.2864  loss_dice_dn_4: 0.7876  loss_bbox_dn_4: 0.2435  loss_giou_dn_4: 0.5303  loss_ce_5: 0.9083  loss_mask_5: 0.2901  loss_dice_5: 0.7658  loss_bbox_5: 0.2779  loss_giou_5: 0.6053  loss_ce_dn_5: 0.1375  loss_mask_dn_5: 0.2867  loss_dice_dn_5: 0.7842  loss_bbox_dn_5: 0.2347  loss_giou_dn_5: 0.5241  loss_ce_6: 0.8283  loss_mask_6: 0.2882  loss_dice_6: 0.8594  loss_bbox_6: 0.2869  loss_giou_6: 0.5935  loss_ce_dn_6: 0.1361  loss_mask_dn_6: 0.2923  loss_dice_dn_6: 0.78  loss_bbox_dn_6: 0.2277  loss_giou_dn_6: 0.5099  loss_ce_7: 0.7921  loss_mask_7: 0.2865  loss_dice_7: 0.7959  loss_bbox_7: 0.2733  loss_giou_7: 0.5918  loss_ce_dn_7: 0.1322  loss_mask_dn_7: 0.2913  loss_dice_dn_7: 0.7705  loss_bbox_dn_7: 0.2233  loss_giou_dn_7: 0.5043  loss_ce_8: 0.7484  loss_mask_8: 0.2824  loss_dice_8: 0.7975  loss_bbox_8: 0.2849  loss_giou_8: 0.5827  loss_ce_dn_8: 0.1285  loss_mask_dn_8: 0.2884  loss_dice_dn_8: 0.7719  loss_bbox_dn_8: 0.2182  loss_giou_dn_8: 0.5001  loss_ce_interm: 1.514  loss_mask_interm: 0.3116  loss_dice_interm: 0.8942  loss_bbox_interm: 0.427  loss_giou_interm: 0.8123    time: 0.8736  last_time: 0.8741  data_time: 0.0119  last_data_time: 0.0136   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:10:25 d2.utils.events]: \u001b[0m eta: 3 days, 16:32:18  iter: 2819  total_loss: 59.53  loss_ce: 0.7358  loss_mask: 0.257  loss_dice: 0.6263  loss_bbox: 0.2234  loss_giou: 0.7029  loss_ce_dn: 0.1041  loss_mask_dn: 0.2506  loss_dice_dn: 0.6526  loss_bbox_dn: 0.2328  loss_giou_dn: 0.5342  loss_ce_0: 1.436  loss_mask_0: 0.2646  loss_dice_0: 0.6547  loss_bbox_0: 0.4271  loss_giou_0: 0.9006  loss_ce_dn_0: 0.7432  loss_mask_dn_0: 0.8229  loss_dice_dn_0: 2.797  loss_bbox_dn_0: 0.6304  loss_giou_dn_0: 0.8755  loss_ce_1: 1.297  loss_mask_1: 0.2824  loss_dice_1: 0.6749  loss_bbox_1: 0.2821  loss_giou_1: 0.7676  loss_ce_dn_1: 0.2232  loss_mask_dn_1: 0.3079  loss_dice_dn_1: 0.8098  loss_bbox_dn_1: 0.3497  loss_giou_dn_1: 0.6525  loss_ce_2: 1.092  loss_mask_2: 0.2527  loss_dice_2: 0.6664  loss_bbox_2: 0.2643  loss_giou_2: 0.7182  loss_ce_dn_2: 0.177  loss_mask_dn_2: 0.2615  loss_dice_dn_2: 0.724  loss_bbox_dn_2: 0.2914  loss_giou_dn_2: 0.5945  loss_ce_3: 0.8896  loss_mask_3: 0.2552  loss_dice_3: 0.6836  loss_bbox_3: 0.2379  loss_giou_3: 0.7183  loss_ce_dn_3: 0.1622  loss_mask_dn_3: 0.2554  loss_dice_dn_3: 0.6892  loss_bbox_dn_3: 0.2689  loss_giou_dn_3: 0.5779  loss_ce_4: 0.8437  loss_mask_4: 0.254  loss_dice_4: 0.6353  loss_bbox_4: 0.2277  loss_giou_4: 0.7071  loss_ce_dn_4: 0.1452  loss_mask_dn_4: 0.2537  loss_dice_dn_4: 0.6733  loss_bbox_dn_4: 0.2478  loss_giou_dn_4: 0.5575  loss_ce_5: 0.7964  loss_mask_5: 0.2683  loss_dice_5: 0.6163  loss_bbox_5: 0.2285  loss_giou_5: 0.7155  loss_ce_dn_5: 0.1306  loss_mask_dn_5: 0.2622  loss_dice_dn_5: 0.651  loss_bbox_dn_5: 0.2454  loss_giou_dn_5: 0.5518  loss_ce_6: 0.7597  loss_mask_6: 0.2578  loss_dice_6: 0.6139  loss_bbox_6: 0.2379  loss_giou_6: 0.6963  loss_ce_dn_6: 0.1164  loss_mask_dn_6: 0.2517  loss_dice_dn_6: 0.6473  loss_bbox_dn_6: 0.2439  loss_giou_dn_6: 0.5407  loss_ce_7: 0.7269  loss_mask_7: 0.2509  loss_dice_7: 0.632  loss_bbox_7: 0.2253  loss_giou_7: 0.7019  loss_ce_dn_7: 0.112  loss_mask_dn_7: 0.2437  loss_dice_dn_7: 0.6577  loss_bbox_dn_7: 0.2364  loss_giou_dn_7: 0.5388  loss_ce_8: 0.7408  loss_mask_8: 0.2588  loss_dice_8: 0.6604  loss_bbox_8: 0.2277  loss_giou_8: 0.7034  loss_ce_dn_8: 0.1031  loss_mask_dn_8: 0.2481  loss_dice_dn_8: 0.6595  loss_bbox_dn_8: 0.2323  loss_giou_dn_8: 0.5336  loss_ce_interm: 1.45  loss_mask_interm: 0.2602  loss_dice_interm: 0.6638  loss_bbox_interm: 0.4158  loss_giou_interm: 0.8962    time: 0.8736  last_time: 0.8774  data_time: 0.0119  last_data_time: 0.0144   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:10:43 d2.utils.events]: \u001b[0m eta: 3 days, 16:31:22  iter: 2839  total_loss: 60.75  loss_ce: 0.7309  loss_mask: 0.2591  loss_dice: 0.7846  loss_bbox: 0.2484  loss_giou: 0.5403  loss_ce_dn: 0.1142  loss_mask_dn: 0.3038  loss_dice_dn: 0.7738  loss_bbox_dn: 0.2319  loss_giou_dn: 0.4602  loss_ce_0: 1.491  loss_mask_0: 0.3368  loss_dice_0: 0.9034  loss_bbox_0: 0.4274  loss_giou_0: 0.7622  loss_ce_dn_0: 0.7963  loss_mask_dn_0: 0.9734  loss_dice_dn_0: 3.005  loss_bbox_dn_0: 0.6736  loss_giou_dn_0: 0.8602  loss_ce_1: 1.324  loss_mask_1: 0.3472  loss_dice_1: 0.8336  loss_bbox_1: 0.2946  loss_giou_1: 0.6152  loss_ce_dn_1: 0.2611  loss_mask_dn_1: 0.4383  loss_dice_dn_1: 0.9665  loss_bbox_dn_1: 0.3777  loss_giou_dn_1: 0.6198  loss_ce_2: 1.161  loss_mask_2: 0.3334  loss_dice_2: 0.8336  loss_bbox_2: 0.2681  loss_giou_2: 0.5738  loss_ce_dn_2: 0.1847  loss_mask_dn_2: 0.3624  loss_dice_dn_2: 0.861  loss_bbox_dn_2: 0.309  loss_giou_dn_2: 0.5459  loss_ce_3: 0.9765  loss_mask_3: 0.2912  loss_dice_3: 0.808  loss_bbox_3: 0.2558  loss_giou_3: 0.5606  loss_ce_dn_3: 0.1564  loss_mask_dn_3: 0.3483  loss_dice_dn_3: 0.8313  loss_bbox_dn_3: 0.2838  loss_giou_dn_3: 0.5133  loss_ce_4: 0.9026  loss_mask_4: 0.2692  loss_dice_4: 0.831  loss_bbox_4: 0.2424  loss_giou_4: 0.5495  loss_ce_dn_4: 0.1452  loss_mask_dn_4: 0.3239  loss_dice_dn_4: 0.8064  loss_bbox_dn_4: 0.2567  loss_giou_dn_4: 0.4887  loss_ce_5: 0.8502  loss_mask_5: 0.2695  loss_dice_5: 0.8474  loss_bbox_5: 0.2625  loss_giou_5: 0.5507  loss_ce_dn_5: 0.1333  loss_mask_dn_5: 0.3166  loss_dice_dn_5: 0.792  loss_bbox_dn_5: 0.2503  loss_giou_dn_5: 0.4816  loss_ce_6: 0.8277  loss_mask_6: 0.2697  loss_dice_6: 0.8099  loss_bbox_6: 0.2645  loss_giou_6: 0.5422  loss_ce_dn_6: 0.1228  loss_mask_dn_6: 0.3066  loss_dice_dn_6: 0.787  loss_bbox_dn_6: 0.2424  loss_giou_dn_6: 0.4693  loss_ce_7: 0.817  loss_mask_7: 0.2671  loss_dice_7: 0.8127  loss_bbox_7: 0.2545  loss_giou_7: 0.5513  loss_ce_dn_7: 0.1174  loss_mask_dn_7: 0.3029  loss_dice_dn_7: 0.7785  loss_bbox_dn_7: 0.2393  loss_giou_dn_7: 0.4658  loss_ce_8: 0.787  loss_mask_8: 0.2636  loss_dice_8: 0.7894  loss_bbox_8: 0.2474  loss_giou_8: 0.5456  loss_ce_dn_8: 0.1188  loss_mask_dn_8: 0.3013  loss_dice_dn_8: 0.7693  loss_bbox_dn_8: 0.2329  loss_giou_dn_8: 0.4624  loss_ce_interm: 1.505  loss_mask_interm: 0.3487  loss_dice_interm: 0.8836  loss_bbox_interm: 0.4214  loss_giou_interm: 0.7554    time: 0.8736  last_time: 0.8783  data_time: 0.0117  last_data_time: 0.0184   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:11:00 d2.utils.events]: \u001b[0m eta: 3 days, 16:32:06  iter: 2859  total_loss: 63.94  loss_ce: 0.8744  loss_mask: 0.2583  loss_dice: 0.7196  loss_bbox: 0.2765  loss_giou: 0.7542  loss_ce_dn: 0.1303  loss_mask_dn: 0.2514  loss_dice_dn: 0.7369  loss_bbox_dn: 0.22  loss_giou_dn: 0.6403  loss_ce_0: 1.594  loss_mask_0: 0.2687  loss_dice_0: 0.7557  loss_bbox_0: 0.4098  loss_giou_0: 0.9983  loss_ce_dn_0: 0.7317  loss_mask_dn_0: 0.9124  loss_dice_dn_0: 2.826  loss_bbox_dn_0: 0.6264  loss_giou_dn_0: 0.8901  loss_ce_1: 1.471  loss_mask_1: 0.2786  loss_dice_1: 0.7777  loss_bbox_1: 0.3463  loss_giou_1: 0.836  loss_ce_dn_1: 0.2701  loss_mask_dn_1: 0.2934  loss_dice_dn_1: 0.8838  loss_bbox_dn_1: 0.3516  loss_giou_dn_1: 0.7064  loss_ce_2: 1.317  loss_mask_2: 0.2458  loss_dice_2: 0.7092  loss_bbox_2: 0.3198  loss_giou_2: 0.7946  loss_ce_dn_2: 0.2212  loss_mask_dn_2: 0.2617  loss_dice_dn_2: 0.7955  loss_bbox_dn_2: 0.2943  loss_giou_dn_2: 0.6831  loss_ce_3: 1.176  loss_mask_3: 0.242  loss_dice_3: 0.7212  loss_bbox_3: 0.2852  loss_giou_3: 0.7693  loss_ce_dn_3: 0.1903  loss_mask_dn_3: 0.2622  loss_dice_dn_3: 0.7574  loss_bbox_dn_3: 0.2621  loss_giou_dn_3: 0.6646  loss_ce_4: 1.076  loss_mask_4: 0.2533  loss_dice_4: 0.7176  loss_bbox_4: 0.2637  loss_giou_4: 0.7586  loss_ce_dn_4: 0.1719  loss_mask_dn_4: 0.2602  loss_dice_dn_4: 0.7713  loss_bbox_dn_4: 0.2435  loss_giou_dn_4: 0.6633  loss_ce_5: 1.059  loss_mask_5: 0.2559  loss_dice_5: 0.6982  loss_bbox_5: 0.2692  loss_giou_5: 0.7584  loss_ce_dn_5: 0.156  loss_mask_dn_5: 0.2532  loss_dice_dn_5: 0.7367  loss_bbox_dn_5: 0.234  loss_giou_dn_5: 0.6497  loss_ce_6: 0.9566  loss_mask_6: 0.2663  loss_dice_6: 0.6795  loss_bbox_6: 0.2635  loss_giou_6: 0.7583  loss_ce_dn_6: 0.1491  loss_mask_dn_6: 0.2612  loss_dice_dn_6: 0.7529  loss_bbox_dn_6: 0.2238  loss_giou_dn_6: 0.646  loss_ce_7: 0.9369  loss_mask_7: 0.2481  loss_dice_7: 0.6864  loss_bbox_7: 0.2554  loss_giou_7: 0.7561  loss_ce_dn_7: 0.1397  loss_mask_dn_7: 0.2571  loss_dice_dn_7: 0.7473  loss_bbox_dn_7: 0.2233  loss_giou_dn_7: 0.6444  loss_ce_8: 0.8765  loss_mask_8: 0.2595  loss_dice_8: 0.7119  loss_bbox_8: 0.2862  loss_giou_8: 0.7532  loss_ce_dn_8: 0.1352  loss_mask_dn_8: 0.2485  loss_dice_dn_8: 0.7368  loss_bbox_dn_8: 0.2191  loss_giou_dn_8: 0.6403  loss_ce_interm: 1.561  loss_mask_interm: 0.2716  loss_dice_interm: 0.7397  loss_bbox_interm: 0.4208  loss_giou_interm: 0.9945    time: 0.8736  last_time: 0.8736  data_time: 0.0126  last_data_time: 0.0135   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:11:18 d2.utils.events]: \u001b[0m eta: 3 days, 16:33:22  iter: 2879  total_loss: 71.19  loss_ce: 0.9666  loss_mask: 0.2605  loss_dice: 1.025  loss_bbox: 0.3284  loss_giou: 0.7047  loss_ce_dn: 0.1101  loss_mask_dn: 0.2574  loss_dice_dn: 0.9691  loss_bbox_dn: 0.2867  loss_giou_dn: 0.5283  loss_ce_0: 1.522  loss_mask_0: 0.2777  loss_dice_0: 0.9894  loss_bbox_0: 0.4814  loss_giou_0: 0.9415  loss_ce_dn_0: 0.7459  loss_mask_dn_0: 0.781  loss_dice_dn_0: 3.134  loss_bbox_dn_0: 0.6442  loss_giou_dn_0: 0.8796  loss_ce_1: 1.466  loss_mask_1: 0.28  loss_dice_1: 0.9926  loss_bbox_1: 0.3523  loss_giou_1: 0.7552  loss_ce_dn_1: 0.2393  loss_mask_dn_1: 0.345  loss_dice_dn_1: 1.069  loss_bbox_dn_1: 0.4058  loss_giou_dn_1: 0.6506  loss_ce_2: 1.279  loss_mask_2: 0.2743  loss_dice_2: 0.9939  loss_bbox_2: 0.315  loss_giou_2: 0.7198  loss_ce_dn_2: 0.1891  loss_mask_dn_2: 0.3162  loss_dice_dn_2: 1.001  loss_bbox_dn_2: 0.3338  loss_giou_dn_2: 0.5932  loss_ce_3: 1.165  loss_mask_3: 0.2659  loss_dice_3: 0.9287  loss_bbox_3: 0.3182  loss_giou_3: 0.7263  loss_ce_dn_3: 0.1616  loss_mask_dn_3: 0.2855  loss_dice_dn_3: 0.996  loss_bbox_dn_3: 0.3156  loss_giou_dn_3: 0.568  loss_ce_4: 1.072  loss_mask_4: 0.266  loss_dice_4: 1.001  loss_bbox_4: 0.3193  loss_giou_4: 0.7192  loss_ce_dn_4: 0.1401  loss_mask_dn_4: 0.2735  loss_dice_dn_4: 0.981  loss_bbox_dn_4: 0.2985  loss_giou_dn_4: 0.5475  loss_ce_5: 0.9859  loss_mask_5: 0.2629  loss_dice_5: 0.9687  loss_bbox_5: 0.3341  loss_giou_5: 0.717  loss_ce_dn_5: 0.1278  loss_mask_dn_5: 0.2619  loss_dice_dn_5: 0.9715  loss_bbox_dn_5: 0.2893  loss_giou_dn_5: 0.5448  loss_ce_6: 0.9253  loss_mask_6: 0.2743  loss_dice_6: 0.9897  loss_bbox_6: 0.3249  loss_giou_6: 0.7028  loss_ce_dn_6: 0.1193  loss_mask_dn_6: 0.2614  loss_dice_dn_6: 0.9493  loss_bbox_dn_6: 0.2888  loss_giou_dn_6: 0.5323  loss_ce_7: 0.9174  loss_mask_7: 0.2578  loss_dice_7: 0.9682  loss_bbox_7: 0.3275  loss_giou_7: 0.6825  loss_ce_dn_7: 0.1152  loss_mask_dn_7: 0.2526  loss_dice_dn_7: 0.935  loss_bbox_dn_7: 0.2885  loss_giou_dn_7: 0.5303  loss_ce_8: 0.9444  loss_mask_8: 0.2594  loss_dice_8: 0.97  loss_bbox_8: 0.3239  loss_giou_8: 0.7042  loss_ce_dn_8: 0.116  loss_mask_dn_8: 0.2582  loss_dice_dn_8: 0.9538  loss_bbox_dn_8: 0.2871  loss_giou_dn_8: 0.5302  loss_ce_interm: 1.525  loss_mask_interm: 0.2771  loss_dice_interm: 1.032  loss_bbox_interm: 0.4775  loss_giou_interm: 0.9385    time: 0.8737  last_time: 0.9052  data_time: 0.0123  last_data_time: 0.0140   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:11:36 d2.utils.events]: \u001b[0m eta: 3 days, 16:32:29  iter: 2899  total_loss: 64.91  loss_ce: 0.8692  loss_mask: 0.2924  loss_dice: 0.8672  loss_bbox: 0.2306  loss_giou: 0.7361  loss_ce_dn: 0.1383  loss_mask_dn: 0.341  loss_dice_dn: 0.837  loss_bbox_dn: 0.237  loss_giou_dn: 0.5461  loss_ce_0: 1.516  loss_mask_0: 0.3228  loss_dice_0: 0.779  loss_bbox_0: 0.4168  loss_giou_0: 0.9529  loss_ce_dn_0: 0.6824  loss_mask_dn_0: 0.9354  loss_dice_dn_0: 2.986  loss_bbox_dn_0: 0.714  loss_giou_dn_0: 0.9306  loss_ce_1: 1.457  loss_mask_1: 0.3094  loss_dice_1: 0.8278  loss_bbox_1: 0.3212  loss_giou_1: 0.8276  loss_ce_dn_1: 0.2487  loss_mask_dn_1: 0.4247  loss_dice_dn_1: 0.997  loss_bbox_dn_1: 0.4065  loss_giou_dn_1: 0.6482  loss_ce_2: 1.257  loss_mask_2: 0.2961  loss_dice_2: 0.9249  loss_bbox_2: 0.2647  loss_giou_2: 0.7876  loss_ce_dn_2: 0.2202  loss_mask_dn_2: 0.3936  loss_dice_dn_2: 0.9156  loss_bbox_dn_2: 0.3236  loss_giou_dn_2: 0.5996  loss_ce_3: 1.164  loss_mask_3: 0.2862  loss_dice_3: 0.8771  loss_bbox_3: 0.2434  loss_giou_3: 0.7691  loss_ce_dn_3: 0.1992  loss_mask_dn_3: 0.3724  loss_dice_dn_3: 0.8842  loss_bbox_dn_3: 0.2848  loss_giou_dn_3: 0.5737  loss_ce_4: 1.065  loss_mask_4: 0.2714  loss_dice_4: 0.8315  loss_bbox_4: 0.2317  loss_giou_4: 0.7689  loss_ce_dn_4: 0.181  loss_mask_dn_4: 0.3711  loss_dice_dn_4: 0.8514  loss_bbox_dn_4: 0.2576  loss_giou_dn_4: 0.5633  loss_ce_5: 0.9821  loss_mask_5: 0.2812  loss_dice_5: 0.8532  loss_bbox_5: 0.2586  loss_giou_5: 0.7561  loss_ce_dn_5: 0.163  loss_mask_dn_5: 0.3663  loss_dice_dn_5: 0.8343  loss_bbox_dn_5: 0.2519  loss_giou_dn_5: 0.5572  loss_ce_6: 0.9064  loss_mask_6: 0.294  loss_dice_6: 0.8831  loss_bbox_6: 0.2573  loss_giou_6: 0.7506  loss_ce_dn_6: 0.1516  loss_mask_dn_6: 0.3484  loss_dice_dn_6: 0.8183  loss_bbox_dn_6: 0.24  loss_giou_dn_6: 0.5456  loss_ce_7: 0.8734  loss_mask_7: 0.2985  loss_dice_7: 0.8685  loss_bbox_7: 0.2512  loss_giou_7: 0.7444  loss_ce_dn_7: 0.1461  loss_mask_dn_7: 0.3464  loss_dice_dn_7: 0.8478  loss_bbox_dn_7: 0.2375  loss_giou_dn_7: 0.5478  loss_ce_8: 0.8951  loss_mask_8: 0.2929  loss_dice_8: 0.8225  loss_bbox_8: 0.2278  loss_giou_8: 0.7497  loss_ce_dn_8: 0.1372  loss_mask_dn_8: 0.3456  loss_dice_dn_8: 0.8418  loss_bbox_dn_8: 0.2368  loss_giou_dn_8: 0.5446  loss_ce_interm: 1.544  loss_mask_interm: 0.3195  loss_dice_interm: 0.8025  loss_bbox_interm: 0.4168  loss_giou_interm: 0.9541    time: 0.8737  last_time: 0.8667  data_time: 0.0119  last_data_time: 0.0165   lr: 1.25e-05  max_mem: 11982M\n",
            "\u001b[32m[02/20 08:11:53 d2.utils.events]: \u001b[0m eta: 3 days, 16:32:58  iter: 2919  total_loss: 66.78  loss_ce: 0.8539  loss_mask: 0.2595  loss_dice: 0.8488  loss_bbox: 0.2822  loss_giou: 0.7788  loss_ce_dn: 0.1419  loss_mask_dn: 0.2589  loss_dice_dn: 0.8594  loss_bbox_dn: 0.2461  loss_giou_dn: 0.634  loss_ce_0: 1.497  loss_mask_0: 0.2951  loss_dice_0: 0.9276  loss_bbox_0: 0.4157  loss_giou_0: 0.9839  loss_ce_dn_0: 0.7063  loss_mask_dn_0: 0.9045  loss_dice_dn_0: 3.186  loss_bbox_dn_0: 0.6314  loss_giou_dn_0: 0.9705  loss_ce_1: 1.412  loss_mask_1: 0.2755  loss_dice_1: 0.8742  loss_bbox_1: 0.3324  loss_giou_1: 0.8125  loss_ce_dn_1: 0.2691  loss_mask_dn_1: 0.3203  loss_dice_dn_1: 0.9309  loss_bbox_dn_1: 0.3446  loss_giou_dn_1: 0.7755  loss_ce_2: 1.251  loss_mask_2: 0.2886  loss_dice_2: 0.9272  loss_bbox_2: 0.2955  loss_giou_2: 0.7966  loss_ce_dn_2: 0.2298  loss_mask_dn_2: 0.2824  loss_dice_dn_2: 0.8958  loss_bbox_dn_2: 0.2941  loss_giou_dn_2: 0.7082  loss_ce_3: 1.074  loss_mask_3: 0.2818  loss_dice_3: 0.8371  loss_bbox_3: 0.3041  loss_giou_3: 0.768  loss_ce_dn_3: 0.1964  loss_mask_dn_3: 0.2797  loss_dice_dn_3: 0.886  loss_bbox_dn_3: 0.263  loss_giou_dn_3: 0.6808  loss_ce_4: 0.9955  loss_mask_4: 0.2705  loss_dice_4: 0.8427  loss_bbox_4: 0.3032  loss_giou_4: 0.7845  loss_ce_dn_4: 0.1738  loss_mask_dn_4: 0.2697  loss_dice_dn_4: 0.875  loss_bbox_dn_4: 0.2534  loss_giou_dn_4: 0.6594  loss_ce_5: 0.9207  loss_mask_5: 0.2637  loss_dice_5: 0.8989  loss_bbox_5: 0.3046  loss_giou_5: 0.7907  loss_ce_dn_5: 0.1666  loss_mask_dn_5: 0.274  loss_dice_dn_5: 0.8669  loss_bbox_dn_5: 0.2503  loss_giou_dn_5: 0.6527  loss_ce_6: 0.9213  loss_mask_6: 0.2716  loss_dice_6: 0.86  loss_bbox_6: 0.2917  loss_giou_6: 0.7832  loss_ce_dn_6: 0.1563  loss_mask_dn_6: 0.2642  loss_dice_dn_6: 0.8742  loss_bbox_dn_6: 0.2449  loss_giou_dn_6: 0.6459  loss_ce_7: 0.8295  loss_mask_7: 0.2659  loss_dice_7: 0.8955  loss_bbox_7: 0.294  loss_giou_7: 0.7641  loss_ce_dn_7: 0.1472  loss_mask_dn_7: 0.2572  loss_dice_dn_7: 0.8585  loss_bbox_dn_7: 0.2462  loss_giou_dn_7: 0.6378  loss_ce_8: 0.8555  loss_mask_8: 0.2644  loss_dice_8: 0.8737  loss_bbox_8: 0.2913  loss_giou_8: 0.7757  loss_ce_dn_8: 0.1417  loss_mask_dn_8: 0.2575  loss_dice_dn_8: 0.8476  loss_bbox_dn_8: 0.2444  loss_giou_dn_8: 0.6334  loss_ce_interm: 1.49  loss_mask_interm: 0.2897  loss_dice_interm: 0.9192  loss_bbox_interm: 0.4173  loss_giou_interm: 0.9787    time: 0.8737  last_time: 0.8853  data_time: 0.0131  last_data_time: 0.0098   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:12:11 d2.utils.events]: \u001b[0m eta: 3 days, 16:32:58  iter: 2939  total_loss: 67.86  loss_ce: 0.8492  loss_mask: 0.3428  loss_dice: 0.9107  loss_bbox: 0.2496  loss_giou: 0.7821  loss_ce_dn: 0.1173  loss_mask_dn: 0.3423  loss_dice_dn: 0.8601  loss_bbox_dn: 0.242  loss_giou_dn: 0.5703  loss_ce_0: 1.407  loss_mask_0: 0.3351  loss_dice_0: 0.7723  loss_bbox_0: 0.3874  loss_giou_0: 0.94  loss_ce_dn_0: 0.7035  loss_mask_dn_0: 0.9106  loss_dice_dn_0: 2.739  loss_bbox_dn_0: 0.681  loss_giou_dn_0: 0.8933  loss_ce_1: 1.297  loss_mask_1: 0.3555  loss_dice_1: 0.8639  loss_bbox_1: 0.2895  loss_giou_1: 0.848  loss_ce_dn_1: 0.2634  loss_mask_dn_1: 0.4298  loss_dice_dn_1: 0.9851  loss_bbox_dn_1: 0.3612  loss_giou_dn_1: 0.6755  loss_ce_2: 1.145  loss_mask_2: 0.3409  loss_dice_2: 0.872  loss_bbox_2: 0.2676  loss_giou_2: 0.8165  loss_ce_dn_2: 0.1982  loss_mask_dn_2: 0.384  loss_dice_dn_2: 0.9523  loss_bbox_dn_2: 0.298  loss_giou_dn_2: 0.6162  loss_ce_3: 1.064  loss_mask_3: 0.3205  loss_dice_3: 0.8463  loss_bbox_3: 0.2596  loss_giou_3: 0.774  loss_ce_dn_3: 0.1605  loss_mask_dn_3: 0.358  loss_dice_dn_3: 0.8984  loss_bbox_dn_3: 0.2674  loss_giou_dn_3: 0.6036  loss_ce_4: 0.9794  loss_mask_4: 0.3238  loss_dice_4: 0.9086  loss_bbox_4: 0.2383  loss_giou_4: 0.776  loss_ce_dn_4: 0.142  loss_mask_dn_4: 0.3331  loss_dice_dn_4: 0.9087  loss_bbox_dn_4: 0.2548  loss_giou_dn_4: 0.5847  loss_ce_5: 0.9161  loss_mask_5: 0.3282  loss_dice_5: 0.8415  loss_bbox_5: 0.2513  loss_giou_5: 0.7823  loss_ce_dn_5: 0.1299  loss_mask_dn_5: 0.3408  loss_dice_dn_5: 0.8662  loss_bbox_dn_5: 0.2528  loss_giou_dn_5: 0.5799  loss_ce_6: 0.8594  loss_mask_6: 0.3476  loss_dice_6: 0.8868  loss_bbox_6: 0.2708  loss_giou_6: 0.7629  loss_ce_dn_6: 0.131  loss_mask_dn_6: 0.341  loss_dice_dn_6: 0.8588  loss_bbox_dn_6: 0.2441  loss_giou_dn_6: 0.5729  loss_ce_7: 0.8616  loss_mask_7: 0.3403  loss_dice_7: 0.9124  loss_bbox_7: 0.2426  loss_giou_7: 0.7701  loss_ce_dn_7: 0.1266  loss_mask_dn_7: 0.3446  loss_dice_dn_7: 0.8776  loss_bbox_dn_7: 0.2429  loss_giou_dn_7: 0.5714  loss_ce_8: 0.8406  loss_mask_8: 0.3389  loss_dice_8: 0.8255  loss_bbox_8: 0.2429  loss_giou_8: 0.7717  loss_ce_dn_8: 0.1162  loss_mask_dn_8: 0.3479  loss_dice_dn_8: 0.8574  loss_bbox_dn_8: 0.2429  loss_giou_dn_8: 0.5727  loss_ce_interm: 1.42  loss_mask_interm: 0.3364  loss_dice_interm: 0.8291  loss_bbox_interm: 0.3924  loss_giou_interm: 0.9441    time: 0.8737  last_time: 0.9123  data_time: 0.0117  last_data_time: 0.0139   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:12:28 d2.utils.events]: \u001b[0m eta: 3 days, 16:33:04  iter: 2959  total_loss: 62.04  loss_ce: 0.7449  loss_mask: 0.2562  loss_dice: 0.7093  loss_bbox: 0.2595  loss_giou: 0.6896  loss_ce_dn: 0.1286  loss_mask_dn: 0.2482  loss_dice_dn: 0.7601  loss_bbox_dn: 0.2145  loss_giou_dn: 0.5353  loss_ce_0: 1.393  loss_mask_0: 0.2748  loss_dice_0: 0.7976  loss_bbox_0: 0.4657  loss_giou_0: 0.9022  loss_ce_dn_0: 0.7096  loss_mask_dn_0: 0.8372  loss_dice_dn_0: 2.932  loss_bbox_dn_0: 0.6151  loss_giou_dn_0: 0.8863  loss_ce_1: 1.331  loss_mask_1: 0.2589  loss_dice_1: 0.755  loss_bbox_1: 0.3389  loss_giou_1: 0.7957  loss_ce_dn_1: 0.2661  loss_mask_dn_1: 0.3077  loss_dice_dn_1: 0.8795  loss_bbox_dn_1: 0.3514  loss_giou_dn_1: 0.6622  loss_ce_2: 1.097  loss_mask_2: 0.2696  loss_dice_2: 0.7163  loss_bbox_2: 0.3032  loss_giou_2: 0.7273  loss_ce_dn_2: 0.2296  loss_mask_dn_2: 0.2912  loss_dice_dn_2: 0.7935  loss_bbox_dn_2: 0.283  loss_giou_dn_2: 0.6048  loss_ce_3: 0.9322  loss_mask_3: 0.2752  loss_dice_3: 0.7395  loss_bbox_3: 0.2747  loss_giou_3: 0.7522  loss_ce_dn_3: 0.1852  loss_mask_dn_3: 0.2621  loss_dice_dn_3: 0.7678  loss_bbox_dn_3: 0.2561  loss_giou_dn_3: 0.5787  loss_ce_4: 0.8738  loss_mask_4: 0.2774  loss_dice_4: 0.7442  loss_bbox_4: 0.2633  loss_giou_4: 0.7012  loss_ce_dn_4: 0.1662  loss_mask_dn_4: 0.2606  loss_dice_dn_4: 0.7737  loss_bbox_dn_4: 0.2405  loss_giou_dn_4: 0.5529  loss_ce_5: 0.8213  loss_mask_5: 0.2762  loss_dice_5: 0.7275  loss_bbox_5: 0.2913  loss_giou_5: 0.7405  loss_ce_dn_5: 0.1628  loss_mask_dn_5: 0.261  loss_dice_dn_5: 0.7703  loss_bbox_dn_5: 0.2307  loss_giou_dn_5: 0.5497  loss_ce_6: 0.756  loss_mask_6: 0.2544  loss_dice_6: 0.6832  loss_bbox_6: 0.2644  loss_giou_6: 0.7131  loss_ce_dn_6: 0.1424  loss_mask_dn_6: 0.2487  loss_dice_dn_6: 0.7591  loss_bbox_dn_6: 0.2235  loss_giou_dn_6: 0.5433  loss_ce_7: 0.7482  loss_mask_7: 0.2555  loss_dice_7: 0.747  loss_bbox_7: 0.2642  loss_giou_7: 0.7145  loss_ce_dn_7: 0.1342  loss_mask_dn_7: 0.2484  loss_dice_dn_7: 0.7579  loss_bbox_dn_7: 0.2174  loss_giou_dn_7: 0.5388  loss_ce_8: 0.7351  loss_mask_8: 0.2506  loss_dice_8: 0.6968  loss_bbox_8: 0.2574  loss_giou_8: 0.6813  loss_ce_dn_8: 0.1305  loss_mask_dn_8: 0.253  loss_dice_dn_8: 0.7376  loss_bbox_dn_8: 0.2144  loss_giou_dn_8: 0.5369  loss_ce_interm: 1.393  loss_mask_interm: 0.2545  loss_dice_interm: 0.7507  loss_bbox_interm: 0.4657  loss_giou_interm: 0.9183    time: 0.8737  last_time: 0.8918  data_time: 0.0133  last_data_time: 0.0167   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:12:46 d2.utils.events]: \u001b[0m eta: 3 days, 16:32:47  iter: 2979  total_loss: 61.78  loss_ce: 0.8105  loss_mask: 0.1983  loss_dice: 0.7039  loss_bbox: 0.2452  loss_giou: 0.6765  loss_ce_dn: 0.1576  loss_mask_dn: 0.2144  loss_dice_dn: 0.7788  loss_bbox_dn: 0.2212  loss_giou_dn: 0.5242  loss_ce_0: 1.446  loss_mask_0: 0.2119  loss_dice_0: 0.8191  loss_bbox_0: 0.3591  loss_giou_0: 0.8923  loss_ce_dn_0: 0.7429  loss_mask_dn_0: 0.8748  loss_dice_dn_0: 2.649  loss_bbox_dn_0: 0.6507  loss_giou_dn_0: 0.9048  loss_ce_1: 1.364  loss_mask_1: 0.213  loss_dice_1: 0.824  loss_bbox_1: 0.3066  loss_giou_1: 0.7461  loss_ce_dn_1: 0.2829  loss_mask_dn_1: 0.2791  loss_dice_dn_1: 0.8588  loss_bbox_dn_1: 0.3429  loss_giou_dn_1: 0.6518  loss_ce_2: 1.155  loss_mask_2: 0.2026  loss_dice_2: 0.7666  loss_bbox_2: 0.2603  loss_giou_2: 0.7377  loss_ce_dn_2: 0.2233  loss_mask_dn_2: 0.2189  loss_dice_dn_2: 0.7904  loss_bbox_dn_2: 0.2632  loss_giou_dn_2: 0.5895  loss_ce_3: 1.062  loss_mask_3: 0.2046  loss_dice_3: 0.7511  loss_bbox_3: 0.2605  loss_giou_3: 0.7249  loss_ce_dn_3: 0.1863  loss_mask_dn_3: 0.2031  loss_dice_dn_3: 0.7629  loss_bbox_dn_3: 0.2399  loss_giou_dn_3: 0.5715  loss_ce_4: 0.9434  loss_mask_4: 0.2133  loss_dice_4: 0.8108  loss_bbox_4: 0.2671  loss_giou_4: 0.7193  loss_ce_dn_4: 0.1784  loss_mask_dn_4: 0.2097  loss_dice_dn_4: 0.7648  loss_bbox_dn_4: 0.2318  loss_giou_dn_4: 0.5478  loss_ce_5: 0.889  loss_mask_5: 0.2072  loss_dice_5: 0.7416  loss_bbox_5: 0.2598  loss_giou_5: 0.7024  loss_ce_dn_5: 0.1686  loss_mask_dn_5: 0.2083  loss_dice_dn_5: 0.7689  loss_bbox_dn_5: 0.2248  loss_giou_dn_5: 0.5402  loss_ce_6: 0.8695  loss_mask_6: 0.2019  loss_dice_6: 0.7603  loss_bbox_6: 0.2454  loss_giou_6: 0.7059  loss_ce_dn_6: 0.1573  loss_mask_dn_6: 0.2048  loss_dice_dn_6: 0.7701  loss_bbox_dn_6: 0.2188  loss_giou_dn_6: 0.5295  loss_ce_7: 0.8204  loss_mask_7: 0.1985  loss_dice_7: 0.7103  loss_bbox_7: 0.244  loss_giou_7: 0.716  loss_ce_dn_7: 0.1506  loss_mask_dn_7: 0.2109  loss_dice_dn_7: 0.7491  loss_bbox_dn_7: 0.2205  loss_giou_dn_7: 0.5284  loss_ce_8: 0.8124  loss_mask_8: 0.2054  loss_dice_8: 0.6819  loss_bbox_8: 0.2441  loss_giou_8: 0.6817  loss_ce_dn_8: 0.1539  loss_mask_dn_8: 0.2086  loss_dice_dn_8: 0.7712  loss_bbox_dn_8: 0.2209  loss_giou_dn_8: 0.5245  loss_ce_interm: 1.425  loss_mask_interm: 0.2072  loss_dice_interm: 0.7899  loss_bbox_interm: 0.362  loss_giou_interm: 0.9036    time: 0.8737  last_time: 0.8522  data_time: 0.0120  last_data_time: 0.0090   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:13:03 d2.utils.events]: \u001b[0m eta: 3 days, 16:32:37  iter: 2999  total_loss: 62.86  loss_ce: 0.8484  loss_mask: 0.2313  loss_dice: 0.9404  loss_bbox: 0.2361  loss_giou: 0.7276  loss_ce_dn: 0.1424  loss_mask_dn: 0.2081  loss_dice_dn: 0.9379  loss_bbox_dn: 0.1878  loss_giou_dn: 0.5337  loss_ce_0: 1.422  loss_mask_0: 0.2236  loss_dice_0: 0.9547  loss_bbox_0: 0.3773  loss_giou_0: 0.9161  loss_ce_dn_0: 0.7895  loss_mask_dn_0: 0.6555  loss_dice_dn_0: 2.677  loss_bbox_dn_0: 0.4988  loss_giou_dn_0: 0.8981  loss_ce_1: 1.333  loss_mask_1: 0.2303  loss_dice_1: 0.9243  loss_bbox_1: 0.3235  loss_giou_1: 0.816  loss_ce_dn_1: 0.2589  loss_mask_dn_1: 0.2731  loss_dice_dn_1: 1.067  loss_bbox_dn_1: 0.2732  loss_giou_dn_1: 0.6591  loss_ce_2: 1.209  loss_mask_2: 0.2359  loss_dice_2: 0.9161  loss_bbox_2: 0.2952  loss_giou_2: 0.7757  loss_ce_dn_2: 0.2104  loss_mask_dn_2: 0.2716  loss_dice_dn_2: 1.052  loss_bbox_dn_2: 0.2274  loss_giou_dn_2: 0.5959  loss_ce_3: 1.086  loss_mask_3: 0.2103  loss_dice_3: 0.9382  loss_bbox_3: 0.2634  loss_giou_3: 0.7499  loss_ce_dn_3: 0.2035  loss_mask_dn_3: 0.2467  loss_dice_dn_3: 1.006  loss_bbox_dn_3: 0.213  loss_giou_dn_3: 0.5618  loss_ce_4: 0.9774  loss_mask_4: 0.219  loss_dice_4: 0.8774  loss_bbox_4: 0.2696  loss_giou_4: 0.743  loss_ce_dn_4: 0.1881  loss_mask_dn_4: 0.2371  loss_dice_dn_4: 0.9544  loss_bbox_dn_4: 0.2053  loss_giou_dn_4: 0.543  loss_ce_5: 0.9429  loss_mask_5: 0.2272  loss_dice_5: 0.9158  loss_bbox_5: 0.2533  loss_giou_5: 0.7402  loss_ce_dn_5: 0.1675  loss_mask_dn_5: 0.2165  loss_dice_dn_5: 0.9424  loss_bbox_dn_5: 0.1987  loss_giou_dn_5: 0.5395  loss_ce_6: 0.9274  loss_mask_6: 0.2283  loss_dice_6: 0.9324  loss_bbox_6: 0.2615  loss_giou_6: 0.7379  loss_ce_dn_6: 0.1604  loss_mask_dn_6: 0.2151  loss_dice_dn_6: 0.9449  loss_bbox_dn_6: 0.1945  loss_giou_dn_6: 0.5322  loss_ce_7: 0.8836  loss_mask_7: 0.2247  loss_dice_7: 0.8969  loss_bbox_7: 0.2326  loss_giou_7: 0.7262  loss_ce_dn_7: 0.1484  loss_mask_dn_7: 0.2099  loss_dice_dn_7: 0.9185  loss_bbox_dn_7: 0.1916  loss_giou_dn_7: 0.5345  loss_ce_8: 0.8786  loss_mask_8: 0.2287  loss_dice_8: 0.9133  loss_bbox_8: 0.2308  loss_giou_8: 0.7213  loss_ce_dn_8: 0.1413  loss_mask_dn_8: 0.2095  loss_dice_dn_8: 0.9118  loss_bbox_dn_8: 0.1884  loss_giou_dn_8: 0.5318  loss_ce_interm: 1.418  loss_mask_interm: 0.216  loss_dice_interm: 0.9312  loss_bbox_interm: 0.3756  loss_giou_interm: 0.9257    time: 0.8737  last_time: 0.8710  data_time: 0.0127  last_data_time: 0.0112   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:13:21 d2.utils.events]: \u001b[0m eta: 3 days, 16:35:54  iter: 3019  total_loss: 59.97  loss_ce: 0.7095  loss_mask: 0.2669  loss_dice: 0.7328  loss_bbox: 0.2583  loss_giou: 0.6287  loss_ce_dn: 0.1291  loss_mask_dn: 0.3035  loss_dice_dn: 0.7107  loss_bbox_dn: 0.2843  loss_giou_dn: 0.5623  loss_ce_0: 1.481  loss_mask_0: 0.2703  loss_dice_0: 0.8251  loss_bbox_0: 0.4089  loss_giou_0: 0.8488  loss_ce_dn_0: 0.7765  loss_mask_dn_0: 0.8555  loss_dice_dn_0: 2.99  loss_bbox_dn_0: 0.6688  loss_giou_dn_0: 0.9749  loss_ce_1: 1.409  loss_mask_1: 0.2929  loss_dice_1: 0.8244  loss_bbox_1: 0.3213  loss_giou_1: 0.701  loss_ce_dn_1: 0.2491  loss_mask_dn_1: 0.3991  loss_dice_dn_1: 0.866  loss_bbox_dn_1: 0.3765  loss_giou_dn_1: 0.714  loss_ce_2: 1.222  loss_mask_2: 0.2959  loss_dice_2: 0.7752  loss_bbox_2: 0.265  loss_giou_2: 0.6583  loss_ce_dn_2: 0.1964  loss_mask_dn_2: 0.3537  loss_dice_dn_2: 0.7765  loss_bbox_dn_2: 0.3382  loss_giou_dn_2: 0.6361  loss_ce_3: 0.9986  loss_mask_3: 0.292  loss_dice_3: 0.7631  loss_bbox_3: 0.2771  loss_giou_3: 0.6367  loss_ce_dn_3: 0.1751  loss_mask_dn_3: 0.3348  loss_dice_dn_3: 0.7219  loss_bbox_dn_3: 0.3115  loss_giou_dn_3: 0.6053  loss_ce_4: 0.9084  loss_mask_4: 0.2887  loss_dice_4: 0.75  loss_bbox_4: 0.2818  loss_giou_4: 0.629  loss_ce_dn_4: 0.1551  loss_mask_dn_4: 0.3312  loss_dice_dn_4: 0.7098  loss_bbox_dn_4: 0.3052  loss_giou_dn_4: 0.5908  loss_ce_5: 0.819  loss_mask_5: 0.261  loss_dice_5: 0.7695  loss_bbox_5: 0.2661  loss_giou_5: 0.6368  loss_ce_dn_5: 0.1381  loss_mask_dn_5: 0.3123  loss_dice_dn_5: 0.7043  loss_bbox_dn_5: 0.2963  loss_giou_dn_5: 0.5739  loss_ce_6: 0.771  loss_mask_6: 0.2682  loss_dice_6: 0.7393  loss_bbox_6: 0.2718  loss_giou_6: 0.6325  loss_ce_dn_6: 0.1331  loss_mask_dn_6: 0.3152  loss_dice_dn_6: 0.7224  loss_bbox_dn_6: 0.2864  loss_giou_dn_6: 0.5668  loss_ce_7: 0.7583  loss_mask_7: 0.2613  loss_dice_7: 0.7523  loss_bbox_7: 0.2525  loss_giou_7: 0.6237  loss_ce_dn_7: 0.127  loss_mask_dn_7: 0.3085  loss_dice_dn_7: 0.7044  loss_bbox_dn_7: 0.2853  loss_giou_dn_7: 0.5663  loss_ce_8: 0.7141  loss_mask_8: 0.277  loss_dice_8: 0.7972  loss_bbox_8: 0.2472  loss_giou_8: 0.6163  loss_ce_dn_8: 0.1299  loss_mask_dn_8: 0.3063  loss_dice_dn_8: 0.7154  loss_bbox_dn_8: 0.2839  loss_giou_dn_8: 0.5629  loss_ce_interm: 1.488  loss_mask_interm: 0.2703  loss_dice_interm: 0.8126  loss_bbox_interm: 0.4054  loss_giou_interm: 0.8487    time: 0.8738  last_time: 0.8822  data_time: 0.0126  last_data_time: 0.0171   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:13:39 d2.utils.events]: \u001b[0m eta: 3 days, 16:36:16  iter: 3039  total_loss: 62.68  loss_ce: 0.8177  loss_mask: 0.224  loss_dice: 1.219  loss_bbox: 0.2461  loss_giou: 0.6571  loss_ce_dn: 0.129  loss_mask_dn: 0.201  loss_dice_dn: 1.053  loss_bbox_dn: 0.2218  loss_giou_dn: 0.5218  loss_ce_0: 1.485  loss_mask_0: 0.2232  loss_dice_0: 1.249  loss_bbox_0: 0.3771  loss_giou_0: 0.9114  loss_ce_dn_0: 0.7343  loss_mask_dn_0: 0.6453  loss_dice_dn_0: 2.95  loss_bbox_dn_0: 0.4897  loss_giou_dn_0: 0.8633  loss_ce_1: 1.45  loss_mask_1: 0.2334  loss_dice_1: 1.205  loss_bbox_1: 0.2958  loss_giou_1: 0.7821  loss_ce_dn_1: 0.2404  loss_mask_dn_1: 0.2508  loss_dice_dn_1: 1.296  loss_bbox_dn_1: 0.2996  loss_giou_dn_1: 0.6418  loss_ce_2: 1.318  loss_mask_2: 0.2292  loss_dice_2: 1.195  loss_bbox_2: 0.2945  loss_giou_2: 0.7475  loss_ce_dn_2: 0.1929  loss_mask_dn_2: 0.2197  loss_dice_dn_2: 1.173  loss_bbox_dn_2: 0.263  loss_giou_dn_2: 0.5817  loss_ce_3: 1.093  loss_mask_3: 0.2143  loss_dice_3: 1.156  loss_bbox_3: 0.2741  loss_giou_3: 0.7166  loss_ce_dn_3: 0.1651  loss_mask_dn_3: 0.2142  loss_dice_dn_3: 1.12  loss_bbox_dn_3: 0.2471  loss_giou_dn_3: 0.5581  loss_ce_4: 1.012  loss_mask_4: 0.2212  loss_dice_4: 1.166  loss_bbox_4: 0.2549  loss_giou_4: 0.6974  loss_ce_dn_4: 0.1509  loss_mask_dn_4: 0.2109  loss_dice_dn_4: 1.104  loss_bbox_dn_4: 0.2345  loss_giou_dn_4: 0.5407  loss_ce_5: 0.9381  loss_mask_5: 0.2266  loss_dice_5: 1.168  loss_bbox_5: 0.2405  loss_giou_5: 0.7008  loss_ce_dn_5: 0.1508  loss_mask_dn_5: 0.2114  loss_dice_dn_5: 1.043  loss_bbox_dn_5: 0.231  loss_giou_dn_5: 0.5373  loss_ce_6: 0.8786  loss_mask_6: 0.2172  loss_dice_6: 1.14  loss_bbox_6: 0.2478  loss_giou_6: 0.6947  loss_ce_dn_6: 0.1384  loss_mask_dn_6: 0.2046  loss_dice_dn_6: 1.083  loss_bbox_dn_6: 0.2258  loss_giou_dn_6: 0.5258  loss_ce_7: 0.832  loss_mask_7: 0.2171  loss_dice_7: 1.231  loss_bbox_7: 0.2471  loss_giou_7: 0.6828  loss_ce_dn_7: 0.1379  loss_mask_dn_7: 0.2056  loss_dice_dn_7: 1.038  loss_bbox_dn_7: 0.224  loss_giou_dn_7: 0.5225  loss_ce_8: 0.8224  loss_mask_8: 0.2163  loss_dice_8: 1.132  loss_bbox_8: 0.2515  loss_giou_8: 0.6727  loss_ce_dn_8: 0.1302  loss_mask_dn_8: 0.1998  loss_dice_dn_8: 1.082  loss_bbox_dn_8: 0.2217  loss_giou_dn_8: 0.5221  loss_ce_interm: 1.464  loss_mask_interm: 0.2153  loss_dice_interm: 1.252  loss_bbox_interm: 0.3763  loss_giou_interm: 0.9235    time: 0.8738  last_time: 0.8616  data_time: 0.0131  last_data_time: 0.0118   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:13:56 d2.utils.events]: \u001b[0m eta: 3 days, 16:36:09  iter: 3059  total_loss: 57.66  loss_ce: 0.7071  loss_mask: 0.2342  loss_dice: 0.5578  loss_bbox: 0.2323  loss_giou: 0.5876  loss_ce_dn: 0.09971  loss_mask_dn: 0.2461  loss_dice_dn: 0.5944  loss_bbox_dn: 0.2341  loss_giou_dn: 0.5461  loss_ce_0: 1.526  loss_mask_0: 0.2625  loss_dice_0: 0.5654  loss_bbox_0: 0.4618  loss_giou_0: 0.815  loss_ce_dn_0: 0.6515  loss_mask_dn_0: 1.198  loss_dice_dn_0: 2.864  loss_bbox_dn_0: 0.7875  loss_giou_dn_0: 0.9774  loss_ce_1: 1.41  loss_mask_1: 0.2846  loss_dice_1: 0.5933  loss_bbox_1: 0.3358  loss_giou_1: 0.7036  loss_ce_dn_1: 0.2418  loss_mask_dn_1: 0.2819  loss_dice_dn_1: 0.6842  loss_bbox_dn_1: 0.4105  loss_giou_dn_1: 0.7099  loss_ce_2: 1.219  loss_mask_2: 0.272  loss_dice_2: 0.5784  loss_bbox_2: 0.301  loss_giou_2: 0.6813  loss_ce_dn_2: 0.199  loss_mask_dn_2: 0.2657  loss_dice_dn_2: 0.6134  loss_bbox_dn_2: 0.323  loss_giou_dn_2: 0.62  loss_ce_3: 1.044  loss_mask_3: 0.2193  loss_dice_3: 0.5554  loss_bbox_3: 0.2755  loss_giou_3: 0.6418  loss_ce_dn_3: 0.1619  loss_mask_dn_3: 0.2541  loss_dice_dn_3: 0.6051  loss_bbox_dn_3: 0.2874  loss_giou_dn_3: 0.5932  loss_ce_4: 0.944  loss_mask_4: 0.2199  loss_dice_4: 0.5956  loss_bbox_4: 0.2551  loss_giou_4: 0.6238  loss_ce_dn_4: 0.1419  loss_mask_dn_4: 0.2449  loss_dice_dn_4: 0.6071  loss_bbox_dn_4: 0.2589  loss_giou_dn_4: 0.5741  loss_ce_5: 0.8831  loss_mask_5: 0.2297  loss_dice_5: 0.5766  loss_bbox_5: 0.2573  loss_giou_5: 0.6102  loss_ce_dn_5: 0.1282  loss_mask_dn_5: 0.2505  loss_dice_dn_5: 0.5974  loss_bbox_dn_5: 0.2506  loss_giou_dn_5: 0.5622  loss_ce_6: 0.8179  loss_mask_6: 0.2341  loss_dice_6: 0.5771  loss_bbox_6: 0.2508  loss_giou_6: 0.5863  loss_ce_dn_6: 0.1091  loss_mask_dn_6: 0.2429  loss_dice_dn_6: 0.585  loss_bbox_dn_6: 0.2368  loss_giou_dn_6: 0.5509  loss_ce_7: 0.742  loss_mask_7: 0.2338  loss_dice_7: 0.58  loss_bbox_7: 0.2393  loss_giou_7: 0.5752  loss_ce_dn_7: 0.1032  loss_mask_dn_7: 0.2412  loss_dice_dn_7: 0.5862  loss_bbox_dn_7: 0.2334  loss_giou_dn_7: 0.5521  loss_ce_8: 0.7167  loss_mask_8: 0.2465  loss_dice_8: 0.5685  loss_bbox_8: 0.2424  loss_giou_8: 0.5703  loss_ce_dn_8: 0.1007  loss_mask_dn_8: 0.246  loss_dice_dn_8: 0.5991  loss_bbox_dn_8: 0.2331  loss_giou_dn_8: 0.5452  loss_ce_interm: 1.51  loss_mask_interm: 0.2651  loss_dice_interm: 0.583  loss_bbox_interm: 0.4723  loss_giou_interm: 0.8268    time: 0.8738  last_time: 0.8826  data_time: 0.0109  last_data_time: 0.0102   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:14:14 d2.utils.events]: \u001b[0m eta: 3 days, 16:36:22  iter: 3079  total_loss: 69.17  loss_ce: 0.8993  loss_mask: 0.2443  loss_dice: 1.009  loss_bbox: 0.2824  loss_giou: 0.6864  loss_ce_dn: 0.1264  loss_mask_dn: 0.2567  loss_dice_dn: 0.9135  loss_bbox_dn: 0.2358  loss_giou_dn: 0.5075  loss_ce_0: 1.54  loss_mask_0: 0.2613  loss_dice_0: 1.161  loss_bbox_0: 0.4317  loss_giou_0: 0.9269  loss_ce_dn_0: 0.7265  loss_mask_dn_0: 0.968  loss_dice_dn_0: 2.846  loss_bbox_dn_0: 0.6571  loss_giou_dn_0: 0.8718  loss_ce_1: 1.407  loss_mask_1: 0.2792  loss_dice_1: 1.146  loss_bbox_1: 0.3488  loss_giou_1: 0.828  loss_ce_dn_1: 0.2241  loss_mask_dn_1: 0.3223  loss_dice_dn_1: 1.182  loss_bbox_dn_1: 0.3727  loss_giou_dn_1: 0.6296  loss_ce_2: 1.269  loss_mask_2: 0.2527  loss_dice_2: 1.108  loss_bbox_2: 0.3422  loss_giou_2: 0.7435  loss_ce_dn_2: 0.1838  loss_mask_dn_2: 0.2877  loss_dice_dn_2: 1.035  loss_bbox_dn_2: 0.3005  loss_giou_dn_2: 0.5525  loss_ce_3: 1.13  loss_mask_3: 0.2594  loss_dice_3: 1.151  loss_bbox_3: 0.3203  loss_giou_3: 0.7401  loss_ce_dn_3: 0.1618  loss_mask_dn_3: 0.2654  loss_dice_dn_3: 0.9618  loss_bbox_dn_3: 0.2708  loss_giou_dn_3: 0.5323  loss_ce_4: 1.008  loss_mask_4: 0.2456  loss_dice_4: 1.098  loss_bbox_4: 0.3092  loss_giou_4: 0.6888  loss_ce_dn_4: 0.1497  loss_mask_dn_4: 0.2713  loss_dice_dn_4: 0.9655  loss_bbox_dn_4: 0.2573  loss_giou_dn_4: 0.5142  loss_ce_5: 0.9717  loss_mask_5: 0.2527  loss_dice_5: 0.9983  loss_bbox_5: 0.2807  loss_giou_5: 0.6993  loss_ce_dn_5: 0.1352  loss_mask_dn_5: 0.2633  loss_dice_dn_5: 0.9184  loss_bbox_dn_5: 0.2471  loss_giou_dn_5: 0.507  loss_ce_6: 0.9802  loss_mask_6: 0.2481  loss_dice_6: 1.066  loss_bbox_6: 0.3116  loss_giou_6: 0.7169  loss_ce_dn_6: 0.1261  loss_mask_dn_6: 0.2632  loss_dice_dn_6: 0.9654  loss_bbox_dn_6: 0.2386  loss_giou_dn_6: 0.5027  loss_ce_7: 0.9086  loss_mask_7: 0.2447  loss_dice_7: 1.051  loss_bbox_7: 0.2835  loss_giou_7: 0.716  loss_ce_dn_7: 0.1308  loss_mask_dn_7: 0.2628  loss_dice_dn_7: 0.9477  loss_bbox_dn_7: 0.2338  loss_giou_dn_7: 0.5092  loss_ce_8: 0.8999  loss_mask_8: 0.2414  loss_dice_8: 1.091  loss_bbox_8: 0.286  loss_giou_8: 0.683  loss_ce_dn_8: 0.1269  loss_mask_dn_8: 0.2605  loss_dice_dn_8: 0.9096  loss_bbox_dn_8: 0.2358  loss_giou_dn_8: 0.5091  loss_ce_interm: 1.5  loss_mask_interm: 0.257  loss_dice_interm: 1.164  loss_bbox_interm: 0.432  loss_giou_interm: 0.9092    time: 0.8738  last_time: 0.8551  data_time: 0.0132  last_data_time: 0.0102   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:14:31 d2.utils.events]: \u001b[0m eta: 3 days, 16:36:32  iter: 3099  total_loss: 63.3  loss_ce: 0.8622  loss_mask: 0.3178  loss_dice: 0.8415  loss_bbox: 0.2411  loss_giou: 0.7435  loss_ce_dn: 0.1459  loss_mask_dn: 0.3013  loss_dice_dn: 0.8525  loss_bbox_dn: 0.2263  loss_giou_dn: 0.6465  loss_ce_0: 1.497  loss_mask_0: 0.3218  loss_dice_0: 0.9393  loss_bbox_0: 0.4397  loss_giou_0: 0.9582  loss_ce_dn_0: 0.8182  loss_mask_dn_0: 1.024  loss_dice_dn_0: 3.097  loss_bbox_dn_0: 0.6617  loss_giou_dn_0: 0.9826  loss_ce_1: 1.408  loss_mask_1: 0.3151  loss_dice_1: 0.9047  loss_bbox_1: 0.3087  loss_giou_1: 0.8475  loss_ce_dn_1: 0.2848  loss_mask_dn_1: 0.3573  loss_dice_dn_1: 0.9964  loss_bbox_dn_1: 0.3543  loss_giou_dn_1: 0.7547  loss_ce_2: 1.206  loss_mask_2: 0.3122  loss_dice_2: 0.9006  loss_bbox_2: 0.2696  loss_giou_2: 0.8027  loss_ce_dn_2: 0.2226  loss_mask_dn_2: 0.3175  loss_dice_dn_2: 0.9082  loss_bbox_dn_2: 0.2834  loss_giou_dn_2: 0.7252  loss_ce_3: 1.03  loss_mask_3: 0.3259  loss_dice_3: 0.7991  loss_bbox_3: 0.2547  loss_giou_3: 0.7673  loss_ce_dn_3: 0.1891  loss_mask_dn_3: 0.3033  loss_dice_dn_3: 0.8921  loss_bbox_dn_3: 0.2655  loss_giou_dn_3: 0.6795  loss_ce_4: 0.9912  loss_mask_4: 0.3399  loss_dice_4: 0.8164  loss_bbox_4: 0.2704  loss_giou_4: 0.7599  loss_ce_dn_4: 0.1756  loss_mask_dn_4: 0.3015  loss_dice_dn_4: 0.8447  loss_bbox_dn_4: 0.2487  loss_giou_dn_4: 0.6605  loss_ce_5: 0.8668  loss_mask_5: 0.3257  loss_dice_5: 0.8652  loss_bbox_5: 0.2566  loss_giou_5: 0.7588  loss_ce_dn_5: 0.1666  loss_mask_dn_5: 0.3002  loss_dice_dn_5: 0.8665  loss_bbox_dn_5: 0.2387  loss_giou_dn_5: 0.656  loss_ce_6: 0.8878  loss_mask_6: 0.3228  loss_dice_6: 0.838  loss_bbox_6: 0.2605  loss_giou_6: 0.7497  loss_ce_dn_6: 0.1526  loss_mask_dn_6: 0.3039  loss_dice_dn_6: 0.8794  loss_bbox_dn_6: 0.2304  loss_giou_dn_6: 0.6525  loss_ce_7: 0.8896  loss_mask_7: 0.3213  loss_dice_7: 0.8393  loss_bbox_7: 0.2429  loss_giou_7: 0.7415  loss_ce_dn_7: 0.15  loss_mask_dn_7: 0.3016  loss_dice_dn_7: 0.8764  loss_bbox_dn_7: 0.2289  loss_giou_dn_7: 0.6512  loss_ce_8: 0.8857  loss_mask_8: 0.3259  loss_dice_8: 0.8425  loss_bbox_8: 0.2396  loss_giou_8: 0.7306  loss_ce_dn_8: 0.146  loss_mask_dn_8: 0.3057  loss_dice_dn_8: 0.8612  loss_bbox_dn_8: 0.225  loss_giou_dn_8: 0.6468  loss_ce_interm: 1.56  loss_mask_interm: 0.3245  loss_dice_interm: 0.8319  loss_bbox_interm: 0.4422  loss_giou_interm: 0.9667    time: 0.8738  last_time: 0.8834  data_time: 0.0135  last_data_time: 0.0097   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:14:49 d2.utils.events]: \u001b[0m eta: 3 days, 16:36:49  iter: 3119  total_loss: 57.6  loss_ce: 0.6707  loss_mask: 0.2573  loss_dice: 0.6181  loss_bbox: 0.2247  loss_giou: 0.6486  loss_ce_dn: 0.1178  loss_mask_dn: 0.2478  loss_dice_dn: 0.6464  loss_bbox_dn: 0.2113  loss_giou_dn: 0.5042  loss_ce_0: 1.396  loss_mask_0: 0.2576  loss_dice_0: 0.6538  loss_bbox_0: 0.3662  loss_giou_0: 0.8655  loss_ce_dn_0: 0.7146  loss_mask_dn_0: 0.8026  loss_dice_dn_0: 2.59  loss_bbox_dn_0: 0.5961  loss_giou_dn_0: 0.9508  loss_ce_1: 1.276  loss_mask_1: 0.2303  loss_dice_1: 0.6515  loss_bbox_1: 0.3234  loss_giou_1: 0.7545  loss_ce_dn_1: 0.2528  loss_mask_dn_1: 0.2753  loss_dice_dn_1: 0.7663  loss_bbox_dn_1: 0.3283  loss_giou_dn_1: 0.6997  loss_ce_2: 1.076  loss_mask_2: 0.2425  loss_dice_2: 0.6358  loss_bbox_2: 0.2379  loss_giou_2: 0.7208  loss_ce_dn_2: 0.1924  loss_mask_dn_2: 0.2568  loss_dice_dn_2: 0.7539  loss_bbox_dn_2: 0.2694  loss_giou_dn_2: 0.5976  loss_ce_3: 0.9214  loss_mask_3: 0.2557  loss_dice_3: 0.6268  loss_bbox_3: 0.2306  loss_giou_3: 0.6968  loss_ce_dn_3: 0.1618  loss_mask_dn_3: 0.2538  loss_dice_dn_3: 0.6888  loss_bbox_dn_3: 0.2337  loss_giou_dn_3: 0.5685  loss_ce_4: 0.8178  loss_mask_4: 0.2442  loss_dice_4: 0.6353  loss_bbox_4: 0.2265  loss_giou_4: 0.6756  loss_ce_dn_4: 0.149  loss_mask_dn_4: 0.2492  loss_dice_dn_4: 0.6472  loss_bbox_dn_4: 0.2211  loss_giou_dn_4: 0.5351  loss_ce_5: 0.7697  loss_mask_5: 0.2325  loss_dice_5: 0.6298  loss_bbox_5: 0.2193  loss_giou_5: 0.6353  loss_ce_dn_5: 0.1372  loss_mask_dn_5: 0.2434  loss_dice_dn_5: 0.648  loss_bbox_dn_5: 0.2144  loss_giou_dn_5: 0.5195  loss_ce_6: 0.6768  loss_mask_6: 0.2412  loss_dice_6: 0.5997  loss_bbox_6: 0.2246  loss_giou_6: 0.6442  loss_ce_dn_6: 0.133  loss_mask_dn_6: 0.2536  loss_dice_dn_6: 0.648  loss_bbox_dn_6: 0.213  loss_giou_dn_6: 0.5108  loss_ce_7: 0.671  loss_mask_7: 0.249  loss_dice_7: 0.6348  loss_bbox_7: 0.2257  loss_giou_7: 0.6792  loss_ce_dn_7: 0.1197  loss_mask_dn_7: 0.2468  loss_dice_dn_7: 0.6396  loss_bbox_dn_7: 0.2111  loss_giou_dn_7: 0.5097  loss_ce_8: 0.6616  loss_mask_8: 0.2558  loss_dice_8: 0.6249  loss_bbox_8: 0.2272  loss_giou_8: 0.633  loss_ce_dn_8: 0.1205  loss_mask_dn_8: 0.2495  loss_dice_dn_8: 0.6617  loss_bbox_dn_8: 0.2103  loss_giou_dn_8: 0.5031  loss_ce_interm: 1.411  loss_mask_interm: 0.2615  loss_dice_interm: 0.6773  loss_bbox_interm: 0.3753  loss_giou_interm: 0.8562    time: 0.8739  last_time: 0.8642  data_time: 0.0137  last_data_time: 0.0115   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:15:06 d2.utils.events]: \u001b[0m eta: 3 days, 16:35:14  iter: 3139  total_loss: 56.02  loss_ce: 0.7172  loss_mask: 0.1887  loss_dice: 0.6607  loss_bbox: 0.2066  loss_giou: 0.8665  loss_ce_dn: 0.1377  loss_mask_dn: 0.1836  loss_dice_dn: 0.6067  loss_bbox_dn: 0.196  loss_giou_dn: 0.654  loss_ce_0: 1.396  loss_mask_0: 0.2083  loss_dice_0: 0.7232  loss_bbox_0: 0.3295  loss_giou_0: 1.031  loss_ce_dn_0: 0.7526  loss_mask_dn_0: 0.7131  loss_dice_dn_0: 2.904  loss_bbox_dn_0: 0.525  loss_giou_dn_0: 0.9437  loss_ce_1: 1.298  loss_mask_1: 0.2133  loss_dice_1: 0.6232  loss_bbox_1: 0.229  loss_giou_1: 0.9323  loss_ce_dn_1: 0.2706  loss_mask_dn_1: 0.2864  loss_dice_dn_1: 0.7709  loss_bbox_dn_1: 0.3129  loss_giou_dn_1: 0.7348  loss_ce_2: 1.13  loss_mask_2: 0.2201  loss_dice_2: 0.6309  loss_bbox_2: 0.1877  loss_giou_2: 0.9226  loss_ce_dn_2: 0.235  loss_mask_dn_2: 0.2138  loss_dice_dn_2: 0.683  loss_bbox_dn_2: 0.2493  loss_giou_dn_2: 0.7005  loss_ce_3: 1.018  loss_mask_3: 0.204  loss_dice_3: 0.6501  loss_bbox_3: 0.2035  loss_giou_3: 0.9005  loss_ce_dn_3: 0.1903  loss_mask_dn_3: 0.1974  loss_dice_dn_3: 0.6491  loss_bbox_dn_3: 0.2256  loss_giou_dn_3: 0.6825  loss_ce_4: 0.9093  loss_mask_4: 0.1928  loss_dice_4: 0.6244  loss_bbox_4: 0.1776  loss_giou_4: 0.8815  loss_ce_dn_4: 0.1891  loss_mask_dn_4: 0.189  loss_dice_dn_4: 0.6234  loss_bbox_dn_4: 0.2096  loss_giou_dn_4: 0.6699  loss_ce_5: 0.7956  loss_mask_5: 0.2008  loss_dice_5: 0.6785  loss_bbox_5: 0.205  loss_giou_5: 0.8759  loss_ce_dn_5: 0.1714  loss_mask_dn_5: 0.1926  loss_dice_dn_5: 0.6151  loss_bbox_dn_5: 0.2032  loss_giou_dn_5: 0.6667  loss_ce_6: 0.8079  loss_mask_6: 0.2011  loss_dice_6: 0.6946  loss_bbox_6: 0.2069  loss_giou_6: 0.85  loss_ce_dn_6: 0.1565  loss_mask_dn_6: 0.1899  loss_dice_dn_6: 0.6266  loss_bbox_dn_6: 0.1955  loss_giou_dn_6: 0.656  loss_ce_7: 0.7743  loss_mask_7: 0.1933  loss_dice_7: 0.632  loss_bbox_7: 0.2013  loss_giou_7: 0.8771  loss_ce_dn_7: 0.1453  loss_mask_dn_7: 0.1915  loss_dice_dn_7: 0.6275  loss_bbox_dn_7: 0.1971  loss_giou_dn_7: 0.6571  loss_ce_8: 0.7343  loss_mask_8: 0.1944  loss_dice_8: 0.7099  loss_bbox_8: 0.1971  loss_giou_8: 0.8605  loss_ce_dn_8: 0.1362  loss_mask_dn_8: 0.1831  loss_dice_dn_8: 0.6177  loss_bbox_dn_8: 0.1961  loss_giou_dn_8: 0.6532  loss_ce_interm: 1.383  loss_mask_interm: 0.2146  loss_dice_interm: 0.6951  loss_bbox_interm: 0.3302  loss_giou_interm: 1.031    time: 0.8738  last_time: 0.8811  data_time: 0.0117  last_data_time: 0.0096   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:15:24 d2.utils.events]: \u001b[0m eta: 3 days, 16:37:21  iter: 3159  total_loss: 61.93  loss_ce: 0.829  loss_mask: 0.2895  loss_dice: 0.8561  loss_bbox: 0.2867  loss_giou: 0.6856  loss_ce_dn: 0.1516  loss_mask_dn: 0.2279  loss_dice_dn: 0.7432  loss_bbox_dn: 0.2294  loss_giou_dn: 0.5361  loss_ce_0: 1.565  loss_mask_0: 0.2738  loss_dice_0: 0.8521  loss_bbox_0: 0.3785  loss_giou_0: 0.8677  loss_ce_dn_0: 0.7275  loss_mask_dn_0: 0.9097  loss_dice_dn_0: 3.16  loss_bbox_dn_0: 0.6029  loss_giou_dn_0: 0.8756  loss_ce_1: 1.468  loss_mask_1: 0.2767  loss_dice_1: 0.8702  loss_bbox_1: 0.3273  loss_giou_1: 0.7738  loss_ce_dn_1: 0.2722  loss_mask_dn_1: 0.3  loss_dice_dn_1: 0.8905  loss_bbox_dn_1: 0.3184  loss_giou_dn_1: 0.6744  loss_ce_2: 1.174  loss_mask_2: 0.248  loss_dice_2: 0.7575  loss_bbox_2: 0.3078  loss_giou_2: 0.692  loss_ce_dn_2: 0.2354  loss_mask_dn_2: 0.2562  loss_dice_dn_2: 0.8008  loss_bbox_dn_2: 0.2596  loss_giou_dn_2: 0.5944  loss_ce_3: 1.036  loss_mask_3: 0.2736  loss_dice_3: 0.7728  loss_bbox_3: 0.3081  loss_giou_3: 0.7101  loss_ce_dn_3: 0.2158  loss_mask_dn_3: 0.2454  loss_dice_dn_3: 0.7992  loss_bbox_dn_3: 0.2447  loss_giou_dn_3: 0.5779  loss_ce_4: 0.9148  loss_mask_4: 0.2486  loss_dice_4: 0.7942  loss_bbox_4: 0.2925  loss_giou_4: 0.6676  loss_ce_dn_4: 0.1834  loss_mask_dn_4: 0.2371  loss_dice_dn_4: 0.8067  loss_bbox_dn_4: 0.2392  loss_giou_dn_4: 0.5502  loss_ce_5: 0.8399  loss_mask_5: 0.2635  loss_dice_5: 0.8568  loss_bbox_5: 0.2996  loss_giou_5: 0.6587  loss_ce_dn_5: 0.1736  loss_mask_dn_5: 0.2308  loss_dice_dn_5: 0.7524  loss_bbox_dn_5: 0.2357  loss_giou_dn_5: 0.5491  loss_ce_6: 0.8402  loss_mask_6: 0.256  loss_dice_6: 0.8573  loss_bbox_6: 0.3101  loss_giou_6: 0.6527  loss_ce_dn_6: 0.1664  loss_mask_dn_6: 0.232  loss_dice_dn_6: 0.7492  loss_bbox_dn_6: 0.229  loss_giou_dn_6: 0.5442  loss_ce_7: 0.8092  loss_mask_7: 0.273  loss_dice_7: 0.8682  loss_bbox_7: 0.292  loss_giou_7: 0.6649  loss_ce_dn_7: 0.1603  loss_mask_dn_7: 0.2307  loss_dice_dn_7: 0.7603  loss_bbox_dn_7: 0.2266  loss_giou_dn_7: 0.5382  loss_ce_8: 0.7955  loss_mask_8: 0.2757  loss_dice_8: 0.8432  loss_bbox_8: 0.2923  loss_giou_8: 0.697  loss_ce_dn_8: 0.153  loss_mask_dn_8: 0.2335  loss_dice_dn_8: 0.7346  loss_bbox_dn_8: 0.2281  loss_giou_dn_8: 0.5353  loss_ce_interm: 1.526  loss_mask_interm: 0.2652  loss_dice_interm: 0.8833  loss_bbox_interm: 0.384  loss_giou_interm: 0.8806    time: 0.8739  last_time: 0.8715  data_time: 0.0128  last_data_time: 0.0155   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:15:42 d2.utils.events]: \u001b[0m eta: 3 days, 16:36:27  iter: 3179  total_loss: 64.26  loss_ce: 0.7573  loss_mask: 0.3135  loss_dice: 0.7066  loss_bbox: 0.2974  loss_giou: 0.673  loss_ce_dn: 0.1464  loss_mask_dn: 0.2848  loss_dice_dn: 0.8041  loss_bbox_dn: 0.2605  loss_giou_dn: 0.5777  loss_ce_0: 1.503  loss_mask_0: 0.34  loss_dice_0: 0.8737  loss_bbox_0: 0.4925  loss_giou_0: 0.8901  loss_ce_dn_0: 0.7461  loss_mask_dn_0: 0.8046  loss_dice_dn_0: 3.233  loss_bbox_dn_0: 0.6966  loss_giou_dn_0: 0.9375  loss_ce_1: 1.449  loss_mask_1: 0.3438  loss_dice_1: 0.767  loss_bbox_1: 0.3707  loss_giou_1: 0.7836  loss_ce_dn_1: 0.257  loss_mask_dn_1: 0.3751  loss_dice_dn_1: 0.9752  loss_bbox_dn_1: 0.3883  loss_giou_dn_1: 0.7145  loss_ce_2: 1.296  loss_mask_2: 0.2999  loss_dice_2: 0.7821  loss_bbox_2: 0.3464  loss_giou_2: 0.7414  loss_ce_dn_2: 0.2173  loss_mask_dn_2: 0.3171  loss_dice_dn_2: 0.8926  loss_bbox_dn_2: 0.3012  loss_giou_dn_2: 0.6472  loss_ce_3: 1.077  loss_mask_3: 0.3033  loss_dice_3: 0.7024  loss_bbox_3: 0.3461  loss_giou_3: 0.6997  loss_ce_dn_3: 0.1916  loss_mask_dn_3: 0.2914  loss_dice_dn_3: 0.8508  loss_bbox_dn_3: 0.2795  loss_giou_dn_3: 0.6136  loss_ce_4: 0.9692  loss_mask_4: 0.324  loss_dice_4: 0.717  loss_bbox_4: 0.3338  loss_giou_4: 0.7056  loss_ce_dn_4: 0.1856  loss_mask_dn_4: 0.2933  loss_dice_dn_4: 0.8396  loss_bbox_dn_4: 0.261  loss_giou_dn_4: 0.5962  loss_ce_5: 0.8698  loss_mask_5: 0.3035  loss_dice_5: 0.7091  loss_bbox_5: 0.3308  loss_giou_5: 0.6976  loss_ce_dn_5: 0.1629  loss_mask_dn_5: 0.2893  loss_dice_dn_5: 0.8229  loss_bbox_dn_5: 0.2583  loss_giou_dn_5: 0.5908  loss_ce_6: 0.8289  loss_mask_6: 0.3211  loss_dice_6: 0.7128  loss_bbox_6: 0.3256  loss_giou_6: 0.6772  loss_ce_dn_6: 0.1429  loss_mask_dn_6: 0.2885  loss_dice_dn_6: 0.8271  loss_bbox_dn_6: 0.26  loss_giou_dn_6: 0.5862  loss_ce_7: 0.7922  loss_mask_7: 0.3024  loss_dice_7: 0.744  loss_bbox_7: 0.3113  loss_giou_7: 0.6851  loss_ce_dn_7: 0.1471  loss_mask_dn_7: 0.2898  loss_dice_dn_7: 0.7917  loss_bbox_dn_7: 0.257  loss_giou_dn_7: 0.5841  loss_ce_8: 0.7581  loss_mask_8: 0.31  loss_dice_8: 0.7123  loss_bbox_8: 0.3057  loss_giou_8: 0.6739  loss_ce_dn_8: 0.1408  loss_mask_dn_8: 0.2828  loss_dice_dn_8: 0.7979  loss_bbox_dn_8: 0.2596  loss_giou_dn_8: 0.5784  loss_ce_interm: 1.505  loss_mask_interm: 0.3379  loss_dice_interm: 0.8717  loss_bbox_interm: 0.4919  loss_giou_interm: 0.8802    time: 0.8739  last_time: 0.8605  data_time: 0.0124  last_data_time: 0.0092   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:15:59 d2.utils.events]: \u001b[0m eta: 3 days, 16:37:00  iter: 3199  total_loss: 66.95  loss_ce: 0.7621  loss_mask: 0.2181  loss_dice: 0.9543  loss_bbox: 0.2516  loss_giou: 0.7522  loss_ce_dn: 0.14  loss_mask_dn: 0.2158  loss_dice_dn: 0.8875  loss_bbox_dn: 0.2221  loss_giou_dn: 0.5758  loss_ce_0: 1.443  loss_mask_0: 0.2206  loss_dice_0: 0.9346  loss_bbox_0: 0.4023  loss_giou_0: 0.9231  loss_ce_dn_0: 0.7549  loss_mask_dn_0: 0.8918  loss_dice_dn_0: 2.964  loss_bbox_dn_0: 0.5959  loss_giou_dn_0: 0.9505  loss_ce_1: 1.412  loss_mask_1: 0.2151  loss_dice_1: 0.9488  loss_bbox_1: 0.3069  loss_giou_1: 0.8417  loss_ce_dn_1: 0.2654  loss_mask_dn_1: 0.2517  loss_dice_dn_1: 0.9339  loss_bbox_dn_1: 0.3202  loss_giou_dn_1: 0.7246  loss_ce_2: 1.263  loss_mask_2: 0.2259  loss_dice_2: 0.9798  loss_bbox_2: 0.2642  loss_giou_2: 0.7797  loss_ce_dn_2: 0.2125  loss_mask_dn_2: 0.255  loss_dice_dn_2: 0.9107  loss_bbox_dn_2: 0.2608  loss_giou_dn_2: 0.6512  loss_ce_3: 1.243  loss_mask_3: 0.2225  loss_dice_3: 0.9831  loss_bbox_3: 0.2644  loss_giou_3: 0.7668  loss_ce_dn_3: 0.193  loss_mask_dn_3: 0.2464  loss_dice_dn_3: 0.9216  loss_bbox_dn_3: 0.2429  loss_giou_dn_3: 0.6208  loss_ce_4: 1.055  loss_mask_4: 0.2228  loss_dice_4: 0.9674  loss_bbox_4: 0.2681  loss_giou_4: 0.7882  loss_ce_dn_4: 0.173  loss_mask_dn_4: 0.2287  loss_dice_dn_4: 0.9014  loss_bbox_dn_4: 0.2319  loss_giou_dn_4: 0.6027  loss_ce_5: 0.9425  loss_mask_5: 0.2236  loss_dice_5: 0.9407  loss_bbox_5: 0.2612  loss_giou_5: 0.7766  loss_ce_dn_5: 0.1585  loss_mask_dn_5: 0.2205  loss_dice_dn_5: 0.8956  loss_bbox_dn_5: 0.2255  loss_giou_dn_5: 0.5934  loss_ce_6: 0.8856  loss_mask_6: 0.2154  loss_dice_6: 0.9646  loss_bbox_6: 0.2608  loss_giou_6: 0.7612  loss_ce_dn_6: 0.1463  loss_mask_dn_6: 0.2182  loss_dice_dn_6: 0.8481  loss_bbox_dn_6: 0.2234  loss_giou_dn_6: 0.5813  loss_ce_7: 0.7994  loss_mask_7: 0.2176  loss_dice_7: 0.8896  loss_bbox_7: 0.258  loss_giou_7: 0.745  loss_ce_dn_7: 0.1423  loss_mask_dn_7: 0.2166  loss_dice_dn_7: 0.8614  loss_bbox_dn_7: 0.2217  loss_giou_dn_7: 0.5811  loss_ce_8: 0.7881  loss_mask_8: 0.2159  loss_dice_8: 0.8833  loss_bbox_8: 0.2548  loss_giou_8: 0.7646  loss_ce_dn_8: 0.1427  loss_mask_dn_8: 0.214  loss_dice_dn_8: 0.8879  loss_bbox_dn_8: 0.222  loss_giou_dn_8: 0.5764  loss_ce_interm: 1.455  loss_mask_interm: 0.2233  loss_dice_interm: 0.9693  loss_bbox_interm: 0.4217  loss_giou_interm: 0.928    time: 0.8740  last_time: 0.8875  data_time: 0.0132  last_data_time: 0.0107   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:16:17 d2.utils.events]: \u001b[0m eta: 3 days, 16:36:53  iter: 3219  total_loss: 63.17  loss_ce: 0.823  loss_mask: 0.2804  loss_dice: 0.8614  loss_bbox: 0.2016  loss_giou: 0.658  loss_ce_dn: 0.1553  loss_mask_dn: 0.2685  loss_dice_dn: 0.7533  loss_bbox_dn: 0.2069  loss_giou_dn: 0.5169  loss_ce_0: 1.547  loss_mask_0: 0.2945  loss_dice_0: 0.8145  loss_bbox_0: 0.431  loss_giou_0: 0.859  loss_ce_dn_0: 0.7661  loss_mask_dn_0: 0.9269  loss_dice_dn_0: 3.23  loss_bbox_dn_0: 0.6005  loss_giou_dn_0: 0.9494  loss_ce_1: 1.368  loss_mask_1: 0.275  loss_dice_1: 0.7769  loss_bbox_1: 0.3162  loss_giou_1: 0.7746  loss_ce_dn_1: 0.2762  loss_mask_dn_1: 0.3225  loss_dice_dn_1: 0.8734  loss_bbox_dn_1: 0.3363  loss_giou_dn_1: 0.7018  loss_ce_2: 1.114  loss_mask_2: 0.271  loss_dice_2: 0.7628  loss_bbox_2: 0.2765  loss_giou_2: 0.7175  loss_ce_dn_2: 0.2204  loss_mask_dn_2: 0.2864  loss_dice_dn_2: 0.8332  loss_bbox_dn_2: 0.2764  loss_giou_dn_2: 0.606  loss_ce_3: 0.9737  loss_mask_3: 0.2744  loss_dice_3: 0.8021  loss_bbox_3: 0.2689  loss_giou_3: 0.6951  loss_ce_dn_3: 0.1881  loss_mask_dn_3: 0.2811  loss_dice_dn_3: 0.8089  loss_bbox_dn_3: 0.2373  loss_giou_dn_3: 0.5706  loss_ce_4: 0.9143  loss_mask_4: 0.277  loss_dice_4: 0.7773  loss_bbox_4: 0.2511  loss_giou_4: 0.6803  loss_ce_dn_4: 0.1689  loss_mask_dn_4: 0.2879  loss_dice_dn_4: 0.7504  loss_bbox_dn_4: 0.2156  loss_giou_dn_4: 0.5434  loss_ce_5: 0.8483  loss_mask_5: 0.2749  loss_dice_5: 0.8135  loss_bbox_5: 0.2267  loss_giou_5: 0.6774  loss_ce_dn_5: 0.1565  loss_mask_dn_5: 0.2799  loss_dice_dn_5: 0.7635  loss_bbox_dn_5: 0.2138  loss_giou_dn_5: 0.538  loss_ce_6: 0.825  loss_mask_6: 0.274  loss_dice_6: 0.8354  loss_bbox_6: 0.2129  loss_giou_6: 0.6711  loss_ce_dn_6: 0.1531  loss_mask_dn_6: 0.274  loss_dice_dn_6: 0.7781  loss_bbox_dn_6: 0.2092  loss_giou_dn_6: 0.5227  loss_ce_7: 0.8491  loss_mask_7: 0.2773  loss_dice_7: 0.757  loss_bbox_7: 0.2174  loss_giou_7: 0.6701  loss_ce_dn_7: 0.1535  loss_mask_dn_7: 0.2633  loss_dice_dn_7: 0.7498  loss_bbox_dn_7: 0.2092  loss_giou_dn_7: 0.5202  loss_ce_8: 0.8205  loss_mask_8: 0.2715  loss_dice_8: 0.8494  loss_bbox_8: 0.2319  loss_giou_8: 0.6594  loss_ce_dn_8: 0.1526  loss_mask_dn_8: 0.2707  loss_dice_dn_8: 0.7458  loss_bbox_dn_8: 0.2057  loss_giou_dn_8: 0.5165  loss_ce_interm: 1.545  loss_mask_interm: 0.2836  loss_dice_interm: 0.7351  loss_bbox_interm: 0.4213  loss_giou_interm: 0.8773    time: 0.8740  last_time: 0.8692  data_time: 0.0119  last_data_time: 0.0120   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:16:35 d2.utils.events]: \u001b[0m eta: 3 days, 16:37:51  iter: 3239  total_loss: 63.29  loss_ce: 0.822  loss_mask: 0.2081  loss_dice: 0.7112  loss_bbox: 0.3243  loss_giou: 0.8176  loss_ce_dn: 0.1386  loss_mask_dn: 0.2044  loss_dice_dn: 0.7738  loss_bbox_dn: 0.2244  loss_giou_dn: 0.7882  loss_ce_0: 1.382  loss_mask_0: 0.2086  loss_dice_0: 0.7313  loss_bbox_0: 0.3808  loss_giou_0: 0.9355  loss_ce_dn_0: 0.7566  loss_mask_dn_0: 0.6646  loss_dice_dn_0: 2.636  loss_bbox_dn_0: 0.5464  loss_giou_dn_0: 1.122  loss_ce_1: 1.337  loss_mask_1: 0.2091  loss_dice_1: 0.6782  loss_bbox_1: 0.2758  loss_giou_1: 0.8744  loss_ce_dn_1: 0.3105  loss_mask_dn_1: 0.2354  loss_dice_dn_1: 0.8878  loss_bbox_dn_1: 0.3121  loss_giou_dn_1: 0.9203  loss_ce_2: 1.113  loss_mask_2: 0.2085  loss_dice_2: 0.6693  loss_bbox_2: 0.3143  loss_giou_2: 0.8519  loss_ce_dn_2: 0.2564  loss_mask_dn_2: 0.2099  loss_dice_dn_2: 0.8469  loss_bbox_dn_2: 0.2672  loss_giou_dn_2: 0.842  loss_ce_3: 1.052  loss_mask_3: 0.2324  loss_dice_3: 0.6821  loss_bbox_3: 0.3172  loss_giou_3: 0.8267  loss_ce_dn_3: 0.2281  loss_mask_dn_3: 0.2134  loss_dice_dn_3: 0.8281  loss_bbox_dn_3: 0.2452  loss_giou_dn_3: 0.8261  loss_ce_4: 0.9728  loss_mask_4: 0.2114  loss_dice_4: 0.7141  loss_bbox_4: 0.3012  loss_giou_4: 0.8162  loss_ce_dn_4: 0.2249  loss_mask_dn_4: 0.2021  loss_dice_dn_4: 0.8172  loss_bbox_dn_4: 0.2358  loss_giou_dn_4: 0.8036  loss_ce_5: 0.9066  loss_mask_5: 0.2091  loss_dice_5: 0.7192  loss_bbox_5: 0.293  loss_giou_5: 0.8354  loss_ce_dn_5: 0.1804  loss_mask_dn_5: 0.1994  loss_dice_dn_5: 0.81  loss_bbox_dn_5: 0.2343  loss_giou_dn_5: 0.8041  loss_ce_6: 0.8716  loss_mask_6: 0.2013  loss_dice_6: 0.6461  loss_bbox_6: 0.3006  loss_giou_6: 0.8279  loss_ce_dn_6: 0.1642  loss_mask_dn_6: 0.1976  loss_dice_dn_6: 0.7749  loss_bbox_dn_6: 0.2298  loss_giou_dn_6: 0.8013  loss_ce_7: 0.8378  loss_mask_7: 0.2046  loss_dice_7: 0.6721  loss_bbox_7: 0.2923  loss_giou_7: 0.8278  loss_ce_dn_7: 0.1625  loss_mask_dn_7: 0.202  loss_dice_dn_7: 0.7632  loss_bbox_dn_7: 0.2233  loss_giou_dn_7: 0.7959  loss_ce_8: 0.8187  loss_mask_8: 0.2069  loss_dice_8: 0.6675  loss_bbox_8: 0.3215  loss_giou_8: 0.8175  loss_ce_dn_8: 0.144  loss_mask_dn_8: 0.2043  loss_dice_dn_8: 0.7645  loss_bbox_dn_8: 0.2236  loss_giou_dn_8: 0.7888  loss_ce_interm: 1.388  loss_mask_interm: 0.211  loss_dice_interm: 0.7285  loss_bbox_interm: 0.3802  loss_giou_interm: 0.9347    time: 0.8740  last_time: 0.8915  data_time: 0.0125  last_data_time: 0.0250   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:16:52 d2.utils.events]: \u001b[0m eta: 3 days, 16:36:18  iter: 3259  total_loss: 65.53  loss_ce: 0.8107  loss_mask: 0.2652  loss_dice: 0.8744  loss_bbox: 0.3355  loss_giou: 0.6952  loss_ce_dn: 0.1217  loss_mask_dn: 0.3096  loss_dice_dn: 0.8202  loss_bbox_dn: 0.2617  loss_giou_dn: 0.5583  loss_ce_0: 1.567  loss_mask_0: 0.3392  loss_dice_0: 0.9093  loss_bbox_0: 0.4384  loss_giou_0: 0.9015  loss_ce_dn_0: 0.7346  loss_mask_dn_0: 0.8388  loss_dice_dn_0: 2.669  loss_bbox_dn_0: 0.7196  loss_giou_dn_0: 0.9655  loss_ce_1: 1.471  loss_mask_1: 0.2643  loss_dice_1: 0.8833  loss_bbox_1: 0.3349  loss_giou_1: 0.8108  loss_ce_dn_1: 0.259  loss_mask_dn_1: 0.3534  loss_dice_dn_1: 0.9565  loss_bbox_dn_1: 0.3948  loss_giou_dn_1: 0.6725  loss_ce_2: 1.254  loss_mask_2: 0.2853  loss_dice_2: 0.902  loss_bbox_2: 0.3318  loss_giou_2: 0.7542  loss_ce_dn_2: 0.2067  loss_mask_dn_2: 0.3394  loss_dice_dn_2: 0.8962  loss_bbox_dn_2: 0.3245  loss_giou_dn_2: 0.5992  loss_ce_3: 1.097  loss_mask_3: 0.2657  loss_dice_3: 0.7673  loss_bbox_3: 0.3197  loss_giou_3: 0.722  loss_ce_dn_3: 0.1677  loss_mask_dn_3: 0.3235  loss_dice_dn_3: 0.8411  loss_bbox_dn_3: 0.2963  loss_giou_dn_3: 0.5912  loss_ce_4: 0.9214  loss_mask_4: 0.2548  loss_dice_4: 0.7421  loss_bbox_4: 0.3484  loss_giou_4: 0.7159  loss_ce_dn_4: 0.1519  loss_mask_dn_4: 0.3168  loss_dice_dn_4: 0.8392  loss_bbox_dn_4: 0.2678  loss_giou_dn_4: 0.5737  loss_ce_5: 0.8532  loss_mask_5: 0.2524  loss_dice_5: 0.8204  loss_bbox_5: 0.3662  loss_giou_5: 0.7244  loss_ce_dn_5: 0.137  loss_mask_dn_5: 0.3109  loss_dice_dn_5: 0.818  loss_bbox_dn_5: 0.27  loss_giou_dn_5: 0.5692  loss_ce_6: 0.8261  loss_mask_6: 0.2592  loss_dice_6: 0.759  loss_bbox_6: 0.3674  loss_giou_6: 0.7066  loss_ce_dn_6: 0.1283  loss_mask_dn_6: 0.3009  loss_dice_dn_6: 0.8068  loss_bbox_dn_6: 0.2668  loss_giou_dn_6: 0.563  loss_ce_7: 0.845  loss_mask_7: 0.2679  loss_dice_7: 0.8306  loss_bbox_7: 0.3237  loss_giou_7: 0.6969  loss_ce_dn_7: 0.1259  loss_mask_dn_7: 0.3  loss_dice_dn_7: 0.8103  loss_bbox_dn_7: 0.2664  loss_giou_dn_7: 0.5642  loss_ce_8: 0.8177  loss_mask_8: 0.2537  loss_dice_8: 0.8342  loss_bbox_8: 0.3433  loss_giou_8: 0.6944  loss_ce_dn_8: 0.1233  loss_mask_dn_8: 0.3028  loss_dice_dn_8: 0.8103  loss_bbox_dn_8: 0.2625  loss_giou_dn_8: 0.5582  loss_ce_interm: 1.582  loss_mask_interm: 0.3411  loss_dice_interm: 0.8851  loss_bbox_interm: 0.4384  loss_giou_interm: 0.9225    time: 0.8739  last_time: 0.8529  data_time: 0.0116  last_data_time: 0.0126   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:17:10 d2.utils.events]: \u001b[0m eta: 3 days, 16:36:36  iter: 3279  total_loss: 64.16  loss_ce: 0.7927  loss_mask: 0.3353  loss_dice: 0.6161  loss_bbox: 0.3203  loss_giou: 0.6892  loss_ce_dn: 0.1532  loss_mask_dn: 0.3474  loss_dice_dn: 0.5701  loss_bbox_dn: 0.287  loss_giou_dn: 0.5991  loss_ce_0: 1.485  loss_mask_0: 0.3722  loss_dice_0: 0.633  loss_bbox_0: 0.4229  loss_giou_0: 0.8978  loss_ce_dn_0: 0.6861  loss_mask_dn_0: 0.9534  loss_dice_dn_0: 3.012  loss_bbox_dn_0: 0.7456  loss_giou_dn_0: 1.005  loss_ce_1: 1.398  loss_mask_1: 0.3432  loss_dice_1: 0.6125  loss_bbox_1: 0.3795  loss_giou_1: 0.8152  loss_ce_dn_1: 0.2476  loss_mask_dn_1: 0.4222  loss_dice_dn_1: 0.6837  loss_bbox_dn_1: 0.4368  loss_giou_dn_1: 0.7399  loss_ce_2: 1.218  loss_mask_2: 0.3422  loss_dice_2: 0.5947  loss_bbox_2: 0.3272  loss_giou_2: 0.7926  loss_ce_dn_2: 0.2029  loss_mask_dn_2: 0.3891  loss_dice_dn_2: 0.6258  loss_bbox_dn_2: 0.3626  loss_giou_dn_2: 0.6717  loss_ce_3: 1.065  loss_mask_3: 0.3337  loss_dice_3: 0.578  loss_bbox_3: 0.3249  loss_giou_3: 0.7569  loss_ce_dn_3: 0.1841  loss_mask_dn_3: 0.384  loss_dice_dn_3: 0.608  loss_bbox_dn_3: 0.3254  loss_giou_dn_3: 0.643  loss_ce_4: 0.9514  loss_mask_4: 0.3638  loss_dice_4: 0.6163  loss_bbox_4: 0.3238  loss_giou_4: 0.7072  loss_ce_dn_4: 0.1733  loss_mask_dn_4: 0.3746  loss_dice_dn_4: 0.596  loss_bbox_dn_4: 0.3117  loss_giou_dn_4: 0.6206  loss_ce_5: 0.8472  loss_mask_5: 0.346  loss_dice_5: 0.5706  loss_bbox_5: 0.3418  loss_giou_5: 0.7539  loss_ce_dn_5: 0.1691  loss_mask_dn_5: 0.3669  loss_dice_dn_5: 0.578  loss_bbox_dn_5: 0.3066  loss_giou_dn_5: 0.6151  loss_ce_6: 0.815  loss_mask_6: 0.3492  loss_dice_6: 0.5774  loss_bbox_6: 0.3268  loss_giou_6: 0.7142  loss_ce_dn_6: 0.1647  loss_mask_dn_6: 0.3661  loss_dice_dn_6: 0.5724  loss_bbox_dn_6: 0.2957  loss_giou_dn_6: 0.6009  loss_ce_7: 0.8429  loss_mask_7: 0.3475  loss_dice_7: 0.6061  loss_bbox_7: 0.3212  loss_giou_7: 0.6991  loss_ce_dn_7: 0.1548  loss_mask_dn_7: 0.3566  loss_dice_dn_7: 0.5705  loss_bbox_dn_7: 0.2895  loss_giou_dn_7: 0.5981  loss_ce_8: 0.7982  loss_mask_8: 0.3378  loss_dice_8: 0.5601  loss_bbox_8: 0.32  loss_giou_8: 0.6958  loss_ce_dn_8: 0.1573  loss_mask_dn_8: 0.3546  loss_dice_dn_8: 0.5751  loss_bbox_dn_8: 0.2868  loss_giou_dn_8: 0.5996  loss_ce_interm: 1.475  loss_mask_interm: 0.3691  loss_dice_interm: 0.6263  loss_bbox_interm: 0.4229  loss_giou_interm: 0.9069    time: 0.8740  last_time: 0.8915  data_time: 0.0116  last_data_time: 0.0117   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:17:27 d2.utils.events]: \u001b[0m eta: 3 days, 16:35:43  iter: 3299  total_loss: 65.65  loss_ce: 0.8871  loss_mask: 0.2833  loss_dice: 0.7528  loss_bbox: 0.2738  loss_giou: 0.6653  loss_ce_dn: 0.149  loss_mask_dn: 0.3057  loss_dice_dn: 0.7033  loss_bbox_dn: 0.2468  loss_giou_dn: 0.5662  loss_ce_0: 1.566  loss_mask_0: 0.2796  loss_dice_0: 0.8174  loss_bbox_0: 0.4262  loss_giou_0: 0.8914  loss_ce_dn_0: 0.7193  loss_mask_dn_0: 0.9528  loss_dice_dn_0: 2.979  loss_bbox_dn_0: 0.6327  loss_giou_dn_0: 0.9383  loss_ce_1: 1.458  loss_mask_1: 0.2905  loss_dice_1: 0.721  loss_bbox_1: 0.3688  loss_giou_1: 0.8001  loss_ce_dn_1: 0.2732  loss_mask_dn_1: 0.3576  loss_dice_dn_1: 0.8334  loss_bbox_dn_1: 0.3648  loss_giou_dn_1: 0.6838  loss_ce_2: 1.279  loss_mask_2: 0.3014  loss_dice_2: 0.7191  loss_bbox_2: 0.3132  loss_giou_2: 0.7311  loss_ce_dn_2: 0.2397  loss_mask_dn_2: 0.3383  loss_dice_dn_2: 0.7633  loss_bbox_dn_2: 0.3045  loss_giou_dn_2: 0.6402  loss_ce_3: 1.142  loss_mask_3: 0.2871  loss_dice_3: 0.729  loss_bbox_3: 0.2883  loss_giou_3: 0.7026  loss_ce_dn_3: 0.2025  loss_mask_dn_3: 0.3285  loss_dice_dn_3: 0.732  loss_bbox_dn_3: 0.2823  loss_giou_dn_3: 0.6076  loss_ce_4: 1.054  loss_mask_4: 0.3015  loss_dice_4: 0.7104  loss_bbox_4: 0.2828  loss_giou_4: 0.6898  loss_ce_dn_4: 0.1849  loss_mask_dn_4: 0.3224  loss_dice_dn_4: 0.7215  loss_bbox_dn_4: 0.2635  loss_giou_dn_4: 0.5978  loss_ce_5: 0.9714  loss_mask_5: 0.2998  loss_dice_5: 0.7473  loss_bbox_5: 0.2882  loss_giou_5: 0.7034  loss_ce_dn_5: 0.1672  loss_mask_dn_5: 0.3128  loss_dice_dn_5: 0.7121  loss_bbox_dn_5: 0.2594  loss_giou_dn_5: 0.5867  loss_ce_6: 1.003  loss_mask_6: 0.2838  loss_dice_6: 0.7038  loss_bbox_6: 0.2669  loss_giou_6: 0.6683  loss_ce_dn_6: 0.1613  loss_mask_dn_6: 0.3016  loss_dice_dn_6: 0.7131  loss_bbox_dn_6: 0.2539  loss_giou_dn_6: 0.5687  loss_ce_7: 0.9556  loss_mask_7: 0.2861  loss_dice_7: 0.7125  loss_bbox_7: 0.2784  loss_giou_7: 0.6658  loss_ce_dn_7: 0.153  loss_mask_dn_7: 0.3013  loss_dice_dn_7: 0.7103  loss_bbox_dn_7: 0.2479  loss_giou_dn_7: 0.5663  loss_ce_8: 0.9297  loss_mask_8: 0.283  loss_dice_8: 0.7367  loss_bbox_8: 0.2864  loss_giou_8: 0.6709  loss_ce_dn_8: 0.1469  loss_mask_dn_8: 0.3064  loss_dice_dn_8: 0.7177  loss_bbox_dn_8: 0.2475  loss_giou_dn_8: 0.5666  loss_ce_interm: 1.514  loss_mask_interm: 0.2954  loss_dice_interm: 0.7911  loss_bbox_interm: 0.4135  loss_giou_interm: 0.8992    time: 0.8739  last_time: 0.8734  data_time: 0.0122  last_data_time: 0.0111   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:17:45 d2.utils.events]: \u001b[0m eta: 3 days, 16:37:57  iter: 3319  total_loss: 65.93  loss_ce: 0.8715  loss_mask: 0.2491  loss_dice: 0.8578  loss_bbox: 0.3484  loss_giou: 0.8339  loss_ce_dn: 0.1215  loss_mask_dn: 0.2251  loss_dice_dn: 0.8475  loss_bbox_dn: 0.2488  loss_giou_dn: 0.6722  loss_ce_0: 1.502  loss_mask_0: 0.2395  loss_dice_0: 0.842  loss_bbox_0: 0.5135  loss_giou_0: 0.9805  loss_ce_dn_0: 0.6961  loss_mask_dn_0: 0.9344  loss_dice_dn_0: 3.266  loss_bbox_dn_0: 0.6551  loss_giou_dn_0: 0.9811  loss_ce_1: 1.386  loss_mask_1: 0.2442  loss_dice_1: 0.891  loss_bbox_1: 0.3512  loss_giou_1: 0.8613  loss_ce_dn_1: 0.2373  loss_mask_dn_1: 0.2804  loss_dice_dn_1: 0.9972  loss_bbox_dn_1: 0.3554  loss_giou_dn_1: 0.7651  loss_ce_2: 1.235  loss_mask_2: 0.2321  loss_dice_2: 0.8238  loss_bbox_2: 0.3445  loss_giou_2: 0.8621  loss_ce_dn_2: 0.1909  loss_mask_dn_2: 0.2621  loss_dice_dn_2: 0.9357  loss_bbox_dn_2: 0.3027  loss_giou_dn_2: 0.7223  loss_ce_3: 1.129  loss_mask_3: 0.241  loss_dice_3: 0.8516  loss_bbox_3: 0.3359  loss_giou_3: 0.8658  loss_ce_dn_3: 0.1635  loss_mask_dn_3: 0.2393  loss_dice_dn_3: 0.8985  loss_bbox_dn_3: 0.2871  loss_giou_dn_3: 0.7053  loss_ce_4: 1.034  loss_mask_4: 0.2532  loss_dice_4: 0.9181  loss_bbox_4: 0.342  loss_giou_4: 0.8667  loss_ce_dn_4: 0.1474  loss_mask_dn_4: 0.2296  loss_dice_dn_4: 0.8885  loss_bbox_dn_4: 0.2709  loss_giou_dn_4: 0.6872  loss_ce_5: 0.9658  loss_mask_5: 0.2616  loss_dice_5: 0.8459  loss_bbox_5: 0.3636  loss_giou_5: 0.8661  loss_ce_dn_5: 0.1341  loss_mask_dn_5: 0.224  loss_dice_dn_5: 0.8659  loss_bbox_dn_5: 0.2579  loss_giou_dn_5: 0.6816  loss_ce_6: 0.8991  loss_mask_6: 0.2555  loss_dice_6: 0.873  loss_bbox_6: 0.3419  loss_giou_6: 0.8312  loss_ce_dn_6: 0.1262  loss_mask_dn_6: 0.2223  loss_dice_dn_6: 0.8655  loss_bbox_dn_6: 0.2494  loss_giou_dn_6: 0.6743  loss_ce_7: 0.8804  loss_mask_7: 0.2505  loss_dice_7: 0.894  loss_bbox_7: 0.3371  loss_giou_7: 0.8369  loss_ce_dn_7: 0.1206  loss_mask_dn_7: 0.22  loss_dice_dn_7: 0.8749  loss_bbox_dn_7: 0.2511  loss_giou_dn_7: 0.6727  loss_ce_8: 0.8823  loss_mask_8: 0.2572  loss_dice_8: 0.8577  loss_bbox_8: 0.3432  loss_giou_8: 0.8313  loss_ce_dn_8: 0.1196  loss_mask_dn_8: 0.2272  loss_dice_dn_8: 0.8495  loss_bbox_dn_8: 0.2481  loss_giou_dn_8: 0.6716  loss_ce_interm: 1.494  loss_mask_interm: 0.2414  loss_dice_interm: 0.9262  loss_bbox_interm: 0.5073  loss_giou_interm: 0.9785    time: 0.8740  last_time: 0.8943  data_time: 0.0128  last_data_time: 0.0129   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:18:02 d2.utils.events]: \u001b[0m eta: 3 days, 16:38:59  iter: 3339  total_loss: 65.46  loss_ce: 0.8295  loss_mask: 0.2849  loss_dice: 0.89  loss_bbox: 0.3194  loss_giou: 0.7052  loss_ce_dn: 0.1157  loss_mask_dn: 0.2705  loss_dice_dn: 0.821  loss_bbox_dn: 0.3135  loss_giou_dn: 0.5997  loss_ce_0: 1.431  loss_mask_0: 0.3315  loss_dice_0: 0.8524  loss_bbox_0: 0.4685  loss_giou_0: 0.8902  loss_ce_dn_0: 0.7476  loss_mask_dn_0: 0.9818  loss_dice_dn_0: 2.97  loss_bbox_dn_0: 0.7787  loss_giou_dn_0: 0.9482  loss_ce_1: 1.398  loss_mask_1: 0.2876  loss_dice_1: 0.8534  loss_bbox_1: 0.344  loss_giou_1: 0.7741  loss_ce_dn_1: 0.2639  loss_mask_dn_1: 0.3642  loss_dice_dn_1: 0.9876  loss_bbox_dn_1: 0.4438  loss_giou_dn_1: 0.7134  loss_ce_2: 1.232  loss_mask_2: 0.2707  loss_dice_2: 0.8629  loss_bbox_2: 0.3237  loss_giou_2: 0.7336  loss_ce_dn_2: 0.2088  loss_mask_dn_2: 0.3305  loss_dice_dn_2: 0.9044  loss_bbox_dn_2: 0.3793  loss_giou_dn_2: 0.6605  loss_ce_3: 1.037  loss_mask_3: 0.2885  loss_dice_3: 0.8263  loss_bbox_3: 0.336  loss_giou_3: 0.7313  loss_ce_dn_3: 0.1763  loss_mask_dn_3: 0.2823  loss_dice_dn_3: 0.8406  loss_bbox_dn_3: 0.3504  loss_giou_dn_3: 0.6352  loss_ce_4: 0.9638  loss_mask_4: 0.2895  loss_dice_4: 0.8914  loss_bbox_4: 0.3135  loss_giou_4: 0.7193  loss_ce_dn_4: 0.1567  loss_mask_dn_4: 0.2819  loss_dice_dn_4: 0.8147  loss_bbox_dn_4: 0.3318  loss_giou_dn_4: 0.6073  loss_ce_5: 0.9061  loss_mask_5: 0.2861  loss_dice_5: 0.887  loss_bbox_5: 0.3156  loss_giou_5: 0.7096  loss_ce_dn_5: 0.141  loss_mask_dn_5: 0.2722  loss_dice_dn_5: 0.8115  loss_bbox_dn_5: 0.3222  loss_giou_dn_5: 0.6057  loss_ce_6: 0.878  loss_mask_6: 0.2886  loss_dice_6: 0.8593  loss_bbox_6: 0.31  loss_giou_6: 0.713  loss_ce_dn_6: 0.1344  loss_mask_dn_6: 0.2739  loss_dice_dn_6: 0.8207  loss_bbox_dn_6: 0.3221  loss_giou_dn_6: 0.5977  loss_ce_7: 0.8379  loss_mask_7: 0.292  loss_dice_7: 0.8946  loss_bbox_7: 0.3344  loss_giou_7: 0.7118  loss_ce_dn_7: 0.1247  loss_mask_dn_7: 0.2729  loss_dice_dn_7: 0.8151  loss_bbox_dn_7: 0.3211  loss_giou_dn_7: 0.6003  loss_ce_8: 0.8519  loss_mask_8: 0.2782  loss_dice_8: 0.8767  loss_bbox_8: 0.3245  loss_giou_8: 0.7099  loss_ce_dn_8: 0.1218  loss_mask_dn_8: 0.2705  loss_dice_dn_8: 0.8264  loss_bbox_dn_8: 0.3135  loss_giou_dn_8: 0.5987  loss_ce_interm: 1.431  loss_mask_interm: 0.3284  loss_dice_interm: 0.8506  loss_bbox_interm: 0.4691  loss_giou_interm: 0.884    time: 0.8741  last_time: 0.8682  data_time: 0.0122  last_data_time: 0.0148   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:18:20 d2.utils.events]: \u001b[0m eta: 3 days, 16:38:00  iter: 3359  total_loss: 63.89  loss_ce: 0.6989  loss_mask: 0.2158  loss_dice: 0.7549  loss_bbox: 0.2249  loss_giou: 0.7082  loss_ce_dn: 0.1237  loss_mask_dn: 0.2394  loss_dice_dn: 0.7494  loss_bbox_dn: 0.2157  loss_giou_dn: 0.5087  loss_ce_0: 1.402  loss_mask_0: 0.2523  loss_dice_0: 0.8175  loss_bbox_0: 0.361  loss_giou_0: 0.8755  loss_ce_dn_0: 0.7142  loss_mask_dn_0: 0.8541  loss_dice_dn_0: 3.049  loss_bbox_dn_0: 0.6787  loss_giou_dn_0: 0.9196  loss_ce_1: 1.39  loss_mask_1: 0.2576  loss_dice_1: 0.7986  loss_bbox_1: 0.2485  loss_giou_1: 0.8002  loss_ce_dn_1: 0.2692  loss_mask_dn_1: 0.3118  loss_dice_dn_1: 0.9031  loss_bbox_dn_1: 0.3472  loss_giou_dn_1: 0.6481  loss_ce_2: 1.208  loss_mask_2: 0.2452  loss_dice_2: 0.8146  loss_bbox_2: 0.2486  loss_giou_2: 0.7371  loss_ce_dn_2: 0.2107  loss_mask_dn_2: 0.2621  loss_dice_dn_2: 0.832  loss_bbox_dn_2: 0.2654  loss_giou_dn_2: 0.5835  loss_ce_3: 0.9412  loss_mask_3: 0.246  loss_dice_3: 0.7931  loss_bbox_3: 0.2749  loss_giou_3: 0.7499  loss_ce_dn_3: 0.1833  loss_mask_dn_3: 0.2506  loss_dice_dn_3: 0.7865  loss_bbox_dn_3: 0.2436  loss_giou_dn_3: 0.5595  loss_ce_4: 0.8344  loss_mask_4: 0.2371  loss_dice_4: 0.805  loss_bbox_4: 0.2391  loss_giou_4: 0.7321  loss_ce_dn_4: 0.1597  loss_mask_dn_4: 0.2511  loss_dice_dn_4: 0.7725  loss_bbox_dn_4: 0.2313  loss_giou_dn_4: 0.537  loss_ce_5: 0.7716  loss_mask_5: 0.2175  loss_dice_5: 0.7896  loss_bbox_5: 0.2393  loss_giou_5: 0.7089  loss_ce_dn_5: 0.1416  loss_mask_dn_5: 0.2449  loss_dice_dn_5: 0.7426  loss_bbox_dn_5: 0.2266  loss_giou_dn_5: 0.5311  loss_ce_6: 0.7258  loss_mask_6: 0.2261  loss_dice_6: 0.8184  loss_bbox_6: 0.2337  loss_giou_6: 0.7026  loss_ce_dn_6: 0.1327  loss_mask_dn_6: 0.2362  loss_dice_dn_6: 0.7358  loss_bbox_dn_6: 0.2199  loss_giou_dn_6: 0.519  loss_ce_7: 0.7001  loss_mask_7: 0.2155  loss_dice_7: 0.7498  loss_bbox_7: 0.2277  loss_giou_7: 0.7097  loss_ce_dn_7: 0.1296  loss_mask_dn_7: 0.2369  loss_dice_dn_7: 0.7596  loss_bbox_dn_7: 0.2186  loss_giou_dn_7: 0.5148  loss_ce_8: 0.6833  loss_mask_8: 0.2201  loss_dice_8: 0.7685  loss_bbox_8: 0.2242  loss_giou_8: 0.6954  loss_ce_dn_8: 0.1306  loss_mask_dn_8: 0.2369  loss_dice_dn_8: 0.7712  loss_bbox_dn_8: 0.216  loss_giou_dn_8: 0.5093  loss_ce_interm: 1.402  loss_mask_interm: 0.2554  loss_dice_interm: 0.8126  loss_bbox_interm: 0.362  loss_giou_interm: 0.8787    time: 0.8740  last_time: 0.8646  data_time: 0.0112  last_data_time: 0.0102   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:18:37 d2.utils.events]: \u001b[0m eta: 3 days, 16:39:13  iter: 3379  total_loss: 62.78  loss_ce: 0.8272  loss_mask: 0.2534  loss_dice: 0.6963  loss_bbox: 0.2486  loss_giou: 0.578  loss_ce_dn: 0.1347  loss_mask_dn: 0.2545  loss_dice_dn: 0.6828  loss_bbox_dn: 0.232  loss_giou_dn: 0.5143  loss_ce_0: 1.566  loss_mask_0: 0.2711  loss_dice_0: 0.7681  loss_bbox_0: 0.4229  loss_giou_0: 0.8819  loss_ce_dn_0: 0.7338  loss_mask_dn_0: 1.033  loss_dice_dn_0: 3.175  loss_bbox_dn_0: 0.7422  loss_giou_dn_0: 0.9159  loss_ce_1: 1.404  loss_mask_1: 0.2934  loss_dice_1: 0.7033  loss_bbox_1: 0.3397  loss_giou_1: 0.6739  loss_ce_dn_1: 0.2653  loss_mask_dn_1: 0.3141  loss_dice_dn_1: 0.8425  loss_bbox_dn_1: 0.4183  loss_giou_dn_1: 0.6275  loss_ce_2: 1.251  loss_mask_2: 0.2678  loss_dice_2: 0.7047  loss_bbox_2: 0.2821  loss_giou_2: 0.6654  loss_ce_dn_2: 0.2082  loss_mask_dn_2: 0.2853  loss_dice_dn_2: 0.792  loss_bbox_dn_2: 0.3386  loss_giou_dn_2: 0.5683  loss_ce_3: 1.058  loss_mask_3: 0.2504  loss_dice_3: 0.7408  loss_bbox_3: 0.314  loss_giou_3: 0.6393  loss_ce_dn_3: 0.1809  loss_mask_dn_3: 0.2735  loss_dice_dn_3: 0.7389  loss_bbox_dn_3: 0.2837  loss_giou_dn_3: 0.548  loss_ce_4: 0.9992  loss_mask_4: 0.2492  loss_dice_4: 0.7379  loss_bbox_4: 0.2552  loss_giou_4: 0.6232  loss_ce_dn_4: 0.168  loss_mask_dn_4: 0.2595  loss_dice_dn_4: 0.722  loss_bbox_dn_4: 0.2619  loss_giou_dn_4: 0.5336  loss_ce_5: 0.9117  loss_mask_5: 0.2493  loss_dice_5: 0.7078  loss_bbox_5: 0.2743  loss_giou_5: 0.6255  loss_ce_dn_5: 0.1563  loss_mask_dn_5: 0.2681  loss_dice_dn_5: 0.715  loss_bbox_dn_5: 0.2467  loss_giou_dn_5: 0.5292  loss_ce_6: 0.9347  loss_mask_6: 0.2492  loss_dice_6: 0.6982  loss_bbox_6: 0.2597  loss_giou_6: 0.5829  loss_ce_dn_6: 0.1378  loss_mask_dn_6: 0.2525  loss_dice_dn_6: 0.708  loss_bbox_dn_6: 0.2379  loss_giou_dn_6: 0.5091  loss_ce_7: 0.8917  loss_mask_7: 0.2569  loss_dice_7: 0.7055  loss_bbox_7: 0.26  loss_giou_7: 0.5758  loss_ce_dn_7: 0.1388  loss_mask_dn_7: 0.2536  loss_dice_dn_7: 0.6841  loss_bbox_dn_7: 0.2389  loss_giou_dn_7: 0.5134  loss_ce_8: 0.8576  loss_mask_8: 0.2587  loss_dice_8: 0.6677  loss_bbox_8: 0.2611  loss_giou_8: 0.5857  loss_ce_dn_8: 0.1365  loss_mask_dn_8: 0.2501  loss_dice_dn_8: 0.6796  loss_bbox_dn_8: 0.2316  loss_giou_dn_8: 0.5125  loss_ce_interm: 1.567  loss_mask_interm: 0.2684  loss_dice_interm: 0.7326  loss_bbox_interm: 0.4223  loss_giou_interm: 0.8801    time: 0.8740  last_time: 0.8748  data_time: 0.0121  last_data_time: 0.0111   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:18:55 d2.utils.events]: \u001b[0m eta: 3 days, 16:39:39  iter: 3399  total_loss: 63.71  loss_ce: 0.7748  loss_mask: 0.2837  loss_dice: 0.8009  loss_bbox: 0.3353  loss_giou: 0.656  loss_ce_dn: 0.1651  loss_mask_dn: 0.2944  loss_dice_dn: 0.7849  loss_bbox_dn: 0.2793  loss_giou_dn: 0.5481  loss_ce_0: 1.436  loss_mask_0: 0.3063  loss_dice_0: 0.9551  loss_bbox_0: 0.454  loss_giou_0: 0.862  loss_ce_dn_0: 0.7989  loss_mask_dn_0: 0.8839  loss_dice_dn_0: 2.865  loss_bbox_dn_0: 0.6685  loss_giou_dn_0: 0.9074  loss_ce_1: 1.393  loss_mask_1: 0.2936  loss_dice_1: 0.9018  loss_bbox_1: 0.3888  loss_giou_1: 0.751  loss_ce_dn_1: 0.2806  loss_mask_dn_1: 0.3828  loss_dice_dn_1: 0.8799  loss_bbox_dn_1: 0.3906  loss_giou_dn_1: 0.6452  loss_ce_2: 1.209  loss_mask_2: 0.2884  loss_dice_2: 0.8983  loss_bbox_2: 0.3581  loss_giou_2: 0.7266  loss_ce_dn_2: 0.2329  loss_mask_dn_2: 0.3552  loss_dice_dn_2: 0.8245  loss_bbox_dn_2: 0.3164  loss_giou_dn_2: 0.6031  loss_ce_3: 1.044  loss_mask_3: 0.279  loss_dice_3: 0.8519  loss_bbox_3: 0.3574  loss_giou_3: 0.6645  loss_ce_dn_3: 0.2064  loss_mask_dn_3: 0.3306  loss_dice_dn_3: 0.7927  loss_bbox_dn_3: 0.3046  loss_giou_dn_3: 0.5805  loss_ce_4: 0.9016  loss_mask_4: 0.268  loss_dice_4: 0.8098  loss_bbox_4: 0.3681  loss_giou_4: 0.6585  loss_ce_dn_4: 0.1864  loss_mask_dn_4: 0.3189  loss_dice_dn_4: 0.7918  loss_bbox_dn_4: 0.2904  loss_giou_dn_4: 0.5626  loss_ce_5: 0.8263  loss_mask_5: 0.2701  loss_dice_5: 0.8092  loss_bbox_5: 0.3801  loss_giou_5: 0.6525  loss_ce_dn_5: 0.1696  loss_mask_dn_5: 0.3067  loss_dice_dn_5: 0.7992  loss_bbox_dn_5: 0.2839  loss_giou_dn_5: 0.5575  loss_ce_6: 0.7925  loss_mask_6: 0.2631  loss_dice_6: 0.8146  loss_bbox_6: 0.3783  loss_giou_6: 0.6647  loss_ce_dn_6: 0.1652  loss_mask_dn_6: 0.3063  loss_dice_dn_6: 0.7594  loss_bbox_dn_6: 0.2819  loss_giou_dn_6: 0.5485  loss_ce_7: 0.8012  loss_mask_7: 0.2662  loss_dice_7: 0.807  loss_bbox_7: 0.3788  loss_giou_7: 0.6603  loss_ce_dn_7: 0.1609  loss_mask_dn_7: 0.3036  loss_dice_dn_7: 0.7579  loss_bbox_dn_7: 0.2814  loss_giou_dn_7: 0.5478  loss_ce_8: 0.7933  loss_mask_8: 0.278  loss_dice_8: 0.7766  loss_bbox_8: 0.3628  loss_giou_8: 0.6693  loss_ce_dn_8: 0.1592  loss_mask_dn_8: 0.3059  loss_dice_dn_8: 0.7732  loss_bbox_dn_8: 0.279  loss_giou_dn_8: 0.5464  loss_ce_interm: 1.425  loss_mask_interm: 0.3094  loss_dice_interm: 0.9483  loss_bbox_interm: 0.4468  loss_giou_interm: 0.8617    time: 0.8740  last_time: 0.8538  data_time: 0.0128  last_data_time: 0.0092   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:19:13 d2.utils.events]: \u001b[0m eta: 3 days, 16:40:46  iter: 3419  total_loss: 63.3  loss_ce: 0.8271  loss_mask: 0.227  loss_dice: 0.8188  loss_bbox: 0.2348  loss_giou: 0.7219  loss_ce_dn: 0.12  loss_mask_dn: 0.2346  loss_dice_dn: 0.7638  loss_bbox_dn: 0.2049  loss_giou_dn: 0.5835  loss_ce_0: 1.464  loss_mask_0: 0.2272  loss_dice_0: 0.8442  loss_bbox_0: 0.3834  loss_giou_0: 0.9354  loss_ce_dn_0: 0.7336  loss_mask_dn_0: 0.7338  loss_dice_dn_0: 2.871  loss_bbox_dn_0: 0.5639  loss_giou_dn_0: 0.9507  loss_ce_1: 1.398  loss_mask_1: 0.2419  loss_dice_1: 0.8548  loss_bbox_1: 0.3115  loss_giou_1: 0.8142  loss_ce_dn_1: 0.2625  loss_mask_dn_1: 0.2953  loss_dice_dn_1: 0.8784  loss_bbox_dn_1: 0.3199  loss_giou_dn_1: 0.7143  loss_ce_2: 1.251  loss_mask_2: 0.2414  loss_dice_2: 0.8138  loss_bbox_2: 0.305  loss_giou_2: 0.7854  loss_ce_dn_2: 0.2149  loss_mask_dn_2: 0.2687  loss_dice_dn_2: 0.823  loss_bbox_dn_2: 0.2719  loss_giou_dn_2: 0.6319  loss_ce_3: 1.044  loss_mask_3: 0.2389  loss_dice_3: 0.7604  loss_bbox_3: 0.2597  loss_giou_3: 0.7968  loss_ce_dn_3: 0.1797  loss_mask_dn_3: 0.2515  loss_dice_dn_3: 0.8082  loss_bbox_dn_3: 0.2338  loss_giou_dn_3: 0.6096  loss_ce_4: 0.9269  loss_mask_4: 0.2454  loss_dice_4: 0.8238  loss_bbox_4: 0.2604  loss_giou_4: 0.7802  loss_ce_dn_4: 0.1696  loss_mask_dn_4: 0.2511  loss_dice_dn_4: 0.7799  loss_bbox_dn_4: 0.2181  loss_giou_dn_4: 0.5963  loss_ce_5: 0.8817  loss_mask_5: 0.2424  loss_dice_5: 0.8118  loss_bbox_5: 0.2408  loss_giou_5: 0.7531  loss_ce_dn_5: 0.1495  loss_mask_dn_5: 0.253  loss_dice_dn_5: 0.8071  loss_bbox_dn_5: 0.2143  loss_giou_dn_5: 0.5931  loss_ce_6: 0.8262  loss_mask_6: 0.2348  loss_dice_6: 0.7564  loss_bbox_6: 0.257  loss_giou_6: 0.7474  loss_ce_dn_6: 0.1339  loss_mask_dn_6: 0.2464  loss_dice_dn_6: 0.7705  loss_bbox_dn_6: 0.2074  loss_giou_dn_6: 0.585  loss_ce_7: 0.7965  loss_mask_7: 0.2339  loss_dice_7: 0.7619  loss_bbox_7: 0.2428  loss_giou_7: 0.7281  loss_ce_dn_7: 0.1291  loss_mask_dn_7: 0.2415  loss_dice_dn_7: 0.7874  loss_bbox_dn_7: 0.207  loss_giou_dn_7: 0.5844  loss_ce_8: 0.8137  loss_mask_8: 0.2273  loss_dice_8: 0.8118  loss_bbox_8: 0.2445  loss_giou_8: 0.7362  loss_ce_dn_8: 0.1209  loss_mask_dn_8: 0.234  loss_dice_dn_8: 0.769  loss_bbox_dn_8: 0.2047  loss_giou_dn_8: 0.5843  loss_ce_interm: 1.45  loss_mask_interm: 0.2274  loss_dice_interm: 0.8921  loss_bbox_interm: 0.3793  loss_giou_interm: 0.9308    time: 0.8741  last_time: 0.8782  data_time: 0.0123  last_data_time: 0.0081   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:19:30 d2.utils.events]: \u001b[0m eta: 3 days, 16:41:24  iter: 3439  total_loss: 59.11  loss_ce: 0.8254  loss_mask: 0.2937  loss_dice: 0.835  loss_bbox: 0.2044  loss_giou: 0.6152  loss_ce_dn: 0.1371  loss_mask_dn: 0.3213  loss_dice_dn: 0.7537  loss_bbox_dn: 0.2258  loss_giou_dn: 0.5287  loss_ce_0: 1.467  loss_mask_0: 0.3316  loss_dice_0: 0.7958  loss_bbox_0: 0.4031  loss_giou_0: 0.8267  loss_ce_dn_0: 0.7117  loss_mask_dn_0: 1.086  loss_dice_dn_0: 3.013  loss_bbox_dn_0: 0.6782  loss_giou_dn_0: 0.9928  loss_ce_1: 1.316  loss_mask_1: 0.3106  loss_dice_1: 0.7959  loss_bbox_1: 0.2739  loss_giou_1: 0.7208  loss_ce_dn_1: 0.2279  loss_mask_dn_1: 0.3721  loss_dice_dn_1: 0.8732  loss_bbox_dn_1: 0.3461  loss_giou_dn_1: 0.6867  loss_ce_2: 1.182  loss_mask_2: 0.3185  loss_dice_2: 0.8458  loss_bbox_2: 0.2524  loss_giou_2: 0.6665  loss_ce_dn_2: 0.1897  loss_mask_dn_2: 0.3394  loss_dice_dn_2: 0.8102  loss_bbox_dn_2: 0.2932  loss_giou_dn_2: 0.6046  loss_ce_3: 1.055  loss_mask_3: 0.3075  loss_dice_3: 0.8863  loss_bbox_3: 0.2493  loss_giou_3: 0.6813  loss_ce_dn_3: 0.1646  loss_mask_dn_3: 0.3339  loss_dice_dn_3: 0.7783  loss_bbox_dn_3: 0.2651  loss_giou_dn_3: 0.5793  loss_ce_4: 0.9654  loss_mask_4: 0.2969  loss_dice_4: 0.8062  loss_bbox_4: 0.228  loss_giou_4: 0.6309  loss_ce_dn_4: 0.1536  loss_mask_dn_4: 0.3165  loss_dice_dn_4: 0.7536  loss_bbox_dn_4: 0.243  loss_giou_dn_4: 0.5559  loss_ce_5: 0.8883  loss_mask_5: 0.295  loss_dice_5: 0.884  loss_bbox_5: 0.2102  loss_giou_5: 0.6267  loss_ce_dn_5: 0.1453  loss_mask_dn_5: 0.3134  loss_dice_dn_5: 0.7366  loss_bbox_dn_5: 0.2349  loss_giou_dn_5: 0.5545  loss_ce_6: 0.8427  loss_mask_6: 0.2964  loss_dice_6: 0.8513  loss_bbox_6: 0.2155  loss_giou_6: 0.6142  loss_ce_dn_6: 0.1436  loss_mask_dn_6: 0.3223  loss_dice_dn_6: 0.741  loss_bbox_dn_6: 0.2293  loss_giou_dn_6: 0.5379  loss_ce_7: 0.8255  loss_mask_7: 0.2999  loss_dice_7: 0.8675  loss_bbox_7: 0.2087  loss_giou_7: 0.618  loss_ce_dn_7: 0.1476  loss_mask_dn_7: 0.3203  loss_dice_dn_7: 0.7452  loss_bbox_dn_7: 0.2271  loss_giou_dn_7: 0.5359  loss_ce_8: 0.8144  loss_mask_8: 0.2897  loss_dice_8: 0.824  loss_bbox_8: 0.2131  loss_giou_8: 0.6128  loss_ce_dn_8: 0.1395  loss_mask_dn_8: 0.3171  loss_dice_dn_8: 0.7427  loss_bbox_dn_8: 0.2262  loss_giou_dn_8: 0.5282  loss_ce_interm: 1.456  loss_mask_interm: 0.3285  loss_dice_interm: 0.8413  loss_bbox_interm: 0.401  loss_giou_interm: 0.8231    time: 0.8741  last_time: 0.8800  data_time: 0.0119  last_data_time: 0.0137   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:19:48 d2.utils.events]: \u001b[0m eta: 3 days, 16:41:47  iter: 3459  total_loss: 57.54  loss_ce: 0.7153  loss_mask: 0.2454  loss_dice: 0.6514  loss_bbox: 0.2929  loss_giou: 0.7179  loss_ce_dn: 0.1106  loss_mask_dn: 0.2383  loss_dice_dn: 0.646  loss_bbox_dn: 0.2353  loss_giou_dn: 0.5527  loss_ce_0: 1.389  loss_mask_0: 0.2524  loss_dice_0: 0.7307  loss_bbox_0: 0.3971  loss_giou_0: 0.935  loss_ce_dn_0: 0.7296  loss_mask_dn_0: 0.8451  loss_dice_dn_0: 3.106  loss_bbox_dn_0: 0.6348  loss_giou_dn_0: 0.9389  loss_ce_1: 1.347  loss_mask_1: 0.2713  loss_dice_1: 0.74  loss_bbox_1: 0.3245  loss_giou_1: 0.8215  loss_ce_dn_1: 0.2298  loss_mask_dn_1: 0.2983  loss_dice_dn_1: 0.7544  loss_bbox_dn_1: 0.369  loss_giou_dn_1: 0.6969  loss_ce_2: 1.133  loss_mask_2: 0.2703  loss_dice_2: 0.6916  loss_bbox_2: 0.2946  loss_giou_2: 0.7647  loss_ce_dn_2: 0.1868  loss_mask_dn_2: 0.2603  loss_dice_dn_2: 0.6945  loss_bbox_dn_2: 0.3072  loss_giou_dn_2: 0.621  loss_ce_3: 0.9794  loss_mask_3: 0.252  loss_dice_3: 0.7261  loss_bbox_3: 0.3338  loss_giou_3: 0.7442  loss_ce_dn_3: 0.1585  loss_mask_dn_3: 0.2492  loss_dice_dn_3: 0.6804  loss_bbox_dn_3: 0.2723  loss_giou_dn_3: 0.5995  loss_ce_4: 0.9004  loss_mask_4: 0.2614  loss_dice_4: 0.7077  loss_bbox_4: 0.3169  loss_giou_4: 0.7362  loss_ce_dn_4: 0.1465  loss_mask_dn_4: 0.2445  loss_dice_dn_4: 0.6555  loss_bbox_dn_4: 0.2515  loss_giou_dn_4: 0.5593  loss_ce_5: 0.7962  loss_mask_5: 0.2553  loss_dice_5: 0.6848  loss_bbox_5: 0.3069  loss_giou_5: 0.7359  loss_ce_dn_5: 0.1262  loss_mask_dn_5: 0.2479  loss_dice_dn_5: 0.6643  loss_bbox_dn_5: 0.2501  loss_giou_dn_5: 0.5632  loss_ce_6: 0.7603  loss_mask_6: 0.2482  loss_dice_6: 0.688  loss_bbox_6: 0.3067  loss_giou_6: 0.7218  loss_ce_dn_6: 0.1142  loss_mask_dn_6: 0.2428  loss_dice_dn_6: 0.633  loss_bbox_dn_6: 0.2372  loss_giou_dn_6: 0.5566  loss_ce_7: 0.7178  loss_mask_7: 0.2408  loss_dice_7: 0.697  loss_bbox_7: 0.3019  loss_giou_7: 0.7153  loss_ce_dn_7: 0.1121  loss_mask_dn_7: 0.2394  loss_dice_dn_7: 0.6387  loss_bbox_dn_7: 0.2373  loss_giou_dn_7: 0.5554  loss_ce_8: 0.7036  loss_mask_8: 0.2501  loss_dice_8: 0.6328  loss_bbox_8: 0.294  loss_giou_8: 0.721  loss_ce_dn_8: 0.1147  loss_mask_dn_8: 0.2375  loss_dice_dn_8: 0.6427  loss_bbox_dn_8: 0.2349  loss_giou_dn_8: 0.5516  loss_ce_interm: 1.391  loss_mask_interm: 0.2575  loss_dice_interm: 0.7222  loss_bbox_interm: 0.4038  loss_giou_interm: 0.944    time: 0.8741  last_time: 0.8808  data_time: 0.0125  last_data_time: 0.0158   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:20:06 d2.utils.events]: \u001b[0m eta: 3 days, 16:43:00  iter: 3479  total_loss: 62.8  loss_ce: 0.8279  loss_mask: 0.2675  loss_dice: 0.7497  loss_bbox: 0.2546  loss_giou: 0.7315  loss_ce_dn: 0.1242  loss_mask_dn: 0.2679  loss_dice_dn: 0.7024  loss_bbox_dn: 0.2314  loss_giou_dn: 0.6204  loss_ce_0: 1.519  loss_mask_0: 0.2749  loss_dice_0: 0.8066  loss_bbox_0: 0.4496  loss_giou_0: 1.032  loss_ce_dn_0: 0.7122  loss_mask_dn_0: 0.8926  loss_dice_dn_0: 3.189  loss_bbox_dn_0: 0.6844  loss_giou_dn_0: 1.014  loss_ce_1: 1.427  loss_mask_1: 0.2736  loss_dice_1: 0.7784  loss_bbox_1: 0.3136  loss_giou_1: 0.8788  loss_ce_dn_1: 0.235  loss_mask_dn_1: 0.3324  loss_dice_dn_1: 0.8216  loss_bbox_dn_1: 0.3729  loss_giou_dn_1: 0.7829  loss_ce_2: 1.222  loss_mask_2: 0.2702  loss_dice_2: 0.7681  loss_bbox_2: 0.2678  loss_giou_2: 0.7914  loss_ce_dn_2: 0.1975  loss_mask_dn_2: 0.3007  loss_dice_dn_2: 0.765  loss_bbox_dn_2: 0.308  loss_giou_dn_2: 0.7104  loss_ce_3: 0.98  loss_mask_3: 0.2636  loss_dice_3: 0.728  loss_bbox_3: 0.2723  loss_giou_3: 0.7936  loss_ce_dn_3: 0.1624  loss_mask_dn_3: 0.2861  loss_dice_dn_3: 0.738  loss_bbox_dn_3: 0.2773  loss_giou_dn_3: 0.6785  loss_ce_4: 0.9301  loss_mask_4: 0.2625  loss_dice_4: 0.7042  loss_bbox_4: 0.267  loss_giou_4: 0.7758  loss_ce_dn_4: 0.1502  loss_mask_dn_4: 0.2781  loss_dice_dn_4: 0.7291  loss_bbox_dn_4: 0.2535  loss_giou_dn_4: 0.658  loss_ce_5: 0.8278  loss_mask_5: 0.2559  loss_dice_5: 0.7556  loss_bbox_5: 0.2767  loss_giou_5: 0.7939  loss_ce_dn_5: 0.1418  loss_mask_dn_5: 0.2798  loss_dice_dn_5: 0.7059  loss_bbox_dn_5: 0.2425  loss_giou_dn_5: 0.6556  loss_ce_6: 0.8416  loss_mask_6: 0.2549  loss_dice_6: 0.7631  loss_bbox_6: 0.27  loss_giou_6: 0.7287  loss_ce_dn_6: 0.1421  loss_mask_dn_6: 0.2734  loss_dice_dn_6: 0.6953  loss_bbox_dn_6: 0.2324  loss_giou_dn_6: 0.6336  loss_ce_7: 0.8341  loss_mask_7: 0.288  loss_dice_7: 0.7608  loss_bbox_7: 0.2577  loss_giou_7: 0.7205  loss_ce_dn_7: 0.1321  loss_mask_dn_7: 0.2778  loss_dice_dn_7: 0.6864  loss_bbox_dn_7: 0.2315  loss_giou_dn_7: 0.6289  loss_ce_8: 0.8262  loss_mask_8: 0.2849  loss_dice_8: 0.7601  loss_bbox_8: 0.2586  loss_giou_8: 0.7239  loss_ce_dn_8: 0.1269  loss_mask_dn_8: 0.2747  loss_dice_dn_8: 0.7031  loss_bbox_dn_8: 0.2309  loss_giou_dn_8: 0.6221  loss_ce_interm: 1.542  loss_mask_interm: 0.2737  loss_dice_interm: 0.8322  loss_bbox_interm: 0.4339  loss_giou_interm: 1.022    time: 0.8742  last_time: 0.8844  data_time: 0.0121  last_data_time: 0.0143   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:20:23 d2.utils.events]: \u001b[0m eta: 3 days, 16:43:13  iter: 3499  total_loss: 65.31  loss_ce: 0.8258  loss_mask: 0.2662  loss_dice: 0.8317  loss_bbox: 0.3363  loss_giou: 0.7663  loss_ce_dn: 0.1321  loss_mask_dn: 0.2622  loss_dice_dn: 0.7383  loss_bbox_dn: 0.2356  loss_giou_dn: 0.6095  loss_ce_0: 1.439  loss_mask_0: 0.2742  loss_dice_0: 0.8195  loss_bbox_0: 0.4589  loss_giou_0: 1.017  loss_ce_dn_0: 0.7126  loss_mask_dn_0: 0.9206  loss_dice_dn_0: 3.133  loss_bbox_dn_0: 0.5884  loss_giou_dn_0: 0.9942  loss_ce_1: 1.367  loss_mask_1: 0.2795  loss_dice_1: 0.86  loss_bbox_1: 0.342  loss_giou_1: 0.8835  loss_ce_dn_1: 0.2565  loss_mask_dn_1: 0.3074  loss_dice_dn_1: 0.8628  loss_bbox_dn_1: 0.3664  loss_giou_dn_1: 0.7687  loss_ce_2: 1.165  loss_mask_2: 0.2914  loss_dice_2: 0.8771  loss_bbox_2: 0.3439  loss_giou_2: 0.8297  loss_ce_dn_2: 0.1858  loss_mask_dn_2: 0.2828  loss_dice_dn_2: 0.8546  loss_bbox_dn_2: 0.3138  loss_giou_dn_2: 0.684  loss_ce_3: 1.033  loss_mask_3: 0.2715  loss_dice_3: 0.8471  loss_bbox_3: 0.3407  loss_giou_3: 0.8054  loss_ce_dn_3: 0.1551  loss_mask_dn_3: 0.276  loss_dice_dn_3: 0.792  loss_bbox_dn_3: 0.2661  loss_giou_dn_3: 0.6474  loss_ce_4: 1.012  loss_mask_4: 0.2761  loss_dice_4: 0.8105  loss_bbox_4: 0.3023  loss_giou_4: 0.7812  loss_ce_dn_4: 0.1488  loss_mask_dn_4: 0.2788  loss_dice_dn_4: 0.7549  loss_bbox_dn_4: 0.257  loss_giou_dn_4: 0.6226  loss_ce_5: 0.9416  loss_mask_5: 0.2611  loss_dice_5: 0.7706  loss_bbox_5: 0.32  loss_giou_5: 0.7978  loss_ce_dn_5: 0.1443  loss_mask_dn_5: 0.2677  loss_dice_dn_5: 0.7645  loss_bbox_dn_5: 0.2501  loss_giou_dn_5: 0.6177  loss_ce_6: 0.8732  loss_mask_6: 0.2717  loss_dice_6: 0.8138  loss_bbox_6: 0.3236  loss_giou_6: 0.7879  loss_ce_dn_6: 0.1429  loss_mask_dn_6: 0.2657  loss_dice_dn_6: 0.7673  loss_bbox_dn_6: 0.2399  loss_giou_dn_6: 0.6167  loss_ce_7: 0.8698  loss_mask_7: 0.2678  loss_dice_7: 0.8207  loss_bbox_7: 0.339  loss_giou_7: 0.7849  loss_ce_dn_7: 0.1409  loss_mask_dn_7: 0.26  loss_dice_dn_7: 0.7326  loss_bbox_dn_7: 0.2397  loss_giou_dn_7: 0.6129  loss_ce_8: 0.8537  loss_mask_8: 0.2697  loss_dice_8: 0.7755  loss_bbox_8: 0.3376  loss_giou_8: 0.7698  loss_ce_dn_8: 0.1325  loss_mask_dn_8: 0.2573  loss_dice_dn_8: 0.7256  loss_bbox_dn_8: 0.2344  loss_giou_dn_8: 0.6105  loss_ce_interm: 1.435  loss_mask_interm: 0.2801  loss_dice_interm: 0.8254  loss_bbox_interm: 0.4603  loss_giou_interm: 0.9749    time: 0.8742  last_time: 0.9316  data_time: 0.0132  last_data_time: 0.0121   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:20:41 d2.utils.events]: \u001b[0m eta: 3 days, 16:42:46  iter: 3519  total_loss: 60.67  loss_ce: 0.7293  loss_mask: 0.256  loss_dice: 0.6401  loss_bbox: 0.2981  loss_giou: 0.6029  loss_ce_dn: 0.1263  loss_mask_dn: 0.2765  loss_dice_dn: 0.6488  loss_bbox_dn: 0.246  loss_giou_dn: 0.4927  loss_ce_0: 1.439  loss_mask_0: 0.2999  loss_dice_0: 0.6604  loss_bbox_0: 0.4873  loss_giou_0: 0.8224  loss_ce_dn_0: 0.6971  loss_mask_dn_0: 0.8602  loss_dice_dn_0: 2.692  loss_bbox_dn_0: 0.6864  loss_giou_dn_0: 0.9395  loss_ce_1: 1.365  loss_mask_1: 0.2932  loss_dice_1: 0.6376  loss_bbox_1: 0.3518  loss_giou_1: 0.6185  loss_ce_dn_1: 0.2278  loss_mask_dn_1: 0.3923  loss_dice_dn_1: 0.7441  loss_bbox_dn_1: 0.4208  loss_giou_dn_1: 0.6307  loss_ce_2: 1.201  loss_mask_2: 0.2938  loss_dice_2: 0.6285  loss_bbox_2: 0.3176  loss_giou_2: 0.602  loss_ce_dn_2: 0.1748  loss_mask_dn_2: 0.3556  loss_dice_dn_2: 0.6763  loss_bbox_dn_2: 0.3195  loss_giou_dn_2: 0.5593  loss_ce_3: 1.009  loss_mask_3: 0.2886  loss_dice_3: 0.6474  loss_bbox_3: 0.3094  loss_giou_3: 0.6136  loss_ce_dn_3: 0.1465  loss_mask_dn_3: 0.3021  loss_dice_dn_3: 0.6456  loss_bbox_dn_3: 0.2894  loss_giou_dn_3: 0.5341  loss_ce_4: 0.9432  loss_mask_4: 0.2716  loss_dice_4: 0.645  loss_bbox_4: 0.2974  loss_giou_4: 0.6115  loss_ce_dn_4: 0.1356  loss_mask_dn_4: 0.3067  loss_dice_dn_4: 0.639  loss_bbox_dn_4: 0.266  loss_giou_dn_4: 0.5125  loss_ce_5: 0.8335  loss_mask_5: 0.271  loss_dice_5: 0.6508  loss_bbox_5: 0.2919  loss_giou_5: 0.6125  loss_ce_dn_5: 0.1268  loss_mask_dn_5: 0.2895  loss_dice_dn_5: 0.6293  loss_bbox_dn_5: 0.2581  loss_giou_dn_5: 0.5058  loss_ce_6: 0.8002  loss_mask_6: 0.2638  loss_dice_6: 0.6392  loss_bbox_6: 0.3004  loss_giou_6: 0.6081  loss_ce_dn_6: 0.1272  loss_mask_dn_6: 0.2832  loss_dice_dn_6: 0.6265  loss_bbox_dn_6: 0.2511  loss_giou_dn_6: 0.4996  loss_ce_7: 0.7653  loss_mask_7: 0.2655  loss_dice_7: 0.6442  loss_bbox_7: 0.2997  loss_giou_7: 0.6026  loss_ce_dn_7: 0.1275  loss_mask_dn_7: 0.2863  loss_dice_dn_7: 0.6174  loss_bbox_dn_7: 0.2477  loss_giou_dn_7: 0.4904  loss_ce_8: 0.7212  loss_mask_8: 0.26  loss_dice_8: 0.6131  loss_bbox_8: 0.3057  loss_giou_8: 0.6037  loss_ce_dn_8: 0.1246  loss_mask_dn_8: 0.2806  loss_dice_dn_8: 0.6314  loss_bbox_dn_8: 0.2476  loss_giou_dn_8: 0.4916  loss_ce_interm: 1.43  loss_mask_interm: 0.3012  loss_dice_interm: 0.6699  loss_bbox_interm: 0.4826  loss_giou_interm: 0.8125    time: 0.8743  last_time: 0.8700  data_time: 0.0154  last_data_time: 0.0133   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:20:59 d2.utils.events]: \u001b[0m eta: 3 days, 16:43:08  iter: 3539  total_loss: 55.68  loss_ce: 0.7069  loss_mask: 0.2242  loss_dice: 0.6512  loss_bbox: 0.1985  loss_giou: 0.7366  loss_ce_dn: 0.1197  loss_mask_dn: 0.2774  loss_dice_dn: 0.6626  loss_bbox_dn: 0.2391  loss_giou_dn: 0.618  loss_ce_0: 1.482  loss_mask_0: 0.2592  loss_dice_0: 0.6595  loss_bbox_0: 0.3963  loss_giou_0: 0.9072  loss_ce_dn_0: 0.6844  loss_mask_dn_0: 0.8204  loss_dice_dn_0: 2.937  loss_bbox_dn_0: 0.6463  loss_giou_dn_0: 0.9932  loss_ce_1: 1.361  loss_mask_1: 0.2425  loss_dice_1: 0.6518  loss_bbox_1: 0.307  loss_giou_1: 0.7995  loss_ce_dn_1: 0.2671  loss_mask_dn_1: 0.3212  loss_dice_dn_1: 0.784  loss_bbox_dn_1: 0.3533  loss_giou_dn_1: 0.7592  loss_ce_2: 1.15  loss_mask_2: 0.2291  loss_dice_2: 0.6173  loss_bbox_2: 0.2569  loss_giou_2: 0.761  loss_ce_dn_2: 0.2061  loss_mask_dn_2: 0.3109  loss_dice_dn_2: 0.6983  loss_bbox_dn_2: 0.282  loss_giou_dn_2: 0.6916  loss_ce_3: 1.004  loss_mask_3: 0.2332  loss_dice_3: 0.6481  loss_bbox_3: 0.2489  loss_giou_3: 0.7355  loss_ce_dn_3: 0.1733  loss_mask_dn_3: 0.297  loss_dice_dn_3: 0.6722  loss_bbox_dn_3: 0.2558  loss_giou_dn_3: 0.6517  loss_ce_4: 0.9186  loss_mask_4: 0.2358  loss_dice_4: 0.6456  loss_bbox_4: 0.2009  loss_giou_4: 0.7109  loss_ce_dn_4: 0.1569  loss_mask_dn_4: 0.2899  loss_dice_dn_4: 0.6622  loss_bbox_dn_4: 0.2499  loss_giou_dn_4: 0.633  loss_ce_5: 0.8071  loss_mask_5: 0.2243  loss_dice_5: 0.6308  loss_bbox_5: 0.2076  loss_giou_5: 0.7508  loss_ce_dn_5: 0.1405  loss_mask_dn_5: 0.2805  loss_dice_dn_5: 0.651  loss_bbox_dn_5: 0.2437  loss_giou_dn_5: 0.6306  loss_ce_6: 0.7601  loss_mask_6: 0.2296  loss_dice_6: 0.6289  loss_bbox_6: 0.1948  loss_giou_6: 0.7461  loss_ce_dn_6: 0.1316  loss_mask_dn_6: 0.2703  loss_dice_dn_6: 0.6498  loss_bbox_dn_6: 0.2415  loss_giou_dn_6: 0.6194  loss_ce_7: 0.7449  loss_mask_7: 0.2236  loss_dice_7: 0.6201  loss_bbox_7: 0.1918  loss_giou_7: 0.7409  loss_ce_dn_7: 0.1228  loss_mask_dn_7: 0.279  loss_dice_dn_7: 0.6486  loss_bbox_dn_7: 0.2399  loss_giou_dn_7: 0.6207  loss_ce_8: 0.7216  loss_mask_8: 0.2281  loss_dice_8: 0.65  loss_bbox_8: 0.2033  loss_giou_8: 0.7374  loss_ce_dn_8: 0.1225  loss_mask_dn_8: 0.2834  loss_dice_dn_8: 0.6453  loss_bbox_dn_8: 0.2395  loss_giou_dn_8: 0.6181  loss_ce_interm: 1.482  loss_mask_interm: 0.2536  loss_dice_interm: 0.6346  loss_bbox_interm: 0.3963  loss_giou_interm: 0.9027    time: 0.8743  last_time: 0.8923  data_time: 0.0124  last_data_time: 0.0114   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:21:16 d2.utils.events]: \u001b[0m eta: 3 days, 16:43:29  iter: 3559  total_loss: 56.73  loss_ce: 0.7276  loss_mask: 0.2254  loss_dice: 0.7332  loss_bbox: 0.2999  loss_giou: 0.5929  loss_ce_dn: 0.09743  loss_mask_dn: 0.2633  loss_dice_dn: 0.7358  loss_bbox_dn: 0.2607  loss_giou_dn: 0.4559  loss_ce_0: 1.391  loss_mask_0: 0.255  loss_dice_0: 0.8183  loss_bbox_0: 0.3737  loss_giou_0: 0.7905  loss_ce_dn_0: 0.6798  loss_mask_dn_0: 0.883  loss_dice_dn_0: 2.724  loss_bbox_dn_0: 0.613  loss_giou_dn_0: 0.8646  loss_ce_1: 1.307  loss_mask_1: 0.2293  loss_dice_1: 0.7818  loss_bbox_1: 0.307  loss_giou_1: 0.6504  loss_ce_dn_1: 0.2043  loss_mask_dn_1: 0.3646  loss_dice_dn_1: 0.8322  loss_bbox_dn_1: 0.3752  loss_giou_dn_1: 0.6187  loss_ce_2: 1.097  loss_mask_2: 0.2328  loss_dice_2: 0.7727  loss_bbox_2: 0.2782  loss_giou_2: 0.6148  loss_ce_dn_2: 0.1675  loss_mask_dn_2: 0.3178  loss_dice_dn_2: 0.8153  loss_bbox_dn_2: 0.3087  loss_giou_dn_2: 0.5193  loss_ce_3: 0.8859  loss_mask_3: 0.2366  loss_dice_3: 0.7148  loss_bbox_3: 0.2648  loss_giou_3: 0.6048  loss_ce_dn_3: 0.1412  loss_mask_dn_3: 0.2804  loss_dice_dn_3: 0.7493  loss_bbox_dn_3: 0.2926  loss_giou_dn_3: 0.4924  loss_ce_4: 0.8336  loss_mask_4: 0.2373  loss_dice_4: 0.714  loss_bbox_4: 0.2939  loss_giou_4: 0.5767  loss_ce_dn_4: 0.1305  loss_mask_dn_4: 0.28  loss_dice_dn_4: 0.7411  loss_bbox_dn_4: 0.2804  loss_giou_dn_4: 0.467  loss_ce_5: 0.7999  loss_mask_5: 0.2354  loss_dice_5: 0.7765  loss_bbox_5: 0.3065  loss_giou_5: 0.5977  loss_ce_dn_5: 0.1127  loss_mask_dn_5: 0.2579  loss_dice_dn_5: 0.7473  loss_bbox_dn_5: 0.2719  loss_giou_dn_5: 0.4693  loss_ce_6: 0.7329  loss_mask_6: 0.2269  loss_dice_6: 0.782  loss_bbox_6: 0.3003  loss_giou_6: 0.5915  loss_ce_dn_6: 0.1041  loss_mask_dn_6: 0.2603  loss_dice_dn_6: 0.7508  loss_bbox_dn_6: 0.2659  loss_giou_dn_6: 0.4609  loss_ce_7: 0.7166  loss_mask_7: 0.2303  loss_dice_7: 0.731  loss_bbox_7: 0.2913  loss_giou_7: 0.5885  loss_ce_dn_7: 0.1018  loss_mask_dn_7: 0.2552  loss_dice_dn_7: 0.7327  loss_bbox_dn_7: 0.2617  loss_giou_dn_7: 0.4614  loss_ce_8: 0.7199  loss_mask_8: 0.2257  loss_dice_8: 0.7397  loss_bbox_8: 0.2938  loss_giou_8: 0.5936  loss_ce_dn_8: 0.09782  loss_mask_dn_8: 0.2664  loss_dice_dn_8: 0.753  loss_bbox_dn_8: 0.2596  loss_giou_dn_8: 0.4551  loss_ce_interm: 1.381  loss_mask_interm: 0.2537  loss_dice_interm: 0.7863  loss_bbox_interm: 0.3827  loss_giou_interm: 0.7868    time: 0.8743  last_time: 0.8876  data_time: 0.0124  last_data_time: 0.0100   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:21:34 d2.utils.events]: \u001b[0m eta: 3 days, 16:41:54  iter: 3579  total_loss: 63.42  loss_ce: 0.8248  loss_mask: 0.3093  loss_dice: 0.7675  loss_bbox: 0.2917  loss_giou: 0.701  loss_ce_dn: 0.1571  loss_mask_dn: 0.2769  loss_dice_dn: 0.7558  loss_bbox_dn: 0.2433  loss_giou_dn: 0.6127  loss_ce_0: 1.506  loss_mask_0: 0.349  loss_dice_0: 0.8601  loss_bbox_0: 0.425  loss_giou_0: 0.9027  loss_ce_dn_0: 0.746  loss_mask_dn_0: 0.9887  loss_dice_dn_0: 2.901  loss_bbox_dn_0: 0.7158  loss_giou_dn_0: 1.011  loss_ce_1: 1.387  loss_mask_1: 0.3154  loss_dice_1: 0.8431  loss_bbox_1: 0.3782  loss_giou_1: 0.7798  loss_ce_dn_1: 0.2675  loss_mask_dn_1: 0.3527  loss_dice_dn_1: 0.95  loss_bbox_dn_1: 0.4282  loss_giou_dn_1: 0.7403  loss_ce_2: 1.261  loss_mask_2: 0.3164  loss_dice_2: 0.848  loss_bbox_2: 0.3338  loss_giou_2: 0.7825  loss_ce_dn_2: 0.2223  loss_mask_dn_2: 0.3202  loss_dice_dn_2: 0.8105  loss_bbox_dn_2: 0.3382  loss_giou_dn_2: 0.6643  loss_ce_3: 1.078  loss_mask_3: 0.3137  loss_dice_3: 0.7792  loss_bbox_3: 0.3044  loss_giou_3: 0.7301  loss_ce_dn_3: 0.1919  loss_mask_dn_3: 0.2818  loss_dice_dn_3: 0.7489  loss_bbox_dn_3: 0.287  loss_giou_dn_3: 0.6483  loss_ce_4: 1.029  loss_mask_4: 0.2595  loss_dice_4: 0.7372  loss_bbox_4: 0.2932  loss_giou_4: 0.7076  loss_ce_dn_4: 0.1842  loss_mask_dn_4: 0.2751  loss_dice_dn_4: 0.7147  loss_bbox_dn_4: 0.2663  loss_giou_dn_4: 0.6257  loss_ce_5: 0.9239  loss_mask_5: 0.2662  loss_dice_5: 0.724  loss_bbox_5: 0.2902  loss_giou_5: 0.7189  loss_ce_dn_5: 0.1751  loss_mask_dn_5: 0.2787  loss_dice_dn_5: 0.7068  loss_bbox_dn_5: 0.2554  loss_giou_dn_5: 0.6223  loss_ce_6: 0.8775  loss_mask_6: 0.2828  loss_dice_6: 0.7241  loss_bbox_6: 0.2945  loss_giou_6: 0.732  loss_ce_dn_6: 0.1609  loss_mask_dn_6: 0.2732  loss_dice_dn_6: 0.7255  loss_bbox_dn_6: 0.252  loss_giou_dn_6: 0.6182  loss_ce_7: 0.8458  loss_mask_7: 0.2968  loss_dice_7: 0.7496  loss_bbox_7: 0.2859  loss_giou_7: 0.6964  loss_ce_dn_7: 0.1536  loss_mask_dn_7: 0.2711  loss_dice_dn_7: 0.7308  loss_bbox_dn_7: 0.2473  loss_giou_dn_7: 0.6162  loss_ce_8: 0.8191  loss_mask_8: 0.3079  loss_dice_8: 0.7597  loss_bbox_8: 0.2909  loss_giou_8: 0.706  loss_ce_dn_8: 0.1559  loss_mask_dn_8: 0.2743  loss_dice_dn_8: 0.754  loss_bbox_dn_8: 0.2461  loss_giou_dn_8: 0.6119  loss_ce_interm: 1.52  loss_mask_interm: 0.3457  loss_dice_interm: 0.8043  loss_bbox_interm: 0.4243  loss_giou_interm: 0.884    time: 0.8743  last_time: 0.8912  data_time: 0.0110  last_data_time: 0.0107   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:21:51 d2.utils.events]: \u001b[0m eta: 3 days, 16:42:39  iter: 3599  total_loss: 64.05  loss_ce: 0.852  loss_mask: 0.3334  loss_dice: 0.759  loss_bbox: 0.2765  loss_giou: 0.7105  loss_ce_dn: 0.125  loss_mask_dn: 0.3439  loss_dice_dn: 0.799  loss_bbox_dn: 0.2885  loss_giou_dn: 0.5777  loss_ce_0: 1.409  loss_mask_0: 0.3255  loss_dice_0: 0.8298  loss_bbox_0: 0.4677  loss_giou_0: 0.9392  loss_ce_dn_0: 0.7372  loss_mask_dn_0: 1.093  loss_dice_dn_0: 2.983  loss_bbox_dn_0: 0.8066  loss_giou_dn_0: 0.9492  loss_ce_1: 1.312  loss_mask_1: 0.3246  loss_dice_1: 0.7554  loss_bbox_1: 0.3785  loss_giou_1: 0.8585  loss_ce_dn_1: 0.2518  loss_mask_dn_1: 0.423  loss_dice_dn_1: 0.9024  loss_bbox_dn_1: 0.4365  loss_giou_dn_1: 0.7002  loss_ce_2: 1.226  loss_mask_2: 0.3567  loss_dice_2: 0.7884  loss_bbox_2: 0.3207  loss_giou_2: 0.7555  loss_ce_dn_2: 0.2106  loss_mask_dn_2: 0.3894  loss_dice_dn_2: 0.8223  loss_bbox_dn_2: 0.3533  loss_giou_dn_2: 0.6466  loss_ce_3: 1.079  loss_mask_3: 0.3301  loss_dice_3: 0.8141  loss_bbox_3: 0.3319  loss_giou_3: 0.7572  loss_ce_dn_3: 0.1728  loss_mask_dn_3: 0.3547  loss_dice_dn_3: 0.8207  loss_bbox_dn_3: 0.3349  loss_giou_dn_3: 0.6263  loss_ce_4: 0.9943  loss_mask_4: 0.3317  loss_dice_4: 0.8266  loss_bbox_4: 0.2704  loss_giou_4: 0.7663  loss_ce_dn_4: 0.1584  loss_mask_dn_4: 0.3461  loss_dice_dn_4: 0.7962  loss_bbox_dn_4: 0.3062  loss_giou_dn_4: 0.5975  loss_ce_5: 0.9307  loss_mask_5: 0.3123  loss_dice_5: 0.7573  loss_bbox_5: 0.3069  loss_giou_5: 0.7385  loss_ce_dn_5: 0.1445  loss_mask_dn_5: 0.351  loss_dice_dn_5: 0.8047  loss_bbox_dn_5: 0.3033  loss_giou_dn_5: 0.5954  loss_ce_6: 0.8631  loss_mask_6: 0.3098  loss_dice_6: 0.726  loss_bbox_6: 0.2885  loss_giou_6: 0.7337  loss_ce_dn_6: 0.147  loss_mask_dn_6: 0.3377  loss_dice_dn_6: 0.7927  loss_bbox_dn_6: 0.2938  loss_giou_dn_6: 0.5878  loss_ce_7: 0.8343  loss_mask_7: 0.3264  loss_dice_7: 0.7993  loss_bbox_7: 0.2893  loss_giou_7: 0.7182  loss_ce_dn_7: 0.1367  loss_mask_dn_7: 0.3433  loss_dice_dn_7: 0.8037  loss_bbox_dn_7: 0.291  loss_giou_dn_7: 0.585  loss_ce_8: 0.8647  loss_mask_8: 0.3241  loss_dice_8: 0.771  loss_bbox_8: 0.2753  loss_giou_8: 0.7157  loss_ce_dn_8: 0.1277  loss_mask_dn_8: 0.3407  loss_dice_dn_8: 0.7951  loss_bbox_dn_8: 0.288  loss_giou_dn_8: 0.5783  loss_ce_interm: 1.409  loss_mask_interm: 0.3189  loss_dice_interm: 0.8381  loss_bbox_interm: 0.4677  loss_giou_interm: 0.9162    time: 0.8743  last_time: 0.8639  data_time: 0.0134  last_data_time: 0.0092   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:22:09 d2.utils.events]: \u001b[0m eta: 3 days, 16:41:58  iter: 3619  total_loss: 61.52  loss_ce: 0.8481  loss_mask: 0.2297  loss_dice: 0.7703  loss_bbox: 0.2593  loss_giou: 0.6906  loss_ce_dn: 0.1184  loss_mask_dn: 0.2509  loss_dice_dn: 0.7209  loss_bbox_dn: 0.1745  loss_giou_dn: 0.5837  loss_ce_0: 1.445  loss_mask_0: 0.2513  loss_dice_0: 0.9064  loss_bbox_0: 0.4556  loss_giou_0: 0.9102  loss_ce_dn_0: 0.7349  loss_mask_dn_0: 1.054  loss_dice_dn_0: 3.139  loss_bbox_dn_0: 0.5945  loss_giou_dn_0: 0.9607  loss_ce_1: 1.436  loss_mask_1: 0.2383  loss_dice_1: 0.7707  loss_bbox_1: 0.328  loss_giou_1: 0.8026  loss_ce_dn_1: 0.2406  loss_mask_dn_1: 0.3071  loss_dice_dn_1: 0.9138  loss_bbox_dn_1: 0.328  loss_giou_dn_1: 0.7228  loss_ce_2: 1.255  loss_mask_2: 0.2178  loss_dice_2: 0.7526  loss_bbox_2: 0.3145  loss_giou_2: 0.7752  loss_ce_dn_2: 0.1917  loss_mask_dn_2: 0.2731  loss_dice_dn_2: 0.801  loss_bbox_dn_2: 0.2483  loss_giou_dn_2: 0.637  loss_ce_3: 1.074  loss_mask_3: 0.2151  loss_dice_3: 0.7107  loss_bbox_3: 0.2964  loss_giou_3: 0.729  loss_ce_dn_3: 0.161  loss_mask_dn_3: 0.2547  loss_dice_dn_3: 0.7901  loss_bbox_dn_3: 0.2085  loss_giou_dn_3: 0.608  loss_ce_4: 0.9478  loss_mask_4: 0.2206  loss_dice_4: 0.7562  loss_bbox_4: 0.2717  loss_giou_4: 0.7121  loss_ce_dn_4: 0.1472  loss_mask_dn_4: 0.2518  loss_dice_dn_4: 0.7204  loss_bbox_dn_4: 0.189  loss_giou_dn_4: 0.5937  loss_ce_5: 0.8555  loss_mask_5: 0.2195  loss_dice_5: 0.7371  loss_bbox_5: 0.2812  loss_giou_5: 0.7179  loss_ce_dn_5: 0.1309  loss_mask_dn_5: 0.257  loss_dice_dn_5: 0.7392  loss_bbox_dn_5: 0.1834  loss_giou_dn_5: 0.5873  loss_ce_6: 0.8619  loss_mask_6: 0.2203  loss_dice_6: 0.8002  loss_bbox_6: 0.2683  loss_giou_6: 0.7034  loss_ce_dn_6: 0.1247  loss_mask_dn_6: 0.2562  loss_dice_dn_6: 0.7183  loss_bbox_dn_6: 0.1767  loss_giou_dn_6: 0.5918  loss_ce_7: 0.846  loss_mask_7: 0.2057  loss_dice_7: 0.7799  loss_bbox_7: 0.2629  loss_giou_7: 0.6934  loss_ce_dn_7: 0.1243  loss_mask_dn_7: 0.2532  loss_dice_dn_7: 0.7229  loss_bbox_dn_7: 0.1755  loss_giou_dn_7: 0.59  loss_ce_8: 0.8452  loss_mask_8: 0.2407  loss_dice_8: 0.8282  loss_bbox_8: 0.2544  loss_giou_8: 0.6881  loss_ce_dn_8: 0.1249  loss_mask_dn_8: 0.2492  loss_dice_dn_8: 0.7084  loss_bbox_dn_8: 0.1735  loss_giou_dn_8: 0.5864  loss_ce_interm: 1.491  loss_mask_interm: 0.2468  loss_dice_interm: 0.8825  loss_bbox_interm: 0.4322  loss_giou_interm: 0.9061    time: 0.8743  last_time: 0.8757  data_time: 0.0123  last_data_time: 0.0151   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:22:26 d2.utils.events]: \u001b[0m eta: 3 days, 16:40:52  iter: 3639  total_loss: 53.68  loss_ce: 0.5872  loss_mask: 0.233  loss_dice: 0.6467  loss_bbox: 0.2309  loss_giou: 0.6532  loss_ce_dn: 0.1101  loss_mask_dn: 0.2355  loss_dice_dn: 0.6306  loss_bbox_dn: 0.211  loss_giou_dn: 0.5892  loss_ce_0: 1.317  loss_mask_0: 0.2609  loss_dice_0: 0.6804  loss_bbox_0: 0.3541  loss_giou_0: 0.9102  loss_ce_dn_0: 0.7307  loss_mask_dn_0: 0.8046  loss_dice_dn_0: 2.814  loss_bbox_dn_0: 0.5991  loss_giou_dn_0: 1.068  loss_ce_1: 1.18  loss_mask_1: 0.2372  loss_dice_1: 0.6631  loss_bbox_1: 0.2483  loss_giou_1: 0.7774  loss_ce_dn_1: 0.2648  loss_mask_dn_1: 0.2888  loss_dice_dn_1: 0.7591  loss_bbox_dn_1: 0.3352  loss_giou_dn_1: 0.7568  loss_ce_2: 0.9879  loss_mask_2: 0.2313  loss_dice_2: 0.6744  loss_bbox_2: 0.2675  loss_giou_2: 0.7146  loss_ce_dn_2: 0.2181  loss_mask_dn_2: 0.2489  loss_dice_dn_2: 0.6784  loss_bbox_dn_2: 0.2771  loss_giou_dn_2: 0.6654  loss_ce_3: 0.8657  loss_mask_3: 0.2281  loss_dice_3: 0.6356  loss_bbox_3: 0.2568  loss_giou_3: 0.6959  loss_ce_dn_3: 0.1885  loss_mask_dn_3: 0.2468  loss_dice_dn_3: 0.6561  loss_bbox_dn_3: 0.2498  loss_giou_dn_3: 0.6393  loss_ce_4: 0.8373  loss_mask_4: 0.2315  loss_dice_4: 0.6673  loss_bbox_4: 0.2473  loss_giou_4: 0.6837  loss_ce_dn_4: 0.1614  loss_mask_dn_4: 0.2478  loss_dice_dn_4: 0.6533  loss_bbox_dn_4: 0.23  loss_giou_dn_4: 0.6205  loss_ce_5: 0.682  loss_mask_5: 0.2367  loss_dice_5: 0.6516  loss_bbox_5: 0.2371  loss_giou_5: 0.671  loss_ce_dn_5: 0.141  loss_mask_dn_5: 0.2362  loss_dice_dn_5: 0.6326  loss_bbox_dn_5: 0.219  loss_giou_dn_5: 0.6103  loss_ce_6: 0.6147  loss_mask_6: 0.2328  loss_dice_6: 0.617  loss_bbox_6: 0.2324  loss_giou_6: 0.6624  loss_ce_dn_6: 0.1296  loss_mask_dn_6: 0.2367  loss_dice_dn_6: 0.6244  loss_bbox_dn_6: 0.2151  loss_giou_dn_6: 0.6012  loss_ce_7: 0.5877  loss_mask_7: 0.2346  loss_dice_7: 0.6066  loss_bbox_7: 0.2339  loss_giou_7: 0.6566  loss_ce_dn_7: 0.1226  loss_mask_dn_7: 0.2427  loss_dice_dn_7: 0.6248  loss_bbox_dn_7: 0.212  loss_giou_dn_7: 0.5965  loss_ce_8: 0.5919  loss_mask_8: 0.2261  loss_dice_8: 0.6102  loss_bbox_8: 0.2369  loss_giou_8: 0.661  loss_ce_dn_8: 0.1123  loss_mask_dn_8: 0.2364  loss_dice_dn_8: 0.6272  loss_bbox_dn_8: 0.211  loss_giou_dn_8: 0.588  loss_ce_interm: 1.322  loss_mask_interm: 0.2628  loss_dice_interm: 0.687  loss_bbox_interm: 0.3541  loss_giou_interm: 0.9152    time: 0.8742  last_time: 0.8689  data_time: 0.0119  last_data_time: 0.0104   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:22:44 d2.utils.events]: \u001b[0m eta: 3 days, 16:40:34  iter: 3659  total_loss: 67.35  loss_ce: 0.7676  loss_mask: 0.3428  loss_dice: 0.8066  loss_bbox: 0.2965  loss_giou: 0.6822  loss_ce_dn: 0.1005  loss_mask_dn: 0.3669  loss_dice_dn: 0.7598  loss_bbox_dn: 0.2814  loss_giou_dn: 0.5884  loss_ce_0: 1.476  loss_mask_0: 0.3986  loss_dice_0: 0.8897  loss_bbox_0: 0.4424  loss_giou_0: 0.9082  loss_ce_dn_0: 0.7481  loss_mask_dn_0: 0.8923  loss_dice_dn_0: 2.946  loss_bbox_dn_0: 0.6957  loss_giou_dn_0: 0.9617  loss_ce_1: 1.359  loss_mask_1: 0.3926  loss_dice_1: 0.8604  loss_bbox_1: 0.3899  loss_giou_1: 0.8186  loss_ce_dn_1: 0.2289  loss_mask_dn_1: 0.4649  loss_dice_dn_1: 0.9399  loss_bbox_dn_1: 0.4053  loss_giou_dn_1: 0.7195  loss_ce_2: 1.157  loss_mask_2: 0.376  loss_dice_2: 0.8455  loss_bbox_2: 0.3817  loss_giou_2: 0.7762  loss_ce_dn_2: 0.1768  loss_mask_dn_2: 0.3931  loss_dice_dn_2: 0.8366  loss_bbox_dn_2: 0.3405  loss_giou_dn_2: 0.6604  loss_ce_3: 0.9392  loss_mask_3: 0.3754  loss_dice_3: 0.8424  loss_bbox_3: 0.3226  loss_giou_3: 0.7813  loss_ce_dn_3: 0.1453  loss_mask_dn_3: 0.374  loss_dice_dn_3: 0.8082  loss_bbox_dn_3: 0.3154  loss_giou_dn_3: 0.6299  loss_ce_4: 0.8923  loss_mask_4: 0.351  loss_dice_4: 0.8469  loss_bbox_4: 0.3067  loss_giou_4: 0.7276  loss_ce_dn_4: 0.1289  loss_mask_dn_4: 0.379  loss_dice_dn_4: 0.8066  loss_bbox_dn_4: 0.2971  loss_giou_dn_4: 0.6092  loss_ce_5: 0.8306  loss_mask_5: 0.3579  loss_dice_5: 0.8694  loss_bbox_5: 0.3127  loss_giou_5: 0.7025  loss_ce_dn_5: 0.1124  loss_mask_dn_5: 0.3671  loss_dice_dn_5: 0.7947  loss_bbox_dn_5: 0.2946  loss_giou_dn_5: 0.6058  loss_ce_6: 0.8073  loss_mask_6: 0.3456  loss_dice_6: 0.8581  loss_bbox_6: 0.3081  loss_giou_6: 0.6812  loss_ce_dn_6: 0.11  loss_mask_dn_6: 0.3716  loss_dice_dn_6: 0.7642  loss_bbox_dn_6: 0.2865  loss_giou_dn_6: 0.5915  loss_ce_7: 0.7537  loss_mask_7: 0.3538  loss_dice_7: 0.8263  loss_bbox_7: 0.3016  loss_giou_7: 0.681  loss_ce_dn_7: 0.1022  loss_mask_dn_7: 0.3722  loss_dice_dn_7: 0.7494  loss_bbox_dn_7: 0.2836  loss_giou_dn_7: 0.5919  loss_ce_8: 0.7613  loss_mask_8: 0.3408  loss_dice_8: 0.7987  loss_bbox_8: 0.2958  loss_giou_8: 0.6849  loss_ce_dn_8: 0.1028  loss_mask_dn_8: 0.3672  loss_dice_dn_8: 0.7592  loss_bbox_dn_8: 0.2823  loss_giou_dn_8: 0.5892  loss_ce_interm: 1.475  loss_mask_interm: 0.4043  loss_dice_interm: 0.8708  loss_bbox_interm: 0.4406  loss_giou_interm: 0.9082    time: 0.8742  last_time: 0.8600  data_time: 0.0113  last_data_time: 0.0121   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:23:01 d2.utils.events]: \u001b[0m eta: 3 days, 16:39:00  iter: 3679  total_loss: 61.01  loss_ce: 0.8064  loss_mask: 0.2728  loss_dice: 0.7957  loss_bbox: 0.2267  loss_giou: 0.7134  loss_ce_dn: 0.1405  loss_mask_dn: 0.2824  loss_dice_dn: 0.6676  loss_bbox_dn: 0.2006  loss_giou_dn: 0.6016  loss_ce_0: 1.484  loss_mask_0: 0.3079  loss_dice_0: 0.8522  loss_bbox_0: 0.4459  loss_giou_0: 0.8878  loss_ce_dn_0: 0.7702  loss_mask_dn_0: 0.9753  loss_dice_dn_0: 2.767  loss_bbox_dn_0: 0.6749  loss_giou_dn_0: 0.9607  loss_ce_1: 1.367  loss_mask_1: 0.2843  loss_dice_1: 0.8577  loss_bbox_1: 0.335  loss_giou_1: 0.8032  loss_ce_dn_1: 0.2627  loss_mask_dn_1: 0.3579  loss_dice_dn_1: 0.7849  loss_bbox_dn_1: 0.3652  loss_giou_dn_1: 0.7102  loss_ce_2: 1.297  loss_mask_2: 0.2894  loss_dice_2: 0.8328  loss_bbox_2: 0.289  loss_giou_2: 0.7733  loss_ce_dn_2: 0.2166  loss_mask_dn_2: 0.2948  loss_dice_dn_2: 0.7009  loss_bbox_dn_2: 0.3056  loss_giou_dn_2: 0.6792  loss_ce_3: 1.102  loss_mask_3: 0.2952  loss_dice_3: 0.8642  loss_bbox_3: 0.2925  loss_giou_3: 0.737  loss_ce_dn_3: 0.1883  loss_mask_dn_3: 0.2818  loss_dice_dn_3: 0.6607  loss_bbox_dn_3: 0.2668  loss_giou_dn_3: 0.6443  loss_ce_4: 1.014  loss_mask_4: 0.2933  loss_dice_4: 0.846  loss_bbox_4: 0.2488  loss_giou_4: 0.7048  loss_ce_dn_4: 0.1722  loss_mask_dn_4: 0.2748  loss_dice_dn_4: 0.6822  loss_bbox_dn_4: 0.2384  loss_giou_dn_4: 0.6357  loss_ce_5: 0.8701  loss_mask_5: 0.2787  loss_dice_5: 0.8026  loss_bbox_5: 0.262  loss_giou_5: 0.7447  loss_ce_dn_5: 0.1574  loss_mask_dn_5: 0.2792  loss_dice_dn_5: 0.6716  loss_bbox_dn_5: 0.2275  loss_giou_dn_5: 0.6081  loss_ce_6: 0.824  loss_mask_6: 0.2858  loss_dice_6: 0.8232  loss_bbox_6: 0.2478  loss_giou_6: 0.743  loss_ce_dn_6: 0.1444  loss_mask_dn_6: 0.2852  loss_dice_dn_6: 0.6523  loss_bbox_dn_6: 0.211  loss_giou_dn_6: 0.608  loss_ce_7: 0.7856  loss_mask_7: 0.2796  loss_dice_7: 0.8026  loss_bbox_7: 0.2348  loss_giou_7: 0.6848  loss_ce_dn_7: 0.1479  loss_mask_dn_7: 0.2813  loss_dice_dn_7: 0.6706  loss_bbox_dn_7: 0.2071  loss_giou_dn_7: 0.6063  loss_ce_8: 0.8089  loss_mask_8: 0.2664  loss_dice_8: 0.7933  loss_bbox_8: 0.2324  loss_giou_8: 0.6839  loss_ce_dn_8: 0.1452  loss_mask_dn_8: 0.2794  loss_dice_dn_8: 0.6727  loss_bbox_dn_8: 0.2011  loss_giou_dn_8: 0.6  loss_ce_interm: 1.472  loss_mask_interm: 0.3094  loss_dice_interm: 0.8723  loss_bbox_interm: 0.4469  loss_giou_interm: 0.8865    time: 0.8742  last_time: 0.9348  data_time: 0.0113  last_data_time: 0.0207   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:23:19 d2.utils.events]: \u001b[0m eta: 3 days, 16:38:42  iter: 3699  total_loss: 68.75  loss_ce: 0.9383  loss_mask: 0.2766  loss_dice: 0.9349  loss_bbox: 0.2769  loss_giou: 0.7806  loss_ce_dn: 0.1497  loss_mask_dn: 0.335  loss_dice_dn: 0.9306  loss_bbox_dn: 0.283  loss_giou_dn: 0.6173  loss_ce_0: 1.557  loss_mask_0: 0.2914  loss_dice_0: 0.9346  loss_bbox_0: 0.4818  loss_giou_0: 0.9542  loss_ce_dn_0: 0.7117  loss_mask_dn_0: 0.8347  loss_dice_dn_0: 3.137  loss_bbox_dn_0: 0.6324  loss_giou_dn_0: 0.9176  loss_ce_1: 1.546  loss_mask_1: 0.2961  loss_dice_1: 1.007  loss_bbox_1: 0.3306  loss_giou_1: 0.831  loss_ce_dn_1: 0.2729  loss_mask_dn_1: 0.3855  loss_dice_dn_1: 1.133  loss_bbox_dn_1: 0.4434  loss_giou_dn_1: 0.7117  loss_ce_2: 1.31  loss_mask_2: 0.3065  loss_dice_2: 0.9953  loss_bbox_2: 0.345  loss_giou_2: 0.778  loss_ce_dn_2: 0.2206  loss_mask_dn_2: 0.3608  loss_dice_dn_2: 1.02  loss_bbox_dn_2: 0.3796  loss_giou_dn_2: 0.6625  loss_ce_3: 1.189  loss_mask_3: 0.2874  loss_dice_3: 0.9996  loss_bbox_3: 0.3228  loss_giou_3: 0.7764  loss_ce_dn_3: 0.198  loss_mask_dn_3: 0.3476  loss_dice_dn_3: 1.001  loss_bbox_dn_3: 0.3516  loss_giou_dn_3: 0.6414  loss_ce_4: 1.099  loss_mask_4: 0.2923  loss_dice_4: 0.9768  loss_bbox_4: 0.3068  loss_giou_4: 0.7792  loss_ce_dn_4: 0.1857  loss_mask_dn_4: 0.3448  loss_dice_dn_4: 0.9382  loss_bbox_dn_4: 0.3186  loss_giou_dn_4: 0.6282  loss_ce_5: 0.9873  loss_mask_5: 0.2937  loss_dice_5: 1.022  loss_bbox_5: 0.3023  loss_giou_5: 0.8079  loss_ce_dn_5: 0.1663  loss_mask_dn_5: 0.3345  loss_dice_dn_5: 0.9313  loss_bbox_dn_5: 0.3023  loss_giou_dn_5: 0.6261  loss_ce_6: 0.9492  loss_mask_6: 0.2955  loss_dice_6: 0.8912  loss_bbox_6: 0.2915  loss_giou_6: 0.7764  loss_ce_dn_6: 0.1618  loss_mask_dn_6: 0.3357  loss_dice_dn_6: 0.9221  loss_bbox_dn_6: 0.2931  loss_giou_dn_6: 0.6178  loss_ce_7: 0.9342  loss_mask_7: 0.2859  loss_dice_7: 0.9377  loss_bbox_7: 0.2901  loss_giou_7: 0.7842  loss_ce_dn_7: 0.1579  loss_mask_dn_7: 0.3396  loss_dice_dn_7: 0.8992  loss_bbox_dn_7: 0.2904  loss_giou_dn_7: 0.6191  loss_ce_8: 0.9337  loss_mask_8: 0.2828  loss_dice_8: 0.9391  loss_bbox_8: 0.279  loss_giou_8: 0.7686  loss_ce_dn_8: 0.1577  loss_mask_dn_8: 0.3359  loss_dice_dn_8: 0.9282  loss_bbox_dn_8: 0.2853  loss_giou_dn_8: 0.6165  loss_ce_interm: 1.614  loss_mask_interm: 0.2904  loss_dice_interm: 0.9829  loss_bbox_interm: 0.4825  loss_giou_interm: 0.9774    time: 0.8742  last_time: 0.8785  data_time: 0.0129  last_data_time: 0.0145   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:23:36 d2.utils.events]: \u001b[0m eta: 3 days, 16:38:51  iter: 3719  total_loss: 56.68  loss_ce: 0.7821  loss_mask: 0.2129  loss_dice: 0.6031  loss_bbox: 0.2627  loss_giou: 0.7376  loss_ce_dn: 0.108  loss_mask_dn: 0.2088  loss_dice_dn: 0.5735  loss_bbox_dn: 0.2111  loss_giou_dn: 0.6084  loss_ce_0: 1.461  loss_mask_0: 0.2339  loss_dice_0: 0.7101  loss_bbox_0: 0.3893  loss_giou_0: 0.9545  loss_ce_dn_0: 0.7702  loss_mask_dn_0: 0.807  loss_dice_dn_0: 2.772  loss_bbox_dn_0: 0.5976  loss_giou_dn_0: 0.9679  loss_ce_1: 1.312  loss_mask_1: 0.2244  loss_dice_1: 0.6545  loss_bbox_1: 0.2678  loss_giou_1: 0.7766  loss_ce_dn_1: 0.2463  loss_mask_dn_1: 0.3092  loss_dice_dn_1: 0.7203  loss_bbox_dn_1: 0.3252  loss_giou_dn_1: 0.7166  loss_ce_2: 1.146  loss_mask_2: 0.2251  loss_dice_2: 0.6332  loss_bbox_2: 0.2619  loss_giou_2: 0.7589  loss_ce_dn_2: 0.1987  loss_mask_dn_2: 0.2716  loss_dice_dn_2: 0.633  loss_bbox_dn_2: 0.2781  loss_giou_dn_2: 0.6718  loss_ce_3: 1.016  loss_mask_3: 0.2227  loss_dice_3: 0.606  loss_bbox_3: 0.2792  loss_giou_3: 0.7529  loss_ce_dn_3: 0.1683  loss_mask_dn_3: 0.247  loss_dice_dn_3: 0.5812  loss_bbox_dn_3: 0.2495  loss_giou_dn_3: 0.6482  loss_ce_4: 0.8992  loss_mask_4: 0.2165  loss_dice_4: 0.6036  loss_bbox_4: 0.2775  loss_giou_4: 0.7518  loss_ce_dn_4: 0.1511  loss_mask_dn_4: 0.2269  loss_dice_dn_4: 0.5698  loss_bbox_dn_4: 0.2348  loss_giou_dn_4: 0.6288  loss_ce_5: 0.8347  loss_mask_5: 0.2099  loss_dice_5: 0.5877  loss_bbox_5: 0.2696  loss_giou_5: 0.7519  loss_ce_dn_5: 0.1389  loss_mask_dn_5: 0.2268  loss_dice_dn_5: 0.5893  loss_bbox_dn_5: 0.2251  loss_giou_dn_5: 0.6202  loss_ce_6: 0.8063  loss_mask_6: 0.23  loss_dice_6: 0.6047  loss_bbox_6: 0.2493  loss_giou_6: 0.7394  loss_ce_dn_6: 0.1207  loss_mask_dn_6: 0.2208  loss_dice_dn_6: 0.5851  loss_bbox_dn_6: 0.2181  loss_giou_dn_6: 0.6108  loss_ce_7: 0.7921  loss_mask_7: 0.2192  loss_dice_7: 0.6196  loss_bbox_7: 0.2638  loss_giou_7: 0.7264  loss_ce_dn_7: 0.1131  loss_mask_dn_7: 0.2112  loss_dice_dn_7: 0.5632  loss_bbox_dn_7: 0.2141  loss_giou_dn_7: 0.6079  loss_ce_8: 0.7507  loss_mask_8: 0.2117  loss_dice_8: 0.6205  loss_bbox_8: 0.2591  loss_giou_8: 0.7383  loss_ce_dn_8: 0.1117  loss_mask_dn_8: 0.2063  loss_dice_dn_8: 0.5614  loss_bbox_dn_8: 0.2117  loss_giou_dn_8: 0.6054  loss_ce_interm: 1.464  loss_mask_interm: 0.2337  loss_dice_interm: 0.7235  loss_bbox_interm: 0.3971  loss_giou_interm: 0.9523    time: 0.8742  last_time: 0.8843  data_time: 0.0128  last_data_time: 0.0121   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:23:54 d2.utils.events]: \u001b[0m eta: 3 days, 16:37:42  iter: 3739  total_loss: 59.65  loss_ce: 0.87  loss_mask: 0.3331  loss_dice: 0.723  loss_bbox: 0.227  loss_giou: 0.6637  loss_ce_dn: 0.1712  loss_mask_dn: 0.3198  loss_dice_dn: 0.7132  loss_bbox_dn: 0.2071  loss_giou_dn: 0.5959  loss_ce_0: 1.44  loss_mask_0: 0.3295  loss_dice_0: 0.7294  loss_bbox_0: 0.3696  loss_giou_0: 0.8067  loss_ce_dn_0: 0.7144  loss_mask_dn_0: 0.9373  loss_dice_dn_0: 2.892  loss_bbox_dn_0: 0.6836  loss_giou_dn_0: 1.023  loss_ce_1: 1.363  loss_mask_1: 0.3367  loss_dice_1: 0.6788  loss_bbox_1: 0.2718  loss_giou_1: 0.7131  loss_ce_dn_1: 0.2754  loss_mask_dn_1: 0.3626  loss_dice_dn_1: 0.7817  loss_bbox_dn_1: 0.338  loss_giou_dn_1: 0.7272  loss_ce_2: 1.184  loss_mask_2: 0.305  loss_dice_2: 0.7235  loss_bbox_2: 0.264  loss_giou_2: 0.6895  loss_ce_dn_2: 0.2296  loss_mask_dn_2: 0.3326  loss_dice_dn_2: 0.7407  loss_bbox_dn_2: 0.2803  loss_giou_dn_2: 0.6718  loss_ce_3: 1.066  loss_mask_3: 0.2883  loss_dice_3: 0.6813  loss_bbox_3: 0.2457  loss_giou_3: 0.6641  loss_ce_dn_3: 0.193  loss_mask_dn_3: 0.3191  loss_dice_dn_3: 0.6958  loss_bbox_dn_3: 0.2486  loss_giou_dn_3: 0.6384  loss_ce_4: 0.9745  loss_mask_4: 0.3105  loss_dice_4: 0.7156  loss_bbox_4: 0.2468  loss_giou_4: 0.6537  loss_ce_dn_4: 0.1887  loss_mask_dn_4: 0.3152  loss_dice_dn_4: 0.7227  loss_bbox_dn_4: 0.2318  loss_giou_dn_4: 0.6226  loss_ce_5: 0.8763  loss_mask_5: 0.3239  loss_dice_5: 0.7169  loss_bbox_5: 0.2433  loss_giou_5: 0.6566  loss_ce_dn_5: 0.189  loss_mask_dn_5: 0.3246  loss_dice_dn_5: 0.7306  loss_bbox_dn_5: 0.2285  loss_giou_dn_5: 0.6178  loss_ce_6: 0.915  loss_mask_6: 0.3272  loss_dice_6: 0.7294  loss_bbox_6: 0.2386  loss_giou_6: 0.6617  loss_ce_dn_6: 0.1791  loss_mask_dn_6: 0.3276  loss_dice_dn_6: 0.7232  loss_bbox_dn_6: 0.2171  loss_giou_dn_6: 0.6093  loss_ce_7: 0.8966  loss_mask_7: 0.336  loss_dice_7: 0.7151  loss_bbox_7: 0.2329  loss_giou_7: 0.6717  loss_ce_dn_7: 0.1777  loss_mask_dn_7: 0.3206  loss_dice_dn_7: 0.7088  loss_bbox_dn_7: 0.2133  loss_giou_dn_7: 0.6033  loss_ce_8: 0.8942  loss_mask_8: 0.3246  loss_dice_8: 0.7001  loss_bbox_8: 0.2288  loss_giou_8: 0.6536  loss_ce_dn_8: 0.1713  loss_mask_dn_8: 0.3213  loss_dice_dn_8: 0.6964  loss_bbox_dn_8: 0.2087  loss_giou_dn_8: 0.5975  loss_ce_interm: 1.462  loss_mask_interm: 0.3164  loss_dice_interm: 0.7471  loss_bbox_interm: 0.3706  loss_giou_interm: 0.8004    time: 0.8742  last_time: 0.8807  data_time: 0.0112  last_data_time: 0.0094   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:24:12 d2.utils.events]: \u001b[0m eta: 3 days, 16:39:01  iter: 3759  total_loss: 64.45  loss_ce: 0.7192  loss_mask: 0.281  loss_dice: 0.7238  loss_bbox: 0.2766  loss_giou: 0.7206  loss_ce_dn: 0.1201  loss_mask_dn: 0.3224  loss_dice_dn: 0.7358  loss_bbox_dn: 0.2246  loss_giou_dn: 0.5845  loss_ce_0: 1.492  loss_mask_0: 0.3537  loss_dice_0: 0.7739  loss_bbox_0: 0.4222  loss_giou_0: 0.8844  loss_ce_dn_0: 0.7266  loss_mask_dn_0: 1.052  loss_dice_dn_0: 3.138  loss_bbox_dn_0: 0.6734  loss_giou_dn_0: 0.9821  loss_ce_1: 1.355  loss_mask_1: 0.3239  loss_dice_1: 0.7388  loss_bbox_1: 0.3208  loss_giou_1: 0.7803  loss_ce_dn_1: 0.2301  loss_mask_dn_1: 0.4032  loss_dice_dn_1: 0.8386  loss_bbox_dn_1: 0.3653  loss_giou_dn_1: 0.7134  loss_ce_2: 1.118  loss_mask_2: 0.3058  loss_dice_2: 0.7278  loss_bbox_2: 0.2857  loss_giou_2: 0.7213  loss_ce_dn_2: 0.1861  loss_mask_dn_2: 0.3456  loss_dice_dn_2: 0.7604  loss_bbox_dn_2: 0.2966  loss_giou_dn_2: 0.6464  loss_ce_3: 1.021  loss_mask_3: 0.2854  loss_dice_3: 0.7519  loss_bbox_3: 0.2667  loss_giou_3: 0.7153  loss_ce_dn_3: 0.1514  loss_mask_dn_3: 0.3308  loss_dice_dn_3: 0.7432  loss_bbox_dn_3: 0.2586  loss_giou_dn_3: 0.6225  loss_ce_4: 0.9105  loss_mask_4: 0.3209  loss_dice_4: 0.7453  loss_bbox_4: 0.2803  loss_giou_4: 0.7139  loss_ce_dn_4: 0.1427  loss_mask_dn_4: 0.3355  loss_dice_dn_4: 0.7316  loss_bbox_dn_4: 0.2437  loss_giou_dn_4: 0.6122  loss_ce_5: 0.8029  loss_mask_5: 0.2884  loss_dice_5: 0.7617  loss_bbox_5: 0.2819  loss_giou_5: 0.7157  loss_ce_dn_5: 0.1312  loss_mask_dn_5: 0.3346  loss_dice_dn_5: 0.7172  loss_bbox_dn_5: 0.2375  loss_giou_dn_5: 0.6025  loss_ce_6: 0.7845  loss_mask_6: 0.2884  loss_dice_6: 0.7426  loss_bbox_6: 0.2817  loss_giou_6: 0.7037  loss_ce_dn_6: 0.119  loss_mask_dn_6: 0.3304  loss_dice_dn_6: 0.7157  loss_bbox_dn_6: 0.2302  loss_giou_dn_6: 0.5887  loss_ce_7: 0.7548  loss_mask_7: 0.2822  loss_dice_7: 0.7163  loss_bbox_7: 0.2691  loss_giou_7: 0.7021  loss_ce_dn_7: 0.1251  loss_mask_dn_7: 0.3285  loss_dice_dn_7: 0.7292  loss_bbox_dn_7: 0.2289  loss_giou_dn_7: 0.5889  loss_ce_8: 0.7547  loss_mask_8: 0.2825  loss_dice_8: 0.7486  loss_bbox_8: 0.2644  loss_giou_8: 0.7331  loss_ce_dn_8: 0.1219  loss_mask_dn_8: 0.3253  loss_dice_dn_8: 0.7048  loss_bbox_dn_8: 0.2266  loss_giou_dn_8: 0.5854  loss_ce_interm: 1.498  loss_mask_interm: 0.3528  loss_dice_interm: 0.7659  loss_bbox_interm: 0.3962  loss_giou_interm: 0.891    time: 0.8743  last_time: 0.8828  data_time: 0.0123  last_data_time: 0.0180   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:24:29 d2.utils.events]: \u001b[0m eta: 3 days, 16:41:18  iter: 3779  total_loss: 54.38  loss_ce: 0.7521  loss_mask: 0.2443  loss_dice: 0.5373  loss_bbox: 0.1925  loss_giou: 0.7532  loss_ce_dn: 0.1177  loss_mask_dn: 0.2361  loss_dice_dn: 0.5194  loss_bbox_dn: 0.2274  loss_giou_dn: 0.673  loss_ce_0: 1.481  loss_mask_0: 0.2935  loss_dice_0: 0.579  loss_bbox_0: 0.3892  loss_giou_0: 0.9469  loss_ce_dn_0: 0.7532  loss_mask_dn_0: 0.6652  loss_dice_dn_0: 2.446  loss_bbox_dn_0: 0.7022  loss_giou_dn_0: 0.9871  loss_ce_1: 1.352  loss_mask_1: 0.2625  loss_dice_1: 0.5851  loss_bbox_1: 0.2457  loss_giou_1: 0.8055  loss_ce_dn_1: 0.2687  loss_mask_dn_1: 0.317  loss_dice_dn_1: 0.6058  loss_bbox_dn_1: 0.3802  loss_giou_dn_1: 0.7819  loss_ce_2: 1.097  loss_mask_2: 0.2699  loss_dice_2: 0.6124  loss_bbox_2: 0.2178  loss_giou_2: 0.7942  loss_ce_dn_2: 0.2106  loss_mask_dn_2: 0.261  loss_dice_dn_2: 0.5873  loss_bbox_dn_2: 0.2928  loss_giou_dn_2: 0.7366  loss_ce_3: 0.9245  loss_mask_3: 0.2529  loss_dice_3: 0.5612  loss_bbox_3: 0.2048  loss_giou_3: 0.7718  loss_ce_dn_3: 0.1791  loss_mask_dn_3: 0.2471  loss_dice_dn_3: 0.5593  loss_bbox_dn_3: 0.2757  loss_giou_dn_3: 0.7129  loss_ce_4: 0.8606  loss_mask_4: 0.243  loss_dice_4: 0.5448  loss_bbox_4: 0.2056  loss_giou_4: 0.7681  loss_ce_dn_4: 0.1601  loss_mask_dn_4: 0.2404  loss_dice_dn_4: 0.5547  loss_bbox_dn_4: 0.2544  loss_giou_dn_4: 0.6895  loss_ce_5: 0.815  loss_mask_5: 0.2459  loss_dice_5: 0.4968  loss_bbox_5: 0.2078  loss_giou_5: 0.758  loss_ce_dn_5: 0.1363  loss_mask_dn_5: 0.2428  loss_dice_dn_5: 0.5535  loss_bbox_dn_5: 0.2478  loss_giou_dn_5: 0.6845  loss_ce_6: 0.7762  loss_mask_6: 0.2397  loss_dice_6: 0.5614  loss_bbox_6: 0.2  loss_giou_6: 0.7465  loss_ce_dn_6: 0.1236  loss_mask_dn_6: 0.2442  loss_dice_dn_6: 0.5502  loss_bbox_dn_6: 0.2333  loss_giou_dn_6: 0.6783  loss_ce_7: 0.7528  loss_mask_7: 0.246  loss_dice_7: 0.5118  loss_bbox_7: 0.2032  loss_giou_7: 0.747  loss_ce_dn_7: 0.1208  loss_mask_dn_7: 0.2354  loss_dice_dn_7: 0.5298  loss_bbox_dn_7: 0.2304  loss_giou_dn_7: 0.6779  loss_ce_8: 0.7562  loss_mask_8: 0.2355  loss_dice_8: 0.49  loss_bbox_8: 0.1915  loss_giou_8: 0.7547  loss_ce_dn_8: 0.1185  loss_mask_dn_8: 0.235  loss_dice_dn_8: 0.5438  loss_bbox_dn_8: 0.2276  loss_giou_dn_8: 0.6727  loss_ce_interm: 1.477  loss_mask_interm: 0.2868  loss_dice_interm: 0.5281  loss_bbox_interm: 0.3891  loss_giou_interm: 0.9469    time: 0.8743  last_time: 0.8922  data_time: 0.0126  last_data_time: 0.0188   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:24:47 d2.utils.events]: \u001b[0m eta: 3 days, 16:40:27  iter: 3799  total_loss: 56.7  loss_ce: 0.788  loss_mask: 0.254  loss_dice: 0.849  loss_bbox: 0.2133  loss_giou: 0.6242  loss_ce_dn: 0.1246  loss_mask_dn: 0.2353  loss_dice_dn: 0.7993  loss_bbox_dn: 0.1898  loss_giou_dn: 0.5275  loss_ce_0: 1.504  loss_mask_0: 0.2484  loss_dice_0: 0.8574  loss_bbox_0: 0.356  loss_giou_0: 0.9038  loss_ce_dn_0: 0.7612  loss_mask_dn_0: 0.7434  loss_dice_dn_0: 2.849  loss_bbox_dn_0: 0.6223  loss_giou_dn_0: 0.9034  loss_ce_1: 1.422  loss_mask_1: 0.2646  loss_dice_1: 0.8471  loss_bbox_1: 0.2387  loss_giou_1: 0.7365  loss_ce_dn_1: 0.2587  loss_mask_dn_1: 0.278  loss_dice_dn_1: 0.9065  loss_bbox_dn_1: 0.3026  loss_giou_dn_1: 0.6349  loss_ce_2: 1.201  loss_mask_2: 0.2606  loss_dice_2: 0.807  loss_bbox_2: 0.2518  loss_giou_2: 0.6795  loss_ce_dn_2: 0.2048  loss_mask_dn_2: 0.269  loss_dice_dn_2: 0.8509  loss_bbox_dn_2: 0.2348  loss_giou_dn_2: 0.5779  loss_ce_3: 1.069  loss_mask_3: 0.26  loss_dice_3: 0.8692  loss_bbox_3: 0.2447  loss_giou_3: 0.6895  loss_ce_dn_3: 0.1791  loss_mask_dn_3: 0.267  loss_dice_dn_3: 0.8016  loss_bbox_dn_3: 0.2038  loss_giou_dn_3: 0.5573  loss_ce_4: 1.019  loss_mask_4: 0.2677  loss_dice_4: 0.7916  loss_bbox_4: 0.2442  loss_giou_4: 0.6787  loss_ce_dn_4: 0.1622  loss_mask_dn_4: 0.2534  loss_dice_dn_4: 0.7976  loss_bbox_dn_4: 0.1937  loss_giou_dn_4: 0.5477  loss_ce_5: 0.8838  loss_mask_5: 0.2609  loss_dice_5: 0.82  loss_bbox_5: 0.2469  loss_giou_5: 0.6647  loss_ce_dn_5: 0.1569  loss_mask_dn_5: 0.2529  loss_dice_dn_5: 0.777  loss_bbox_dn_5: 0.1881  loss_giou_dn_5: 0.5403  loss_ce_6: 0.867  loss_mask_6: 0.2602  loss_dice_6: 0.8586  loss_bbox_6: 0.219  loss_giou_6: 0.6612  loss_ce_dn_6: 0.1391  loss_mask_dn_6: 0.2436  loss_dice_dn_6: 0.7969  loss_bbox_dn_6: 0.1876  loss_giou_dn_6: 0.5371  loss_ce_7: 0.8419  loss_mask_7: 0.2567  loss_dice_7: 0.8248  loss_bbox_7: 0.217  loss_giou_7: 0.6233  loss_ce_dn_7: 0.1284  loss_mask_dn_7: 0.2389  loss_dice_dn_7: 0.7759  loss_bbox_dn_7: 0.1895  loss_giou_dn_7: 0.5337  loss_ce_8: 0.8046  loss_mask_8: 0.257  loss_dice_8: 0.8232  loss_bbox_8: 0.2161  loss_giou_8: 0.6343  loss_ce_dn_8: 0.1251  loss_mask_dn_8: 0.2372  loss_dice_dn_8: 0.7727  loss_bbox_dn_8: 0.1903  loss_giou_dn_8: 0.5278  loss_ce_interm: 1.495  loss_mask_interm: 0.2507  loss_dice_interm: 0.8491  loss_bbox_interm: 0.3473  loss_giou_interm: 0.8642    time: 0.8743  last_time: 0.8712  data_time: 0.0116  last_data_time: 0.0081   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:25:04 d2.utils.events]: \u001b[0m eta: 3 days, 16:39:29  iter: 3819  total_loss: 65.1  loss_ce: 0.7249  loss_mask: 0.2891  loss_dice: 0.8248  loss_bbox: 0.2978  loss_giou: 0.7676  loss_ce_dn: 0.1398  loss_mask_dn: 0.3024  loss_dice_dn: 0.7695  loss_bbox_dn: 0.3065  loss_giou_dn: 0.6756  loss_ce_0: 1.492  loss_mask_0: 0.2934  loss_dice_0: 0.8462  loss_bbox_0: 0.4265  loss_giou_0: 0.9219  loss_ce_dn_0: 0.7219  loss_mask_dn_0: 0.8063  loss_dice_dn_0: 3.071  loss_bbox_dn_0: 0.6231  loss_giou_dn_0: 1.005  loss_ce_1: 1.389  loss_mask_1: 0.35  loss_dice_1: 0.8329  loss_bbox_1: 0.3257  loss_giou_1: 0.8448  loss_ce_dn_1: 0.2707  loss_mask_dn_1: 0.4246  loss_dice_dn_1: 0.8633  loss_bbox_dn_1: 0.4015  loss_giou_dn_1: 0.7939  loss_ce_2: 1.222  loss_mask_2: 0.3428  loss_dice_2: 0.8542  loss_bbox_2: 0.3119  loss_giou_2: 0.8144  loss_ce_dn_2: 0.2262  loss_mask_dn_2: 0.3374  loss_dice_dn_2: 0.8296  loss_bbox_dn_2: 0.3615  loss_giou_dn_2: 0.7332  loss_ce_3: 1.035  loss_mask_3: 0.2997  loss_dice_3: 0.833  loss_bbox_3: 0.2828  loss_giou_3: 0.7883  loss_ce_dn_3: 0.1922  loss_mask_dn_3: 0.3206  loss_dice_dn_3: 0.8069  loss_bbox_dn_3: 0.3365  loss_giou_dn_3: 0.7128  loss_ce_4: 0.9446  loss_mask_4: 0.3041  loss_dice_4: 0.807  loss_bbox_4: 0.2971  loss_giou_4: 0.7798  loss_ce_dn_4: 0.1858  loss_mask_dn_4: 0.3064  loss_dice_dn_4: 0.7929  loss_bbox_dn_4: 0.3208  loss_giou_dn_4: 0.699  loss_ce_5: 0.822  loss_mask_5: 0.2964  loss_dice_5: 0.8147  loss_bbox_5: 0.2995  loss_giou_5: 0.7605  loss_ce_dn_5: 0.1673  loss_mask_dn_5: 0.3015  loss_dice_dn_5: 0.7897  loss_bbox_dn_5: 0.3184  loss_giou_dn_5: 0.6924  loss_ce_6: 0.8074  loss_mask_6: 0.2961  loss_dice_6: 0.842  loss_bbox_6: 0.2921  loss_giou_6: 0.7549  loss_ce_dn_6: 0.1573  loss_mask_dn_6: 0.2993  loss_dice_dn_6: 0.7843  loss_bbox_dn_6: 0.3063  loss_giou_dn_6: 0.6819  loss_ce_7: 0.7692  loss_mask_7: 0.2873  loss_dice_7: 0.8418  loss_bbox_7: 0.2962  loss_giou_7: 0.7709  loss_ce_dn_7: 0.1478  loss_mask_dn_7: 0.2948  loss_dice_dn_7: 0.772  loss_bbox_dn_7: 0.307  loss_giou_dn_7: 0.6811  loss_ce_8: 0.7584  loss_mask_8: 0.2881  loss_dice_8: 0.8424  loss_bbox_8: 0.2908  loss_giou_8: 0.7523  loss_ce_dn_8: 0.1496  loss_mask_dn_8: 0.299  loss_dice_dn_8: 0.7802  loss_bbox_dn_8: 0.3052  loss_giou_dn_8: 0.6765  loss_ce_interm: 1.49  loss_mask_interm: 0.2929  loss_dice_interm: 0.8312  loss_bbox_interm: 0.4285  loss_giou_interm: 0.9451    time: 0.8743  last_time: 0.8828  data_time: 0.0124  last_data_time: 0.0172   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:25:22 d2.utils.events]: \u001b[0m eta: 3 days, 16:40:36  iter: 3839  total_loss: 62.94  loss_ce: 0.8114  loss_mask: 0.2538  loss_dice: 0.6432  loss_bbox: 0.2432  loss_giou: 0.7986  loss_ce_dn: 0.1338  loss_mask_dn: 0.263  loss_dice_dn: 0.6624  loss_bbox_dn: 0.2159  loss_giou_dn: 0.6106  loss_ce_0: 1.449  loss_mask_0: 0.2576  loss_dice_0: 0.77  loss_bbox_0: 0.3533  loss_giou_0: 0.9638  loss_ce_dn_0: 0.7084  loss_mask_dn_0: 1.038  loss_dice_dn_0: 3.173  loss_bbox_dn_0: 0.6579  loss_giou_dn_0: 0.9999  loss_ce_1: 1.393  loss_mask_1: 0.2672  loss_dice_1: 0.7626  loss_bbox_1: 0.2556  loss_giou_1: 0.8676  loss_ce_dn_1: 0.2702  loss_mask_dn_1: 0.3651  loss_dice_dn_1: 0.7993  loss_bbox_dn_1: 0.3439  loss_giou_dn_1: 0.7573  loss_ce_2: 1.166  loss_mask_2: 0.2496  loss_dice_2: 0.7562  loss_bbox_2: 0.2477  loss_giou_2: 0.8736  loss_ce_dn_2: 0.2269  loss_mask_dn_2: 0.3218  loss_dice_dn_2: 0.7262  loss_bbox_dn_2: 0.2815  loss_giou_dn_2: 0.6876  loss_ce_3: 0.994  loss_mask_3: 0.2495  loss_dice_3: 0.7363  loss_bbox_3: 0.263  loss_giou_3: 0.864  loss_ce_dn_3: 0.2033  loss_mask_dn_3: 0.2906  loss_dice_dn_3: 0.6917  loss_bbox_dn_3: 0.2519  loss_giou_dn_3: 0.66  loss_ce_4: 0.9405  loss_mask_4: 0.2511  loss_dice_4: 0.6549  loss_bbox_4: 0.2607  loss_giou_4: 0.8516  loss_ce_dn_4: 0.1761  loss_mask_dn_4: 0.2774  loss_dice_dn_4: 0.6823  loss_bbox_dn_4: 0.2401  loss_giou_dn_4: 0.6435  loss_ce_5: 0.8726  loss_mask_5: 0.2487  loss_dice_5: 0.7175  loss_bbox_5: 0.2607  loss_giou_5: 0.839  loss_ce_dn_5: 0.1654  loss_mask_dn_5: 0.2777  loss_dice_dn_5: 0.6766  loss_bbox_dn_5: 0.2299  loss_giou_dn_5: 0.6343  loss_ce_6: 0.8802  loss_mask_6: 0.252  loss_dice_6: 0.7033  loss_bbox_6: 0.252  loss_giou_6: 0.8287  loss_ce_dn_6: 0.155  loss_mask_dn_6: 0.2808  loss_dice_dn_6: 0.6779  loss_bbox_dn_6: 0.225  loss_giou_dn_6: 0.6201  loss_ce_7: 0.8446  loss_mask_7: 0.2648  loss_dice_7: 0.7131  loss_bbox_7: 0.2486  loss_giou_7: 0.8679  loss_ce_dn_7: 0.1487  loss_mask_dn_7: 0.2681  loss_dice_dn_7: 0.6697  loss_bbox_dn_7: 0.2215  loss_giou_dn_7: 0.6167  loss_ce_8: 0.8189  loss_mask_8: 0.2567  loss_dice_8: 0.6681  loss_bbox_8: 0.2461  loss_giou_8: 0.8535  loss_ce_dn_8: 0.1345  loss_mask_dn_8: 0.2658  loss_dice_dn_8: 0.6656  loss_bbox_dn_8: 0.2162  loss_giou_dn_8: 0.61  loss_ce_interm: 1.476  loss_mask_interm: 0.2561  loss_dice_interm: 0.7552  loss_bbox_interm: 0.3502  loss_giou_interm: 0.9621    time: 0.8744  last_time: 0.8984  data_time: 0.0133  last_data_time: 0.0099   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:25:40 d2.utils.events]: \u001b[0m eta: 3 days, 16:40:08  iter: 3859  total_loss: 55.73  loss_ce: 0.7948  loss_mask: 0.246  loss_dice: 0.6039  loss_bbox: 0.2174  loss_giou: 0.6072  loss_ce_dn: 0.1154  loss_mask_dn: 0.2614  loss_dice_dn: 0.5935  loss_bbox_dn: 0.2003  loss_giou_dn: 0.5477  loss_ce_0: 1.425  loss_mask_0: 0.2852  loss_dice_0: 0.5774  loss_bbox_0: 0.3269  loss_giou_0: 0.9003  loss_ce_dn_0: 0.6972  loss_mask_dn_0: 0.8594  loss_dice_dn_0: 2.763  loss_bbox_dn_0: 0.5125  loss_giou_dn_0: 0.9808  loss_ce_1: 1.349  loss_mask_1: 0.2613  loss_dice_1: 0.62  loss_bbox_1: 0.2471  loss_giou_1: 0.7095  loss_ce_dn_1: 0.2354  loss_mask_dn_1: 0.296  loss_dice_dn_1: 0.7258  loss_bbox_dn_1: 0.2819  loss_giou_dn_1: 0.6998  loss_ce_2: 1.18  loss_mask_2: 0.2355  loss_dice_2: 0.6257  loss_bbox_2: 0.2406  loss_giou_2: 0.6695  loss_ce_dn_2: 0.189  loss_mask_dn_2: 0.2815  loss_dice_dn_2: 0.6664  loss_bbox_dn_2: 0.2321  loss_giou_dn_2: 0.618  loss_ce_3: 1.027  loss_mask_3: 0.2559  loss_dice_3: 0.5933  loss_bbox_3: 0.2449  loss_giou_3: 0.6866  loss_ce_dn_3: 0.1564  loss_mask_dn_3: 0.2697  loss_dice_dn_3: 0.6372  loss_bbox_dn_3: 0.218  loss_giou_dn_3: 0.5887  loss_ce_4: 0.8863  loss_mask_4: 0.2391  loss_dice_4: 0.6016  loss_bbox_4: 0.2369  loss_giou_4: 0.6557  loss_ce_dn_4: 0.1473  loss_mask_dn_4: 0.2581  loss_dice_dn_4: 0.5983  loss_bbox_dn_4: 0.212  loss_giou_dn_4: 0.5683  loss_ce_5: 0.8546  loss_mask_5: 0.2592  loss_dice_5: 0.6377  loss_bbox_5: 0.2372  loss_giou_5: 0.6214  loss_ce_dn_5: 0.1309  loss_mask_dn_5: 0.2618  loss_dice_dn_5: 0.6043  loss_bbox_dn_5: 0.2116  loss_giou_dn_5: 0.5642  loss_ce_6: 0.832  loss_mask_6: 0.2491  loss_dice_6: 0.5595  loss_bbox_6: 0.2376  loss_giou_6: 0.6152  loss_ce_dn_6: 0.1264  loss_mask_dn_6: 0.2664  loss_dice_dn_6: 0.6043  loss_bbox_dn_6: 0.205  loss_giou_dn_6: 0.5577  loss_ce_7: 0.8226  loss_mask_7: 0.2478  loss_dice_7: 0.6752  loss_bbox_7: 0.2145  loss_giou_7: 0.6114  loss_ce_dn_7: 0.1199  loss_mask_dn_7: 0.2567  loss_dice_dn_7: 0.5981  loss_bbox_dn_7: 0.2007  loss_giou_dn_7: 0.5566  loss_ce_8: 0.7921  loss_mask_8: 0.2422  loss_dice_8: 0.6645  loss_bbox_8: 0.2264  loss_giou_8: 0.6047  loss_ce_dn_8: 0.1146  loss_mask_dn_8: 0.2552  loss_dice_dn_8: 0.5907  loss_bbox_dn_8: 0.2004  loss_giou_dn_8: 0.5465  loss_ce_interm: 1.428  loss_mask_interm: 0.2853  loss_dice_interm: 0.676  loss_bbox_interm: 0.3285  loss_giou_interm: 0.9243    time: 0.8744  last_time: 0.9000  data_time: 0.0110  last_data_time: 0.0124   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:25:57 d2.utils.events]: \u001b[0m eta: 3 days, 16:38:18  iter: 3879  total_loss: 56.58  loss_ce: 0.6735  loss_mask: 0.2589  loss_dice: 0.7409  loss_bbox: 0.2506  loss_giou: 0.6459  loss_ce_dn: 0.1178  loss_mask_dn: 0.2912  loss_dice_dn: 0.6676  loss_bbox_dn: 0.2235  loss_giou_dn: 0.4776  loss_ce_0: 1.443  loss_mask_0: 0.2473  loss_dice_0: 0.8349  loss_bbox_0: 0.3818  loss_giou_0: 0.8341  loss_ce_dn_0: 0.7538  loss_mask_dn_0: 0.8503  loss_dice_dn_0: 3.101  loss_bbox_dn_0: 0.6326  loss_giou_dn_0: 0.9146  loss_ce_1: 1.22  loss_mask_1: 0.2729  loss_dice_1: 0.7959  loss_bbox_1: 0.3102  loss_giou_1: 0.7136  loss_ce_dn_1: 0.2382  loss_mask_dn_1: 0.3119  loss_dice_dn_1: 0.8361  loss_bbox_dn_1: 0.3936  loss_giou_dn_1: 0.6619  loss_ce_2: 1.042  loss_mask_2: 0.2585  loss_dice_2: 0.7606  loss_bbox_2: 0.2729  loss_giou_2: 0.6844  loss_ce_dn_2: 0.1847  loss_mask_dn_2: 0.2868  loss_dice_dn_2: 0.7479  loss_bbox_dn_2: 0.3296  loss_giou_dn_2: 0.5626  loss_ce_3: 0.9081  loss_mask_3: 0.2555  loss_dice_3: 0.7807  loss_bbox_3: 0.26  loss_giou_3: 0.6723  loss_ce_dn_3: 0.1574  loss_mask_dn_3: 0.3002  loss_dice_dn_3: 0.7089  loss_bbox_dn_3: 0.262  loss_giou_dn_3: 0.5222  loss_ce_4: 0.8438  loss_mask_4: 0.2509  loss_dice_4: 0.7664  loss_bbox_4: 0.259  loss_giou_4: 0.6831  loss_ce_dn_4: 0.1452  loss_mask_dn_4: 0.2967  loss_dice_dn_4: 0.6755  loss_bbox_dn_4: 0.2449  loss_giou_dn_4: 0.498  loss_ce_5: 0.7493  loss_mask_5: 0.2646  loss_dice_5: 0.7544  loss_bbox_5: 0.2582  loss_giou_5: 0.6667  loss_ce_dn_5: 0.1274  loss_mask_dn_5: 0.2941  loss_dice_dn_5: 0.6564  loss_bbox_dn_5: 0.2326  loss_giou_dn_5: 0.4879  loss_ce_6: 0.6785  loss_mask_6: 0.2509  loss_dice_6: 0.7813  loss_bbox_6: 0.2511  loss_giou_6: 0.6524  loss_ce_dn_6: 0.1203  loss_mask_dn_6: 0.2918  loss_dice_dn_6: 0.6858  loss_bbox_dn_6: 0.2314  loss_giou_dn_6: 0.4782  loss_ce_7: 0.699  loss_mask_7: 0.2564  loss_dice_7: 0.7907  loss_bbox_7: 0.2591  loss_giou_7: 0.6588  loss_ce_dn_7: 0.1195  loss_mask_dn_7: 0.293  loss_dice_dn_7: 0.6526  loss_bbox_dn_7: 0.2314  loss_giou_dn_7: 0.4741  loss_ce_8: 0.6493  loss_mask_8: 0.2561  loss_dice_8: 0.7621  loss_bbox_8: 0.2546  loss_giou_8: 0.6545  loss_ce_dn_8: 0.119  loss_mask_dn_8: 0.29  loss_dice_dn_8: 0.6518  loss_bbox_dn_8: 0.2296  loss_giou_dn_8: 0.4764  loss_ce_interm: 1.462  loss_mask_interm: 0.2417  loss_dice_interm: 0.8061  loss_bbox_interm: 0.3818  loss_giou_interm: 0.8329    time: 0.8744  last_time: 0.8867  data_time: 0.0128  last_data_time: 0.0144   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:26:15 d2.utils.events]: \u001b[0m eta: 3 days, 16:37:41  iter: 3899  total_loss: 53.09  loss_ce: 0.6252  loss_mask: 0.1554  loss_dice: 0.5611  loss_bbox: 0.1957  loss_giou: 0.5972  loss_ce_dn: 0.09713  loss_mask_dn: 0.1563  loss_dice_dn: 0.6264  loss_bbox_dn: 0.1772  loss_giou_dn: 0.4994  loss_ce_0: 1.341  loss_mask_0: 0.174  loss_dice_0: 0.6573  loss_bbox_0: 0.295  loss_giou_0: 0.8049  loss_ce_dn_0: 0.7471  loss_mask_dn_0: 0.7362  loss_dice_dn_0: 2.68  loss_bbox_dn_0: 0.5083  loss_giou_dn_0: 0.8979  loss_ce_1: 1.18  loss_mask_1: 0.1788  loss_dice_1: 0.6739  loss_bbox_1: 0.2393  loss_giou_1: 0.7118  loss_ce_dn_1: 0.2331  loss_mask_dn_1: 0.2211  loss_dice_dn_1: 0.6979  loss_bbox_dn_1: 0.2664  loss_giou_dn_1: 0.6391  loss_ce_2: 1.039  loss_mask_2: 0.1704  loss_dice_2: 0.6771  loss_bbox_2: 0.2147  loss_giou_2: 0.6625  loss_ce_dn_2: 0.1845  loss_mask_dn_2: 0.1696  loss_dice_dn_2: 0.6516  loss_bbox_dn_2: 0.2046  loss_giou_dn_2: 0.5697  loss_ce_3: 0.8688  loss_mask_3: 0.1664  loss_dice_3: 0.6371  loss_bbox_3: 0.2159  loss_giou_3: 0.65  loss_ce_dn_3: 0.1491  loss_mask_dn_3: 0.1674  loss_dice_dn_3: 0.6417  loss_bbox_dn_3: 0.1933  loss_giou_dn_3: 0.5351  loss_ce_4: 0.7708  loss_mask_4: 0.1556  loss_dice_4: 0.5897  loss_bbox_4: 0.2177  loss_giou_4: 0.638  loss_ce_dn_4: 0.1392  loss_mask_dn_4: 0.1638  loss_dice_dn_4: 0.6287  loss_bbox_dn_4: 0.1834  loss_giou_dn_4: 0.5158  loss_ce_5: 0.6944  loss_mask_5: 0.1615  loss_dice_5: 0.6426  loss_bbox_5: 0.2248  loss_giou_5: 0.6312  loss_ce_dn_5: 0.1261  loss_mask_dn_5: 0.1619  loss_dice_dn_5: 0.6221  loss_bbox_dn_5: 0.183  loss_giou_dn_5: 0.5087  loss_ce_6: 0.6635  loss_mask_6: 0.1562  loss_dice_6: 0.6276  loss_bbox_6: 0.2105  loss_giou_6: 0.6041  loss_ce_dn_6: 0.1187  loss_mask_dn_6: 0.1565  loss_dice_dn_6: 0.617  loss_bbox_dn_6: 0.1788  loss_giou_dn_6: 0.5017  loss_ce_7: 0.5913  loss_mask_7: 0.1543  loss_dice_7: 0.5963  loss_bbox_7: 0.1919  loss_giou_7: 0.5949  loss_ce_dn_7: 0.1091  loss_mask_dn_7: 0.1545  loss_dice_dn_7: 0.6238  loss_bbox_dn_7: 0.1788  loss_giou_dn_7: 0.5026  loss_ce_8: 0.6005  loss_mask_8: 0.1596  loss_dice_8: 0.5929  loss_bbox_8: 0.1938  loss_giou_8: 0.5953  loss_ce_dn_8: 0.1032  loss_mask_dn_8: 0.1515  loss_dice_dn_8: 0.6401  loss_bbox_dn_8: 0.1765  loss_giou_dn_8: 0.4979  loss_ce_interm: 1.341  loss_mask_interm: 0.173  loss_dice_interm: 0.6671  loss_bbox_interm: 0.297  loss_giou_interm: 0.8172    time: 0.8743  last_time: 0.8714  data_time: 0.0108  last_data_time: 0.0120   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:26:32 d2.utils.events]: \u001b[0m eta: 3 days, 16:36:47  iter: 3919  total_loss: 61.66  loss_ce: 0.8333  loss_mask: 0.2477  loss_dice: 0.6864  loss_bbox: 0.2093  loss_giou: 0.8892  loss_ce_dn: 0.1521  loss_mask_dn: 0.2528  loss_dice_dn: 0.6281  loss_bbox_dn: 0.2137  loss_giou_dn: 0.621  loss_ce_0: 1.515  loss_mask_0: 0.2571  loss_dice_0: 0.6921  loss_bbox_0: 0.3994  loss_giou_0: 0.9911  loss_ce_dn_0: 0.6776  loss_mask_dn_0: 0.7624  loss_dice_dn_0: 3.082  loss_bbox_dn_0: 0.4918  loss_giou_dn_0: 0.9996  loss_ce_1: 1.468  loss_mask_1: 0.2539  loss_dice_1: 0.715  loss_bbox_1: 0.3143  loss_giou_1: 0.9196  loss_ce_dn_1: 0.265  loss_mask_dn_1: 0.3252  loss_dice_dn_1: 0.8107  loss_bbox_dn_1: 0.2988  loss_giou_dn_1: 0.7609  loss_ce_2: 1.283  loss_mask_2: 0.2672  loss_dice_2: 0.7695  loss_bbox_2: 0.2772  loss_giou_2: 0.8906  loss_ce_dn_2: 0.2204  loss_mask_dn_2: 0.2922  loss_dice_dn_2: 0.707  loss_bbox_dn_2: 0.2557  loss_giou_dn_2: 0.6948  loss_ce_3: 1.135  loss_mask_3: 0.2631  loss_dice_3: 0.7107  loss_bbox_3: 0.2482  loss_giou_3: 0.8703  loss_ce_dn_3: 0.1943  loss_mask_dn_3: 0.2647  loss_dice_dn_3: 0.6898  loss_bbox_dn_3: 0.2369  loss_giou_dn_3: 0.6646  loss_ce_4: 1.009  loss_mask_4: 0.261  loss_dice_4: 0.7773  loss_bbox_4: 0.2624  loss_giou_4: 0.8755  loss_ce_dn_4: 0.1826  loss_mask_dn_4: 0.2556  loss_dice_dn_4: 0.6517  loss_bbox_dn_4: 0.2248  loss_giou_dn_4: 0.6475  loss_ce_5: 0.9483  loss_mask_5: 0.2469  loss_dice_5: 0.6817  loss_bbox_5: 0.2019  loss_giou_5: 0.8971  loss_ce_dn_5: 0.1773  loss_mask_dn_5: 0.2526  loss_dice_dn_5: 0.6373  loss_bbox_dn_5: 0.2199  loss_giou_dn_5: 0.6435  loss_ce_6: 0.8822  loss_mask_6: 0.2475  loss_dice_6: 0.6758  loss_bbox_6: 0.1935  loss_giou_6: 0.85  loss_ce_dn_6: 0.1685  loss_mask_dn_6: 0.2564  loss_dice_dn_6: 0.6233  loss_bbox_dn_6: 0.22  loss_giou_dn_6: 0.6315  loss_ce_7: 0.8717  loss_mask_7: 0.2445  loss_dice_7: 0.6509  loss_bbox_7: 0.2027  loss_giou_7: 0.8917  loss_ce_dn_7: 0.1602  loss_mask_dn_7: 0.2509  loss_dice_dn_7: 0.6272  loss_bbox_dn_7: 0.2166  loss_giou_dn_7: 0.6297  loss_ce_8: 0.8134  loss_mask_8: 0.2479  loss_dice_8: 0.701  loss_bbox_8: 0.2065  loss_giou_8: 0.8864  loss_ce_dn_8: 0.156  loss_mask_dn_8: 0.2504  loss_dice_dn_8: 0.6085  loss_bbox_dn_8: 0.2142  loss_giou_dn_8: 0.6229  loss_ce_interm: 1.503  loss_mask_interm: 0.2537  loss_dice_interm: 0.7112  loss_bbox_interm: 0.4333  loss_giou_interm: 1.041    time: 0.8743  last_time: 0.8476  data_time: 0.0135  last_data_time: 0.0124   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:26:50 d2.utils.events]: \u001b[0m eta: 3 days, 16:38:24  iter: 3939  total_loss: 68.3  loss_ce: 0.8784  loss_mask: 0.2221  loss_dice: 0.975  loss_bbox: 0.3236  loss_giou: 0.6653  loss_ce_dn: 0.126  loss_mask_dn: 0.2348  loss_dice_dn: 0.987  loss_bbox_dn: 0.2864  loss_giou_dn: 0.5384  loss_ce_0: 1.481  loss_mask_0: 0.2241  loss_dice_0: 1.08  loss_bbox_0: 0.4383  loss_giou_0: 0.856  loss_ce_dn_0: 0.7245  loss_mask_dn_0: 0.8605  loss_dice_dn_0: 3.018  loss_bbox_dn_0: 0.6535  loss_giou_dn_0: 0.8781  loss_ce_1: 1.464  loss_mask_1: 0.2347  loss_dice_1: 0.9848  loss_bbox_1: 0.3358  loss_giou_1: 0.7504  loss_ce_dn_1: 0.2498  loss_mask_dn_1: 0.2958  loss_dice_dn_1: 1.132  loss_bbox_dn_1: 0.3824  loss_giou_dn_1: 0.6693  loss_ce_2: 1.309  loss_mask_2: 0.2262  loss_dice_2: 1.015  loss_bbox_2: 0.3296  loss_giou_2: 0.717  loss_ce_dn_2: 0.1987  loss_mask_dn_2: 0.2621  loss_dice_dn_2: 1.017  loss_bbox_dn_2: 0.3223  loss_giou_dn_2: 0.616  loss_ce_3: 1.164  loss_mask_3: 0.224  loss_dice_3: 0.9968  loss_bbox_3: 0.3116  loss_giou_3: 0.7029  loss_ce_dn_3: 0.164  loss_mask_dn_3: 0.235  loss_dice_dn_3: 0.9995  loss_bbox_dn_3: 0.3095  loss_giou_dn_3: 0.58  loss_ce_4: 1.068  loss_mask_4: 0.2301  loss_dice_4: 0.967  loss_bbox_4: 0.313  loss_giou_4: 0.6806  loss_ce_dn_4: 0.1477  loss_mask_dn_4: 0.2359  loss_dice_dn_4: 0.9873  loss_bbox_dn_4: 0.3024  loss_giou_dn_4: 0.5677  loss_ce_5: 0.9509  loss_mask_5: 0.2202  loss_dice_5: 0.98  loss_bbox_5: 0.3289  loss_giou_5: 0.7234  loss_ce_dn_5: 0.1402  loss_mask_dn_5: 0.2325  loss_dice_dn_5: 0.9616  loss_bbox_dn_5: 0.2978  loss_giou_dn_5: 0.5587  loss_ce_6: 0.9274  loss_mask_6: 0.2205  loss_dice_6: 0.9882  loss_bbox_6: 0.3139  loss_giou_6: 0.7003  loss_ce_dn_6: 0.1398  loss_mask_dn_6: 0.2354  loss_dice_dn_6: 0.9674  loss_bbox_dn_6: 0.2925  loss_giou_dn_6: 0.553  loss_ce_7: 0.8848  loss_mask_7: 0.2198  loss_dice_7: 0.9761  loss_bbox_7: 0.3155  loss_giou_7: 0.7033  loss_ce_dn_7: 0.1339  loss_mask_dn_7: 0.2284  loss_dice_dn_7: 0.9451  loss_bbox_dn_7: 0.2884  loss_giou_dn_7: 0.5479  loss_ce_8: 0.8988  loss_mask_8: 0.2218  loss_dice_8: 0.9871  loss_bbox_8: 0.3237  loss_giou_8: 0.6873  loss_ce_dn_8: 0.1283  loss_mask_dn_8: 0.2357  loss_dice_dn_8: 0.9623  loss_bbox_dn_8: 0.2859  loss_giou_dn_8: 0.5389  loss_ce_interm: 1.505  loss_mask_interm: 0.223  loss_dice_interm: 1.018  loss_bbox_interm: 0.439  loss_giou_interm: 0.8511    time: 0.8743  last_time: 0.9013  data_time: 0.0127  last_data_time: 0.0103   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:27:07 d2.utils.events]: \u001b[0m eta: 3 days, 16:36:03  iter: 3959  total_loss: 56.95  loss_ce: 0.7083  loss_mask: 0.1979  loss_dice: 0.6951  loss_bbox: 0.2469  loss_giou: 0.7178  loss_ce_dn: 0.1538  loss_mask_dn: 0.2109  loss_dice_dn: 0.6985  loss_bbox_dn: 0.1879  loss_giou_dn: 0.6124  loss_ce_0: 1.433  loss_mask_0: 0.2699  loss_dice_0: 0.9096  loss_bbox_0: 0.3529  loss_giou_0: 0.8859  loss_ce_dn_0: 0.7427  loss_mask_dn_0: 0.8486  loss_dice_dn_0: 3.089  loss_bbox_dn_0: 0.5219  loss_giou_dn_0: 0.9607  loss_ce_1: 1.382  loss_mask_1: 0.2261  loss_dice_1: 0.7477  loss_bbox_1: 0.2733  loss_giou_1: 0.7745  loss_ce_dn_1: 0.2978  loss_mask_dn_1: 0.2565  loss_dice_dn_1: 0.8857  loss_bbox_dn_1: 0.3007  loss_giou_dn_1: 0.7466  loss_ce_2: 1.213  loss_mask_2: 0.2116  loss_dice_2: 0.776  loss_bbox_2: 0.2272  loss_giou_2: 0.7451  loss_ce_dn_2: 0.2247  loss_mask_dn_2: 0.2369  loss_dice_dn_2: 0.7499  loss_bbox_dn_2: 0.2469  loss_giou_dn_2: 0.6698  loss_ce_3: 1.038  loss_mask_3: 0.2124  loss_dice_3: 0.6891  loss_bbox_3: 0.2321  loss_giou_3: 0.7305  loss_ce_dn_3: 0.1896  loss_mask_dn_3: 0.2256  loss_dice_dn_3: 0.7627  loss_bbox_dn_3: 0.2201  loss_giou_dn_3: 0.6553  loss_ce_4: 0.9523  loss_mask_4: 0.2118  loss_dice_4: 0.7062  loss_bbox_4: 0.2196  loss_giou_4: 0.7179  loss_ce_dn_4: 0.1711  loss_mask_dn_4: 0.218  loss_dice_dn_4: 0.7298  loss_bbox_dn_4: 0.2068  loss_giou_dn_4: 0.6332  loss_ce_5: 0.8042  loss_mask_5: 0.1986  loss_dice_5: 0.7313  loss_bbox_5: 0.2316  loss_giou_5: 0.7388  loss_ce_dn_5: 0.1564  loss_mask_dn_5: 0.2155  loss_dice_dn_5: 0.6947  loss_bbox_dn_5: 0.1991  loss_giou_dn_5: 0.6238  loss_ce_6: 0.7743  loss_mask_6: 0.2166  loss_dice_6: 0.7097  loss_bbox_6: 0.2452  loss_giou_6: 0.7227  loss_ce_dn_6: 0.1655  loss_mask_dn_6: 0.2138  loss_dice_dn_6: 0.6794  loss_bbox_dn_6: 0.1947  loss_giou_dn_6: 0.6181  loss_ce_7: 0.7479  loss_mask_7: 0.2038  loss_dice_7: 0.7034  loss_bbox_7: 0.2378  loss_giou_7: 0.722  loss_ce_dn_7: 0.1603  loss_mask_dn_7: 0.2131  loss_dice_dn_7: 0.6812  loss_bbox_dn_7: 0.1909  loss_giou_dn_7: 0.6164  loss_ce_8: 0.7186  loss_mask_8: 0.1977  loss_dice_8: 0.7267  loss_bbox_8: 0.2381  loss_giou_8: 0.7201  loss_ce_dn_8: 0.1528  loss_mask_dn_8: 0.2089  loss_dice_dn_8: 0.6839  loss_bbox_dn_8: 0.1865  loss_giou_dn_8: 0.612  loss_ce_interm: 1.431  loss_mask_interm: 0.2666  loss_dice_interm: 0.8739  loss_bbox_interm: 0.3564  loss_giou_interm: 0.9008    time: 0.8743  last_time: 0.8703  data_time: 0.0121  last_data_time: 0.0105   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:27:25 d2.utils.events]: \u001b[0m eta: 3 days, 16:35:46  iter: 3979  total_loss: 59.94  loss_ce: 0.7615  loss_mask: 0.2124  loss_dice: 0.6508  loss_bbox: 0.2577  loss_giou: 0.6789  loss_ce_dn: 0.1114  loss_mask_dn: 0.1864  loss_dice_dn: 0.7611  loss_bbox_dn: 0.2079  loss_giou_dn: 0.5424  loss_ce_0: 1.491  loss_mask_0: 0.2137  loss_dice_0: 0.728  loss_bbox_0: 0.416  loss_giou_0: 0.8928  loss_ce_dn_0: 0.7614  loss_mask_dn_0: 0.9411  loss_dice_dn_0: 3.271  loss_bbox_dn_0: 0.592  loss_giou_dn_0: 0.9244  loss_ce_1: 1.302  loss_mask_1: 0.2315  loss_dice_1: 0.7124  loss_bbox_1: 0.2911  loss_giou_1: 0.6921  loss_ce_dn_1: 0.284  loss_mask_dn_1: 0.2757  loss_dice_dn_1: 0.9158  loss_bbox_dn_1: 0.2993  loss_giou_dn_1: 0.6584  loss_ce_2: 1.164  loss_mask_2: 0.216  loss_dice_2: 0.6809  loss_bbox_2: 0.2616  loss_giou_2: 0.6877  loss_ce_dn_2: 0.2244  loss_mask_dn_2: 0.2113  loss_dice_dn_2: 0.851  loss_bbox_dn_2: 0.2428  loss_giou_dn_2: 0.602  loss_ce_3: 1.018  loss_mask_3: 0.2165  loss_dice_3: 0.7013  loss_bbox_3: 0.2681  loss_giou_3: 0.6969  loss_ce_dn_3: 0.1835  loss_mask_dn_3: 0.2007  loss_dice_dn_3: 0.8039  loss_bbox_dn_3: 0.2375  loss_giou_dn_3: 0.5714  loss_ce_4: 0.9392  loss_mask_4: 0.207  loss_dice_4: 0.6708  loss_bbox_4: 0.2693  loss_giou_4: 0.6698  loss_ce_dn_4: 0.1524  loss_mask_dn_4: 0.1953  loss_dice_dn_4: 0.8105  loss_bbox_dn_4: 0.2164  loss_giou_dn_4: 0.5519  loss_ce_5: 0.885  loss_mask_5: 0.2062  loss_dice_5: 0.6861  loss_bbox_5: 0.2639  loss_giou_5: 0.6412  loss_ce_dn_5: 0.1324  loss_mask_dn_5: 0.197  loss_dice_dn_5: 0.7884  loss_bbox_dn_5: 0.2131  loss_giou_dn_5: 0.5499  loss_ce_6: 0.8085  loss_mask_6: 0.2032  loss_dice_6: 0.692  loss_bbox_6: 0.2512  loss_giou_6: 0.6535  loss_ce_dn_6: 0.1295  loss_mask_dn_6: 0.1975  loss_dice_dn_6: 0.7987  loss_bbox_dn_6: 0.2087  loss_giou_dn_6: 0.544  loss_ce_7: 0.7733  loss_mask_7: 0.2146  loss_dice_7: 0.6497  loss_bbox_7: 0.26  loss_giou_7: 0.6837  loss_ce_dn_7: 0.1229  loss_mask_dn_7: 0.1906  loss_dice_dn_7: 0.7907  loss_bbox_dn_7: 0.208  loss_giou_dn_7: 0.5477  loss_ce_8: 0.7634  loss_mask_8: 0.2016  loss_dice_8: 0.6951  loss_bbox_8: 0.2593  loss_giou_8: 0.6637  loss_ce_dn_8: 0.1163  loss_mask_dn_8: 0.1915  loss_dice_dn_8: 0.7883  loss_bbox_dn_8: 0.2071  loss_giou_dn_8: 0.544  loss_ce_interm: 1.488  loss_mask_interm: 0.2177  loss_dice_interm: 0.733  loss_bbox_interm: 0.4118  loss_giou_interm: 0.8882    time: 0.8743  last_time: 0.8673  data_time: 0.0147  last_data_time: 0.0126   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:27:42 d2.utils.events]: \u001b[0m eta: 3 days, 16:34:59  iter: 3999  total_loss: 58.58  loss_ce: 0.6861  loss_mask: 0.2923  loss_dice: 0.7604  loss_bbox: 0.2469  loss_giou: 0.663  loss_ce_dn: 0.1173  loss_mask_dn: 0.307  loss_dice_dn: 0.6995  loss_bbox_dn: 0.2487  loss_giou_dn: 0.5235  loss_ce_0: 1.452  loss_mask_0: 0.3073  loss_dice_0: 0.7328  loss_bbox_0: 0.4193  loss_giou_0: 0.8596  loss_ce_dn_0: 0.6831  loss_mask_dn_0: 0.9072  loss_dice_dn_0: 2.937  loss_bbox_dn_0: 0.7112  loss_giou_dn_0: 0.9378  loss_ce_1: 1.242  loss_mask_1: 0.3234  loss_dice_1: 0.7155  loss_bbox_1: 0.3184  loss_giou_1: 0.7042  loss_ce_dn_1: 0.2452  loss_mask_dn_1: 0.3888  loss_dice_dn_1: 0.7906  loss_bbox_dn_1: 0.3946  loss_giou_dn_1: 0.6659  loss_ce_2: 1.132  loss_mask_2: 0.3223  loss_dice_2: 0.722  loss_bbox_2: 0.2738  loss_giou_2: 0.7102  loss_ce_dn_2: 0.1965  loss_mask_dn_2: 0.3594  loss_dice_dn_2: 0.743  loss_bbox_dn_2: 0.3171  loss_giou_dn_2: 0.5886  loss_ce_3: 0.9809  loss_mask_3: 0.3183  loss_dice_3: 0.747  loss_bbox_3: 0.2534  loss_giou_3: 0.7268  loss_ce_dn_3: 0.1707  loss_mask_dn_3: 0.3271  loss_dice_dn_3: 0.6893  loss_bbox_dn_3: 0.2875  loss_giou_dn_3: 0.5582  loss_ce_4: 0.8952  loss_mask_4: 0.3124  loss_dice_4: 0.8043  loss_bbox_4: 0.2657  loss_giou_4: 0.6949  loss_ce_dn_4: 0.1442  loss_mask_dn_4: 0.3115  loss_dice_dn_4: 0.6916  loss_bbox_dn_4: 0.2602  loss_giou_dn_4: 0.5444  loss_ce_5: 0.7787  loss_mask_5: 0.3054  loss_dice_5: 0.7043  loss_bbox_5: 0.2557  loss_giou_5: 0.6508  loss_ce_dn_5: 0.1284  loss_mask_dn_5: 0.3057  loss_dice_dn_5: 0.6822  loss_bbox_dn_5: 0.2542  loss_giou_dn_5: 0.542  loss_ce_6: 0.6935  loss_mask_6: 0.2951  loss_dice_6: 0.7235  loss_bbox_6: 0.2383  loss_giou_6: 0.6433  loss_ce_dn_6: 0.1179  loss_mask_dn_6: 0.3101  loss_dice_dn_6: 0.6834  loss_bbox_dn_6: 0.2492  loss_giou_dn_6: 0.5299  loss_ce_7: 0.6753  loss_mask_7: 0.2965  loss_dice_7: 0.7467  loss_bbox_7: 0.2388  loss_giou_7: 0.672  loss_ce_dn_7: 0.1117  loss_mask_dn_7: 0.3044  loss_dice_dn_7: 0.6944  loss_bbox_dn_7: 0.2492  loss_giou_dn_7: 0.5279  loss_ce_8: 0.6605  loss_mask_8: 0.2923  loss_dice_8: 0.7324  loss_bbox_8: 0.2388  loss_giou_8: 0.6244  loss_ce_dn_8: 0.114  loss_mask_dn_8: 0.306  loss_dice_dn_8: 0.6836  loss_bbox_dn_8: 0.2483  loss_giou_dn_8: 0.5228  loss_ce_interm: 1.448  loss_mask_interm: 0.3116  loss_dice_interm: 0.724  loss_bbox_interm: 0.4237  loss_giou_interm: 0.86    time: 0.8743  last_time: 0.8828  data_time: 0.0124  last_data_time: 0.0179   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:28:00 d2.utils.events]: \u001b[0m eta: 3 days, 16:33:07  iter: 4019  total_loss: 67.32  loss_ce: 0.7968  loss_mask: 0.2545  loss_dice: 1.055  loss_bbox: 0.3225  loss_giou: 0.6871  loss_ce_dn: 0.0896  loss_mask_dn: 0.2575  loss_dice_dn: 1.001  loss_bbox_dn: 0.2698  loss_giou_dn: 0.5464  loss_ce_0: 1.442  loss_mask_0: 0.2595  loss_dice_0: 1.168  loss_bbox_0: 0.4373  loss_giou_0: 0.9072  loss_ce_dn_0: 0.7516  loss_mask_dn_0: 0.798  loss_dice_dn_0: 2.91  loss_bbox_dn_0: 0.6557  loss_giou_dn_0: 0.8649  loss_ce_1: 1.348  loss_mask_1: 0.2638  loss_dice_1: 1.036  loss_bbox_1: 0.3869  loss_giou_1: 0.7637  loss_ce_dn_1: 0.2302  loss_mask_dn_1: 0.3382  loss_dice_dn_1: 1.205  loss_bbox_dn_1: 0.3746  loss_giou_dn_1: 0.6573  loss_ce_2: 1.204  loss_mask_2: 0.2655  loss_dice_2: 1.069  loss_bbox_2: 0.3445  loss_giou_2: 0.7181  loss_ce_dn_2: 0.1815  loss_mask_dn_2: 0.3039  loss_dice_dn_2: 1.091  loss_bbox_dn_2: 0.3167  loss_giou_dn_2: 0.6094  loss_ce_3: 1.116  loss_mask_3: 0.2698  loss_dice_3: 1.098  loss_bbox_3: 0.3312  loss_giou_3: 0.7003  loss_ce_dn_3: 0.1452  loss_mask_dn_3: 0.2889  loss_dice_dn_3: 1.048  loss_bbox_dn_3: 0.2935  loss_giou_dn_3: 0.5751  loss_ce_4: 0.9989  loss_mask_4: 0.2466  loss_dice_4: 1.055  loss_bbox_4: 0.3256  loss_giou_4: 0.7053  loss_ce_dn_4: 0.1229  loss_mask_dn_4: 0.2687  loss_dice_dn_4: 1.022  loss_bbox_dn_4: 0.2887  loss_giou_dn_4: 0.5576  loss_ce_5: 0.8689  loss_mask_5: 0.2519  loss_dice_5: 1.008  loss_bbox_5: 0.3215  loss_giou_5: 0.7268  loss_ce_dn_5: 0.1037  loss_mask_dn_5: 0.2731  loss_dice_dn_5: 1.01  loss_bbox_dn_5: 0.2844  loss_giou_dn_5: 0.5538  loss_ce_6: 0.8932  loss_mask_6: 0.2656  loss_dice_6: 1.025  loss_bbox_6: 0.3201  loss_giou_6: 0.7027  loss_ce_dn_6: 0.09869  loss_mask_dn_6: 0.2729  loss_dice_dn_6: 1.011  loss_bbox_dn_6: 0.2769  loss_giou_dn_6: 0.5491  loss_ce_7: 0.8374  loss_mask_7: 0.2522  loss_dice_7: 1.073  loss_bbox_7: 0.3153  loss_giou_7: 0.6879  loss_ce_dn_7: 0.09392  loss_mask_dn_7: 0.2679  loss_dice_dn_7: 1  loss_bbox_dn_7: 0.2723  loss_giou_dn_7: 0.549  loss_ce_8: 0.7616  loss_mask_8: 0.254  loss_dice_8: 1.076  loss_bbox_8: 0.3137  loss_giou_8: 0.6815  loss_ce_dn_8: 0.09275  loss_mask_dn_8: 0.2586  loss_dice_dn_8: 1.026  loss_bbox_dn_8: 0.2699  loss_giou_dn_8: 0.5469  loss_ce_interm: 1.456  loss_mask_interm: 0.2623  loss_dice_interm: 1.152  loss_bbox_interm: 0.4454  loss_giou_interm: 0.8827    time: 0.8743  last_time: 0.8560  data_time: 0.0136  last_data_time: 0.0108   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:28:17 d2.utils.events]: \u001b[0m eta: 3 days, 16:32:47  iter: 4039  total_loss: 64.56  loss_ce: 0.765  loss_mask: 0.2655  loss_dice: 0.8116  loss_bbox: 0.2695  loss_giou: 0.7771  loss_ce_dn: 0.1287  loss_mask_dn: 0.259  loss_dice_dn: 0.7875  loss_bbox_dn: 0.2326  loss_giou_dn: 0.6337  loss_ce_0: 1.482  loss_mask_0: 0.2791  loss_dice_0: 0.7863  loss_bbox_0: 0.4191  loss_giou_0: 0.9557  loss_ce_dn_0: 0.6903  loss_mask_dn_0: 0.7835  loss_dice_dn_0: 3.137  loss_bbox_dn_0: 0.5895  loss_giou_dn_0: 0.9954  loss_ce_1: 1.346  loss_mask_1: 0.2707  loss_dice_1: 0.7482  loss_bbox_1: 0.3016  loss_giou_1: 0.8754  loss_ce_dn_1: 0.2688  loss_mask_dn_1: 0.3244  loss_dice_dn_1: 0.8855  loss_bbox_dn_1: 0.3473  loss_giou_dn_1: 0.7593  loss_ce_2: 1.167  loss_mask_2: 0.2682  loss_dice_2: 0.7805  loss_bbox_2: 0.2865  loss_giou_2: 0.8415  loss_ce_dn_2: 0.2156  loss_mask_dn_2: 0.2931  loss_dice_dn_2: 0.8354  loss_bbox_dn_2: 0.2955  loss_giou_dn_2: 0.7001  loss_ce_3: 1.049  loss_mask_3: 0.2709  loss_dice_3: 0.7874  loss_bbox_3: 0.2571  loss_giou_3: 0.8307  loss_ce_dn_3: 0.1849  loss_mask_dn_3: 0.2686  loss_dice_dn_3: 0.8051  loss_bbox_dn_3: 0.2686  loss_giou_dn_3: 0.6801  loss_ce_4: 0.914  loss_mask_4: 0.2747  loss_dice_4: 0.7979  loss_bbox_4: 0.2589  loss_giou_4: 0.8042  loss_ce_dn_4: 0.1648  loss_mask_dn_4: 0.2637  loss_dice_dn_4: 0.793  loss_bbox_dn_4: 0.249  loss_giou_dn_4: 0.6589  loss_ce_5: 0.8396  loss_mask_5: 0.2704  loss_dice_5: 0.8019  loss_bbox_5: 0.2664  loss_giou_5: 0.8134  loss_ce_dn_5: 0.1496  loss_mask_dn_5: 0.2503  loss_dice_dn_5: 0.7891  loss_bbox_dn_5: 0.2432  loss_giou_dn_5: 0.651  loss_ce_6: 0.7729  loss_mask_6: 0.2744  loss_dice_6: 0.7707  loss_bbox_6: 0.273  loss_giou_6: 0.8158  loss_ce_dn_6: 0.137  loss_mask_dn_6: 0.2461  loss_dice_dn_6: 0.7868  loss_bbox_dn_6: 0.2329  loss_giou_dn_6: 0.6412  loss_ce_7: 0.7293  loss_mask_7: 0.2721  loss_dice_7: 0.8168  loss_bbox_7: 0.2784  loss_giou_7: 0.8051  loss_ce_dn_7: 0.1311  loss_mask_dn_7: 0.2438  loss_dice_dn_7: 0.7971  loss_bbox_dn_7: 0.2346  loss_giou_dn_7: 0.6364  loss_ce_8: 0.7807  loss_mask_8: 0.2762  loss_dice_8: 0.774  loss_bbox_8: 0.2543  loss_giou_8: 0.794  loss_ce_dn_8: 0.1293  loss_mask_dn_8: 0.2441  loss_dice_dn_8: 0.7891  loss_bbox_dn_8: 0.2316  loss_giou_dn_8: 0.6332  loss_ce_interm: 1.495  loss_mask_interm: 0.2752  loss_dice_interm: 0.8168  loss_bbox_interm: 0.4355  loss_giou_interm: 0.9844    time: 0.8743  last_time: 0.8741  data_time: 0.0125  last_data_time: 0.0133   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:28:35 d2.utils.events]: \u001b[0m eta: 3 days, 16:33:05  iter: 4059  total_loss: 64  loss_ce: 0.8359  loss_mask: 0.2346  loss_dice: 0.8626  loss_bbox: 0.2787  loss_giou: 0.7136  loss_ce_dn: 0.1318  loss_mask_dn: 0.2237  loss_dice_dn: 0.9071  loss_bbox_dn: 0.2159  loss_giou_dn: 0.5471  loss_ce_0: 1.526  loss_mask_0: 0.2734  loss_dice_0: 0.975  loss_bbox_0: 0.4099  loss_giou_0: 0.8738  loss_ce_dn_0: 0.7443  loss_mask_dn_0: 0.7143  loss_dice_dn_0: 3.13  loss_bbox_dn_0: 0.5279  loss_giou_dn_0: 0.9259  loss_ce_1: 1.36  loss_mask_1: 0.285  loss_dice_1: 0.9379  loss_bbox_1: 0.3366  loss_giou_1: 0.8097  loss_ce_dn_1: 0.255  loss_mask_dn_1: 0.2919  loss_dice_dn_1: 1.015  loss_bbox_dn_1: 0.3153  loss_giou_dn_1: 0.6959  loss_ce_2: 1.213  loss_mask_2: 0.2661  loss_dice_2: 0.9066  loss_bbox_2: 0.3086  loss_giou_2: 0.7633  loss_ce_dn_2: 0.1979  loss_mask_dn_2: 0.2544  loss_dice_dn_2: 0.9503  loss_bbox_dn_2: 0.277  loss_giou_dn_2: 0.6291  loss_ce_3: 0.9857  loss_mask_3: 0.2513  loss_dice_3: 0.9017  loss_bbox_3: 0.2913  loss_giou_3: 0.7252  loss_ce_dn_3: 0.1695  loss_mask_dn_3: 0.2335  loss_dice_dn_3: 0.8973  loss_bbox_dn_3: 0.2535  loss_giou_dn_3: 0.5936  loss_ce_4: 0.9069  loss_mask_4: 0.2456  loss_dice_4: 0.8847  loss_bbox_4: 0.2741  loss_giou_4: 0.7121  loss_ce_dn_4: 0.1601  loss_mask_dn_4: 0.2226  loss_dice_dn_4: 0.8845  loss_bbox_dn_4: 0.2273  loss_giou_dn_4: 0.5715  loss_ce_5: 0.8995  loss_mask_5: 0.2325  loss_dice_5: 0.8787  loss_bbox_5: 0.273  loss_giou_5: 0.6899  loss_ce_dn_5: 0.1533  loss_mask_dn_5: 0.2218  loss_dice_dn_5: 0.8781  loss_bbox_dn_5: 0.2217  loss_giou_dn_5: 0.5654  loss_ce_6: 0.8966  loss_mask_6: 0.2331  loss_dice_6: 0.8852  loss_bbox_6: 0.258  loss_giou_6: 0.7185  loss_ce_dn_6: 0.143  loss_mask_dn_6: 0.2241  loss_dice_dn_6: 0.8742  loss_bbox_dn_6: 0.2192  loss_giou_dn_6: 0.5541  loss_ce_7: 0.8537  loss_mask_7: 0.2313  loss_dice_7: 0.8766  loss_bbox_7: 0.282  loss_giou_7: 0.7101  loss_ce_dn_7: 0.1385  loss_mask_dn_7: 0.2231  loss_dice_dn_7: 0.8716  loss_bbox_dn_7: 0.2172  loss_giou_dn_7: 0.5481  loss_ce_8: 0.8541  loss_mask_8: 0.2294  loss_dice_8: 0.8816  loss_bbox_8: 0.2802  loss_giou_8: 0.682  loss_ce_dn_8: 0.1371  loss_mask_dn_8: 0.2226  loss_dice_dn_8: 0.8782  loss_bbox_dn_8: 0.2158  loss_giou_dn_8: 0.5455  loss_ce_interm: 1.527  loss_mask_interm: 0.2722  loss_dice_interm: 1.02  loss_bbox_interm: 0.4087  loss_giou_interm: 0.8638    time: 0.8743  last_time: 0.8818  data_time: 0.0123  last_data_time: 0.0124   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:28:52 d2.utils.events]: \u001b[0m eta: 3 days, 16:32:12  iter: 4079  total_loss: 64.66  loss_ce: 0.8398  loss_mask: 0.3332  loss_dice: 0.8537  loss_bbox: 0.3175  loss_giou: 0.6856  loss_ce_dn: 0.1149  loss_mask_dn: 0.3835  loss_dice_dn: 0.9335  loss_bbox_dn: 0.2619  loss_giou_dn: 0.5452  loss_ce_0: 1.496  loss_mask_0: 0.2926  loss_dice_0: 0.9765  loss_bbox_0: 0.4642  loss_giou_0: 0.8575  loss_ce_dn_0: 0.739  loss_mask_dn_0: 1.128  loss_dice_dn_0: 3.253  loss_bbox_dn_0: 0.7205  loss_giou_dn_0: 0.941  loss_ce_1: 1.418  loss_mask_1: 0.3338  loss_dice_1: 0.9588  loss_bbox_1: 0.366  loss_giou_1: 0.7427  loss_ce_dn_1: 0.2601  loss_mask_dn_1: 0.3695  loss_dice_dn_1: 1.093  loss_bbox_dn_1: 0.4168  loss_giou_dn_1: 0.675  loss_ce_2: 1.245  loss_mask_2: 0.3133  loss_dice_2: 0.8987  loss_bbox_2: 0.354  loss_giou_2: 0.7212  loss_ce_dn_2: 0.1945  loss_mask_dn_2: 0.3521  loss_dice_dn_2: 1.015  loss_bbox_dn_2: 0.3174  loss_giou_dn_2: 0.6014  loss_ce_3: 1.055  loss_mask_3: 0.3199  loss_dice_3: 0.8821  loss_bbox_3: 0.3457  loss_giou_3: 0.7159  loss_ce_dn_3: 0.1677  loss_mask_dn_3: 0.3665  loss_dice_dn_3: 0.9722  loss_bbox_dn_3: 0.2961  loss_giou_dn_3: 0.5811  loss_ce_4: 0.9559  loss_mask_4: 0.3267  loss_dice_4: 0.8716  loss_bbox_4: 0.3369  loss_giou_4: 0.6838  loss_ce_dn_4: 0.156  loss_mask_dn_4: 0.3766  loss_dice_dn_4: 0.9631  loss_bbox_dn_4: 0.2732  loss_giou_dn_4: 0.5645  loss_ce_5: 0.8889  loss_mask_5: 0.3169  loss_dice_5: 0.9006  loss_bbox_5: 0.3253  loss_giou_5: 0.6714  loss_ce_dn_5: 0.1329  loss_mask_dn_5: 0.3731  loss_dice_dn_5: 0.946  loss_bbox_dn_5: 0.2703  loss_giou_dn_5: 0.5528  loss_ce_6: 0.8734  loss_mask_6: 0.3275  loss_dice_6: 0.9098  loss_bbox_6: 0.3146  loss_giou_6: 0.6601  loss_ce_dn_6: 0.125  loss_mask_dn_6: 0.3834  loss_dice_dn_6: 0.9326  loss_bbox_dn_6: 0.2652  loss_giou_dn_6: 0.5503  loss_ce_7: 0.8357  loss_mask_7: 0.3395  loss_dice_7: 0.8902  loss_bbox_7: 0.3255  loss_giou_7: 0.6752  loss_ce_dn_7: 0.1188  loss_mask_dn_7: 0.3902  loss_dice_dn_7: 0.9329  loss_bbox_dn_7: 0.2622  loss_giou_dn_7: 0.5478  loss_ce_8: 0.8636  loss_mask_8: 0.3446  loss_dice_8: 0.8942  loss_bbox_8: 0.3096  loss_giou_8: 0.6651  loss_ce_dn_8: 0.1139  loss_mask_dn_8: 0.3809  loss_dice_dn_8: 0.9392  loss_bbox_dn_8: 0.2607  loss_giou_dn_8: 0.5479  loss_ce_interm: 1.496  loss_mask_interm: 0.3624  loss_dice_interm: 0.9313  loss_bbox_interm: 0.4633  loss_giou_interm: 0.8616    time: 0.8743  last_time: 0.8637  data_time: 0.0131  last_data_time: 0.0139   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:29:10 d2.utils.events]: \u001b[0m eta: 3 days, 16:31:51  iter: 4099  total_loss: 66.16  loss_ce: 0.8665  loss_mask: 0.2496  loss_dice: 0.6968  loss_bbox: 0.2453  loss_giou: 0.7286  loss_ce_dn: 0.198  loss_mask_dn: 0.254  loss_dice_dn: 0.6991  loss_bbox_dn: 0.2073  loss_giou_dn: 0.6382  loss_ce_0: 1.433  loss_mask_0: 0.2667  loss_dice_0: 0.8051  loss_bbox_0: 0.4003  loss_giou_0: 0.9493  loss_ce_dn_0: 0.7834  loss_mask_dn_0: 0.7805  loss_dice_dn_0: 3.041  loss_bbox_dn_0: 0.5351  loss_giou_dn_0: 0.9811  loss_ce_1: 1.384  loss_mask_1: 0.2624  loss_dice_1: 0.6756  loss_bbox_1: 0.3244  loss_giou_1: 0.8359  loss_ce_dn_1: 0.3349  loss_mask_dn_1: 0.3403  loss_dice_dn_1: 0.8441  loss_bbox_dn_1: 0.3282  loss_giou_dn_1: 0.7746  loss_ce_2: 1.233  loss_mask_2: 0.2636  loss_dice_2: 0.6727  loss_bbox_2: 0.2832  loss_giou_2: 0.8104  loss_ce_dn_2: 0.2852  loss_mask_dn_2: 0.2737  loss_dice_dn_2: 0.7648  loss_bbox_dn_2: 0.2631  loss_giou_dn_2: 0.7024  loss_ce_3: 1.092  loss_mask_3: 0.2482  loss_dice_3: 0.6901  loss_bbox_3: 0.2854  loss_giou_3: 0.7926  loss_ce_dn_3: 0.2387  loss_mask_dn_3: 0.2591  loss_dice_dn_3: 0.6998  loss_bbox_dn_3: 0.2303  loss_giou_dn_3: 0.6782  loss_ce_4: 1.035  loss_mask_4: 0.2468  loss_dice_4: 0.6893  loss_bbox_4: 0.2732  loss_giou_4: 0.7715  loss_ce_dn_4: 0.2211  loss_mask_dn_4: 0.2581  loss_dice_dn_4: 0.7305  loss_bbox_dn_4: 0.2196  loss_giou_dn_4: 0.6592  loss_ce_5: 0.8997  loss_mask_5: 0.2462  loss_dice_5: 0.7102  loss_bbox_5: 0.2636  loss_giou_5: 0.7899  loss_ce_dn_5: 0.2017  loss_mask_dn_5: 0.2532  loss_dice_dn_5: 0.7185  loss_bbox_dn_5: 0.218  loss_giou_dn_5: 0.6557  loss_ce_6: 0.9232  loss_mask_6: 0.2573  loss_dice_6: 0.7109  loss_bbox_6: 0.2509  loss_giou_6: 0.7515  loss_ce_dn_6: 0.1986  loss_mask_dn_6: 0.2557  loss_dice_dn_6: 0.6827  loss_bbox_dn_6: 0.2139  loss_giou_dn_6: 0.6446  loss_ce_7: 0.9061  loss_mask_7: 0.2559  loss_dice_7: 0.7448  loss_bbox_7: 0.2525  loss_giou_7: 0.7479  loss_ce_dn_7: 0.1964  loss_mask_dn_7: 0.2513  loss_dice_dn_7: 0.6776  loss_bbox_dn_7: 0.2102  loss_giou_dn_7: 0.6405  loss_ce_8: 0.8995  loss_mask_8: 0.2559  loss_dice_8: 0.7138  loss_bbox_8: 0.25  loss_giou_8: 0.7471  loss_ce_dn_8: 0.2023  loss_mask_dn_8: 0.2578  loss_dice_dn_8: 0.6911  loss_bbox_dn_8: 0.2082  loss_giou_dn_8: 0.6384  loss_ce_interm: 1.441  loss_mask_interm: 0.2702  loss_dice_interm: 0.7823  loss_bbox_interm: 0.3987  loss_giou_interm: 0.9475    time: 0.8743  last_time: 0.8896  data_time: 0.0133  last_data_time: 0.0155   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:29:27 d2.utils.events]: \u001b[0m eta: 3 days, 16:30:16  iter: 4119  total_loss: 68.95  loss_ce: 0.854  loss_mask: 0.2214  loss_dice: 0.9293  loss_bbox: 0.2619  loss_giou: 0.7026  loss_ce_dn: 0.1102  loss_mask_dn: 0.216  loss_dice_dn: 0.9937  loss_bbox_dn: 0.2472  loss_giou_dn: 0.5465  loss_ce_0: 1.456  loss_mask_0: 0.2372  loss_dice_0: 0.9806  loss_bbox_0: 0.438  loss_giou_0: 0.8725  loss_ce_dn_0: 0.7076  loss_mask_dn_0: 0.641  loss_dice_dn_0: 3.075  loss_bbox_dn_0: 0.5488  loss_giou_dn_0: 0.91  loss_ce_1: 1.393  loss_mask_1: 0.2377  loss_dice_1: 0.9451  loss_bbox_1: 0.3414  loss_giou_1: 0.7654  loss_ce_dn_1: 0.2557  loss_mask_dn_1: 0.2745  loss_dice_dn_1: 1.13  loss_bbox_dn_1: 0.3434  loss_giou_dn_1: 0.6778  loss_ce_2: 1.229  loss_mask_2: 0.2449  loss_dice_2: 0.907  loss_bbox_2: 0.2835  loss_giou_2: 0.7361  loss_ce_dn_2: 0.2072  loss_mask_dn_2: 0.2572  loss_dice_dn_2: 1.059  loss_bbox_dn_2: 0.3032  loss_giou_dn_2: 0.6099  loss_ce_3: 1.11  loss_mask_3: 0.2387  loss_dice_3: 0.9801  loss_bbox_3: 0.2829  loss_giou_3: 0.7417  loss_ce_dn_3: 0.1702  loss_mask_dn_3: 0.2355  loss_dice_dn_3: 1.032  loss_bbox_dn_3: 0.2924  loss_giou_dn_3: 0.5821  loss_ce_4: 1.01  loss_mask_4: 0.2365  loss_dice_4: 0.9661  loss_bbox_4: 0.2818  loss_giou_4: 0.7256  loss_ce_dn_4: 0.1569  loss_mask_dn_4: 0.2274  loss_dice_dn_4: 1.004  loss_bbox_dn_4: 0.2696  loss_giou_dn_4: 0.5625  loss_ce_5: 0.9171  loss_mask_5: 0.2288  loss_dice_5: 0.92  loss_bbox_5: 0.2709  loss_giou_5: 0.7167  loss_ce_dn_5: 0.1371  loss_mask_dn_5: 0.2209  loss_dice_dn_5: 1.006  loss_bbox_dn_5: 0.2608  loss_giou_dn_5: 0.5564  loss_ce_6: 0.8957  loss_mask_6: 0.2303  loss_dice_6: 0.9318  loss_bbox_6: 0.2646  loss_giou_6: 0.7084  loss_ce_dn_6: 0.1274  loss_mask_dn_6: 0.2178  loss_dice_dn_6: 0.9903  loss_bbox_dn_6: 0.2514  loss_giou_dn_6: 0.5502  loss_ce_7: 0.8652  loss_mask_7: 0.224  loss_dice_7: 0.91  loss_bbox_7: 0.2597  loss_giou_7: 0.7114  loss_ce_dn_7: 0.1204  loss_mask_dn_7: 0.2138  loss_dice_dn_7: 0.9979  loss_bbox_dn_7: 0.2498  loss_giou_dn_7: 0.5474  loss_ce_8: 0.8061  loss_mask_8: 0.2179  loss_dice_8: 0.9249  loss_bbox_8: 0.2746  loss_giou_8: 0.7109  loss_ce_dn_8: 0.1147  loss_mask_dn_8: 0.2142  loss_dice_dn_8: 0.9964  loss_bbox_dn_8: 0.2476  loss_giou_dn_8: 0.5464  loss_ce_interm: 1.467  loss_mask_interm: 0.2381  loss_dice_interm: 0.9813  loss_bbox_interm: 0.4407  loss_giou_interm: 0.887    time: 0.8743  last_time: 0.8694  data_time: 0.0130  last_data_time: 0.0160   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:29:45 d2.utils.events]: \u001b[0m eta: 3 days, 16:30:42  iter: 4139  total_loss: 66.59  loss_ce: 0.9373  loss_mask: 0.257  loss_dice: 0.827  loss_bbox: 0.2731  loss_giou: 0.8245  loss_ce_dn: 0.1537  loss_mask_dn: 0.2381  loss_dice_dn: 0.7997  loss_bbox_dn: 0.2138  loss_giou_dn: 0.6673  loss_ce_0: 1.557  loss_mask_0: 0.2606  loss_dice_0: 0.8672  loss_bbox_0: 0.3788  loss_giou_0: 1  loss_ce_dn_0: 0.7638  loss_mask_dn_0: 0.9591  loss_dice_dn_0: 3.253  loss_bbox_dn_0: 0.536  loss_giou_dn_0: 0.9921  loss_ce_1: 1.41  loss_mask_1: 0.2638  loss_dice_1: 0.8952  loss_bbox_1: 0.3032  loss_giou_1: 0.9191  loss_ce_dn_1: 0.297  loss_mask_dn_1: 0.3356  loss_dice_dn_1: 0.9762  loss_bbox_dn_1: 0.321  loss_giou_dn_1: 0.7838  loss_ce_2: 1.264  loss_mask_2: 0.2493  loss_dice_2: 0.8319  loss_bbox_2: 0.2964  loss_giou_2: 0.8659  loss_ce_dn_2: 0.2488  loss_mask_dn_2: 0.2751  loss_dice_dn_2: 0.8676  loss_bbox_dn_2: 0.2779  loss_giou_dn_2: 0.7344  loss_ce_3: 1.166  loss_mask_3: 0.2471  loss_dice_3: 0.811  loss_bbox_3: 0.2953  loss_giou_3: 0.8606  loss_ce_dn_3: 0.1987  loss_mask_dn_3: 0.2621  loss_dice_dn_3: 0.8454  loss_bbox_dn_3: 0.2522  loss_giou_dn_3: 0.7084  loss_ce_4: 1.103  loss_mask_4: 0.2513  loss_dice_4: 0.8014  loss_bbox_4: 0.2537  loss_giou_4: 0.8367  loss_ce_dn_4: 0.1972  loss_mask_dn_4: 0.2712  loss_dice_dn_4: 0.824  loss_bbox_dn_4: 0.2365  loss_giou_dn_4: 0.6894  loss_ce_5: 0.9885  loss_mask_5: 0.2504  loss_dice_5: 0.8195  loss_bbox_5: 0.2742  loss_giou_5: 0.8125  loss_ce_dn_5: 0.1763  loss_mask_dn_5: 0.2539  loss_dice_dn_5: 0.8067  loss_bbox_dn_5: 0.2263  loss_giou_dn_5: 0.6801  loss_ce_6: 0.9537  loss_mask_6: 0.259  loss_dice_6: 0.7807  loss_bbox_6: 0.2605  loss_giou_6: 0.822  loss_ce_dn_6: 0.1622  loss_mask_dn_6: 0.2533  loss_dice_dn_6: 0.8134  loss_bbox_dn_6: 0.2212  loss_giou_dn_6: 0.6684  loss_ce_7: 0.9043  loss_mask_7: 0.2543  loss_dice_7: 0.8813  loss_bbox_7: 0.2642  loss_giou_7: 0.822  loss_ce_dn_7: 0.1671  loss_mask_dn_7: 0.2388  loss_dice_dn_7: 0.8226  loss_bbox_dn_7: 0.2188  loss_giou_dn_7: 0.6704  loss_ce_8: 0.9368  loss_mask_8: 0.2578  loss_dice_8: 0.8316  loss_bbox_8: 0.2603  loss_giou_8: 0.8236  loss_ce_dn_8: 0.1564  loss_mask_dn_8: 0.238  loss_dice_dn_8: 0.8201  loss_bbox_dn_8: 0.2153  loss_giou_dn_8: 0.6671  loss_ce_interm: 1.559  loss_mask_interm: 0.2593  loss_dice_interm: 0.8941  loss_bbox_interm: 0.3778  loss_giou_interm: 1.004    time: 0.8743  last_time: 0.8627  data_time: 0.0143  last_data_time: 0.0116   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:30:02 d2.utils.events]: \u001b[0m eta: 3 days, 16:27:40  iter: 4159  total_loss: 65.61  loss_ce: 0.7055  loss_mask: 0.2911  loss_dice: 0.7452  loss_bbox: 0.249  loss_giou: 0.6509  loss_ce_dn: 0.1053  loss_mask_dn: 0.2808  loss_dice_dn: 0.7933  loss_bbox_dn: 0.2655  loss_giou_dn: 0.5387  loss_ce_0: 1.528  loss_mask_0: 0.3279  loss_dice_0: 0.7787  loss_bbox_0: 0.4921  loss_giou_0: 0.851  loss_ce_dn_0: 0.8033  loss_mask_dn_0: 0.9961  loss_dice_dn_0: 2.845  loss_bbox_dn_0: 0.7375  loss_giou_dn_0: 0.8942  loss_ce_1: 1.454  loss_mask_1: 0.3008  loss_dice_1: 0.7636  loss_bbox_1: 0.3449  loss_giou_1: 0.7484  loss_ce_dn_1: 0.2743  loss_mask_dn_1: 0.3155  loss_dice_dn_1: 0.9424  loss_bbox_dn_1: 0.42  loss_giou_dn_1: 0.666  loss_ce_2: 1.247  loss_mask_2: 0.2963  loss_dice_2: 0.8114  loss_bbox_2: 0.2884  loss_giou_2: 0.7144  loss_ce_dn_2: 0.2003  loss_mask_dn_2: 0.2842  loss_dice_dn_2: 0.9028  loss_bbox_dn_2: 0.3397  loss_giou_dn_2: 0.5985  loss_ce_3: 1.038  loss_mask_3: 0.2956  loss_dice_3: 0.7829  loss_bbox_3: 0.265  loss_giou_3: 0.7102  loss_ce_dn_3: 0.1592  loss_mask_dn_3: 0.2837  loss_dice_dn_3: 0.8234  loss_bbox_dn_3: 0.3118  loss_giou_dn_3: 0.5796  loss_ce_4: 0.8957  loss_mask_4: 0.297  loss_dice_4: 0.7085  loss_bbox_4: 0.3034  loss_giou_4: 0.6833  loss_ce_dn_4: 0.1455  loss_mask_dn_4: 0.2759  loss_dice_dn_4: 0.7976  loss_bbox_dn_4: 0.2866  loss_giou_dn_4: 0.5623  loss_ce_5: 0.8231  loss_mask_5: 0.2816  loss_dice_5: 0.7154  loss_bbox_5: 0.2413  loss_giou_5: 0.6562  loss_ce_dn_5: 0.1315  loss_mask_dn_5: 0.2759  loss_dice_dn_5: 0.7741  loss_bbox_dn_5: 0.2777  loss_giou_dn_5: 0.5566  loss_ce_6: 0.7695  loss_mask_6: 0.2796  loss_dice_6: 0.6963  loss_bbox_6: 0.2317  loss_giou_6: 0.6596  loss_ce_dn_6: 0.1212  loss_mask_dn_6: 0.2808  loss_dice_dn_6: 0.7913  loss_bbox_dn_6: 0.2699  loss_giou_dn_6: 0.5486  loss_ce_7: 0.7393  loss_mask_7: 0.2891  loss_dice_7: 0.7223  loss_bbox_7: 0.2411  loss_giou_7: 0.6591  loss_ce_dn_7: 0.1121  loss_mask_dn_7: 0.2796  loss_dice_dn_7: 0.7702  loss_bbox_dn_7: 0.2684  loss_giou_dn_7: 0.546  loss_ce_8: 0.7291  loss_mask_8: 0.2867  loss_dice_8: 0.7227  loss_bbox_8: 0.2294  loss_giou_8: 0.648  loss_ce_dn_8: 0.1074  loss_mask_dn_8: 0.2798  loss_dice_dn_8: 0.7602  loss_bbox_dn_8: 0.2664  loss_giou_dn_8: 0.5397  loss_ce_interm: 1.508  loss_mask_interm: 0.3358  loss_dice_interm: 0.8051  loss_bbox_interm: 0.4773  loss_giou_interm: 0.8621    time: 0.8743  last_time: 0.8651  data_time: 0.0111  last_data_time: 0.0127   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:30:20 d2.utils.events]: \u001b[0m eta: 3 days, 16:27:23  iter: 4179  total_loss: 56.62  loss_ce: 0.6686  loss_mask: 0.2387  loss_dice: 0.638  loss_bbox: 0.2421  loss_giou: 0.6416  loss_ce_dn: 0.1067  loss_mask_dn: 0.2399  loss_dice_dn: 0.6673  loss_bbox_dn: 0.2228  loss_giou_dn: 0.5528  loss_ce_0: 1.363  loss_mask_0: 0.266  loss_dice_0: 0.8202  loss_bbox_0: 0.4394  loss_giou_0: 0.8879  loss_ce_dn_0: 0.7428  loss_mask_dn_0: 0.9433  loss_dice_dn_0: 2.81  loss_bbox_dn_0: 0.6589  loss_giou_dn_0: 0.9293  loss_ce_1: 1.256  loss_mask_1: 0.2632  loss_dice_1: 0.7187  loss_bbox_1: 0.3168  loss_giou_1: 0.7203  loss_ce_dn_1: 0.2479  loss_mask_dn_1: 0.298  loss_dice_dn_1: 0.725  loss_bbox_dn_1: 0.3752  loss_giou_dn_1: 0.6817  loss_ce_2: 1.148  loss_mask_2: 0.272  loss_dice_2: 0.689  loss_bbox_2: 0.2793  loss_giou_2: 0.6593  loss_ce_dn_2: 0.1774  loss_mask_dn_2: 0.268  loss_dice_dn_2: 0.7003  loss_bbox_dn_2: 0.3106  loss_giou_dn_2: 0.597  loss_ce_3: 0.9664  loss_mask_3: 0.2469  loss_dice_3: 0.6729  loss_bbox_3: 0.2651  loss_giou_3: 0.6588  loss_ce_dn_3: 0.1518  loss_mask_dn_3: 0.2558  loss_dice_dn_3: 0.6728  loss_bbox_dn_3: 0.2808  loss_giou_dn_3: 0.5792  loss_ce_4: 0.8158  loss_mask_4: 0.2525  loss_dice_4: 0.653  loss_bbox_4: 0.264  loss_giou_4: 0.6513  loss_ce_dn_4: 0.1272  loss_mask_dn_4: 0.2445  loss_dice_dn_4: 0.6635  loss_bbox_dn_4: 0.2559  loss_giou_dn_4: 0.567  loss_ce_5: 0.7388  loss_mask_5: 0.2559  loss_dice_5: 0.6526  loss_bbox_5: 0.2356  loss_giou_5: 0.6569  loss_ce_dn_5: 0.1135  loss_mask_dn_5: 0.2391  loss_dice_dn_5: 0.6566  loss_bbox_dn_5: 0.2441  loss_giou_dn_5: 0.5661  loss_ce_6: 0.7707  loss_mask_6: 0.2515  loss_dice_6: 0.6427  loss_bbox_6: 0.2539  loss_giou_6: 0.6654  loss_ce_dn_6: 0.09955  loss_mask_dn_6: 0.2416  loss_dice_dn_6: 0.6559  loss_bbox_dn_6: 0.2305  loss_giou_dn_6: 0.5605  loss_ce_7: 0.7531  loss_mask_7: 0.248  loss_dice_7: 0.6487  loss_bbox_7: 0.2479  loss_giou_7: 0.6459  loss_ce_dn_7: 0.1015  loss_mask_dn_7: 0.2404  loss_dice_dn_7: 0.6522  loss_bbox_dn_7: 0.2303  loss_giou_dn_7: 0.5576  loss_ce_8: 0.7509  loss_mask_8: 0.2422  loss_dice_8: 0.6507  loss_bbox_8: 0.2421  loss_giou_8: 0.6486  loss_ce_dn_8: 0.102  loss_mask_dn_8: 0.2385  loss_dice_dn_8: 0.6602  loss_bbox_dn_8: 0.2236  loss_giou_dn_8: 0.5537  loss_ce_interm: 1.356  loss_mask_interm: 0.2713  loss_dice_interm: 0.809  loss_bbox_interm: 0.4411  loss_giou_interm: 0.8677    time: 0.8742  last_time: 0.8601  data_time: 0.0111  last_data_time: 0.0112   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:30:37 d2.utils.events]: \u001b[0m eta: 3 days, 16:26:33  iter: 4199  total_loss: 57.48  loss_ce: 0.7559  loss_mask: 0.2475  loss_dice: 0.7723  loss_bbox: 0.2559  loss_giou: 0.6204  loss_ce_dn: 0.1075  loss_mask_dn: 0.2388  loss_dice_dn: 0.7499  loss_bbox_dn: 0.2293  loss_giou_dn: 0.481  loss_ce_0: 1.431  loss_mask_0: 0.2725  loss_dice_0: 0.8376  loss_bbox_0: 0.3841  loss_giou_0: 0.8294  loss_ce_dn_0: 0.7738  loss_mask_dn_0: 0.8016  loss_dice_dn_0: 2.772  loss_bbox_dn_0: 0.6523  loss_giou_dn_0: 0.8653  loss_ce_1: 1.383  loss_mask_1: 0.2682  loss_dice_1: 0.8459  loss_bbox_1: 0.3024  loss_giou_1: 0.7038  loss_ce_dn_1: 0.2561  loss_mask_dn_1: 0.3008  loss_dice_dn_1: 0.8727  loss_bbox_dn_1: 0.3457  loss_giou_dn_1: 0.633  loss_ce_2: 1.187  loss_mask_2: 0.2787  loss_dice_2: 0.7948  loss_bbox_2: 0.2922  loss_giou_2: 0.6662  loss_ce_dn_2: 0.1983  loss_mask_dn_2: 0.2745  loss_dice_dn_2: 0.8339  loss_bbox_dn_2: 0.3052  loss_giou_dn_2: 0.5516  loss_ce_3: 1.021  loss_mask_3: 0.2663  loss_dice_3: 0.8207  loss_bbox_3: 0.2465  loss_giou_3: 0.6492  loss_ce_dn_3: 0.1677  loss_mask_dn_3: 0.2523  loss_dice_dn_3: 0.7822  loss_bbox_dn_3: 0.2798  loss_giou_dn_3: 0.5255  loss_ce_4: 0.894  loss_mask_4: 0.2632  loss_dice_4: 0.7945  loss_bbox_4: 0.2715  loss_giou_4: 0.6329  loss_ce_dn_4: 0.1531  loss_mask_dn_4: 0.2466  loss_dice_dn_4: 0.7745  loss_bbox_dn_4: 0.2541  loss_giou_dn_4: 0.5151  loss_ce_5: 0.802  loss_mask_5: 0.2628  loss_dice_5: 0.8041  loss_bbox_5: 0.2799  loss_giou_5: 0.6245  loss_ce_dn_5: 0.1359  loss_mask_dn_5: 0.2458  loss_dice_dn_5: 0.7649  loss_bbox_dn_5: 0.2493  loss_giou_dn_5: 0.5101  loss_ce_6: 0.8025  loss_mask_6: 0.2685  loss_dice_6: 0.7743  loss_bbox_6: 0.2687  loss_giou_6: 0.6308  loss_ce_dn_6: 0.1166  loss_mask_dn_6: 0.2456  loss_dice_dn_6: 0.7507  loss_bbox_dn_6: 0.2335  loss_giou_dn_6: 0.4881  loss_ce_7: 0.7651  loss_mask_7: 0.2522  loss_dice_7: 0.7513  loss_bbox_7: 0.2597  loss_giou_7: 0.6516  loss_ce_dn_7: 0.1078  loss_mask_dn_7: 0.2439  loss_dice_dn_7: 0.7391  loss_bbox_dn_7: 0.2334  loss_giou_dn_7: 0.4889  loss_ce_8: 0.7557  loss_mask_8: 0.2538  loss_dice_8: 0.7707  loss_bbox_8: 0.2661  loss_giou_8: 0.6256  loss_ce_dn_8: 0.1117  loss_mask_dn_8: 0.2402  loss_dice_dn_8: 0.7499  loss_bbox_dn_8: 0.2282  loss_giou_dn_8: 0.4826  loss_ce_interm: 1.431  loss_mask_interm: 0.2638  loss_dice_interm: 0.8282  loss_bbox_interm: 0.3759  loss_giou_interm: 0.8317    time: 0.8743  last_time: 0.8653  data_time: 0.0128  last_data_time: 0.0144   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:30:55 d2.utils.events]: \u001b[0m eta: 3 days, 16:25:53  iter: 4219  total_loss: 64.23  loss_ce: 0.7671  loss_mask: 0.3487  loss_dice: 0.7381  loss_bbox: 0.2853  loss_giou: 0.5762  loss_ce_dn: 0.1329  loss_mask_dn: 0.3196  loss_dice_dn: 0.801  loss_bbox_dn: 0.2717  loss_giou_dn: 0.4637  loss_ce_0: 1.528  loss_mask_0: 0.3548  loss_dice_0: 0.8349  loss_bbox_0: 0.4319  loss_giou_0: 0.8557  loss_ce_dn_0: 0.7088  loss_mask_dn_0: 0.7191  loss_dice_dn_0: 2.952  loss_bbox_dn_0: 0.574  loss_giou_dn_0: 0.8686  loss_ce_1: 1.413  loss_mask_1: 0.3513  loss_dice_1: 0.8717  loss_bbox_1: 0.3103  loss_giou_1: 0.7133  loss_ce_dn_1: 0.2305  loss_mask_dn_1: 0.3762  loss_dice_dn_1: 0.9766  loss_bbox_dn_1: 0.3621  loss_giou_dn_1: 0.6168  loss_ce_2: 1.285  loss_mask_2: 0.3617  loss_dice_2: 0.7742  loss_bbox_2: 0.3116  loss_giou_2: 0.6545  loss_ce_dn_2: 0.192  loss_mask_dn_2: 0.3515  loss_dice_dn_2: 0.8589  loss_bbox_dn_2: 0.3182  loss_giou_dn_2: 0.5515  loss_ce_3: 1.111  loss_mask_3: 0.333  loss_dice_3: 0.7936  loss_bbox_3: 0.3029  loss_giou_3: 0.6295  loss_ce_dn_3: 0.1615  loss_mask_dn_3: 0.3249  loss_dice_dn_3: 0.8  loss_bbox_dn_3: 0.2984  loss_giou_dn_3: 0.5162  loss_ce_4: 0.9102  loss_mask_4: 0.333  loss_dice_4: 0.8234  loss_bbox_4: 0.2808  loss_giou_4: 0.6181  loss_ce_dn_4: 0.1507  loss_mask_dn_4: 0.3223  loss_dice_dn_4: 0.8084  loss_bbox_dn_4: 0.2881  loss_giou_dn_4: 0.4942  loss_ce_5: 0.8422  loss_mask_5: 0.3207  loss_dice_5: 0.7506  loss_bbox_5: 0.2883  loss_giou_5: 0.5955  loss_ce_dn_5: 0.1446  loss_mask_dn_5: 0.3227  loss_dice_dn_5: 0.7939  loss_bbox_dn_5: 0.2857  loss_giou_dn_5: 0.4862  loss_ce_6: 0.807  loss_mask_6: 0.3557  loss_dice_6: 0.7871  loss_bbox_6: 0.2833  loss_giou_6: 0.6053  loss_ce_dn_6: 0.1342  loss_mask_dn_6: 0.3239  loss_dice_dn_6: 0.7842  loss_bbox_dn_6: 0.2783  loss_giou_dn_6: 0.4701  loss_ce_7: 0.7844  loss_mask_7: 0.3549  loss_dice_7: 0.7809  loss_bbox_7: 0.2736  loss_giou_7: 0.5979  loss_ce_dn_7: 0.1259  loss_mask_dn_7: 0.3202  loss_dice_dn_7: 0.7885  loss_bbox_dn_7: 0.2768  loss_giou_dn_7: 0.4674  loss_ce_8: 0.7868  loss_mask_8: 0.3344  loss_dice_8: 0.7357  loss_bbox_8: 0.2845  loss_giou_8: 0.6062  loss_ce_dn_8: 0.1293  loss_mask_dn_8: 0.3193  loss_dice_dn_8: 0.7991  loss_bbox_dn_8: 0.2723  loss_giou_dn_8: 0.4631  loss_ce_interm: 1.533  loss_mask_interm: 0.3586  loss_dice_interm: 0.839  loss_bbox_interm: 0.4491  loss_giou_interm: 0.8336    time: 0.8742  last_time: 0.8972  data_time: 0.0111  last_data_time: 0.0247   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:31:12 d2.utils.events]: \u001b[0m eta: 3 days, 16:24:46  iter: 4239  total_loss: 59.36  loss_ce: 0.7307  loss_mask: 0.2781  loss_dice: 0.7608  loss_bbox: 0.2419  loss_giou: 0.7038  loss_ce_dn: 0.1271  loss_mask_dn: 0.2604  loss_dice_dn: 0.7293  loss_bbox_dn: 0.2431  loss_giou_dn: 0.6584  loss_ce_0: 1.412  loss_mask_0: 0.2595  loss_dice_0: 0.8151  loss_bbox_0: 0.3974  loss_giou_0: 0.8963  loss_ce_dn_0: 0.7145  loss_mask_dn_0: 1.009  loss_dice_dn_0: 3.053  loss_bbox_dn_0: 0.5806  loss_giou_dn_0: 0.9815  loss_ce_1: 1.32  loss_mask_1: 0.2841  loss_dice_1: 0.8018  loss_bbox_1: 0.3153  loss_giou_1: 0.7792  loss_ce_dn_1: 0.2514  loss_mask_dn_1: 0.3615  loss_dice_dn_1: 0.9073  loss_bbox_dn_1: 0.3443  loss_giou_dn_1: 0.75  loss_ce_2: 1.178  loss_mask_2: 0.3101  loss_dice_2: 0.7466  loss_bbox_2: 0.2881  loss_giou_2: 0.7332  loss_ce_dn_2: 0.2058  loss_mask_dn_2: 0.2965  loss_dice_dn_2: 0.8082  loss_bbox_dn_2: 0.2808  loss_giou_dn_2: 0.7024  loss_ce_3: 1.015  loss_mask_3: 0.2848  loss_dice_3: 0.7627  loss_bbox_3: 0.266  loss_giou_3: 0.7406  loss_ce_dn_3: 0.1798  loss_mask_dn_3: 0.2848  loss_dice_dn_3: 0.7624  loss_bbox_dn_3: 0.258  loss_giou_dn_3: 0.6817  loss_ce_4: 0.8673  loss_mask_4: 0.2818  loss_dice_4: 0.7313  loss_bbox_4: 0.2447  loss_giou_4: 0.724  loss_ce_dn_4: 0.1635  loss_mask_dn_4: 0.2776  loss_dice_dn_4: 0.7477  loss_bbox_dn_4: 0.2473  loss_giou_dn_4: 0.6666  loss_ce_5: 0.8595  loss_mask_5: 0.2729  loss_dice_5: 0.7528  loss_bbox_5: 0.2402  loss_giou_5: 0.7003  loss_ce_dn_5: 0.1571  loss_mask_dn_5: 0.2575  loss_dice_dn_5: 0.73  loss_bbox_dn_5: 0.2421  loss_giou_dn_5: 0.6611  loss_ce_6: 0.7871  loss_mask_6: 0.2837  loss_dice_6: 0.7548  loss_bbox_6: 0.2443  loss_giou_6: 0.7175  loss_ce_dn_6: 0.1502  loss_mask_dn_6: 0.2634  loss_dice_dn_6: 0.7212  loss_bbox_dn_6: 0.2427  loss_giou_dn_6: 0.6525  loss_ce_7: 0.7685  loss_mask_7: 0.2811  loss_dice_7: 0.7373  loss_bbox_7: 0.2429  loss_giou_7: 0.7046  loss_ce_dn_7: 0.1361  loss_mask_dn_7: 0.2578  loss_dice_dn_7: 0.7412  loss_bbox_dn_7: 0.2393  loss_giou_dn_7: 0.6555  loss_ce_8: 0.732  loss_mask_8: 0.2788  loss_dice_8: 0.7684  loss_bbox_8: 0.2484  loss_giou_8: 0.6969  loss_ce_dn_8: 0.1306  loss_mask_dn_8: 0.2536  loss_dice_dn_8: 0.7286  loss_bbox_dn_8: 0.2412  loss_giou_dn_8: 0.6565  loss_ce_interm: 1.416  loss_mask_interm: 0.266  loss_dice_interm: 0.8167  loss_bbox_interm: 0.4005  loss_giou_interm: 0.8739    time: 0.8742  last_time: 0.8621  data_time: 0.0122  last_data_time: 0.0116   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:31:30 d2.utils.events]: \u001b[0m eta: 3 days, 16:25:33  iter: 4259  total_loss: 68.6  loss_ce: 0.9548  loss_mask: 0.2192  loss_dice: 0.8067  loss_bbox: 0.2505  loss_giou: 0.8761  loss_ce_dn: 0.1597  loss_mask_dn: 0.2375  loss_dice_dn: 0.8215  loss_bbox_dn: 0.1942  loss_giou_dn: 0.717  loss_ce_0: 1.544  loss_mask_0: 0.2398  loss_dice_0: 1.011  loss_bbox_0: 0.3438  loss_giou_0: 1.063  loss_ce_dn_0: 0.7331  loss_mask_dn_0: 0.8704  loss_dice_dn_0: 2.872  loss_bbox_dn_0: 0.5  loss_giou_dn_0: 0.9963  loss_ce_1: 1.428  loss_mask_1: 0.2339  loss_dice_1: 0.8839  loss_bbox_1: 0.292  loss_giou_1: 0.9413  loss_ce_dn_1: 0.3026  loss_mask_dn_1: 0.2798  loss_dice_dn_1: 0.9239  loss_bbox_dn_1: 0.2762  loss_giou_dn_1: 0.8165  loss_ce_2: 1.281  loss_mask_2: 0.2277  loss_dice_2: 0.9188  loss_bbox_2: 0.246  loss_giou_2: 0.8888  loss_ce_dn_2: 0.2599  loss_mask_dn_2: 0.2682  loss_dice_dn_2: 0.8856  loss_bbox_dn_2: 0.2197  loss_giou_dn_2: 0.7518  loss_ce_3: 1.094  loss_mask_3: 0.2243  loss_dice_3: 0.8795  loss_bbox_3: 0.2612  loss_giou_3: 0.8634  loss_ce_dn_3: 0.2387  loss_mask_dn_3: 0.2495  loss_dice_dn_3: 0.8435  loss_bbox_dn_3: 0.2112  loss_giou_dn_3: 0.734  loss_ce_4: 1.056  loss_mask_4: 0.2216  loss_dice_4: 0.9338  loss_bbox_4: 0.263  loss_giou_4: 0.8634  loss_ce_dn_4: 0.2088  loss_mask_dn_4: 0.2401  loss_dice_dn_4: 0.834  loss_bbox_dn_4: 0.2045  loss_giou_dn_4: 0.7235  loss_ce_5: 0.9893  loss_mask_5: 0.2337  loss_dice_5: 0.8556  loss_bbox_5: 0.272  loss_giou_5: 0.8835  loss_ce_dn_5: 0.2033  loss_mask_dn_5: 0.241  loss_dice_dn_5: 0.8396  loss_bbox_dn_5: 0.1993  loss_giou_dn_5: 0.7269  loss_ce_6: 0.9729  loss_mask_6: 0.2248  loss_dice_6: 0.8797  loss_bbox_6: 0.2604  loss_giou_6: 0.8705  loss_ce_dn_6: 0.1853  loss_mask_dn_6: 0.2393  loss_dice_dn_6: 0.8441  loss_bbox_dn_6: 0.1942  loss_giou_dn_6: 0.7236  loss_ce_7: 0.9382  loss_mask_7: 0.2338  loss_dice_7: 0.8934  loss_bbox_7: 0.2566  loss_giou_7: 0.8736  loss_ce_dn_7: 0.1749  loss_mask_dn_7: 0.2356  loss_dice_dn_7: 0.8309  loss_bbox_dn_7: 0.1953  loss_giou_dn_7: 0.7222  loss_ce_8: 0.9613  loss_mask_8: 0.2121  loss_dice_8: 0.8335  loss_bbox_8: 0.2496  loss_giou_8: 0.8698  loss_ce_dn_8: 0.1665  loss_mask_dn_8: 0.2378  loss_dice_dn_8: 0.8116  loss_bbox_dn_8: 0.1957  loss_giou_dn_8: 0.7173  loss_ce_interm: 1.538  loss_mask_interm: 0.227  loss_dice_interm: 0.9226  loss_bbox_interm: 0.3404  loss_giou_interm: 1.072    time: 0.8742  last_time: 0.8480  data_time: 0.0128  last_data_time: 0.0112   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:31:47 d2.utils.events]: \u001b[0m eta: 3 days, 16:24:01  iter: 4279  total_loss: 61.45  loss_ce: 0.7692  loss_mask: 0.2672  loss_dice: 0.8032  loss_bbox: 0.3173  loss_giou: 0.7789  loss_ce_dn: 0.1179  loss_mask_dn: 0.2443  loss_dice_dn: 0.7951  loss_bbox_dn: 0.234  loss_giou_dn: 0.6323  loss_ce_0: 1.472  loss_mask_0: 0.3046  loss_dice_0: 0.7645  loss_bbox_0: 0.4137  loss_giou_0: 0.8911  loss_ce_dn_0: 0.7622  loss_mask_dn_0: 0.7839  loss_dice_dn_0: 2.851  loss_bbox_dn_0: 0.5601  loss_giou_dn_0: 1.004  loss_ce_1: 1.408  loss_mask_1: 0.2788  loss_dice_1: 0.7692  loss_bbox_1: 0.3159  loss_giou_1: 0.806  loss_ce_dn_1: 0.2684  loss_mask_dn_1: 0.2934  loss_dice_dn_1: 0.9511  loss_bbox_dn_1: 0.3088  loss_giou_dn_1: 0.7676  loss_ce_2: 1.221  loss_mask_2: 0.292  loss_dice_2: 0.8081  loss_bbox_2: 0.2895  loss_giou_2: 0.7602  loss_ce_dn_2: 0.2059  loss_mask_dn_2: 0.2803  loss_dice_dn_2: 0.8658  loss_bbox_dn_2: 0.2508  loss_giou_dn_2: 0.7082  loss_ce_3: 1.075  loss_mask_3: 0.287  loss_dice_3: 0.783  loss_bbox_3: 0.2998  loss_giou_3: 0.7738  loss_ce_dn_3: 0.1843  loss_mask_dn_3: 0.2672  loss_dice_dn_3: 0.823  loss_bbox_dn_3: 0.2414  loss_giou_dn_3: 0.6739  loss_ce_4: 1.002  loss_mask_4: 0.2833  loss_dice_4: 0.779  loss_bbox_4: 0.3141  loss_giou_4: 0.7532  loss_ce_dn_4: 0.1649  loss_mask_dn_4: 0.2618  loss_dice_dn_4: 0.7988  loss_bbox_dn_4: 0.2328  loss_giou_dn_4: 0.6542  loss_ce_5: 0.8485  loss_mask_5: 0.2631  loss_dice_5: 0.7692  loss_bbox_5: 0.3163  loss_giou_5: 0.7603  loss_ce_dn_5: 0.146  loss_mask_dn_5: 0.251  loss_dice_dn_5: 0.7665  loss_bbox_dn_5: 0.2335  loss_giou_dn_5: 0.646  loss_ce_6: 0.8463  loss_mask_6: 0.2614  loss_dice_6: 0.7741  loss_bbox_6: 0.2902  loss_giou_6: 0.7586  loss_ce_dn_6: 0.13  loss_mask_dn_6: 0.2491  loss_dice_dn_6: 0.7828  loss_bbox_dn_6: 0.2319  loss_giou_dn_6: 0.6409  loss_ce_7: 0.8251  loss_mask_7: 0.2675  loss_dice_7: 0.8372  loss_bbox_7: 0.2908  loss_giou_7: 0.7389  loss_ce_dn_7: 0.1214  loss_mask_dn_7: 0.2466  loss_dice_dn_7: 0.7808  loss_bbox_dn_7: 0.2319  loss_giou_dn_7: 0.6377  loss_ce_8: 0.7818  loss_mask_8: 0.2705  loss_dice_8: 0.8165  loss_bbox_8: 0.3165  loss_giou_8: 0.7543  loss_ce_dn_8: 0.1161  loss_mask_dn_8: 0.2433  loss_dice_dn_8: 0.7898  loss_bbox_dn_8: 0.2339  loss_giou_dn_8: 0.6326  loss_ce_interm: 1.467  loss_mask_interm: 0.3076  loss_dice_interm: 0.7635  loss_bbox_interm: 0.423  loss_giou_interm: 0.8929    time: 0.8742  last_time: 0.8794  data_time: 0.0122  last_data_time: 0.0189   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:32:05 d2.utils.events]: \u001b[0m eta: 3 days, 16:23:09  iter: 4299  total_loss: 62.83  loss_ce: 0.6989  loss_mask: 0.3035  loss_dice: 0.8256  loss_bbox: 0.3342  loss_giou: 0.7014  loss_ce_dn: 0.129  loss_mask_dn: 0.2856  loss_dice_dn: 0.7165  loss_bbox_dn: 0.2871  loss_giou_dn: 0.5321  loss_ce_0: 1.447  loss_mask_0: 0.2828  loss_dice_0: 0.8724  loss_bbox_0: 0.4847  loss_giou_0: 0.8895  loss_ce_dn_0: 0.7592  loss_mask_dn_0: 0.9809  loss_dice_dn_0: 3.068  loss_bbox_dn_0: 0.6776  loss_giou_dn_0: 0.8955  loss_ce_1: 1.388  loss_mask_1: 0.3215  loss_dice_1: 0.86  loss_bbox_1: 0.3184  loss_giou_1: 0.7685  loss_ce_dn_1: 0.2782  loss_mask_dn_1: 0.3514  loss_dice_dn_1: 0.8501  loss_bbox_dn_1: 0.4039  loss_giou_dn_1: 0.6755  loss_ce_2: 1.179  loss_mask_2: 0.3039  loss_dice_2: 0.8441  loss_bbox_2: 0.3283  loss_giou_2: 0.7375  loss_ce_dn_2: 0.2304  loss_mask_dn_2: 0.3322  loss_dice_dn_2: 0.8145  loss_bbox_dn_2: 0.3532  loss_giou_dn_2: 0.6024  loss_ce_3: 0.9926  loss_mask_3: 0.3089  loss_dice_3: 0.798  loss_bbox_3: 0.307  loss_giou_3: 0.6982  loss_ce_dn_3: 0.1968  loss_mask_dn_3: 0.2945  loss_dice_dn_3: 0.7534  loss_bbox_dn_3: 0.3163  loss_giou_dn_3: 0.578  loss_ce_4: 0.9011  loss_mask_4: 0.2884  loss_dice_4: 0.8445  loss_bbox_4: 0.278  loss_giou_4: 0.7026  loss_ce_dn_4: 0.1806  loss_mask_dn_4: 0.2883  loss_dice_dn_4: 0.7403  loss_bbox_dn_4: 0.3079  loss_giou_dn_4: 0.5533  loss_ce_5: 0.8133  loss_mask_5: 0.2797  loss_dice_5: 0.7891  loss_bbox_5: 0.2868  loss_giou_5: 0.689  loss_ce_dn_5: 0.1573  loss_mask_dn_5: 0.2872  loss_dice_dn_5: 0.7178  loss_bbox_dn_5: 0.301  loss_giou_dn_5: 0.5512  loss_ce_6: 0.7699  loss_mask_6: 0.2975  loss_dice_6: 0.8021  loss_bbox_6: 0.3145  loss_giou_6: 0.69  loss_ce_dn_6: 0.1405  loss_mask_dn_6: 0.2878  loss_dice_dn_6: 0.7308  loss_bbox_dn_6: 0.2933  loss_giou_dn_6: 0.5407  loss_ce_7: 0.7436  loss_mask_7: 0.3036  loss_dice_7: 0.8202  loss_bbox_7: 0.3164  loss_giou_7: 0.6875  loss_ce_dn_7: 0.133  loss_mask_dn_7: 0.2876  loss_dice_dn_7: 0.7392  loss_bbox_dn_7: 0.2918  loss_giou_dn_7: 0.5396  loss_ce_8: 0.7071  loss_mask_8: 0.2994  loss_dice_8: 0.747  loss_bbox_8: 0.3081  loss_giou_8: 0.6832  loss_ce_dn_8: 0.1323  loss_mask_dn_8: 0.2845  loss_dice_dn_8: 0.7156  loss_bbox_dn_8: 0.2882  loss_giou_dn_8: 0.5338  loss_ce_interm: 1.469  loss_mask_interm: 0.284  loss_dice_interm: 0.8842  loss_bbox_interm: 0.4806  loss_giou_interm: 0.8738    time: 0.8743  last_time: 0.8654  data_time: 0.0130  last_data_time: 0.0219   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:32:23 d2.utils.events]: \u001b[0m eta: 3 days, 16:21:15  iter: 4319  total_loss: 61.28  loss_ce: 0.7358  loss_mask: 0.2967  loss_dice: 0.7982  loss_bbox: 0.2156  loss_giou: 0.6846  loss_ce_dn: 0.1089  loss_mask_dn: 0.2789  loss_dice_dn: 0.7454  loss_bbox_dn: 0.2252  loss_giou_dn: 0.5386  loss_ce_0: 1.384  loss_mask_0: 0.2941  loss_dice_0: 0.8159  loss_bbox_0: 0.4277  loss_giou_0: 0.9122  loss_ce_dn_0: 0.6965  loss_mask_dn_0: 0.7714  loss_dice_dn_0: 2.837  loss_bbox_dn_0: 0.682  loss_giou_dn_0: 0.9227  loss_ce_1: 1.303  loss_mask_1: 0.3004  loss_dice_1: 0.7857  loss_bbox_1: 0.284  loss_giou_1: 0.7796  loss_ce_dn_1: 0.2365  loss_mask_dn_1: 0.3482  loss_dice_dn_1: 0.8701  loss_bbox_dn_1: 0.357  loss_giou_dn_1: 0.6671  loss_ce_2: 1.076  loss_mask_2: 0.3079  loss_dice_2: 0.7672  loss_bbox_2: 0.2628  loss_giou_2: 0.764  loss_ce_dn_2: 0.1838  loss_mask_dn_2: 0.3144  loss_dice_dn_2: 0.7974  loss_bbox_dn_2: 0.2874  loss_giou_dn_2: 0.6099  loss_ce_3: 0.935  loss_mask_3: 0.2971  loss_dice_3: 0.7835  loss_bbox_3: 0.2531  loss_giou_3: 0.7514  loss_ce_dn_3: 0.1587  loss_mask_dn_3: 0.288  loss_dice_dn_3: 0.7434  loss_bbox_dn_3: 0.2538  loss_giou_dn_3: 0.5789  loss_ce_4: 0.9228  loss_mask_4: 0.3067  loss_dice_4: 0.7546  loss_bbox_4: 0.2137  loss_giou_4: 0.7286  loss_ce_dn_4: 0.1356  loss_mask_dn_4: 0.2882  loss_dice_dn_4: 0.7571  loss_bbox_dn_4: 0.2327  loss_giou_dn_4: 0.5621  loss_ce_5: 0.796  loss_mask_5: 0.3035  loss_dice_5: 0.78  loss_bbox_5: 0.2151  loss_giou_5: 0.7026  loss_ce_dn_5: 0.1249  loss_mask_dn_5: 0.2856  loss_dice_dn_5: 0.7346  loss_bbox_dn_5: 0.2301  loss_giou_dn_5: 0.5533  loss_ce_6: 0.7614  loss_mask_6: 0.3029  loss_dice_6: 0.8036  loss_bbox_6: 0.2203  loss_giou_6: 0.6783  loss_ce_dn_6: 0.1149  loss_mask_dn_6: 0.2771  loss_dice_dn_6: 0.7572  loss_bbox_dn_6: 0.2263  loss_giou_dn_6: 0.5431  loss_ce_7: 0.755  loss_mask_7: 0.299  loss_dice_7: 0.7906  loss_bbox_7: 0.2204  loss_giou_7: 0.6908  loss_ce_dn_7: 0.1132  loss_mask_dn_7: 0.2731  loss_dice_dn_7: 0.7754  loss_bbox_dn_7: 0.2247  loss_giou_dn_7: 0.5429  loss_ce_8: 0.7455  loss_mask_8: 0.2917  loss_dice_8: 0.783  loss_bbox_8: 0.2167  loss_giou_8: 0.6767  loss_ce_dn_8: 0.1107  loss_mask_dn_8: 0.2741  loss_dice_dn_8: 0.7356  loss_bbox_dn_8: 0.2262  loss_giou_dn_8: 0.5384  loss_ce_interm: 1.369  loss_mask_interm: 0.2891  loss_dice_interm: 0.8128  loss_bbox_interm: 0.431  loss_giou_interm: 0.9192    time: 0.8743  last_time: 0.8671  data_time: 0.0121  last_data_time: 0.0106   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:32:40 d2.utils.events]: \u001b[0m eta: 3 days, 16:19:13  iter: 4339  total_loss: 59.37  loss_ce: 0.6186  loss_mask: 0.183  loss_dice: 0.6918  loss_bbox: 0.2134  loss_giou: 0.6484  loss_ce_dn: 0.122  loss_mask_dn: 0.222  loss_dice_dn: 0.7214  loss_bbox_dn: 0.205  loss_giou_dn: 0.5824  loss_ce_0: 1.395  loss_mask_0: 0.2216  loss_dice_0: 0.7064  loss_bbox_0: 0.3655  loss_giou_0: 0.9255  loss_ce_dn_0: 0.6989  loss_mask_dn_0: 0.9107  loss_dice_dn_0: 2.773  loss_bbox_dn_0: 0.6286  loss_giou_dn_0: 0.9701  loss_ce_1: 1.301  loss_mask_1: 0.2135  loss_dice_1: 0.725  loss_bbox_1: 0.268  loss_giou_1: 0.7481  loss_ce_dn_1: 0.2634  loss_mask_dn_1: 0.3015  loss_dice_dn_1: 0.8667  loss_bbox_dn_1: 0.3855  loss_giou_dn_1: 0.694  loss_ce_2: 1.112  loss_mask_2: 0.1863  loss_dice_2: 0.6469  loss_bbox_2: 0.23  loss_giou_2: 0.6764  loss_ce_dn_2: 0.2023  loss_mask_dn_2: 0.2489  loss_dice_dn_2: 0.7779  loss_bbox_dn_2: 0.2742  loss_giou_dn_2: 0.6273  loss_ce_3: 0.9291  loss_mask_3: 0.1887  loss_dice_3: 0.6807  loss_bbox_3: 0.2311  loss_giou_3: 0.6692  loss_ce_dn_3: 0.1767  loss_mask_dn_3: 0.241  loss_dice_dn_3: 0.7391  loss_bbox_dn_3: 0.2351  loss_giou_dn_3: 0.607  loss_ce_4: 0.8221  loss_mask_4: 0.1806  loss_dice_4: 0.7168  loss_bbox_4: 0.2107  loss_giou_4: 0.6808  loss_ce_dn_4: 0.1539  loss_mask_dn_4: 0.2294  loss_dice_dn_4: 0.7392  loss_bbox_dn_4: 0.2188  loss_giou_dn_4: 0.5979  loss_ce_5: 0.7116  loss_mask_5: 0.1848  loss_dice_5: 0.6697  loss_bbox_5: 0.2043  loss_giou_5: 0.6536  loss_ce_dn_5: 0.1413  loss_mask_dn_5: 0.2279  loss_dice_dn_5: 0.7289  loss_bbox_dn_5: 0.2129  loss_giou_dn_5: 0.5945  loss_ce_6: 0.6974  loss_mask_6: 0.1842  loss_dice_6: 0.6744  loss_bbox_6: 0.2128  loss_giou_6: 0.666  loss_ce_dn_6: 0.1364  loss_mask_dn_6: 0.2228  loss_dice_dn_6: 0.7189  loss_bbox_dn_6: 0.2083  loss_giou_dn_6: 0.5924  loss_ce_7: 0.6402  loss_mask_7: 0.1777  loss_dice_7: 0.6757  loss_bbox_7: 0.216  loss_giou_7: 0.6482  loss_ce_dn_7: 0.1297  loss_mask_dn_7: 0.2198  loss_dice_dn_7: 0.729  loss_bbox_dn_7: 0.2061  loss_giou_dn_7: 0.5891  loss_ce_8: 0.634  loss_mask_8: 0.19  loss_dice_8: 0.6653  loss_bbox_8: 0.2156  loss_giou_8: 0.6331  loss_ce_dn_8: 0.1235  loss_mask_dn_8: 0.2251  loss_dice_dn_8: 0.7225  loss_bbox_dn_8: 0.2044  loss_giou_dn_8: 0.5828  loss_ce_interm: 1.395  loss_mask_interm: 0.2221  loss_dice_interm: 0.7255  loss_bbox_interm: 0.3633  loss_giou_interm: 0.8701    time: 0.8742  last_time: 0.8915  data_time: 0.0116  last_data_time: 0.0123   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:32:58 d2.utils.events]: \u001b[0m eta: 3 days, 16:18:56  iter: 4359  total_loss: 57.84  loss_ce: 0.7243  loss_mask: 0.2753  loss_dice: 0.6557  loss_bbox: 0.2417  loss_giou: 0.6022  loss_ce_dn: 0.1018  loss_mask_dn: 0.3051  loss_dice_dn: 0.6274  loss_bbox_dn: 0.2388  loss_giou_dn: 0.4996  loss_ce_0: 1.412  loss_mask_0: 0.2966  loss_dice_0: 0.6931  loss_bbox_0: 0.4321  loss_giou_0: 0.7778  loss_ce_dn_0: 0.75  loss_mask_dn_0: 0.9611  loss_dice_dn_0: 2.926  loss_bbox_dn_0: 0.6706  loss_giou_dn_0: 0.8875  loss_ce_1: 1.264  loss_mask_1: 0.2802  loss_dice_1: 0.6247  loss_bbox_1: 0.3061  loss_giou_1: 0.6685  loss_ce_dn_1: 0.2371  loss_mask_dn_1: 0.3374  loss_dice_dn_1: 0.7243  loss_bbox_dn_1: 0.389  loss_giou_dn_1: 0.6454  loss_ce_2: 1.101  loss_mask_2: 0.2785  loss_dice_2: 0.712  loss_bbox_2: 0.2558  loss_giou_2: 0.6421  loss_ce_dn_2: 0.1714  loss_mask_dn_2: 0.3119  loss_dice_dn_2: 0.6862  loss_bbox_dn_2: 0.3247  loss_giou_dn_2: 0.5629  loss_ce_3: 0.8817  loss_mask_3: 0.2587  loss_dice_3: 0.636  loss_bbox_3: 0.2515  loss_giou_3: 0.6295  loss_ce_dn_3: 0.1456  loss_mask_dn_3: 0.3074  loss_dice_dn_3: 0.6478  loss_bbox_dn_3: 0.2833  loss_giou_dn_3: 0.5361  loss_ce_4: 0.8618  loss_mask_4: 0.2772  loss_dice_4: 0.6221  loss_bbox_4: 0.2485  loss_giou_4: 0.6042  loss_ce_dn_4: 0.141  loss_mask_dn_4: 0.2891  loss_dice_dn_4: 0.6495  loss_bbox_dn_4: 0.2626  loss_giou_dn_4: 0.5136  loss_ce_5: 0.8017  loss_mask_5: 0.2714  loss_dice_5: 0.6462  loss_bbox_5: 0.2512  loss_giou_5: 0.6039  loss_ce_dn_5: 0.1211  loss_mask_dn_5: 0.2932  loss_dice_dn_5: 0.6357  loss_bbox_dn_5: 0.2503  loss_giou_dn_5: 0.5163  loss_ce_6: 0.758  loss_mask_6: 0.2783  loss_dice_6: 0.6683  loss_bbox_6: 0.2443  loss_giou_6: 0.5888  loss_ce_dn_6: 0.1149  loss_mask_dn_6: 0.297  loss_dice_dn_6: 0.6287  loss_bbox_dn_6: 0.2473  loss_giou_dn_6: 0.506  loss_ce_7: 0.7185  loss_mask_7: 0.2647  loss_dice_7: 0.6512  loss_bbox_7: 0.2384  loss_giou_7: 0.605  loss_ce_dn_7: 0.1068  loss_mask_dn_7: 0.2946  loss_dice_dn_7: 0.6106  loss_bbox_dn_7: 0.2432  loss_giou_dn_7: 0.5041  loss_ce_8: 0.7534  loss_mask_8: 0.2701  loss_dice_8: 0.6284  loss_bbox_8: 0.2477  loss_giou_8: 0.5992  loss_ce_dn_8: 0.107  loss_mask_dn_8: 0.3043  loss_dice_dn_8: 0.6094  loss_bbox_dn_8: 0.2405  loss_giou_dn_8: 0.5  loss_ce_interm: 1.398  loss_mask_interm: 0.2951  loss_dice_interm: 0.7267  loss_bbox_interm: 0.4108  loss_giou_interm: 0.7591    time: 0.8742  last_time: 0.8841  data_time: 0.0131  last_data_time: 0.0162   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:33:15 d2.utils.events]: \u001b[0m eta: 3 days, 16:18:08  iter: 4379  total_loss: 61.72  loss_ce: 0.7165  loss_mask: 0.2529  loss_dice: 0.6886  loss_bbox: 0.2152  loss_giou: 0.7851  loss_ce_dn: 0.128  loss_mask_dn: 0.2863  loss_dice_dn: 0.6924  loss_bbox_dn: 0.211  loss_giou_dn: 0.6918  loss_ce_0: 1.359  loss_mask_0: 0.2677  loss_dice_0: 0.7589  loss_bbox_0: 0.3395  loss_giou_0: 0.9685  loss_ce_dn_0: 0.6786  loss_mask_dn_0: 0.7199  loss_dice_dn_0: 2.699  loss_bbox_dn_0: 0.5895  loss_giou_dn_0: 1.116  loss_ce_1: 1.295  loss_mask_1: 0.2586  loss_dice_1: 0.7164  loss_bbox_1: 0.2613  loss_giou_1: 0.8741  loss_ce_dn_1: 0.2277  loss_mask_dn_1: 0.2694  loss_dice_dn_1: 0.7605  loss_bbox_dn_1: 0.3325  loss_giou_dn_1: 0.8425  loss_ce_2: 1.133  loss_mask_2: 0.2572  loss_dice_2: 0.7073  loss_bbox_2: 0.246  loss_giou_2: 0.8093  loss_ce_dn_2: 0.1826  loss_mask_dn_2: 0.2546  loss_dice_dn_2: 0.7343  loss_bbox_dn_2: 0.2862  loss_giou_dn_2: 0.7748  loss_ce_3: 0.9486  loss_mask_3: 0.265  loss_dice_3: 0.7093  loss_bbox_3: 0.2272  loss_giou_3: 0.8115  loss_ce_dn_3: 0.1561  loss_mask_dn_3: 0.2545  loss_dice_dn_3: 0.7149  loss_bbox_dn_3: 0.2596  loss_giou_dn_3: 0.7315  loss_ce_4: 0.8116  loss_mask_4: 0.2622  loss_dice_4: 0.705  loss_bbox_4: 0.2315  loss_giou_4: 0.7897  loss_ce_dn_4: 0.14  loss_mask_dn_4: 0.2607  loss_dice_dn_4: 0.6973  loss_bbox_dn_4: 0.2419  loss_giou_dn_4: 0.7156  loss_ce_5: 0.7677  loss_mask_5: 0.2546  loss_dice_5: 0.7177  loss_bbox_5: 0.2276  loss_giou_5: 0.7918  loss_ce_dn_5: 0.1322  loss_mask_dn_5: 0.2926  loss_dice_dn_5: 0.6838  loss_bbox_dn_5: 0.2336  loss_giou_dn_5: 0.7049  loss_ce_6: 0.7416  loss_mask_6: 0.2558  loss_dice_6: 0.6992  loss_bbox_6: 0.2142  loss_giou_6: 0.7807  loss_ce_dn_6: 0.1327  loss_mask_dn_6: 0.2923  loss_dice_dn_6: 0.6902  loss_bbox_dn_6: 0.2182  loss_giou_dn_6: 0.6949  loss_ce_7: 0.7253  loss_mask_7: 0.2587  loss_dice_7: 0.7274  loss_bbox_7: 0.2196  loss_giou_7: 0.7802  loss_ce_dn_7: 0.137  loss_mask_dn_7: 0.2868  loss_dice_dn_7: 0.6885  loss_bbox_dn_7: 0.2179  loss_giou_dn_7: 0.6936  loss_ce_8: 0.7139  loss_mask_8: 0.2541  loss_dice_8: 0.6906  loss_bbox_8: 0.2177  loss_giou_8: 0.775  loss_ce_dn_8: 0.1327  loss_mask_dn_8: 0.2878  loss_dice_dn_8: 0.7028  loss_bbox_dn_8: 0.2129  loss_giou_dn_8: 0.6877  loss_ce_interm: 1.335  loss_mask_interm: 0.2774  loss_dice_interm: 0.7565  loss_bbox_interm: 0.3491  loss_giou_interm: 0.9679    time: 0.8742  last_time: 0.8849  data_time: 0.0118  last_data_time: 0.0143   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:33:33 d2.utils.events]: \u001b[0m eta: 3 days, 16:18:27  iter: 4399  total_loss: 59.86  loss_ce: 0.853  loss_mask: 0.2097  loss_dice: 0.7985  loss_bbox: 0.2059  loss_giou: 0.8131  loss_ce_dn: 0.139  loss_mask_dn: 0.2067  loss_dice_dn: 0.6634  loss_bbox_dn: 0.2049  loss_giou_dn: 0.6546  loss_ce_0: 1.535  loss_mask_0: 0.2194  loss_dice_0: 0.8207  loss_bbox_0: 0.3535  loss_giou_0: 1.04  loss_ce_dn_0: 0.738  loss_mask_dn_0: 0.6589  loss_dice_dn_0: 3.104  loss_bbox_dn_0: 0.487  loss_giou_dn_0: 1.01  loss_ce_1: 1.384  loss_mask_1: 0.2132  loss_dice_1: 0.7994  loss_bbox_1: 0.2629  loss_giou_1: 0.8784  loss_ce_dn_1: 0.2826  loss_mask_dn_1: 0.2515  loss_dice_dn_1: 0.7478  loss_bbox_dn_1: 0.2925  loss_giou_dn_1: 0.7538  loss_ce_2: 1.205  loss_mask_2: 0.2133  loss_dice_2: 0.7526  loss_bbox_2: 0.2293  loss_giou_2: 0.8335  loss_ce_dn_2: 0.2268  loss_mask_dn_2: 0.2244  loss_dice_dn_2: 0.7025  loss_bbox_dn_2: 0.2526  loss_giou_dn_2: 0.7116  loss_ce_3: 1.102  loss_mask_3: 0.2169  loss_dice_3: 0.7941  loss_bbox_3: 0.2228  loss_giou_3: 0.8442  loss_ce_dn_3: 0.1944  loss_mask_dn_3: 0.2168  loss_dice_dn_3: 0.6703  loss_bbox_dn_3: 0.2296  loss_giou_dn_3: 0.6839  loss_ce_4: 0.9825  loss_mask_4: 0.2155  loss_dice_4: 0.8039  loss_bbox_4: 0.2129  loss_giou_4: 0.8291  loss_ce_dn_4: 0.1682  loss_mask_dn_4: 0.2146  loss_dice_dn_4: 0.6724  loss_bbox_dn_4: 0.2131  loss_giou_dn_4: 0.6678  loss_ce_5: 0.8989  loss_mask_5: 0.2106  loss_dice_5: 0.7317  loss_bbox_5: 0.2155  loss_giou_5: 0.818  loss_ce_dn_5: 0.1514  loss_mask_dn_5: 0.2127  loss_dice_dn_5: 0.6472  loss_bbox_dn_5: 0.2135  loss_giou_dn_5: 0.6647  loss_ce_6: 0.9038  loss_mask_6: 0.2092  loss_dice_6: 0.8001  loss_bbox_6: 0.2124  loss_giou_6: 0.8058  loss_ce_dn_6: 0.1451  loss_mask_dn_6: 0.2119  loss_dice_dn_6: 0.6744  loss_bbox_dn_6: 0.2086  loss_giou_dn_6: 0.6578  loss_ce_7: 0.8605  loss_mask_7: 0.2036  loss_dice_7: 0.7273  loss_bbox_7: 0.2033  loss_giou_7: 0.8194  loss_ce_dn_7: 0.1425  loss_mask_dn_7: 0.2051  loss_dice_dn_7: 0.6548  loss_bbox_dn_7: 0.2041  loss_giou_dn_7: 0.6566  loss_ce_8: 0.8487  loss_mask_8: 0.1972  loss_dice_8: 0.754  loss_bbox_8: 0.2035  loss_giou_8: 0.8268  loss_ce_dn_8: 0.1426  loss_mask_dn_8: 0.206  loss_dice_dn_8: 0.6675  loss_bbox_dn_8: 0.2048  loss_giou_dn_8: 0.6533  loss_ce_interm: 1.524  loss_mask_interm: 0.2225  loss_dice_interm: 0.8075  loss_bbox_interm: 0.3555  loss_giou_interm: 1.032    time: 0.8742  last_time: 0.8753  data_time: 0.0140  last_data_time: 0.0169   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:33:50 d2.utils.events]: \u001b[0m eta: 3 days, 16:15:14  iter: 4419  total_loss: 61.21  loss_ce: 0.7682  loss_mask: 0.2518  loss_dice: 0.7513  loss_bbox: 0.2417  loss_giou: 0.849  loss_ce_dn: 0.1312  loss_mask_dn: 0.2527  loss_dice_dn: 0.7476  loss_bbox_dn: 0.1694  loss_giou_dn: 0.6521  loss_ce_0: 1.52  loss_mask_0: 0.2464  loss_dice_0: 0.7931  loss_bbox_0: 0.3395  loss_giou_0: 0.9872  loss_ce_dn_0: 0.709  loss_mask_dn_0: 0.7295  loss_dice_dn_0: 2.907  loss_bbox_dn_0: 0.5335  loss_giou_dn_0: 1.001  loss_ce_1: 1.389  loss_mask_1: 0.2509  loss_dice_1: 0.8123  loss_bbox_1: 0.2609  loss_giou_1: 0.8901  loss_ce_dn_1: 0.2596  loss_mask_dn_1: 0.2993  loss_dice_dn_1: 0.7931  loss_bbox_dn_1: 0.2983  loss_giou_dn_1: 0.7942  loss_ce_2: 1.21  loss_mask_2: 0.2618  loss_dice_2: 0.7635  loss_bbox_2: 0.2812  loss_giou_2: 0.8391  loss_ce_dn_2: 0.1995  loss_mask_dn_2: 0.2864  loss_dice_dn_2: 0.8005  loss_bbox_dn_2: 0.2392  loss_giou_dn_2: 0.7149  loss_ce_3: 1.019  loss_mask_3: 0.2598  loss_dice_3: 0.7526  loss_bbox_3: 0.2418  loss_giou_3: 0.842  loss_ce_dn_3: 0.1706  loss_mask_dn_3: 0.2641  loss_dice_dn_3: 0.76  loss_bbox_dn_3: 0.2131  loss_giou_dn_3: 0.6914  loss_ce_4: 0.965  loss_mask_4: 0.2619  loss_dice_4: 0.7969  loss_bbox_4: 0.2396  loss_giou_4: 0.8371  loss_ce_dn_4: 0.1604  loss_mask_dn_4: 0.2563  loss_dice_dn_4: 0.7484  loss_bbox_dn_4: 0.1912  loss_giou_dn_4: 0.6796  loss_ce_5: 0.8508  loss_mask_5: 0.2655  loss_dice_5: 0.7916  loss_bbox_5: 0.2434  loss_giou_5: 0.8624  loss_ce_dn_5: 0.1467  loss_mask_dn_5: 0.2538  loss_dice_dn_5: 0.7485  loss_bbox_dn_5: 0.1803  loss_giou_dn_5: 0.6668  loss_ce_6: 0.8065  loss_mask_6: 0.2623  loss_dice_6: 0.7431  loss_bbox_6: 0.2334  loss_giou_6: 0.8387  loss_ce_dn_6: 0.1438  loss_mask_dn_6: 0.2507  loss_dice_dn_6: 0.7438  loss_bbox_dn_6: 0.1718  loss_giou_dn_6: 0.6588  loss_ce_7: 0.8074  loss_mask_7: 0.2584  loss_dice_7: 0.7299  loss_bbox_7: 0.237  loss_giou_7: 0.859  loss_ce_dn_7: 0.1358  loss_mask_dn_7: 0.2524  loss_dice_dn_7: 0.7597  loss_bbox_dn_7: 0.1708  loss_giou_dn_7: 0.6569  loss_ce_8: 0.7777  loss_mask_8: 0.2562  loss_dice_8: 0.7216  loss_bbox_8: 0.2335  loss_giou_8: 0.8457  loss_ce_dn_8: 0.1326  loss_mask_dn_8: 0.2518  loss_dice_dn_8: 0.7513  loss_bbox_dn_8: 0.1706  loss_giou_dn_8: 0.6534  loss_ce_interm: 1.505  loss_mask_interm: 0.248  loss_dice_interm: 0.7989  loss_bbox_interm: 0.3523  loss_giou_interm: 0.9937    time: 0.8742  last_time: 0.8705  data_time: 0.0110  last_data_time: 0.0082   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:34:07 d2.utils.events]: \u001b[0m eta: 3 days, 16:15:10  iter: 4439  total_loss: 66.43  loss_ce: 0.7664  loss_mask: 0.2789  loss_dice: 0.7494  loss_bbox: 0.2827  loss_giou: 0.6448  loss_ce_dn: 0.1128  loss_mask_dn: 0.2976  loss_dice_dn: 0.7819  loss_bbox_dn: 0.2498  loss_giou_dn: 0.5329  loss_ce_0: 1.471  loss_mask_0: 0.3057  loss_dice_0: 0.886  loss_bbox_0: 0.4185  loss_giou_0: 0.8525  loss_ce_dn_0: 0.7961  loss_mask_dn_0: 0.9147  loss_dice_dn_0: 3.068  loss_bbox_dn_0: 0.7069  loss_giou_dn_0: 0.9566  loss_ce_1: 1.344  loss_mask_1: 0.3097  loss_dice_1: 0.8891  loss_bbox_1: 0.3334  loss_giou_1: 0.7  loss_ce_dn_1: 0.2414  loss_mask_dn_1: 0.3609  loss_dice_dn_1: 0.932  loss_bbox_dn_1: 0.4077  loss_giou_dn_1: 0.6688  loss_ce_2: 1.192  loss_mask_2: 0.2912  loss_dice_2: 0.8835  loss_bbox_2: 0.3173  loss_giou_2: 0.6754  loss_ce_dn_2: 0.2017  loss_mask_dn_2: 0.339  loss_dice_dn_2: 0.8513  loss_bbox_dn_2: 0.3286  loss_giou_dn_2: 0.6061  loss_ce_3: 1.026  loss_mask_3: 0.2952  loss_dice_3: 0.8288  loss_bbox_3: 0.3154  loss_giou_3: 0.678  loss_ce_dn_3: 0.1583  loss_mask_dn_3: 0.3116  loss_dice_dn_3: 0.802  loss_bbox_dn_3: 0.2916  loss_giou_dn_3: 0.5777  loss_ce_4: 0.9505  loss_mask_4: 0.2789  loss_dice_4: 0.7722  loss_bbox_4: 0.2938  loss_giou_4: 0.6742  loss_ce_dn_4: 0.1468  loss_mask_dn_4: 0.3015  loss_dice_dn_4: 0.7934  loss_bbox_dn_4: 0.2767  loss_giou_dn_4: 0.5594  loss_ce_5: 0.8094  loss_mask_5: 0.2659  loss_dice_5: 0.8107  loss_bbox_5: 0.2954  loss_giou_5: 0.6551  loss_ce_dn_5: 0.1323  loss_mask_dn_5: 0.3011  loss_dice_dn_5: 0.7852  loss_bbox_dn_5: 0.2699  loss_giou_dn_5: 0.5507  loss_ce_6: 0.8136  loss_mask_6: 0.2683  loss_dice_6: 0.7929  loss_bbox_6: 0.2962  loss_giou_6: 0.647  loss_ce_dn_6: 0.1226  loss_mask_dn_6: 0.2997  loss_dice_dn_6: 0.77  loss_bbox_dn_6: 0.2572  loss_giou_dn_6: 0.5374  loss_ce_7: 0.7759  loss_mask_7: 0.2772  loss_dice_7: 0.8219  loss_bbox_7: 0.2967  loss_giou_7: 0.6442  loss_ce_dn_7: 0.1201  loss_mask_dn_7: 0.2939  loss_dice_dn_7: 0.7748  loss_bbox_dn_7: 0.2554  loss_giou_dn_7: 0.5398  loss_ce_8: 0.7773  loss_mask_8: 0.2818  loss_dice_8: 0.8054  loss_bbox_8: 0.2824  loss_giou_8: 0.6378  loss_ce_dn_8: 0.1137  loss_mask_dn_8: 0.2936  loss_dice_dn_8: 0.7744  loss_bbox_dn_8: 0.2511  loss_giou_dn_8: 0.5333  loss_ce_interm: 1.483  loss_mask_interm: 0.3117  loss_dice_interm: 0.886  loss_bbox_interm: 0.4145  loss_giou_interm: 0.853    time: 0.8741  last_time: 0.8746  data_time: 0.0116  last_data_time: 0.0141   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:34:25 d2.utils.events]: \u001b[0m eta: 3 days, 16:13:43  iter: 4459  total_loss: 61.9  loss_ce: 0.7953  loss_mask: 0.2146  loss_dice: 0.8086  loss_bbox: 0.2489  loss_giou: 0.695  loss_ce_dn: 0.1115  loss_mask_dn: 0.2321  loss_dice_dn: 0.7764  loss_bbox_dn: 0.1943  loss_giou_dn: 0.6088  loss_ce_0: 1.461  loss_mask_0: 0.2193  loss_dice_0: 0.8685  loss_bbox_0: 0.361  loss_giou_0: 0.89  loss_ce_dn_0: 0.6885  loss_mask_dn_0: 0.7448  loss_dice_dn_0: 3.128  loss_bbox_dn_0: 0.5719  loss_giou_dn_0: 0.9905  loss_ce_1: 1.349  loss_mask_1: 0.2361  loss_dice_1: 0.8443  loss_bbox_1: 0.2733  loss_giou_1: 0.7699  loss_ce_dn_1: 0.24  loss_mask_dn_1: 0.2827  loss_dice_dn_1: 0.8541  loss_bbox_dn_1: 0.305  loss_giou_dn_1: 0.7431  loss_ce_2: 1.209  loss_mask_2: 0.2117  loss_dice_2: 0.8273  loss_bbox_2: 0.2507  loss_giou_2: 0.727  loss_ce_dn_2: 0.1872  loss_mask_dn_2: 0.2511  loss_dice_dn_2: 0.8182  loss_bbox_dn_2: 0.2444  loss_giou_dn_2: 0.687  loss_ce_3: 0.9794  loss_mask_3: 0.2033  loss_dice_3: 0.8118  loss_bbox_3: 0.2485  loss_giou_3: 0.708  loss_ce_dn_3: 0.1453  loss_mask_dn_3: 0.2484  loss_dice_dn_3: 0.7746  loss_bbox_dn_3: 0.2282  loss_giou_dn_3: 0.6624  loss_ce_4: 0.8676  loss_mask_4: 0.2046  loss_dice_4: 0.808  loss_bbox_4: 0.2584  loss_giou_4: 0.7079  loss_ce_dn_4: 0.128  loss_mask_dn_4: 0.2352  loss_dice_dn_4: 0.7594  loss_bbox_dn_4: 0.2098  loss_giou_dn_4: 0.6392  loss_ce_5: 0.7738  loss_mask_5: 0.1999  loss_dice_5: 0.791  loss_bbox_5: 0.2587  loss_giou_5: 0.7057  loss_ce_dn_5: 0.1193  loss_mask_dn_5: 0.2349  loss_dice_dn_5: 0.758  loss_bbox_dn_5: 0.2049  loss_giou_dn_5: 0.6298  loss_ce_6: 0.7913  loss_mask_6: 0.194  loss_dice_6: 0.7941  loss_bbox_6: 0.2428  loss_giou_6: 0.6999  loss_ce_dn_6: 0.118  loss_mask_dn_6: 0.2275  loss_dice_dn_6: 0.7761  loss_bbox_dn_6: 0.2027  loss_giou_dn_6: 0.6176  loss_ce_7: 0.7975  loss_mask_7: 0.212  loss_dice_7: 0.8069  loss_bbox_7: 0.2512  loss_giou_7: 0.6995  loss_ce_dn_7: 0.1163  loss_mask_dn_7: 0.23  loss_dice_dn_7: 0.7654  loss_bbox_dn_7: 0.1989  loss_giou_dn_7: 0.615  loss_ce_8: 0.7795  loss_mask_8: 0.2095  loss_dice_8: 0.7759  loss_bbox_8: 0.2522  loss_giou_8: 0.6928  loss_ce_dn_8: 0.1132  loss_mask_dn_8: 0.2332  loss_dice_dn_8: 0.7625  loss_bbox_dn_8: 0.1952  loss_giou_dn_8: 0.6107  loss_ce_interm: 1.457  loss_mask_interm: 0.2139  loss_dice_interm: 0.8322  loss_bbox_interm: 0.3703  loss_giou_interm: 0.8901    time: 0.8741  last_time: 0.8555  data_time: 0.0120  last_data_time: 0.0105   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:34:42 d2.utils.events]: \u001b[0m eta: 3 days, 16:14:17  iter: 4479  total_loss: 58.63  loss_ce: 0.7585  loss_mask: 0.2927  loss_dice: 0.7697  loss_bbox: 0.2761  loss_giou: 0.7993  loss_ce_dn: 0.1652  loss_mask_dn: 0.2876  loss_dice_dn: 0.7362  loss_bbox_dn: 0.2879  loss_giou_dn: 0.694  loss_ce_0: 1.463  loss_mask_0: 0.313  loss_dice_0: 0.8628  loss_bbox_0: 0.4314  loss_giou_0: 0.9885  loss_ce_dn_0: 0.6914  loss_mask_dn_0: 0.9791  loss_dice_dn_0: 3.003  loss_bbox_dn_0: 0.6714  loss_giou_dn_0: 1.046  loss_ce_1: 1.327  loss_mask_1: 0.3121  loss_dice_1: 0.8018  loss_bbox_1: 0.3412  loss_giou_1: 0.8853  loss_ce_dn_1: 0.2881  loss_mask_dn_1: 0.3758  loss_dice_dn_1: 0.8991  loss_bbox_dn_1: 0.397  loss_giou_dn_1: 0.835  loss_ce_2: 1.156  loss_mask_2: 0.3139  loss_dice_2: 0.8433  loss_bbox_2: 0.3053  loss_giou_2: 0.8181  loss_ce_dn_2: 0.2335  loss_mask_dn_2: 0.314  loss_dice_dn_2: 0.8193  loss_bbox_dn_2: 0.3264  loss_giou_dn_2: 0.7577  loss_ce_3: 0.9879  loss_mask_3: 0.3078  loss_dice_3: 0.8263  loss_bbox_3: 0.288  loss_giou_3: 0.8272  loss_ce_dn_3: 0.2154  loss_mask_dn_3: 0.3024  loss_dice_dn_3: 0.7655  loss_bbox_dn_3: 0.2916  loss_giou_dn_3: 0.7346  loss_ce_4: 0.9086  loss_mask_4: 0.3064  loss_dice_4: 0.8452  loss_bbox_4: 0.2913  loss_giou_4: 0.8144  loss_ce_dn_4: 0.2034  loss_mask_dn_4: 0.2926  loss_dice_dn_4: 0.7618  loss_bbox_dn_4: 0.2869  loss_giou_dn_4: 0.7181  loss_ce_5: 0.8109  loss_mask_5: 0.3049  loss_dice_5: 0.8319  loss_bbox_5: 0.2769  loss_giou_5: 0.8181  loss_ce_dn_5: 0.1853  loss_mask_dn_5: 0.2873  loss_dice_dn_5: 0.7646  loss_bbox_dn_5: 0.2817  loss_giou_dn_5: 0.7145  loss_ce_6: 0.7665  loss_mask_6: 0.3029  loss_dice_6: 0.8052  loss_bbox_6: 0.2722  loss_giou_6: 0.8054  loss_ce_dn_6: 0.1765  loss_mask_dn_6: 0.2877  loss_dice_dn_6: 0.7354  loss_bbox_dn_6: 0.2841  loss_giou_dn_6: 0.7052  loss_ce_7: 0.764  loss_mask_7: 0.2941  loss_dice_7: 0.7768  loss_bbox_7: 0.2754  loss_giou_7: 0.8061  loss_ce_dn_7: 0.1757  loss_mask_dn_7: 0.2865  loss_dice_dn_7: 0.7354  loss_bbox_dn_7: 0.2864  loss_giou_dn_7: 0.6999  loss_ce_8: 0.7407  loss_mask_8: 0.2894  loss_dice_8: 0.7783  loss_bbox_8: 0.2763  loss_giou_8: 0.8084  loss_ce_dn_8: 0.1632  loss_mask_dn_8: 0.2887  loss_dice_dn_8: 0.7295  loss_bbox_dn_8: 0.2878  loss_giou_dn_8: 0.6943  loss_ce_interm: 1.447  loss_mask_interm: 0.316  loss_dice_interm: 0.8573  loss_bbox_interm: 0.4235  loss_giou_interm: 0.9773    time: 0.8741  last_time: 0.8788  data_time: 0.0126  last_data_time: 0.0174   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:35:00 d2.utils.events]: \u001b[0m eta: 3 days, 16:12:50  iter: 4499  total_loss: 65.96  loss_ce: 0.8677  loss_mask: 0.3154  loss_dice: 0.6338  loss_bbox: 0.27  loss_giou: 0.712  loss_ce_dn: 0.1301  loss_mask_dn: 0.31  loss_dice_dn: 0.6459  loss_bbox_dn: 0.2763  loss_giou_dn: 0.5589  loss_ce_0: 1.595  loss_mask_0: 0.2957  loss_dice_0: 0.6625  loss_bbox_0: 0.426  loss_giou_0: 0.9128  loss_ce_dn_0: 0.7473  loss_mask_dn_0: 1.055  loss_dice_dn_0: 3.127  loss_bbox_dn_0: 0.7035  loss_giou_dn_0: 0.9465  loss_ce_1: 1.456  loss_mask_1: 0.3112  loss_dice_1: 0.602  loss_bbox_1: 0.3247  loss_giou_1: 0.7997  loss_ce_dn_1: 0.2937  loss_mask_dn_1: 0.3838  loss_dice_dn_1: 0.816  loss_bbox_dn_1: 0.4014  loss_giou_dn_1: 0.7263  loss_ce_2: 1.326  loss_mask_2: 0.2854  loss_dice_2: 0.614  loss_bbox_2: 0.285  loss_giou_2: 0.7424  loss_ce_dn_2: 0.2344  loss_mask_dn_2: 0.3526  loss_dice_dn_2: 0.74  loss_bbox_dn_2: 0.3413  loss_giou_dn_2: 0.6515  loss_ce_3: 1.16  loss_mask_3: 0.297  loss_dice_3: 0.617  loss_bbox_3: 0.2812  loss_giou_3: 0.7304  loss_ce_dn_3: 0.1956  loss_mask_dn_3: 0.3383  loss_dice_dn_3: 0.7111  loss_bbox_dn_3: 0.3069  loss_giou_dn_3: 0.6145  loss_ce_4: 1.041  loss_mask_4: 0.3145  loss_dice_4: 0.6554  loss_bbox_4: 0.2794  loss_giou_4: 0.71  loss_ce_dn_4: 0.1747  loss_mask_dn_4: 0.3386  loss_dice_dn_4: 0.6721  loss_bbox_dn_4: 0.2889  loss_giou_dn_4: 0.5912  loss_ce_5: 0.9495  loss_mask_5: 0.328  loss_dice_5: 0.6616  loss_bbox_5: 0.2909  loss_giou_5: 0.7033  loss_ce_dn_5: 0.1583  loss_mask_dn_5: 0.3244  loss_dice_dn_5: 0.6584  loss_bbox_dn_5: 0.2762  loss_giou_dn_5: 0.5833  loss_ce_6: 0.8951  loss_mask_6: 0.318  loss_dice_6: 0.6418  loss_bbox_6: 0.2889  loss_giou_6: 0.7245  loss_ce_dn_6: 0.1423  loss_mask_dn_6: 0.3185  loss_dice_dn_6: 0.6366  loss_bbox_dn_6: 0.2786  loss_giou_dn_6: 0.5675  loss_ce_7: 0.9124  loss_mask_7: 0.3189  loss_dice_7: 0.6498  loss_bbox_7: 0.2613  loss_giou_7: 0.7432  loss_ce_dn_7: 0.138  loss_mask_dn_7: 0.3127  loss_dice_dn_7: 0.6334  loss_bbox_dn_7: 0.278  loss_giou_dn_7: 0.5654  loss_ce_8: 0.877  loss_mask_8: 0.3098  loss_dice_8: 0.6359  loss_bbox_8: 0.2539  loss_giou_8: 0.7268  loss_ce_dn_8: 0.1349  loss_mask_dn_8: 0.3088  loss_dice_dn_8: 0.638  loss_bbox_dn_8: 0.2782  loss_giou_dn_8: 0.5564  loss_ce_interm: 1.596  loss_mask_interm: 0.2963  loss_dice_interm: 0.6775  loss_bbox_interm: 0.4099  loss_giou_interm: 0.923    time: 0.8741  last_time: 0.8476  data_time: 0.0123  last_data_time: 0.0076   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:35:17 d2.utils.events]: \u001b[0m eta: 3 days, 16:11:14  iter: 4519  total_loss: 68.73  loss_ce: 0.9249  loss_mask: 0.3573  loss_dice: 0.8077  loss_bbox: 0.322  loss_giou: 0.7241  loss_ce_dn: 0.1588  loss_mask_dn: 0.3774  loss_dice_dn: 0.7506  loss_bbox_dn: 0.2773  loss_giou_dn: 0.591  loss_ce_0: 1.574  loss_mask_0: 0.3706  loss_dice_0: 0.8113  loss_bbox_0: 0.5068  loss_giou_0: 0.9683  loss_ce_dn_0: 0.7608  loss_mask_dn_0: 1.049  loss_dice_dn_0: 3.287  loss_bbox_dn_0: 0.6543  loss_giou_dn_0: 0.9578  loss_ce_1: 1.476  loss_mask_1: 0.3559  loss_dice_1: 0.7903  loss_bbox_1: 0.4058  loss_giou_1: 0.8297  loss_ce_dn_1: 0.302  loss_mask_dn_1: 0.452  loss_dice_dn_1: 0.9229  loss_bbox_dn_1: 0.3896  loss_giou_dn_1: 0.7168  loss_ce_2: 1.279  loss_mask_2: 0.357  loss_dice_2: 0.7909  loss_bbox_2: 0.3717  loss_giou_2: 0.8073  loss_ce_dn_2: 0.2338  loss_mask_dn_2: 0.4068  loss_dice_dn_2: 0.8421  loss_bbox_dn_2: 0.3215  loss_giou_dn_2: 0.6562  loss_ce_3: 1.109  loss_mask_3: 0.3398  loss_dice_3: 0.8319  loss_bbox_3: 0.3806  loss_giou_3: 0.7486  loss_ce_dn_3: 0.2069  loss_mask_dn_3: 0.3782  loss_dice_dn_3: 0.7889  loss_bbox_dn_3: 0.3018  loss_giou_dn_3: 0.6319  loss_ce_4: 0.9925  loss_mask_4: 0.3554  loss_dice_4: 0.8249  loss_bbox_4: 0.3567  loss_giou_4: 0.7283  loss_ce_dn_4: 0.1851  loss_mask_dn_4: 0.3732  loss_dice_dn_4: 0.7669  loss_bbox_dn_4: 0.2903  loss_giou_dn_4: 0.6108  loss_ce_5: 0.9624  loss_mask_5: 0.35  loss_dice_5: 0.8645  loss_bbox_5: 0.3291  loss_giou_5: 0.7263  loss_ce_dn_5: 0.176  loss_mask_dn_5: 0.3696  loss_dice_dn_5: 0.7816  loss_bbox_dn_5: 0.2919  loss_giou_dn_5: 0.6108  loss_ce_6: 0.955  loss_mask_6: 0.35  loss_dice_6: 0.8344  loss_bbox_6: 0.2881  loss_giou_6: 0.7124  loss_ce_dn_6: 0.1651  loss_mask_dn_6: 0.366  loss_dice_dn_6: 0.7547  loss_bbox_dn_6: 0.284  loss_giou_dn_6: 0.602  loss_ce_7: 0.932  loss_mask_7: 0.3521  loss_dice_7: 0.8128  loss_bbox_7: 0.3161  loss_giou_7: 0.7391  loss_ce_dn_7: 0.1523  loss_mask_dn_7: 0.3717  loss_dice_dn_7: 0.7894  loss_bbox_dn_7: 0.2832  loss_giou_dn_7: 0.5984  loss_ce_8: 0.9332  loss_mask_8: 0.3535  loss_dice_8: 0.8485  loss_bbox_8: 0.2856  loss_giou_8: 0.7336  loss_ce_dn_8: 0.1603  loss_mask_dn_8: 0.3729  loss_dice_dn_8: 0.7546  loss_bbox_dn_8: 0.278  loss_giou_dn_8: 0.5911  loss_ce_interm: 1.581  loss_mask_interm: 0.3673  loss_dice_interm: 0.8346  loss_bbox_interm: 0.4992  loss_giou_interm: 0.9673    time: 0.8741  last_time: 0.8631  data_time: 0.0119  last_data_time: 0.0127   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:35:35 d2.utils.events]: \u001b[0m eta: 3 days, 16:10:21  iter: 4539  total_loss: 67.36  loss_ce: 0.8282  loss_mask: 0.2469  loss_dice: 0.9517  loss_bbox: 0.2938  loss_giou: 0.7648  loss_ce_dn: 0.1537  loss_mask_dn: 0.2679  loss_dice_dn: 1.01  loss_bbox_dn: 0.2948  loss_giou_dn: 0.5945  loss_ce_0: 1.542  loss_mask_0: 0.248  loss_dice_0: 0.9802  loss_bbox_0: 0.4078  loss_giou_0: 0.944  loss_ce_dn_0: 0.7028  loss_mask_dn_0: 0.8786  loss_dice_dn_0: 3.122  loss_bbox_dn_0: 0.6613  loss_giou_dn_0: 0.9525  loss_ce_1: 1.45  loss_mask_1: 0.279  loss_dice_1: 0.9388  loss_bbox_1: 0.3352  loss_giou_1: 0.8387  loss_ce_dn_1: 0.2595  loss_mask_dn_1: 0.3247  loss_dice_dn_1: 1.115  loss_bbox_dn_1: 0.4237  loss_giou_dn_1: 0.7101  loss_ce_2: 1.26  loss_mask_2: 0.2572  loss_dice_2: 0.9361  loss_bbox_2: 0.2875  loss_giou_2: 0.8049  loss_ce_dn_2: 0.2121  loss_mask_dn_2: 0.3148  loss_dice_dn_2: 1.04  loss_bbox_dn_2: 0.3539  loss_giou_dn_2: 0.6416  loss_ce_3: 1.099  loss_mask_3: 0.3158  loss_dice_3: 0.9444  loss_bbox_3: 0.2855  loss_giou_3: 0.7946  loss_ce_dn_3: 0.1742  loss_mask_dn_3: 0.2937  loss_dice_dn_3: 1  loss_bbox_dn_3: 0.3277  loss_giou_dn_3: 0.627  loss_ce_4: 0.9576  loss_mask_4: 0.2611  loss_dice_4: 1.013  loss_bbox_4: 0.2707  loss_giou_4: 0.7777  loss_ce_dn_4: 0.1696  loss_mask_dn_4: 0.2727  loss_dice_dn_4: 1.018  loss_bbox_dn_4: 0.3211  loss_giou_dn_4: 0.6063  loss_ce_5: 0.941  loss_mask_5: 0.2571  loss_dice_5: 0.9238  loss_bbox_5: 0.2745  loss_giou_5: 0.7857  loss_ce_dn_5: 0.1676  loss_mask_dn_5: 0.2621  loss_dice_dn_5: 1.01  loss_bbox_dn_5: 0.3124  loss_giou_dn_5: 0.5992  loss_ce_6: 0.8835  loss_mask_6: 0.2718  loss_dice_6: 0.9427  loss_bbox_6: 0.274  loss_giou_6: 0.7892  loss_ce_dn_6: 0.1598  loss_mask_dn_6: 0.2713  loss_dice_dn_6: 1.011  loss_bbox_dn_6: 0.3038  loss_giou_dn_6: 0.5943  loss_ce_7: 0.827  loss_mask_7: 0.2553  loss_dice_7: 0.9608  loss_bbox_7: 0.2801  loss_giou_7: 0.7543  loss_ce_dn_7: 0.157  loss_mask_dn_7: 0.2709  loss_dice_dn_7: 1.004  loss_bbox_dn_7: 0.2998  loss_giou_dn_7: 0.594  loss_ce_8: 0.8187  loss_mask_8: 0.2513  loss_dice_8: 0.916  loss_bbox_8: 0.2884  loss_giou_8: 0.7507  loss_ce_dn_8: 0.1559  loss_mask_dn_8: 0.2743  loss_dice_dn_8: 1.003  loss_bbox_dn_8: 0.2958  loss_giou_dn_8: 0.5917  loss_ce_interm: 1.542  loss_mask_interm: 0.2591  loss_dice_interm: 0.9977  loss_bbox_interm: 0.4186  loss_giou_interm: 0.9323    time: 0.8741  last_time: 0.8773  data_time: 0.0117  last_data_time: 0.0162   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:35:52 d2.utils.events]: \u001b[0m eta: 3 days, 16:10:23  iter: 4559  total_loss: 65.85  loss_ce: 0.8045  loss_mask: 0.1899  loss_dice: 0.8195  loss_bbox: 0.2041  loss_giou: 0.7268  loss_ce_dn: 0.1319  loss_mask_dn: 0.2155  loss_dice_dn: 0.8226  loss_bbox_dn: 0.2009  loss_giou_dn: 0.5581  loss_ce_0: 1.517  loss_mask_0: 0.2114  loss_dice_0: 0.8383  loss_bbox_0: 0.3402  loss_giou_0: 0.9595  loss_ce_dn_0: 0.6947  loss_mask_dn_0: 0.7608  loss_dice_dn_0: 3.322  loss_bbox_dn_0: 0.5383  loss_giou_dn_0: 0.925  loss_ce_1: 1.381  loss_mask_1: 0.2124  loss_dice_1: 0.8387  loss_bbox_1: 0.2984  loss_giou_1: 0.7792  loss_ce_dn_1: 0.2632  loss_mask_dn_1: 0.2691  loss_dice_dn_1: 0.9323  loss_bbox_dn_1: 0.3273  loss_giou_dn_1: 0.7031  loss_ce_2: 1.183  loss_mask_2: 0.1975  loss_dice_2: 0.8548  loss_bbox_2: 0.2605  loss_giou_2: 0.7715  loss_ce_dn_2: 0.2051  loss_mask_dn_2: 0.2401  loss_dice_dn_2: 0.8939  loss_bbox_dn_2: 0.269  loss_giou_dn_2: 0.6418  loss_ce_3: 1.053  loss_mask_3: 0.2048  loss_dice_3: 0.8171  loss_bbox_3: 0.2443  loss_giou_3: 0.7717  loss_ce_dn_3: 0.1703  loss_mask_dn_3: 0.2299  loss_dice_dn_3: 0.8461  loss_bbox_dn_3: 0.2511  loss_giou_dn_3: 0.6059  loss_ce_4: 0.9532  loss_mask_4: 0.1938  loss_dice_4: 0.7955  loss_bbox_4: 0.2329  loss_giou_4: 0.7657  loss_ce_dn_4: 0.1592  loss_mask_dn_4: 0.2165  loss_dice_dn_4: 0.8423  loss_bbox_dn_4: 0.2275  loss_giou_dn_4: 0.5812  loss_ce_5: 0.8763  loss_mask_5: 0.1921  loss_dice_5: 0.8202  loss_bbox_5: 0.229  loss_giou_5: 0.7056  loss_ce_dn_5: 0.1397  loss_mask_dn_5: 0.2144  loss_dice_dn_5: 0.8297  loss_bbox_dn_5: 0.2211  loss_giou_dn_5: 0.5788  loss_ce_6: 0.8582  loss_mask_6: 0.1979  loss_dice_6: 0.8114  loss_bbox_6: 0.2204  loss_giou_6: 0.7517  loss_ce_dn_6: 0.1337  loss_mask_dn_6: 0.213  loss_dice_dn_6: 0.8007  loss_bbox_dn_6: 0.2117  loss_giou_dn_6: 0.5677  loss_ce_7: 0.8109  loss_mask_7: 0.1937  loss_dice_7: 0.8079  loss_bbox_7: 0.2234  loss_giou_7: 0.7477  loss_ce_dn_7: 0.1348  loss_mask_dn_7: 0.2114  loss_dice_dn_7: 0.7974  loss_bbox_dn_7: 0.2074  loss_giou_dn_7: 0.5667  loss_ce_8: 0.8093  loss_mask_8: 0.1905  loss_dice_8: 0.8247  loss_bbox_8: 0.2129  loss_giou_8: 0.7275  loss_ce_dn_8: 0.1362  loss_mask_dn_8: 0.2124  loss_dice_dn_8: 0.8154  loss_bbox_dn_8: 0.1996  loss_giou_dn_8: 0.5584  loss_ce_interm: 1.502  loss_mask_interm: 0.2105  loss_dice_interm: 0.8509  loss_bbox_interm: 0.3415  loss_giou_interm: 0.9491    time: 0.8741  last_time: 0.9101  data_time: 0.0132  last_data_time: 0.0121   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:36:10 d2.utils.events]: \u001b[0m eta: 3 days, 16:11:40  iter: 4579  total_loss: 62.69  loss_ce: 0.863  loss_mask: 0.2777  loss_dice: 0.7821  loss_bbox: 0.2451  loss_giou: 0.7992  loss_ce_dn: 0.1746  loss_mask_dn: 0.2694  loss_dice_dn: 0.7534  loss_bbox_dn: 0.2338  loss_giou_dn: 0.6766  loss_ce_0: 1.534  loss_mask_0: 0.28  loss_dice_0: 0.8163  loss_bbox_0: 0.3862  loss_giou_0: 1.019  loss_ce_dn_0: 0.7299  loss_mask_dn_0: 1.098  loss_dice_dn_0: 3.091  loss_bbox_dn_0: 0.6271  loss_giou_dn_0: 1.039  loss_ce_1: 1.375  loss_mask_1: 0.2682  loss_dice_1: 0.8018  loss_bbox_1: 0.3265  loss_giou_1: 0.9132  loss_ce_dn_1: 0.2964  loss_mask_dn_1: 0.3295  loss_dice_dn_1: 0.8431  loss_bbox_dn_1: 0.3379  loss_giou_dn_1: 0.7965  loss_ce_2: 1.206  loss_mask_2: 0.2579  loss_dice_2: 0.829  loss_bbox_2: 0.3155  loss_giou_2: 0.8628  loss_ce_dn_2: 0.2397  loss_mask_dn_2: 0.2868  loss_dice_dn_2: 0.8373  loss_bbox_dn_2: 0.2818  loss_giou_dn_2: 0.7244  loss_ce_3: 1.026  loss_mask_3: 0.2734  loss_dice_3: 0.7517  loss_bbox_3: 0.2984  loss_giou_3: 0.8408  loss_ce_dn_3: 0.2096  loss_mask_dn_3: 0.2744  loss_dice_dn_3: 0.7868  loss_bbox_dn_3: 0.275  loss_giou_dn_3: 0.7052  loss_ce_4: 0.9798  loss_mask_4: 0.2656  loss_dice_4: 0.7917  loss_bbox_4: 0.2652  loss_giou_4: 0.8039  loss_ce_dn_4: 0.1923  loss_mask_dn_4: 0.2737  loss_dice_dn_4: 0.7913  loss_bbox_dn_4: 0.2556  loss_giou_dn_4: 0.6873  loss_ce_5: 0.9165  loss_mask_5: 0.272  loss_dice_5: 0.8194  loss_bbox_5: 0.2535  loss_giou_5: 0.8275  loss_ce_dn_5: 0.1921  loss_mask_dn_5: 0.2736  loss_dice_dn_5: 0.795  loss_bbox_dn_5: 0.2438  loss_giou_dn_5: 0.6806  loss_ce_6: 0.8512  loss_mask_6: 0.2717  loss_dice_6: 0.8307  loss_bbox_6: 0.2516  loss_giou_6: 0.8085  loss_ce_dn_6: 0.1833  loss_mask_dn_6: 0.2686  loss_dice_dn_6: 0.7858  loss_bbox_dn_6: 0.2351  loss_giou_dn_6: 0.6762  loss_ce_7: 0.8576  loss_mask_7: 0.2588  loss_dice_7: 0.7707  loss_bbox_7: 0.2447  loss_giou_7: 0.7994  loss_ce_dn_7: 0.1815  loss_mask_dn_7: 0.268  loss_dice_dn_7: 0.785  loss_bbox_dn_7: 0.2331  loss_giou_dn_7: 0.6753  loss_ce_8: 0.8546  loss_mask_8: 0.2722  loss_dice_8: 0.8149  loss_bbox_8: 0.2461  loss_giou_8: 0.7898  loss_ce_dn_8: 0.1737  loss_mask_dn_8: 0.2719  loss_dice_dn_8: 0.7928  loss_bbox_dn_8: 0.2335  loss_giou_dn_8: 0.6729  loss_ce_interm: 1.547  loss_mask_interm: 0.2804  loss_dice_interm: 0.8043  loss_bbox_interm: 0.3852  loss_giou_interm: 1.03    time: 0.8741  last_time: 0.8745  data_time: 0.0125  last_data_time: 0.0182   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:36:27 d2.utils.events]: \u001b[0m eta: 3 days, 16:10:04  iter: 4599  total_loss: 66.78  loss_ce: 0.8038  loss_mask: 0.256  loss_dice: 0.8107  loss_bbox: 0.3064  loss_giou: 0.7672  loss_ce_dn: 0.1346  loss_mask_dn: 0.2653  loss_dice_dn: 0.8551  loss_bbox_dn: 0.2458  loss_giou_dn: 0.578  loss_ce_0: 1.493  loss_mask_0: 0.296  loss_dice_0: 0.771  loss_bbox_0: 0.4531  loss_giou_0: 0.9647  loss_ce_dn_0: 0.7574  loss_mask_dn_0: 0.8836  loss_dice_dn_0: 3.006  loss_bbox_dn_0: 0.682  loss_giou_dn_0: 0.9736  loss_ce_1: 1.429  loss_mask_1: 0.2598  loss_dice_1: 0.8909  loss_bbox_1: 0.3563  loss_giou_1: 0.8468  loss_ce_dn_1: 0.2563  loss_mask_dn_1: 0.3692  loss_dice_dn_1: 0.9459  loss_bbox_dn_1: 0.4088  loss_giou_dn_1: 0.7004  loss_ce_2: 1.256  loss_mask_2: 0.2543  loss_dice_2: 0.7609  loss_bbox_2: 0.3098  loss_giou_2: 0.8154  loss_ce_dn_2: 0.2311  loss_mask_dn_2: 0.3135  loss_dice_dn_2: 0.8921  loss_bbox_dn_2: 0.3417  loss_giou_dn_2: 0.6365  loss_ce_3: 1.053  loss_mask_3: 0.2553  loss_dice_3: 0.7763  loss_bbox_3: 0.2943  loss_giou_3: 0.7901  loss_ce_dn_3: 0.1958  loss_mask_dn_3: 0.2693  loss_dice_dn_3: 0.8655  loss_bbox_dn_3: 0.2983  loss_giou_dn_3: 0.618  loss_ce_4: 0.9357  loss_mask_4: 0.2572  loss_dice_4: 0.7559  loss_bbox_4: 0.2756  loss_giou_4: 0.7467  loss_ce_dn_4: 0.1653  loss_mask_dn_4: 0.2759  loss_dice_dn_4: 0.803  loss_bbox_dn_4: 0.2767  loss_giou_dn_4: 0.5943  loss_ce_5: 0.8095  loss_mask_5: 0.2468  loss_dice_5: 0.8039  loss_bbox_5: 0.2745  loss_giou_5: 0.7609  loss_ce_dn_5: 0.1557  loss_mask_dn_5: 0.2719  loss_dice_dn_5: 0.8619  loss_bbox_dn_5: 0.2661  loss_giou_dn_5: 0.5876  loss_ce_6: 0.7777  loss_mask_6: 0.2538  loss_dice_6: 0.7113  loss_bbox_6: 0.3184  loss_giou_6: 0.7685  loss_ce_dn_6: 0.1495  loss_mask_dn_6: 0.2711  loss_dice_dn_6: 0.8503  loss_bbox_dn_6: 0.2515  loss_giou_dn_6: 0.5836  loss_ce_7: 0.8121  loss_mask_7: 0.2523  loss_dice_7: 0.8256  loss_bbox_7: 0.3089  loss_giou_7: 0.7794  loss_ce_dn_7: 0.1404  loss_mask_dn_7: 0.2668  loss_dice_dn_7: 0.8293  loss_bbox_dn_7: 0.2485  loss_giou_dn_7: 0.5804  loss_ce_8: 0.7558  loss_mask_8: 0.2559  loss_dice_8: 0.8208  loss_bbox_8: 0.3154  loss_giou_8: 0.7581  loss_ce_dn_8: 0.1422  loss_mask_dn_8: 0.2577  loss_dice_dn_8: 0.8369  loss_bbox_dn_8: 0.2445  loss_giou_dn_8: 0.5783  loss_ce_interm: 1.52  loss_mask_interm: 0.2956  loss_dice_interm: 0.8221  loss_bbox_interm: 0.453  loss_giou_interm: 0.9503    time: 0.8741  last_time: 0.8831  data_time: 0.0119  last_data_time: 0.0166   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:36:45 d2.utils.events]: \u001b[0m eta: 3 days, 16:11:06  iter: 4619  total_loss: 68.01  loss_ce: 0.9866  loss_mask: 0.1663  loss_dice: 0.9995  loss_bbox: 0.2142  loss_giou: 0.9401  loss_ce_dn: 0.135  loss_mask_dn: 0.1585  loss_dice_dn: 1.05  loss_bbox_dn: 0.1641  loss_giou_dn: 0.6992  loss_ce_0: 1.473  loss_mask_0: 0.1659  loss_dice_0: 1.058  loss_bbox_0: 0.3011  loss_giou_0: 1.161  loss_ce_dn_0: 0.6935  loss_mask_dn_0: 0.539  loss_dice_dn_0: 3.138  loss_bbox_dn_0: 0.3485  loss_giou_dn_0: 0.9254  loss_ce_1: 1.417  loss_mask_1: 0.1654  loss_dice_1: 0.9664  loss_bbox_1: 0.2211  loss_giou_1: 1.042  loss_ce_dn_1: 0.2732  loss_mask_dn_1: 0.1903  loss_dice_dn_1: 1.09  loss_bbox_dn_1: 0.213  loss_giou_dn_1: 0.7616  loss_ce_2: 1.304  loss_mask_2: 0.1756  loss_dice_2: 1.049  loss_bbox_2: 0.2364  loss_giou_2: 1.016  loss_ce_dn_2: 0.2118  loss_mask_dn_2: 0.1681  loss_dice_dn_2: 1.091  loss_bbox_dn_2: 0.1871  loss_giou_dn_2: 0.7316  loss_ce_3: 1.166  loss_mask_3: 0.1832  loss_dice_3: 0.9816  loss_bbox_3: 0.2289  loss_giou_3: 1.009  loss_ce_dn_3: 0.1801  loss_mask_dn_3: 0.1658  loss_dice_dn_3: 1.077  loss_bbox_dn_3: 0.1803  loss_giou_dn_3: 0.7194  loss_ce_4: 1.121  loss_mask_4: 0.1672  loss_dice_4: 0.9636  loss_bbox_4: 0.2184  loss_giou_4: 0.9975  loss_ce_dn_4: 0.1702  loss_mask_dn_4: 0.1571  loss_dice_dn_4: 1.05  loss_bbox_dn_4: 0.1789  loss_giou_dn_4: 0.7021  loss_ce_5: 1.054  loss_mask_5: 0.1676  loss_dice_5: 1.038  loss_bbox_5: 0.2241  loss_giou_5: 0.977  loss_ce_dn_5: 0.1567  loss_mask_dn_5: 0.1559  loss_dice_dn_5: 1.03  loss_bbox_dn_5: 0.173  loss_giou_dn_5: 0.6946  loss_ce_6: 0.9838  loss_mask_6: 0.1648  loss_dice_6: 1.014  loss_bbox_6: 0.2155  loss_giou_6: 0.9563  loss_ce_dn_6: 0.1547  loss_mask_dn_6: 0.1536  loss_dice_dn_6: 1.049  loss_bbox_dn_6: 0.1671  loss_giou_dn_6: 0.6962  loss_ce_7: 1.03  loss_mask_7: 0.1653  loss_dice_7: 0.9794  loss_bbox_7: 0.2224  loss_giou_7: 0.9538  loss_ce_dn_7: 0.1499  loss_mask_dn_7: 0.1575  loss_dice_dn_7: 1.058  loss_bbox_dn_7: 0.1641  loss_giou_dn_7: 0.6974  loss_ce_8: 0.9846  loss_mask_8: 0.1674  loss_dice_8: 1.147  loss_bbox_8: 0.2095  loss_giou_8: 0.9511  loss_ce_dn_8: 0.143  loss_mask_dn_8: 0.1573  loss_dice_dn_8: 1.06  loss_bbox_dn_8: 0.1638  loss_giou_dn_8: 0.6982  loss_ce_interm: 1.487  loss_mask_interm: 0.1624  loss_dice_interm: 1.033  loss_bbox_interm: 0.2811  loss_giou_interm: 1.165    time: 0.8741  last_time: 0.8848  data_time: 0.0153  last_data_time: 0.0161   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:37:03 d2.utils.events]: \u001b[0m eta: 3 days, 16:11:50  iter: 4639  total_loss: 68.65  loss_ce: 0.8091  loss_mask: 0.2759  loss_dice: 0.961  loss_bbox: 0.2712  loss_giou: 0.7374  loss_ce_dn: 0.1156  loss_mask_dn: 0.3026  loss_dice_dn: 0.9443  loss_bbox_dn: 0.2204  loss_giou_dn: 0.5839  loss_ce_0: 1.403  loss_mask_0: 0.2851  loss_dice_0: 0.9171  loss_bbox_0: 0.4476  loss_giou_0: 0.9133  loss_ce_dn_0: 0.7458  loss_mask_dn_0: 0.7576  loss_dice_dn_0: 3.209  loss_bbox_dn_0: 0.543  loss_giou_dn_0: 0.9288  loss_ce_1: 1.305  loss_mask_1: 0.2838  loss_dice_1: 0.9613  loss_bbox_1: 0.3155  loss_giou_1: 0.7893  loss_ce_dn_1: 0.2316  loss_mask_dn_1: 0.3185  loss_dice_dn_1: 1.137  loss_bbox_dn_1: 0.3266  loss_giou_dn_1: 0.699  loss_ce_2: 1.233  loss_mask_2: 0.2735  loss_dice_2: 0.9901  loss_bbox_2: 0.2872  loss_giou_2: 0.7628  loss_ce_dn_2: 0.1944  loss_mask_dn_2: 0.3206  loss_dice_dn_2: 1.047  loss_bbox_dn_2: 0.2659  loss_giou_dn_2: 0.6313  loss_ce_3: 1.035  loss_mask_3: 0.2837  loss_dice_3: 1.003  loss_bbox_3: 0.2928  loss_giou_3: 0.7465  loss_ce_dn_3: 0.1685  loss_mask_dn_3: 0.3061  loss_dice_dn_3: 1.001  loss_bbox_dn_3: 0.2524  loss_giou_dn_3: 0.6165  loss_ce_4: 0.986  loss_mask_4: 0.2766  loss_dice_4: 0.9532  loss_bbox_4: 0.2943  loss_giou_4: 0.7418  loss_ce_dn_4: 0.1486  loss_mask_dn_4: 0.3117  loss_dice_dn_4: 0.9641  loss_bbox_dn_4: 0.2426  loss_giou_dn_4: 0.5999  loss_ce_5: 0.9043  loss_mask_5: 0.2796  loss_dice_5: 0.9746  loss_bbox_5: 0.2908  loss_giou_5: 0.7433  loss_ce_dn_5: 0.136  loss_mask_dn_5: 0.3058  loss_dice_dn_5: 0.9599  loss_bbox_dn_5: 0.2344  loss_giou_dn_5: 0.5954  loss_ce_6: 0.8492  loss_mask_6: 0.2673  loss_dice_6: 0.947  loss_bbox_6: 0.2865  loss_giou_6: 0.7417  loss_ce_dn_6: 0.1293  loss_mask_dn_6: 0.3094  loss_dice_dn_6: 0.9497  loss_bbox_dn_6: 0.2267  loss_giou_dn_6: 0.5883  loss_ce_7: 0.7999  loss_mask_7: 0.285  loss_dice_7: 0.9725  loss_bbox_7: 0.27  loss_giou_7: 0.7439  loss_ce_dn_7: 0.1242  loss_mask_dn_7: 0.306  loss_dice_dn_7: 0.9869  loss_bbox_dn_7: 0.2229  loss_giou_dn_7: 0.5855  loss_ce_8: 0.7884  loss_mask_8: 0.2743  loss_dice_8: 0.9873  loss_bbox_8: 0.2699  loss_giou_8: 0.7405  loss_ce_dn_8: 0.1193  loss_mask_dn_8: 0.3036  loss_dice_dn_8: 0.9557  loss_bbox_dn_8: 0.2208  loss_giou_dn_8: 0.5835  loss_ce_interm: 1.403  loss_mask_interm: 0.2962  loss_dice_interm: 0.944  loss_bbox_interm: 0.4243  loss_giou_interm: 0.9088    time: 0.8742  last_time: 0.8561  data_time: 0.0132  last_data_time: 0.0073   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:37:20 d2.utils.events]: \u001b[0m eta: 3 days, 16:11:45  iter: 4659  total_loss: 59.63  loss_ce: 0.6931  loss_mask: 0.3176  loss_dice: 0.7376  loss_bbox: 0.3035  loss_giou: 0.6492  loss_ce_dn: 0.1176  loss_mask_dn: 0.3439  loss_dice_dn: 0.7086  loss_bbox_dn: 0.2791  loss_giou_dn: 0.5378  loss_ce_0: 1.401  loss_mask_0: 0.3558  loss_dice_0: 0.8398  loss_bbox_0: 0.4855  loss_giou_0: 0.8666  loss_ce_dn_0: 0.7385  loss_mask_dn_0: 1.007  loss_dice_dn_0: 2.97  loss_bbox_dn_0: 0.6584  loss_giou_dn_0: 0.9305  loss_ce_1: 1.327  loss_mask_1: 0.3547  loss_dice_1: 0.8172  loss_bbox_1: 0.3769  loss_giou_1: 0.7382  loss_ce_dn_1: 0.2598  loss_mask_dn_1: 0.4214  loss_dice_dn_1: 0.8702  loss_bbox_dn_1: 0.4103  loss_giou_dn_1: 0.6965  loss_ce_2: 1.135  loss_mask_2: 0.3336  loss_dice_2: 0.7769  loss_bbox_2: 0.353  loss_giou_2: 0.6984  loss_ce_dn_2: 0.2038  loss_mask_dn_2: 0.3698  loss_dice_dn_2: 0.7782  loss_bbox_dn_2: 0.347  loss_giou_dn_2: 0.6218  loss_ce_3: 0.976  loss_mask_3: 0.3399  loss_dice_3: 0.7741  loss_bbox_3: 0.3369  loss_giou_3: 0.7013  loss_ce_dn_3: 0.1753  loss_mask_dn_3: 0.3667  loss_dice_dn_3: 0.7896  loss_bbox_dn_3: 0.3244  loss_giou_dn_3: 0.5866  loss_ce_4: 0.864  loss_mask_4: 0.3185  loss_dice_4: 0.733  loss_bbox_4: 0.3013  loss_giou_4: 0.6828  loss_ce_dn_4: 0.1521  loss_mask_dn_4: 0.3631  loss_dice_dn_4: 0.7725  loss_bbox_dn_4: 0.3056  loss_giou_dn_4: 0.5673  loss_ce_5: 0.7439  loss_mask_5: 0.3371  loss_dice_5: 0.7387  loss_bbox_5: 0.3192  loss_giou_5: 0.6719  loss_ce_dn_5: 0.1423  loss_mask_dn_5: 0.3611  loss_dice_dn_5: 0.7424  loss_bbox_dn_5: 0.293  loss_giou_dn_5: 0.5602  loss_ce_6: 0.6887  loss_mask_6: 0.326  loss_dice_6: 0.7176  loss_bbox_6: 0.3063  loss_giou_6: 0.6458  loss_ce_dn_6: 0.132  loss_mask_dn_6: 0.3498  loss_dice_dn_6: 0.7493  loss_bbox_dn_6: 0.285  loss_giou_dn_6: 0.5482  loss_ce_7: 0.6932  loss_mask_7: 0.3146  loss_dice_7: 0.7127  loss_bbox_7: 0.2993  loss_giou_7: 0.6496  loss_ce_dn_7: 0.1185  loss_mask_dn_7: 0.3418  loss_dice_dn_7: 0.7321  loss_bbox_dn_7: 0.2818  loss_giou_dn_7: 0.5405  loss_ce_8: 0.7039  loss_mask_8: 0.3094  loss_dice_8: 0.701  loss_bbox_8: 0.3044  loss_giou_8: 0.6423  loss_ce_dn_8: 0.1203  loss_mask_dn_8: 0.3425  loss_dice_dn_8: 0.7246  loss_bbox_dn_8: 0.2791  loss_giou_dn_8: 0.5394  loss_ce_interm: 1.413  loss_mask_interm: 0.3595  loss_dice_interm: 0.8604  loss_bbox_interm: 0.4858  loss_giou_interm: 0.8661    time: 0.8741  last_time: 0.8525  data_time: 0.0114  last_data_time: 0.0083   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:37:38 d2.utils.events]: \u001b[0m eta: 3 days, 16:12:03  iter: 4679  total_loss: 59.24  loss_ce: 0.7493  loss_mask: 0.25  loss_dice: 0.8493  loss_bbox: 0.2681  loss_giou: 0.6831  loss_ce_dn: 0.1259  loss_mask_dn: 0.279  loss_dice_dn: 0.9052  loss_bbox_dn: 0.2325  loss_giou_dn: 0.5152  loss_ce_0: 1.39  loss_mask_0: 0.2945  loss_dice_0: 0.8675  loss_bbox_0: 0.3894  loss_giou_0: 0.8748  loss_ce_dn_0: 0.7229  loss_mask_dn_0: 0.94  loss_dice_dn_0: 2.723  loss_bbox_dn_0: 0.6852  loss_giou_dn_0: 0.9674  loss_ce_1: 1.326  loss_mask_1: 0.2849  loss_dice_1: 0.8341  loss_bbox_1: 0.3072  loss_giou_1: 0.7355  loss_ce_dn_1: 0.2544  loss_mask_dn_1: 0.3594  loss_dice_dn_1: 1.032  loss_bbox_dn_1: 0.3444  loss_giou_dn_1: 0.6644  loss_ce_2: 1.113  loss_mask_2: 0.267  loss_dice_2: 0.8702  loss_bbox_2: 0.2869  loss_giou_2: 0.7007  loss_ce_dn_2: 0.1945  loss_mask_dn_2: 0.3215  loss_dice_dn_2: 0.9763  loss_bbox_dn_2: 0.2845  loss_giou_dn_2: 0.6005  loss_ce_3: 0.9602  loss_mask_3: 0.2532  loss_dice_3: 0.881  loss_bbox_3: 0.2777  loss_giou_3: 0.6754  loss_ce_dn_3: 0.1709  loss_mask_dn_3: 0.303  loss_dice_dn_3: 0.949  loss_bbox_dn_3: 0.2538  loss_giou_dn_3: 0.568  loss_ce_4: 0.876  loss_mask_4: 0.2542  loss_dice_4: 0.8682  loss_bbox_4: 0.2712  loss_giou_4: 0.6588  loss_ce_dn_4: 0.1533  loss_mask_dn_4: 0.2931  loss_dice_dn_4: 0.9266  loss_bbox_dn_4: 0.2399  loss_giou_dn_4: 0.5376  loss_ce_5: 0.8441  loss_mask_5: 0.2599  loss_dice_5: 0.8784  loss_bbox_5: 0.2699  loss_giou_5: 0.6785  loss_ce_dn_5: 0.1488  loss_mask_dn_5: 0.2866  loss_dice_dn_5: 0.9075  loss_bbox_dn_5: 0.2337  loss_giou_dn_5: 0.5319  loss_ce_6: 0.8015  loss_mask_6: 0.2584  loss_dice_6: 0.8334  loss_bbox_6: 0.2697  loss_giou_6: 0.6917  loss_ce_dn_6: 0.1354  loss_mask_dn_6: 0.2881  loss_dice_dn_6: 0.9124  loss_bbox_dn_6: 0.2295  loss_giou_dn_6: 0.5215  loss_ce_7: 0.7305  loss_mask_7: 0.2617  loss_dice_7: 0.7979  loss_bbox_7: 0.2709  loss_giou_7: 0.6871  loss_ce_dn_7: 0.1294  loss_mask_dn_7: 0.2867  loss_dice_dn_7: 0.9108  loss_bbox_dn_7: 0.2306  loss_giou_dn_7: 0.5163  loss_ce_8: 0.7482  loss_mask_8: 0.2615  loss_dice_8: 0.8416  loss_bbox_8: 0.2693  loss_giou_8: 0.6782  loss_ce_dn_8: 0.1251  loss_mask_dn_8: 0.2817  loss_dice_dn_8: 0.9084  loss_bbox_dn_8: 0.2334  loss_giou_dn_8: 0.5146  loss_ce_interm: 1.4  loss_mask_interm: 0.2903  loss_dice_interm: 0.8729  loss_bbox_interm: 0.3894  loss_giou_interm: 0.8885    time: 0.8741  last_time: 0.8728  data_time: 0.0113  last_data_time: 0.0087   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:37:55 d2.utils.events]: \u001b[0m eta: 3 days, 16:11:45  iter: 4699  total_loss: 58.44  loss_ce: 0.7334  loss_mask: 0.2819  loss_dice: 0.608  loss_bbox: 0.2465  loss_giou: 0.7785  loss_ce_dn: 0.1469  loss_mask_dn: 0.302  loss_dice_dn: 0.6074  loss_bbox_dn: 0.2524  loss_giou_dn: 0.7252  loss_ce_0: 1.488  loss_mask_0: 0.3259  loss_dice_0: 0.6947  loss_bbox_0: 0.4437  loss_giou_0: 0.9665  loss_ce_dn_0: 0.7214  loss_mask_dn_0: 0.8415  loss_dice_dn_0: 2.984  loss_bbox_dn_0: 0.7305  loss_giou_dn_0: 1.062  loss_ce_1: 1.374  loss_mask_1: 0.322  loss_dice_1: 0.6505  loss_bbox_1: 0.3104  loss_giou_1: 0.8611  loss_ce_dn_1: 0.2723  loss_mask_dn_1: 0.3528  loss_dice_dn_1: 0.6811  loss_bbox_dn_1: 0.4037  loss_giou_dn_1: 0.8526  loss_ce_2: 1.195  loss_mask_2: 0.3178  loss_dice_2: 0.5916  loss_bbox_2: 0.2714  loss_giou_2: 0.812  loss_ce_dn_2: 0.2216  loss_mask_dn_2: 0.3254  loss_dice_dn_2: 0.6164  loss_bbox_dn_2: 0.3332  loss_giou_dn_2: 0.7912  loss_ce_3: 1.045  loss_mask_3: 0.289  loss_dice_3: 0.5924  loss_bbox_3: 0.2722  loss_giou_3: 0.8067  loss_ce_dn_3: 0.1924  loss_mask_dn_3: 0.3195  loss_dice_dn_3: 0.6017  loss_bbox_dn_3: 0.3115  loss_giou_dn_3: 0.7639  loss_ce_4: 0.8497  loss_mask_4: 0.3039  loss_dice_4: 0.592  loss_bbox_4: 0.2563  loss_giou_4: 0.7921  loss_ce_dn_4: 0.1801  loss_mask_dn_4: 0.3164  loss_dice_dn_4: 0.5938  loss_bbox_dn_4: 0.2901  loss_giou_dn_4: 0.7445  loss_ce_5: 0.7746  loss_mask_5: 0.2923  loss_dice_5: 0.6462  loss_bbox_5: 0.2541  loss_giou_5: 0.7768  loss_ce_dn_5: 0.1678  loss_mask_dn_5: 0.3047  loss_dice_dn_5: 0.5787  loss_bbox_dn_5: 0.2811  loss_giou_dn_5: 0.7393  loss_ce_6: 0.7758  loss_mask_6: 0.2906  loss_dice_6: 0.5813  loss_bbox_6: 0.2522  loss_giou_6: 0.7822  loss_ce_dn_6: 0.1606  loss_mask_dn_6: 0.3033  loss_dice_dn_6: 0.5832  loss_bbox_dn_6: 0.263  loss_giou_dn_6: 0.7289  loss_ce_7: 0.7616  loss_mask_7: 0.287  loss_dice_7: 0.5732  loss_bbox_7: 0.2506  loss_giou_7: 0.7852  loss_ce_dn_7: 0.152  loss_mask_dn_7: 0.3017  loss_dice_dn_7: 0.6115  loss_bbox_dn_7: 0.2522  loss_giou_dn_7: 0.7283  loss_ce_8: 0.7427  loss_mask_8: 0.2921  loss_dice_8: 0.6232  loss_bbox_8: 0.2464  loss_giou_8: 0.7794  loss_ce_dn_8: 0.146  loss_mask_dn_8: 0.3028  loss_dice_dn_8: 0.5929  loss_bbox_dn_8: 0.2529  loss_giou_dn_8: 0.7265  loss_ce_interm: 1.488  loss_mask_interm: 0.3248  loss_dice_interm: 0.6813  loss_bbox_interm: 0.4181  loss_giou_interm: 0.9665    time: 0.8741  last_time: 0.8506  data_time: 0.0121  last_data_time: 0.0124   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:38:13 d2.utils.events]: \u001b[0m eta: 3 days, 16:10:18  iter: 4719  total_loss: 58.18  loss_ce: 0.7178  loss_mask: 0.2718  loss_dice: 0.5879  loss_bbox: 0.3076  loss_giou: 0.6365  loss_ce_dn: 0.1259  loss_mask_dn: 0.2738  loss_dice_dn: 0.6417  loss_bbox_dn: 0.2977  loss_giou_dn: 0.5025  loss_ce_0: 1.48  loss_mask_0: 0.2784  loss_dice_0: 0.5788  loss_bbox_0: 0.4626  loss_giou_0: 0.7998  loss_ce_dn_0: 0.7421  loss_mask_dn_0: 0.8838  loss_dice_dn_0: 2.452  loss_bbox_dn_0: 0.756  loss_giou_dn_0: 0.9431  loss_ce_1: 1.345  loss_mask_1: 0.28  loss_dice_1: 0.5859  loss_bbox_1: 0.298  loss_giou_1: 0.6995  loss_ce_dn_1: 0.2827  loss_mask_dn_1: 0.3191  loss_dice_dn_1: 0.7898  loss_bbox_dn_1: 0.4367  loss_giou_dn_1: 0.646  loss_ce_2: 1.16  loss_mask_2: 0.282  loss_dice_2: 0.5978  loss_bbox_2: 0.2937  loss_giou_2: 0.6426  loss_ce_dn_2: 0.2249  loss_mask_dn_2: 0.2909  loss_dice_dn_2: 0.6866  loss_bbox_dn_2: 0.3441  loss_giou_dn_2: 0.5673  loss_ce_3: 0.9345  loss_mask_3: 0.2846  loss_dice_3: 0.6124  loss_bbox_3: 0.2495  loss_giou_3: 0.6295  loss_ce_dn_3: 0.1717  loss_mask_dn_3: 0.2892  loss_dice_dn_3: 0.6566  loss_bbox_dn_3: 0.3323  loss_giou_dn_3: 0.5408  loss_ce_4: 0.8608  loss_mask_4: 0.276  loss_dice_4: 0.6019  loss_bbox_4: 0.2496  loss_giou_4: 0.649  loss_ce_dn_4: 0.1587  loss_mask_dn_4: 0.2803  loss_dice_dn_4: 0.6575  loss_bbox_dn_4: 0.3024  loss_giou_dn_4: 0.5211  loss_ce_5: 0.8092  loss_mask_5: 0.2763  loss_dice_5: 0.6076  loss_bbox_5: 0.2722  loss_giou_5: 0.6422  loss_ce_dn_5: 0.14  loss_mask_dn_5: 0.2786  loss_dice_dn_5: 0.6427  loss_bbox_dn_5: 0.3032  loss_giou_dn_5: 0.5109  loss_ce_6: 0.7645  loss_mask_6: 0.2776  loss_dice_6: 0.5884  loss_bbox_6: 0.3066  loss_giou_6: 0.6348  loss_ce_dn_6: 0.1349  loss_mask_dn_6: 0.276  loss_dice_dn_6: 0.6398  loss_bbox_dn_6: 0.2989  loss_giou_dn_6: 0.505  loss_ce_7: 0.7628  loss_mask_7: 0.2715  loss_dice_7: 0.5959  loss_bbox_7: 0.2833  loss_giou_7: 0.6259  loss_ce_dn_7: 0.1287  loss_mask_dn_7: 0.2755  loss_dice_dn_7: 0.6374  loss_bbox_dn_7: 0.2952  loss_giou_dn_7: 0.5043  loss_ce_8: 0.7151  loss_mask_8: 0.2682  loss_dice_8: 0.6036  loss_bbox_8: 0.2838  loss_giou_8: 0.636  loss_ce_dn_8: 0.1261  loss_mask_dn_8: 0.2777  loss_dice_dn_8: 0.6421  loss_bbox_dn_8: 0.2969  loss_giou_dn_8: 0.5037  loss_ce_interm: 1.48  loss_mask_interm: 0.2841  loss_dice_interm: 0.589  loss_bbox_interm: 0.4615  loss_giou_interm: 0.811    time: 0.8741  last_time: 0.8750  data_time: 0.0108  last_data_time: 0.0083   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:38:30 d2.utils.events]: \u001b[0m eta: 3 days, 16:10:31  iter: 4739  total_loss: 64.74  loss_ce: 0.7709  loss_mask: 0.2538  loss_dice: 0.6537  loss_bbox: 0.2159  loss_giou: 0.7689  loss_ce_dn: 0.1137  loss_mask_dn: 0.2548  loss_dice_dn: 0.6388  loss_bbox_dn: 0.2176  loss_giou_dn: 0.5763  loss_ce_0: 1.389  loss_mask_0: 0.2772  loss_dice_0: 0.6995  loss_bbox_0: 0.3379  loss_giou_0: 0.9481  loss_ce_dn_0: 0.676  loss_mask_dn_0: 0.9008  loss_dice_dn_0: 2.996  loss_bbox_dn_0: 0.6222  loss_giou_dn_0: 0.9369  loss_ce_1: 1.356  loss_mask_1: 0.2706  loss_dice_1: 0.6585  loss_bbox_1: 0.2554  loss_giou_1: 0.8293  loss_ce_dn_1: 0.2287  loss_mask_dn_1: 0.2891  loss_dice_dn_1: 0.7854  loss_bbox_dn_1: 0.3332  loss_giou_dn_1: 0.6903  loss_ce_2: 1.155  loss_mask_2: 0.2587  loss_dice_2: 0.6398  loss_bbox_2: 0.2537  loss_giou_2: 0.7952  loss_ce_dn_2: 0.1868  loss_mask_dn_2: 0.2561  loss_dice_dn_2: 0.7143  loss_bbox_dn_2: 0.2751  loss_giou_dn_2: 0.6234  loss_ce_3: 1.026  loss_mask_3: 0.2399  loss_dice_3: 0.6322  loss_bbox_3: 0.2387  loss_giou_3: 0.7814  loss_ce_dn_3: 0.1637  loss_mask_dn_3: 0.2617  loss_dice_dn_3: 0.6727  loss_bbox_dn_3: 0.238  loss_giou_dn_3: 0.5949  loss_ce_4: 0.9422  loss_mask_4: 0.247  loss_dice_4: 0.6335  loss_bbox_4: 0.1935  loss_giou_4: 0.7694  loss_ce_dn_4: 0.1485  loss_mask_dn_4: 0.2506  loss_dice_dn_4: 0.6487  loss_bbox_dn_4: 0.2278  loss_giou_dn_4: 0.5804  loss_ce_5: 0.8312  loss_mask_5: 0.2376  loss_dice_5: 0.6483  loss_bbox_5: 0.2014  loss_giou_5: 0.7939  loss_ce_dn_5: 0.1352  loss_mask_dn_5: 0.2487  loss_dice_dn_5: 0.6433  loss_bbox_dn_5: 0.2246  loss_giou_dn_5: 0.5821  loss_ce_6: 0.7829  loss_mask_6: 0.2496  loss_dice_6: 0.6495  loss_bbox_6: 0.2039  loss_giou_6: 0.7859  loss_ce_dn_6: 0.1213  loss_mask_dn_6: 0.2484  loss_dice_dn_6: 0.6442  loss_bbox_dn_6: 0.2189  loss_giou_dn_6: 0.5735  loss_ce_7: 0.7566  loss_mask_7: 0.2512  loss_dice_7: 0.6385  loss_bbox_7: 0.209  loss_giou_7: 0.783  loss_ce_dn_7: 0.1163  loss_mask_dn_7: 0.2515  loss_dice_dn_7: 0.64  loss_bbox_dn_7: 0.2137  loss_giou_dn_7: 0.5726  loss_ce_8: 0.7509  loss_mask_8: 0.2549  loss_dice_8: 0.6122  loss_bbox_8: 0.1993  loss_giou_8: 0.7663  loss_ce_dn_8: 0.114  loss_mask_dn_8: 0.2547  loss_dice_dn_8: 0.6327  loss_bbox_dn_8: 0.2189  loss_giou_dn_8: 0.5671  loss_ce_interm: 1.398  loss_mask_interm: 0.2789  loss_dice_interm: 0.7004  loss_bbox_interm: 0.3422  loss_giou_interm: 0.9439    time: 0.8741  last_time: 0.8856  data_time: 0.0126  last_data_time: 0.0163   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:38:48 d2.utils.events]: \u001b[0m eta: 3 days, 16:10:05  iter: 4759  total_loss: 65.71  loss_ce: 0.8072  loss_mask: 0.399  loss_dice: 0.801  loss_bbox: 0.2303  loss_giou: 0.7558  loss_ce_dn: 0.1445  loss_mask_dn: 0.3715  loss_dice_dn: 0.6697  loss_bbox_dn: 0.2545  loss_giou_dn: 0.6155  loss_ce_0: 1.554  loss_mask_0: 0.3734  loss_dice_0: 0.8076  loss_bbox_0: 0.3514  loss_giou_0: 0.9844  loss_ce_dn_0: 0.6966  loss_mask_dn_0: 0.8996  loss_dice_dn_0: 3.077  loss_bbox_dn_0: 0.5995  loss_giou_dn_0: 1.002  loss_ce_1: 1.416  loss_mask_1: 0.3999  loss_dice_1: 0.7673  loss_bbox_1: 0.2706  loss_giou_1: 0.8087  loss_ce_dn_1: 0.2903  loss_mask_dn_1: 0.4491  loss_dice_dn_1: 0.8541  loss_bbox_dn_1: 0.3756  loss_giou_dn_1: 0.7666  loss_ce_2: 1.223  loss_mask_2: 0.3856  loss_dice_2: 0.7665  loss_bbox_2: 0.244  loss_giou_2: 0.7856  loss_ce_dn_2: 0.2408  loss_mask_dn_2: 0.3917  loss_dice_dn_2: 0.7671  loss_bbox_dn_2: 0.3168  loss_giou_dn_2: 0.7016  loss_ce_3: 1.055  loss_mask_3: 0.3758  loss_dice_3: 0.8247  loss_bbox_3: 0.2302  loss_giou_3: 0.736  loss_ce_dn_3: 0.2062  loss_mask_dn_3: 0.3868  loss_dice_dn_3: 0.7421  loss_bbox_dn_3: 0.2871  loss_giou_dn_3: 0.6636  loss_ce_4: 0.9499  loss_mask_4: 0.3654  loss_dice_4: 0.8088  loss_bbox_4: 0.2339  loss_giou_4: 0.7853  loss_ce_dn_4: 0.191  loss_mask_dn_4: 0.3752  loss_dice_dn_4: 0.7019  loss_bbox_dn_4: 0.2698  loss_giou_dn_4: 0.6445  loss_ce_5: 0.8142  loss_mask_5: 0.3698  loss_dice_5: 0.7875  loss_bbox_5: 0.2367  loss_giou_5: 0.779  loss_ce_dn_5: 0.1732  loss_mask_dn_5: 0.376  loss_dice_dn_5: 0.7196  loss_bbox_dn_5: 0.2611  loss_giou_dn_5: 0.6345  loss_ce_6: 0.7766  loss_mask_6: 0.3912  loss_dice_6: 0.8142  loss_bbox_6: 0.2348  loss_giou_6: 0.7702  loss_ce_dn_6: 0.1632  loss_mask_dn_6: 0.3761  loss_dice_dn_6: 0.6946  loss_bbox_dn_6: 0.2615  loss_giou_dn_6: 0.6273  loss_ce_7: 0.7584  loss_mask_7: 0.3848  loss_dice_7: 0.7779  loss_bbox_7: 0.2352  loss_giou_7: 0.7581  loss_ce_dn_7: 0.1494  loss_mask_dn_7: 0.3769  loss_dice_dn_7: 0.6933  loss_bbox_dn_7: 0.2576  loss_giou_dn_7: 0.6206  loss_ce_8: 0.7941  loss_mask_8: 0.3918  loss_dice_8: 0.7831  loss_bbox_8: 0.2294  loss_giou_8: 0.7567  loss_ce_dn_8: 0.1355  loss_mask_dn_8: 0.3716  loss_dice_dn_8: 0.6724  loss_bbox_dn_8: 0.254  loss_giou_dn_8: 0.6154  loss_ce_interm: 1.54  loss_mask_interm: 0.3742  loss_dice_interm: 0.804  loss_bbox_interm: 0.3504  loss_giou_interm: 0.9588    time: 0.8741  last_time: 0.8540  data_time: 0.0129  last_data_time: 0.0120   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:39:06 d2.utils.events]: \u001b[0m eta: 3 days, 16:09:26  iter: 4779  total_loss: 63.56  loss_ce: 0.862  loss_mask: 0.1643  loss_dice: 0.8719  loss_bbox: 0.277  loss_giou: 0.6884  loss_ce_dn: 0.1443  loss_mask_dn: 0.2058  loss_dice_dn: 0.9006  loss_bbox_dn: 0.2236  loss_giou_dn: 0.5167  loss_ce_0: 1.402  loss_mask_0: 0.2087  loss_dice_0: 0.9911  loss_bbox_0: 0.3875  loss_giou_0: 0.8773  loss_ce_dn_0: 0.7137  loss_mask_dn_0: 0.7904  loss_dice_dn_0: 3.233  loss_bbox_dn_0: 0.5015  loss_giou_dn_0: 0.8812  loss_ce_1: 1.305  loss_mask_1: 0.1975  loss_dice_1: 1.017  loss_bbox_1: 0.3145  loss_giou_1: 0.7687  loss_ce_dn_1: 0.2402  loss_mask_dn_1: 0.2756  loss_dice_dn_1: 1.048  loss_bbox_dn_1: 0.3112  loss_giou_dn_1: 0.6293  loss_ce_2: 1.176  loss_mask_2: 0.2017  loss_dice_2: 1.007  loss_bbox_2: 0.2888  loss_giou_2: 0.7174  loss_ce_dn_2: 0.2012  loss_mask_dn_2: 0.2362  loss_dice_dn_2: 0.9866  loss_bbox_dn_2: 0.2665  loss_giou_dn_2: 0.5745  loss_ce_3: 1.073  loss_mask_3: 0.1968  loss_dice_3: 0.9563  loss_bbox_3: 0.2998  loss_giou_3: 0.7016  loss_ce_dn_3: 0.1753  loss_mask_dn_3: 0.2328  loss_dice_dn_3: 0.9108  loss_bbox_dn_3: 0.2419  loss_giou_dn_3: 0.5571  loss_ce_4: 0.976  loss_mask_4: 0.199  loss_dice_4: 0.9165  loss_bbox_4: 0.272  loss_giou_4: 0.6946  loss_ce_dn_4: 0.1681  loss_mask_dn_4: 0.2236  loss_dice_dn_4: 0.9098  loss_bbox_dn_4: 0.2272  loss_giou_dn_4: 0.5347  loss_ce_5: 0.9324  loss_mask_5: 0.1812  loss_dice_5: 0.9545  loss_bbox_5: 0.2904  loss_giou_5: 0.6837  loss_ce_dn_5: 0.1502  loss_mask_dn_5: 0.214  loss_dice_dn_5: 0.9035  loss_bbox_dn_5: 0.2228  loss_giou_dn_5: 0.5306  loss_ce_6: 0.8396  loss_mask_6: 0.1774  loss_dice_6: 0.9174  loss_bbox_6: 0.2841  loss_giou_6: 0.6735  loss_ce_dn_6: 0.1421  loss_mask_dn_6: 0.2146  loss_dice_dn_6: 0.8792  loss_bbox_dn_6: 0.2156  loss_giou_dn_6: 0.5195  loss_ce_7: 0.8813  loss_mask_7: 0.165  loss_dice_7: 0.9752  loss_bbox_7: 0.2804  loss_giou_7: 0.6741  loss_ce_dn_7: 0.1452  loss_mask_dn_7: 0.2103  loss_dice_dn_7: 0.8915  loss_bbox_dn_7: 0.2226  loss_giou_dn_7: 0.5221  loss_ce_8: 0.8303  loss_mask_8: 0.1633  loss_dice_8: 0.9647  loss_bbox_8: 0.266  loss_giou_8: 0.669  loss_ce_dn_8: 0.1433  loss_mask_dn_8: 0.2078  loss_dice_dn_8: 0.8913  loss_bbox_dn_8: 0.2196  loss_giou_dn_8: 0.5174  loss_ce_interm: 1.391  loss_mask_interm: 0.2093  loss_dice_interm: 0.9769  loss_bbox_interm: 0.3861  loss_giou_interm: 0.876    time: 0.8741  last_time: 0.8845  data_time: 0.0127  last_data_time: 0.0145   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:39:23 d2.utils.events]: \u001b[0m eta: 3 days, 16:09:38  iter: 4799  total_loss: 65.03  loss_ce: 0.8038  loss_mask: 0.3085  loss_dice: 0.8405  loss_bbox: 0.4013  loss_giou: 0.7163  loss_ce_dn: 0.1156  loss_mask_dn: 0.3002  loss_dice_dn: 0.8357  loss_bbox_dn: 0.3258  loss_giou_dn: 0.6258  loss_ce_0: 1.485  loss_mask_0: 0.3107  loss_dice_0: 0.8531  loss_bbox_0: 0.5048  loss_giou_0: 0.8984  loss_ce_dn_0: 0.692  loss_mask_dn_0: 0.8811  loss_dice_dn_0: 3.284  loss_bbox_dn_0: 0.6502  loss_giou_dn_0: 0.9573  loss_ce_1: 1.348  loss_mask_1: 0.3124  loss_dice_1: 0.8555  loss_bbox_1: 0.3729  loss_giou_1: 0.7929  loss_ce_dn_1: 0.2276  loss_mask_dn_1: 0.393  loss_dice_dn_1: 0.9451  loss_bbox_dn_1: 0.413  loss_giou_dn_1: 0.7229  loss_ce_2: 1.218  loss_mask_2: 0.2765  loss_dice_2: 0.8201  loss_bbox_2: 0.3629  loss_giou_2: 0.737  loss_ce_dn_2: 0.1922  loss_mask_dn_2: 0.3371  loss_dice_dn_2: 0.8982  loss_bbox_dn_2: 0.3591  loss_giou_dn_2: 0.6788  loss_ce_3: 1.077  loss_mask_3: 0.3206  loss_dice_3: 0.8376  loss_bbox_3: 0.401  loss_giou_3: 0.7283  loss_ce_dn_3: 0.1592  loss_mask_dn_3: 0.3246  loss_dice_dn_3: 0.8763  loss_bbox_dn_3: 0.3459  loss_giou_dn_3: 0.6586  loss_ce_4: 0.9585  loss_mask_4: 0.3141  loss_dice_4: 0.8584  loss_bbox_4: 0.3645  loss_giou_4: 0.7178  loss_ce_dn_4: 0.1496  loss_mask_dn_4: 0.3295  loss_dice_dn_4: 0.8508  loss_bbox_dn_4: 0.3365  loss_giou_dn_4: 0.647  loss_ce_5: 0.8723  loss_mask_5: 0.3137  loss_dice_5: 0.8492  loss_bbox_5: 0.3803  loss_giou_5: 0.7104  loss_ce_dn_5: 0.1393  loss_mask_dn_5: 0.3117  loss_dice_dn_5: 0.8575  loss_bbox_dn_5: 0.3322  loss_giou_dn_5: 0.643  loss_ce_6: 0.8472  loss_mask_6: 0.3228  loss_dice_6: 0.8433  loss_bbox_6: 0.3568  loss_giou_6: 0.7125  loss_ce_dn_6: 0.1271  loss_mask_dn_6: 0.3158  loss_dice_dn_6: 0.8529  loss_bbox_dn_6: 0.3281  loss_giou_dn_6: 0.6328  loss_ce_7: 0.8443  loss_mask_7: 0.3238  loss_dice_7: 0.8644  loss_bbox_7: 0.3587  loss_giou_7: 0.704  loss_ce_dn_7: 0.1253  loss_mask_dn_7: 0.3105  loss_dice_dn_7: 0.8383  loss_bbox_dn_7: 0.3286  loss_giou_dn_7: 0.6305  loss_ce_8: 0.7887  loss_mask_8: 0.3177  loss_dice_8: 0.8381  loss_bbox_8: 0.374  loss_giou_8: 0.6929  loss_ce_dn_8: 0.1188  loss_mask_dn_8: 0.3071  loss_dice_dn_8: 0.8408  loss_bbox_dn_8: 0.3255  loss_giou_dn_8: 0.6265  loss_ce_interm: 1.491  loss_mask_interm: 0.3126  loss_dice_interm: 0.8599  loss_bbox_interm: 0.5062  loss_giou_interm: 0.8967    time: 0.8741  last_time: 0.8750  data_time: 0.0126  last_data_time: 0.0090   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:39:41 d2.utils.events]: \u001b[0m eta: 3 days, 16:09:26  iter: 4819  total_loss: 61.46  loss_ce: 0.7515  loss_mask: 0.2043  loss_dice: 0.7208  loss_bbox: 0.2735  loss_giou: 0.6837  loss_ce_dn: 0.1243  loss_mask_dn: 0.2024  loss_dice_dn: 0.7245  loss_bbox_dn: 0.2254  loss_giou_dn: 0.5343  loss_ce_0: 1.402  loss_mask_0: 0.2213  loss_dice_0: 0.7756  loss_bbox_0: 0.4133  loss_giou_0: 0.8742  loss_ce_dn_0: 0.7096  loss_mask_dn_0: 0.8129  loss_dice_dn_0: 2.927  loss_bbox_dn_0: 0.5965  loss_giou_dn_0: 0.9068  loss_ce_1: 1.379  loss_mask_1: 0.226  loss_dice_1: 0.7729  loss_bbox_1: 0.3183  loss_giou_1: 0.8057  loss_ce_dn_1: 0.2447  loss_mask_dn_1: 0.2641  loss_dice_dn_1: 0.9047  loss_bbox_dn_1: 0.3471  loss_giou_dn_1: 0.6617  loss_ce_2: 1.213  loss_mask_2: 0.2237  loss_dice_2: 0.7743  loss_bbox_2: 0.2964  loss_giou_2: 0.7653  loss_ce_dn_2: 0.2066  loss_mask_dn_2: 0.2266  loss_dice_dn_2: 0.8083  loss_bbox_dn_2: 0.2889  loss_giou_dn_2: 0.5979  loss_ce_3: 0.9711  loss_mask_3: 0.2187  loss_dice_3: 0.7415  loss_bbox_3: 0.279  loss_giou_3: 0.7427  loss_ce_dn_3: 0.1847  loss_mask_dn_3: 0.2197  loss_dice_dn_3: 0.7319  loss_bbox_dn_3: 0.2641  loss_giou_dn_3: 0.5658  loss_ce_4: 0.8692  loss_mask_4: 0.2146  loss_dice_4: 0.7613  loss_bbox_4: 0.2878  loss_giou_4: 0.7571  loss_ce_dn_4: 0.1684  loss_mask_dn_4: 0.2165  loss_dice_dn_4: 0.7335  loss_bbox_dn_4: 0.2397  loss_giou_dn_4: 0.5458  loss_ce_5: 0.7843  loss_mask_5: 0.2119  loss_dice_5: 0.7889  loss_bbox_5: 0.2978  loss_giou_5: 0.7475  loss_ce_dn_5: 0.1453  loss_mask_dn_5: 0.2174  loss_dice_dn_5: 0.736  loss_bbox_dn_5: 0.2329  loss_giou_dn_5: 0.5416  loss_ce_6: 0.7309  loss_mask_6: 0.2131  loss_dice_6: 0.7403  loss_bbox_6: 0.2725  loss_giou_6: 0.7125  loss_ce_dn_6: 0.1419  loss_mask_dn_6: 0.2098  loss_dice_dn_6: 0.7318  loss_bbox_dn_6: 0.2309  loss_giou_dn_6: 0.5357  loss_ce_7: 0.7452  loss_mask_7: 0.2098  loss_dice_7: 0.7562  loss_bbox_7: 0.2677  loss_giou_7: 0.7206  loss_ce_dn_7: 0.1322  loss_mask_dn_7: 0.2052  loss_dice_dn_7: 0.7399  loss_bbox_dn_7: 0.2262  loss_giou_dn_7: 0.5332  loss_ce_8: 0.7391  loss_mask_8: 0.2112  loss_dice_8: 0.737  loss_bbox_8: 0.2759  loss_giou_8: 0.713  loss_ce_dn_8: 0.1296  loss_mask_dn_8: 0.2064  loss_dice_dn_8: 0.7087  loss_bbox_dn_8: 0.2246  loss_giou_dn_8: 0.5324  loss_ce_interm: 1.454  loss_mask_interm: 0.216  loss_dice_interm: 0.7764  loss_bbox_interm: 0.4078  loss_giou_interm: 0.8741    time: 0.8741  last_time: 0.8555  data_time: 0.0122  last_data_time: 0.0129   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:39:58 d2.utils.events]: \u001b[0m eta: 3 days, 16:06:35  iter: 4839  total_loss: 58.03  loss_ce: 0.6831  loss_mask: 0.2602  loss_dice: 0.534  loss_bbox: 0.2431  loss_giou: 0.7667  loss_ce_dn: 0.1378  loss_mask_dn: 0.2589  loss_dice_dn: 0.5669  loss_bbox_dn: 0.2158  loss_giou_dn: 0.6667  loss_ce_0: 1.485  loss_mask_0: 0.3204  loss_dice_0: 0.6314  loss_bbox_0: 0.3886  loss_giou_0: 0.9617  loss_ce_dn_0: 0.7384  loss_mask_dn_0: 0.868  loss_dice_dn_0: 3.026  loss_bbox_dn_0: 0.5964  loss_giou_dn_0: 1.019  loss_ce_1: 1.423  loss_mask_1: 0.3033  loss_dice_1: 0.6552  loss_bbox_1: 0.3212  loss_giou_1: 0.8517  loss_ce_dn_1: 0.3102  loss_mask_dn_1: 0.3378  loss_dice_dn_1: 0.743  loss_bbox_dn_1: 0.3427  loss_giou_dn_1: 0.7836  loss_ce_2: 1.221  loss_mask_2: 0.2798  loss_dice_2: 0.5954  loss_bbox_2: 0.2789  loss_giou_2: 0.8583  loss_ce_dn_2: 0.2517  loss_mask_dn_2: 0.3061  loss_dice_dn_2: 0.6648  loss_bbox_dn_2: 0.268  loss_giou_dn_2: 0.7278  loss_ce_3: 1.013  loss_mask_3: 0.2683  loss_dice_3: 0.5673  loss_bbox_3: 0.2676  loss_giou_3: 0.7994  loss_ce_dn_3: 0.2053  loss_mask_dn_3: 0.2779  loss_dice_dn_3: 0.6074  loss_bbox_dn_3: 0.2565  loss_giou_dn_3: 0.7014  loss_ce_4: 0.9169  loss_mask_4: 0.2646  loss_dice_4: 0.5451  loss_bbox_4: 0.2657  loss_giou_4: 0.7788  loss_ce_dn_4: 0.1798  loss_mask_dn_4: 0.2695  loss_dice_dn_4: 0.5906  loss_bbox_dn_4: 0.2373  loss_giou_dn_4: 0.6897  loss_ce_5: 0.8161  loss_mask_5: 0.264  loss_dice_5: 0.591  loss_bbox_5: 0.266  loss_giou_5: 0.7755  loss_ce_dn_5: 0.1502  loss_mask_dn_5: 0.2645  loss_dice_dn_5: 0.586  loss_bbox_dn_5: 0.2245  loss_giou_dn_5: 0.6791  loss_ce_6: 0.7224  loss_mask_6: 0.2663  loss_dice_6: 0.5562  loss_bbox_6: 0.2537  loss_giou_6: 0.7763  loss_ce_dn_6: 0.1497  loss_mask_dn_6: 0.2631  loss_dice_dn_6: 0.5837  loss_bbox_dn_6: 0.2254  loss_giou_dn_6: 0.6724  loss_ce_7: 0.6856  loss_mask_7: 0.2641  loss_dice_7: 0.5588  loss_bbox_7: 0.2517  loss_giou_7: 0.7717  loss_ce_dn_7: 0.1424  loss_mask_dn_7: 0.2608  loss_dice_dn_7: 0.5738  loss_bbox_dn_7: 0.2196  loss_giou_dn_7: 0.6705  loss_ce_8: 0.7115  loss_mask_8: 0.2627  loss_dice_8: 0.5688  loss_bbox_8: 0.2391  loss_giou_8: 0.7633  loss_ce_dn_8: 0.1394  loss_mask_dn_8: 0.2596  loss_dice_dn_8: 0.5604  loss_bbox_dn_8: 0.2175  loss_giou_dn_8: 0.6661  loss_ce_interm: 1.51  loss_mask_interm: 0.3306  loss_dice_interm: 0.6298  loss_bbox_interm: 0.3891  loss_giou_interm: 0.9506    time: 0.8741  last_time: 0.8610  data_time: 0.0114  last_data_time: 0.0078   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:40:16 d2.utils.events]: \u001b[0m eta: 3 days, 16:07:18  iter: 4859  total_loss: 60.44  loss_ce: 0.7874  loss_mask: 0.2919  loss_dice: 0.8257  loss_bbox: 0.2658  loss_giou: 0.7037  loss_ce_dn: 0.1295  loss_mask_dn: 0.3138  loss_dice_dn: 0.8368  loss_bbox_dn: 0.2724  loss_giou_dn: 0.5629  loss_ce_0: 1.496  loss_mask_0: 0.2645  loss_dice_0: 0.93  loss_bbox_0: 0.3962  loss_giou_0: 0.9142  loss_ce_dn_0: 0.6989  loss_mask_dn_0: 0.7767  loss_dice_dn_0: 2.947  loss_bbox_dn_0: 0.5886  loss_giou_dn_0: 0.9128  loss_ce_1: 1.372  loss_mask_1: 0.2826  loss_dice_1: 0.8534  loss_bbox_1: 0.2947  loss_giou_1: 0.8146  loss_ce_dn_1: 0.2646  loss_mask_dn_1: 0.3676  loss_dice_dn_1: 0.9717  loss_bbox_dn_1: 0.357  loss_giou_dn_1: 0.6934  loss_ce_2: 1.214  loss_mask_2: 0.2896  loss_dice_2: 0.839  loss_bbox_2: 0.2698  loss_giou_2: 0.7669  loss_ce_dn_2: 0.2129  loss_mask_dn_2: 0.3474  loss_dice_dn_2: 0.8612  loss_bbox_dn_2: 0.3066  loss_giou_dn_2: 0.6202  loss_ce_3: 1.095  loss_mask_3: 0.2875  loss_dice_3: 0.8055  loss_bbox_3: 0.2779  loss_giou_3: 0.7447  loss_ce_dn_3: 0.1833  loss_mask_dn_3: 0.3253  loss_dice_dn_3: 0.892  loss_bbox_dn_3: 0.2877  loss_giou_dn_3: 0.5974  loss_ce_4: 0.9599  loss_mask_4: 0.2872  loss_dice_4: 0.8306  loss_bbox_4: 0.2606  loss_giou_4: 0.7354  loss_ce_dn_4: 0.1737  loss_mask_dn_4: 0.3314  loss_dice_dn_4: 0.8852  loss_bbox_dn_4: 0.2844  loss_giou_dn_4: 0.5864  loss_ce_5: 0.9042  loss_mask_5: 0.2906  loss_dice_5: 0.8036  loss_bbox_5: 0.263  loss_giou_5: 0.7395  loss_ce_dn_5: 0.1589  loss_mask_dn_5: 0.3285  loss_dice_dn_5: 0.8812  loss_bbox_dn_5: 0.2816  loss_giou_dn_5: 0.5783  loss_ce_6: 0.806  loss_mask_6: 0.2835  loss_dice_6: 0.814  loss_bbox_6: 0.2739  loss_giou_6: 0.7244  loss_ce_dn_6: 0.1453  loss_mask_dn_6: 0.3223  loss_dice_dn_6: 0.835  loss_bbox_dn_6: 0.2802  loss_giou_dn_6: 0.5665  loss_ce_7: 0.8161  loss_mask_7: 0.3024  loss_dice_7: 0.8141  loss_bbox_7: 0.2677  loss_giou_7: 0.7238  loss_ce_dn_7: 0.1302  loss_mask_dn_7: 0.3214  loss_dice_dn_7: 0.8274  loss_bbox_dn_7: 0.2736  loss_giou_dn_7: 0.5608  loss_ce_8: 0.7739  loss_mask_8: 0.2925  loss_dice_8: 0.7768  loss_bbox_8: 0.2652  loss_giou_8: 0.6966  loss_ce_dn_8: 0.1312  loss_mask_dn_8: 0.3113  loss_dice_dn_8: 0.7939  loss_bbox_dn_8: 0.273  loss_giou_dn_8: 0.5611  loss_ce_interm: 1.479  loss_mask_interm: 0.2637  loss_dice_interm: 0.8736  loss_bbox_interm: 0.4038  loss_giou_interm: 0.9206    time: 0.8741  last_time: 0.8904  data_time: 0.0154  last_data_time: 0.0148   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:40:33 d2.utils.events]: \u001b[0m eta: 3 days, 16:05:17  iter: 4879  total_loss: 64.14  loss_ce: 0.7213  loss_mask: 0.2684  loss_dice: 0.7193  loss_bbox: 0.2646  loss_giou: 0.7275  loss_ce_dn: 0.1212  loss_mask_dn: 0.285  loss_dice_dn: 0.7443  loss_bbox_dn: 0.2574  loss_giou_dn: 0.652  loss_ce_0: 1.51  loss_mask_0: 0.3203  loss_dice_0: 0.7814  loss_bbox_0: 0.4248  loss_giou_0: 0.9437  loss_ce_dn_0: 0.7208  loss_mask_dn_0: 1.056  loss_dice_dn_0: 3.226  loss_bbox_dn_0: 0.6669  loss_giou_dn_0: 1  loss_ce_1: 1.385  loss_mask_1: 0.2891  loss_dice_1: 0.7348  loss_bbox_1: 0.321  loss_giou_1: 0.8256  loss_ce_dn_1: 0.262  loss_mask_dn_1: 0.3614  loss_dice_dn_1: 0.8769  loss_bbox_dn_1: 0.3808  loss_giou_dn_1: 0.7911  loss_ce_2: 1.225  loss_mask_2: 0.2784  loss_dice_2: 0.6951  loss_bbox_2: 0.3331  loss_giou_2: 0.8261  loss_ce_dn_2: 0.184  loss_mask_dn_2: 0.3125  loss_dice_dn_2: 0.7509  loss_bbox_dn_2: 0.3136  loss_giou_dn_2: 0.712  loss_ce_3: 1.021  loss_mask_3: 0.2941  loss_dice_3: 0.7624  loss_bbox_3: 0.3082  loss_giou_3: 0.8229  loss_ce_dn_3: 0.1666  loss_mask_dn_3: 0.2933  loss_dice_dn_3: 0.751  loss_bbox_dn_3: 0.2868  loss_giou_dn_3: 0.691  loss_ce_4: 0.9326  loss_mask_4: 0.2705  loss_dice_4: 0.7375  loss_bbox_4: 0.3215  loss_giou_4: 0.7653  loss_ce_dn_4: 0.1587  loss_mask_dn_4: 0.2824  loss_dice_dn_4: 0.763  loss_bbox_dn_4: 0.2735  loss_giou_dn_4: 0.6613  loss_ce_5: 0.8136  loss_mask_5: 0.2842  loss_dice_5: 0.7353  loss_bbox_5: 0.3302  loss_giou_5: 0.7676  loss_ce_dn_5: 0.1438  loss_mask_dn_5: 0.2804  loss_dice_dn_5: 0.748  loss_bbox_dn_5: 0.2691  loss_giou_dn_5: 0.6626  loss_ce_6: 0.7741  loss_mask_6: 0.289  loss_dice_6: 0.7805  loss_bbox_6: 0.2867  loss_giou_6: 0.7549  loss_ce_dn_6: 0.1372  loss_mask_dn_6: 0.2777  loss_dice_dn_6: 0.7507  loss_bbox_dn_6: 0.2593  loss_giou_dn_6: 0.6504  loss_ce_7: 0.7726  loss_mask_7: 0.2674  loss_dice_7: 0.7556  loss_bbox_7: 0.2773  loss_giou_7: 0.7527  loss_ce_dn_7: 0.1248  loss_mask_dn_7: 0.2779  loss_dice_dn_7: 0.7539  loss_bbox_dn_7: 0.2578  loss_giou_dn_7: 0.6528  loss_ce_8: 0.7335  loss_mask_8: 0.2772  loss_dice_8: 0.7307  loss_bbox_8: 0.2659  loss_giou_8: 0.7324  loss_ce_dn_8: 0.1243  loss_mask_dn_8: 0.28  loss_dice_dn_8: 0.7454  loss_bbox_dn_8: 0.2584  loss_giou_dn_8: 0.651  loss_ce_interm: 1.522  loss_mask_interm: 0.3155  loss_dice_interm: 0.7663  loss_bbox_interm: 0.4219  loss_giou_interm: 0.9387    time: 0.8741  last_time: 0.8949  data_time: 0.0120  last_data_time: 0.0118   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:40:51 d2.utils.events]: \u001b[0m eta: 3 days, 16:05:43  iter: 4899  total_loss: 58.35  loss_ce: 0.7978  loss_mask: 0.2393  loss_dice: 0.6757  loss_bbox: 0.2184  loss_giou: 0.6023  loss_ce_dn: 0.1371  loss_mask_dn: 0.2452  loss_dice_dn: 0.6848  loss_bbox_dn: 0.2302  loss_giou_dn: 0.4939  loss_ce_0: 1.467  loss_mask_0: 0.2651  loss_dice_0: 0.7549  loss_bbox_0: 0.4259  loss_giou_0: 0.8574  loss_ce_dn_0: 0.7449  loss_mask_dn_0: 0.8867  loss_dice_dn_0: 3.01  loss_bbox_dn_0: 0.5641  loss_giou_dn_0: 0.9389  loss_ce_1: 1.39  loss_mask_1: 0.2537  loss_dice_1: 0.7558  loss_bbox_1: 0.3266  loss_giou_1: 0.7147  loss_ce_dn_1: 0.2436  loss_mask_dn_1: 0.2948  loss_dice_dn_1: 0.8208  loss_bbox_dn_1: 0.363  loss_giou_dn_1: 0.6398  loss_ce_2: 1.252  loss_mask_2: 0.2639  loss_dice_2: 0.7312  loss_bbox_2: 0.2716  loss_giou_2: 0.6896  loss_ce_dn_2: 0.1997  loss_mask_dn_2: 0.2952  loss_dice_dn_2: 0.7412  loss_bbox_dn_2: 0.2839  loss_giou_dn_2: 0.56  loss_ce_3: 1.119  loss_mask_3: 0.2489  loss_dice_3: 0.756  loss_bbox_3: 0.2471  loss_giou_3: 0.6644  loss_ce_dn_3: 0.1762  loss_mask_dn_3: 0.2514  loss_dice_dn_3: 0.7091  loss_bbox_dn_3: 0.2588  loss_giou_dn_3: 0.5347  loss_ce_4: 0.9789  loss_mask_4: 0.2454  loss_dice_4: 0.7416  loss_bbox_4: 0.2248  loss_giou_4: 0.5917  loss_ce_dn_4: 0.1576  loss_mask_dn_4: 0.2442  loss_dice_dn_4: 0.7079  loss_bbox_dn_4: 0.2461  loss_giou_dn_4: 0.5179  loss_ce_5: 0.9129  loss_mask_5: 0.2384  loss_dice_5: 0.7092  loss_bbox_5: 0.2282  loss_giou_5: 0.6127  loss_ce_dn_5: 0.1484  loss_mask_dn_5: 0.2494  loss_dice_dn_5: 0.707  loss_bbox_dn_5: 0.2414  loss_giou_dn_5: 0.513  loss_ce_6: 0.8297  loss_mask_6: 0.244  loss_dice_6: 0.7348  loss_bbox_6: 0.2224  loss_giou_6: 0.5964  loss_ce_dn_6: 0.1367  loss_mask_dn_6: 0.245  loss_dice_dn_6: 0.6835  loss_bbox_dn_6: 0.2344  loss_giou_dn_6: 0.5047  loss_ce_7: 0.8672  loss_mask_7: 0.2316  loss_dice_7: 0.7042  loss_bbox_7: 0.2187  loss_giou_7: 0.6325  loss_ce_dn_7: 0.136  loss_mask_dn_7: 0.2478  loss_dice_dn_7: 0.6943  loss_bbox_dn_7: 0.2344  loss_giou_dn_7: 0.5037  loss_ce_8: 0.8129  loss_mask_8: 0.2376  loss_dice_8: 0.6947  loss_bbox_8: 0.2167  loss_giou_8: 0.6163  loss_ce_dn_8: 0.1338  loss_mask_dn_8: 0.2448  loss_dice_dn_8: 0.7126  loss_bbox_dn_8: 0.2297  loss_giou_dn_8: 0.4953  loss_ce_interm: 1.44  loss_mask_interm: 0.2834  loss_dice_interm: 0.7623  loss_bbox_interm: 0.4259  loss_giou_interm: 0.885    time: 0.8741  last_time: 0.8777  data_time: 0.0119  last_data_time: 0.0120   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:41:08 d2.utils.events]: \u001b[0m eta: 3 days, 16:05:48  iter: 4919  total_loss: 63.32  loss_ce: 0.7548  loss_mask: 0.2602  loss_dice: 0.767  loss_bbox: 0.2581  loss_giou: 0.7121  loss_ce_dn: 0.1121  loss_mask_dn: 0.263  loss_dice_dn: 0.8009  loss_bbox_dn: 0.2526  loss_giou_dn: 0.523  loss_ce_0: 1.357  loss_mask_0: 0.2798  loss_dice_0: 0.9394  loss_bbox_0: 0.3872  loss_giou_0: 0.9411  loss_ce_dn_0: 0.6711  loss_mask_dn_0: 0.8139  loss_dice_dn_0: 2.929  loss_bbox_dn_0: 0.6056  loss_giou_dn_0: 0.9069  loss_ce_1: 1.261  loss_mask_1: 0.2885  loss_dice_1: 0.8409  loss_bbox_1: 0.2889  loss_giou_1: 0.8178  loss_ce_dn_1: 0.2176  loss_mask_dn_1: 0.3197  loss_dice_dn_1: 1.047  loss_bbox_dn_1: 0.3764  loss_giou_dn_1: 0.6643  loss_ce_2: 1.092  loss_mask_2: 0.2531  loss_dice_2: 0.8293  loss_bbox_2: 0.2782  loss_giou_2: 0.7695  loss_ce_dn_2: 0.1696  loss_mask_dn_2: 0.2767  loss_dice_dn_2: 0.9279  loss_bbox_dn_2: 0.3161  loss_giou_dn_2: 0.6005  loss_ce_3: 0.9261  loss_mask_3: 0.2497  loss_dice_3: 0.7765  loss_bbox_3: 0.2514  loss_giou_3: 0.7599  loss_ce_dn_3: 0.1582  loss_mask_dn_3: 0.2682  loss_dice_dn_3: 0.8321  loss_bbox_dn_3: 0.2891  loss_giou_dn_3: 0.5814  loss_ce_4: 0.8609  loss_mask_4: 0.2529  loss_dice_4: 0.7875  loss_bbox_4: 0.291  loss_giou_4: 0.7187  loss_ce_dn_4: 0.1429  loss_mask_dn_4: 0.2704  loss_dice_dn_4: 0.8178  loss_bbox_dn_4: 0.271  loss_giou_dn_4: 0.5488  loss_ce_5: 0.8024  loss_mask_5: 0.2541  loss_dice_5: 0.8361  loss_bbox_5: 0.2931  loss_giou_5: 0.729  loss_ce_dn_5: 0.1306  loss_mask_dn_5: 0.2673  loss_dice_dn_5: 0.8104  loss_bbox_dn_5: 0.2617  loss_giou_dn_5: 0.5331  loss_ce_6: 0.7525  loss_mask_6: 0.2628  loss_dice_6: 0.8073  loss_bbox_6: 0.2882  loss_giou_6: 0.7262  loss_ce_dn_6: 0.1287  loss_mask_dn_6: 0.2644  loss_dice_dn_6: 0.8062  loss_bbox_dn_6: 0.2578  loss_giou_dn_6: 0.5223  loss_ce_7: 0.7186  loss_mask_7: 0.2672  loss_dice_7: 0.8278  loss_bbox_7: 0.2876  loss_giou_7: 0.7185  loss_ce_dn_7: 0.125  loss_mask_dn_7: 0.2656  loss_dice_dn_7: 0.7975  loss_bbox_dn_7: 0.2567  loss_giou_dn_7: 0.5236  loss_ce_8: 0.7312  loss_mask_8: 0.2654  loss_dice_8: 0.8058  loss_bbox_8: 0.2873  loss_giou_8: 0.7088  loss_ce_dn_8: 0.1254  loss_mask_dn_8: 0.2636  loss_dice_dn_8: 0.8032  loss_bbox_dn_8: 0.2528  loss_giou_dn_8: 0.5221  loss_ce_interm: 1.367  loss_mask_interm: 0.2929  loss_dice_interm: 0.8636  loss_bbox_interm: 0.3872  loss_giou_interm: 0.946    time: 0.8741  last_time: 0.8454  data_time: 0.0124  last_data_time: 0.0104   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:41:26 d2.utils.events]: \u001b[0m eta: 3 days, 16:05:17  iter: 4939  total_loss: 59.44  loss_ce: 0.7098  loss_mask: 0.2271  loss_dice: 0.8403  loss_bbox: 0.3327  loss_giou: 0.6006  loss_ce_dn: 0.1047  loss_mask_dn: 0.2161  loss_dice_dn: 0.801  loss_bbox_dn: 0.2181  loss_giou_dn: 0.4938  loss_ce_0: 1.371  loss_mask_0: 0.2403  loss_dice_0: 0.953  loss_bbox_0: 0.4418  loss_giou_0: 0.8471  loss_ce_dn_0: 0.7999  loss_mask_dn_0: 0.7158  loss_dice_dn_0: 2.924  loss_bbox_dn_0: 0.6274  loss_giou_dn_0: 0.902  loss_ce_1: 1.286  loss_mask_1: 0.2461  loss_dice_1: 0.9436  loss_bbox_1: 0.3498  loss_giou_1: 0.6573  loss_ce_dn_1: 0.263  loss_mask_dn_1: 0.2673  loss_dice_dn_1: 0.9377  loss_bbox_dn_1: 0.36  loss_giou_dn_1: 0.6407  loss_ce_2: 1.164  loss_mask_2: 0.2497  loss_dice_2: 0.8097  loss_bbox_2: 0.3415  loss_giou_2: 0.6432  loss_ce_dn_2: 0.2089  loss_mask_dn_2: 0.2386  loss_dice_dn_2: 0.8749  loss_bbox_dn_2: 0.2845  loss_giou_dn_2: 0.5578  loss_ce_3: 1.007  loss_mask_3: 0.2474  loss_dice_3: 0.9193  loss_bbox_3: 0.3094  loss_giou_3: 0.636  loss_ce_dn_3: 0.1724  loss_mask_dn_3: 0.2204  loss_dice_dn_3: 0.8074  loss_bbox_dn_3: 0.2591  loss_giou_dn_3: 0.5295  loss_ce_4: 0.8471  loss_mask_4: 0.2473  loss_dice_4: 0.831  loss_bbox_4: 0.2997  loss_giou_4: 0.6315  loss_ce_dn_4: 0.1526  loss_mask_dn_4: 0.2209  loss_dice_dn_4: 0.8165  loss_bbox_dn_4: 0.2384  loss_giou_dn_4: 0.511  loss_ce_5: 0.7321  loss_mask_5: 0.2361  loss_dice_5: 0.8605  loss_bbox_5: 0.2907  loss_giou_5: 0.6124  loss_ce_dn_5: 0.1399  loss_mask_dn_5: 0.2295  loss_dice_dn_5: 0.7942  loss_bbox_dn_5: 0.2312  loss_giou_dn_5: 0.5034  loss_ce_6: 0.7266  loss_mask_6: 0.2226  loss_dice_6: 0.8943  loss_bbox_6: 0.3029  loss_giou_6: 0.5993  loss_ce_dn_6: 0.1201  loss_mask_dn_6: 0.2185  loss_dice_dn_6: 0.7968  loss_bbox_dn_6: 0.222  loss_giou_dn_6: 0.4966  loss_ce_7: 0.7477  loss_mask_7: 0.2311  loss_dice_7: 0.8547  loss_bbox_7: 0.2862  loss_giou_7: 0.6067  loss_ce_dn_7: 0.1143  loss_mask_dn_7: 0.2168  loss_dice_dn_7: 0.8055  loss_bbox_dn_7: 0.2199  loss_giou_dn_7: 0.4935  loss_ce_8: 0.7473  loss_mask_8: 0.2261  loss_dice_8: 0.8582  loss_bbox_8: 0.2853  loss_giou_8: 0.6001  loss_ce_dn_8: 0.1089  loss_mask_dn_8: 0.2185  loss_dice_dn_8: 0.8206  loss_bbox_dn_8: 0.2181  loss_giou_dn_8: 0.4945  loss_ce_interm: 1.371  loss_mask_interm: 0.2431  loss_dice_interm: 0.9312  loss_bbox_interm: 0.4714  loss_giou_interm: 0.8374    time: 0.8741  last_time: 0.8566  data_time: 0.0129  last_data_time: 0.0092   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:41:43 d2.utils.events]: \u001b[0m eta: 3 days, 16:07:21  iter: 4959  total_loss: 57.5  loss_ce: 0.6393  loss_mask: 0.2121  loss_dice: 0.8205  loss_bbox: 0.2566  loss_giou: 0.6273  loss_ce_dn: 0.1071  loss_mask_dn: 0.2353  loss_dice_dn: 0.8219  loss_bbox_dn: 0.2083  loss_giou_dn: 0.4898  loss_ce_0: 1.371  loss_mask_0: 0.2261  loss_dice_0: 0.8254  loss_bbox_0: 0.4147  loss_giou_0: 0.8754  loss_ce_dn_0: 0.7721  loss_mask_dn_0: 0.8011  loss_dice_dn_0: 2.913  loss_bbox_dn_0: 0.6959  loss_giou_dn_0: 0.8831  loss_ce_1: 1.261  loss_mask_1: 0.2267  loss_dice_1: 0.8477  loss_bbox_1: 0.3038  loss_giou_1: 0.7008  loss_ce_dn_1: 0.2398  loss_mask_dn_1: 0.281  loss_dice_dn_1: 0.9195  loss_bbox_dn_1: 0.3699  loss_giou_dn_1: 0.651  loss_ce_2: 1.146  loss_mask_2: 0.2204  loss_dice_2: 0.8316  loss_bbox_2: 0.2806  loss_giou_2: 0.6694  loss_ce_dn_2: 0.1959  loss_mask_dn_2: 0.2457  loss_dice_dn_2: 0.8351  loss_bbox_dn_2: 0.2963  loss_giou_dn_2: 0.5599  loss_ce_3: 0.8566  loss_mask_3: 0.2125  loss_dice_3: 0.855  loss_bbox_3: 0.2661  loss_giou_3: 0.6706  loss_ce_dn_3: 0.1578  loss_mask_dn_3: 0.2261  loss_dice_dn_3: 0.8442  loss_bbox_dn_3: 0.2572  loss_giou_dn_3: 0.5262  loss_ce_4: 0.7832  loss_mask_4: 0.2165  loss_dice_4: 0.8528  loss_bbox_4: 0.2637  loss_giou_4: 0.6434  loss_ce_dn_4: 0.1498  loss_mask_dn_4: 0.2297  loss_dice_dn_4: 0.7758  loss_bbox_dn_4: 0.2333  loss_giou_dn_4: 0.5148  loss_ce_5: 0.7099  loss_mask_5: 0.2212  loss_dice_5: 0.8467  loss_bbox_5: 0.2592  loss_giou_5: 0.6485  loss_ce_dn_5: 0.1333  loss_mask_dn_5: 0.2336  loss_dice_dn_5: 0.8174  loss_bbox_dn_5: 0.2184  loss_giou_dn_5: 0.5078  loss_ce_6: 0.6892  loss_mask_6: 0.2151  loss_dice_6: 0.8606  loss_bbox_6: 0.2595  loss_giou_6: 0.6333  loss_ce_dn_6: 0.1183  loss_mask_dn_6: 0.237  loss_dice_dn_6: 0.7905  loss_bbox_dn_6: 0.2098  loss_giou_dn_6: 0.4932  loss_ce_7: 0.6732  loss_mask_7: 0.2081  loss_dice_7: 0.8121  loss_bbox_7: 0.2589  loss_giou_7: 0.6423  loss_ce_dn_7: 0.1152  loss_mask_dn_7: 0.2388  loss_dice_dn_7: 0.7887  loss_bbox_dn_7: 0.2061  loss_giou_dn_7: 0.4913  loss_ce_8: 0.6442  loss_mask_8: 0.2161  loss_dice_8: 0.8409  loss_bbox_8: 0.2617  loss_giou_8: 0.639  loss_ce_dn_8: 0.1064  loss_mask_dn_8: 0.2345  loss_dice_dn_8: 0.7944  loss_bbox_dn_8: 0.2081  loss_giou_dn_8: 0.4871  loss_ce_interm: 1.405  loss_mask_interm: 0.2274  loss_dice_interm: 0.8686  loss_bbox_interm: 0.4071  loss_giou_interm: 0.8806    time: 0.8741  last_time: 0.8469  data_time: 0.0128  last_data_time: 0.0108   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:42:01 d2.utils.events]: \u001b[0m eta: 3 days, 16:07:04  iter: 4979  total_loss: 64.47  loss_ce: 0.6832  loss_mask: 0.2934  loss_dice: 0.8425  loss_bbox: 0.3112  loss_giou: 0.7246  loss_ce_dn: 0.1114  loss_mask_dn: 0.2832  loss_dice_dn: 0.8124  loss_bbox_dn: 0.265  loss_giou_dn: 0.5895  loss_ce_0: 1.401  loss_mask_0: 0.2952  loss_dice_0: 0.9764  loss_bbox_0: 0.477  loss_giou_0: 0.9599  loss_ce_dn_0: 0.7352  loss_mask_dn_0: 1.055  loss_dice_dn_0: 2.604  loss_bbox_dn_0: 0.7  loss_giou_dn_0: 0.9297  loss_ce_1: 1.301  loss_mask_1: 0.2916  loss_dice_1: 0.9067  loss_bbox_1: 0.3798  loss_giou_1: 0.8123  loss_ce_dn_1: 0.2753  loss_mask_dn_1: 0.3682  loss_dice_dn_1: 0.9828  loss_bbox_dn_1: 0.3937  loss_giou_dn_1: 0.7063  loss_ce_2: 1.157  loss_mask_2: 0.2889  loss_dice_2: 0.8504  loss_bbox_2: 0.3127  loss_giou_2: 0.7871  loss_ce_dn_2: 0.2217  loss_mask_dn_2: 0.3282  loss_dice_dn_2: 0.8787  loss_bbox_dn_2: 0.3308  loss_giou_dn_2: 0.647  loss_ce_3: 0.9551  loss_mask_3: 0.2958  loss_dice_3: 0.7939  loss_bbox_3: 0.2986  loss_giou_3: 0.7429  loss_ce_dn_3: 0.1612  loss_mask_dn_3: 0.3121  loss_dice_dn_3: 0.8612  loss_bbox_dn_3: 0.298  loss_giou_dn_3: 0.6264  loss_ce_4: 0.8381  loss_mask_4: 0.2888  loss_dice_4: 0.7768  loss_bbox_4: 0.3005  loss_giou_4: 0.738  loss_ce_dn_4: 0.1553  loss_mask_dn_4: 0.3026  loss_dice_dn_4: 0.8586  loss_bbox_dn_4: 0.2797  loss_giou_dn_4: 0.61  loss_ce_5: 0.8016  loss_mask_5: 0.2938  loss_dice_5: 0.8751  loss_bbox_5: 0.3063  loss_giou_5: 0.7416  loss_ce_dn_5: 0.1398  loss_mask_dn_5: 0.2981  loss_dice_dn_5: 0.8563  loss_bbox_dn_5: 0.2734  loss_giou_dn_5: 0.6036  loss_ce_6: 0.8201  loss_mask_6: 0.2876  loss_dice_6: 0.8212  loss_bbox_6: 0.3201  loss_giou_6: 0.7207  loss_ce_dn_6: 0.1294  loss_mask_dn_6: 0.2951  loss_dice_dn_6: 0.8342  loss_bbox_dn_6: 0.2659  loss_giou_dn_6: 0.5967  loss_ce_7: 0.7555  loss_mask_7: 0.2847  loss_dice_7: 0.8722  loss_bbox_7: 0.3029  loss_giou_7: 0.7257  loss_ce_dn_7: 0.1162  loss_mask_dn_7: 0.2848  loss_dice_dn_7: 0.8213  loss_bbox_dn_7: 0.2645  loss_giou_dn_7: 0.5968  loss_ce_8: 0.6966  loss_mask_8: 0.287  loss_dice_8: 0.8707  loss_bbox_8: 0.3058  loss_giou_8: 0.7247  loss_ce_dn_8: 0.1173  loss_mask_dn_8: 0.2836  loss_dice_dn_8: 0.8326  loss_bbox_dn_8: 0.2655  loss_giou_dn_8: 0.5902  loss_ce_interm: 1.402  loss_mask_interm: 0.3063  loss_dice_interm: 0.9387  loss_bbox_interm: 0.4768  loss_giou_interm: 0.9495    time: 0.8741  last_time: 0.8702  data_time: 0.0122  last_data_time: 0.0116   lr: 1.25e-05  max_mem: 12027M\n",
            "\u001b[32m[02/20 08:42:18 fvcore.common.checkpoint]: \u001b[0mSaving checkpoint to ./output/model_0004999.pth\n",
            "\u001b[32m[02/20 08:42:20 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[02/20 08:42:20 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[02/20 08:42:20 d2.data.common]: \u001b[0mSerializing 2756 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[02/20 08:42:20 d2.data.common]: \u001b[0mSerialized dataset takes 2.70 MiB\n",
            "\u001b[32m[02/20 08:42:20 d2.evaluation.evaluator]: \u001b[0mStart inference on 2756 batches\n",
            "\u001b[32m[02/20 08:42:25 d2.evaluation.evaluator]: \u001b[0mInference done 11/2756. Dataloading: 0.0025 s/iter. Inference: 0.2983 s/iter. Eval: 0.0785 s/iter. Total: 0.3793 s/iter. ETA=0:17:21\n",
            "\u001b[32m[02/20 08:42:30 d2.evaluation.evaluator]: \u001b[0mInference done 24/2756. Dataloading: 0.0031 s/iter. Inference: 0.2954 s/iter. Eval: 0.0960 s/iter. Total: 0.3946 s/iter. ETA=0:17:58\n",
            "\u001b[32m[02/20 08:42:35 d2.evaluation.evaluator]: \u001b[0mInference done 37/2756. Dataloading: 0.0033 s/iter. Inference: 0.2964 s/iter. Eval: 0.0956 s/iter. Total: 0.3954 s/iter. ETA=0:17:55\n",
            "\u001b[32m[02/20 08:42:40 d2.evaluation.evaluator]: \u001b[0mInference done 50/2756. Dataloading: 0.0034 s/iter. Inference: 0.2953 s/iter. Eval: 0.0936 s/iter. Total: 0.3924 s/iter. ETA=0:17:41\n",
            "\u001b[32m[02/20 08:42:45 d2.evaluation.evaluator]: \u001b[0mInference done 63/2756. Dataloading: 0.0033 s/iter. Inference: 0.2950 s/iter. Eval: 0.0973 s/iter. Total: 0.3957 s/iter. ETA=0:17:45\n",
            "\u001b[32m[02/20 08:42:50 d2.evaluation.evaluator]: \u001b[0mInference done 76/2756. Dataloading: 0.0033 s/iter. Inference: 0.2915 s/iter. Eval: 0.0999 s/iter. Total: 0.3947 s/iter. ETA=0:17:37\n",
            "\u001b[32m[02/20 08:42:56 d2.evaluation.evaluator]: \u001b[0mInference done 89/2756. Dataloading: 0.0033 s/iter. Inference: 0.2924 s/iter. Eval: 0.0992 s/iter. Total: 0.3950 s/iter. ETA=0:17:33\n",
            "\u001b[32m[02/20 08:43:01 d2.evaluation.evaluator]: \u001b[0mInference done 102/2756. Dataloading: 0.0032 s/iter. Inference: 0.2924 s/iter. Eval: 0.1004 s/iter. Total: 0.3961 s/iter. ETA=0:17:31\n",
            "\u001b[32m[02/20 08:43:06 d2.evaluation.evaluator]: \u001b[0mInference done 115/2756. Dataloading: 0.0032 s/iter. Inference: 0.2918 s/iter. Eval: 0.1019 s/iter. Total: 0.3969 s/iter. ETA=0:17:28\n",
            "\u001b[32m[02/20 08:43:11 d2.evaluation.evaluator]: \u001b[0mInference done 128/2756. Dataloading: 0.0032 s/iter. Inference: 0.2916 s/iter. Eval: 0.1031 s/iter. Total: 0.3980 s/iter. ETA=0:17:25\n",
            "\u001b[32m[02/20 08:43:16 d2.evaluation.evaluator]: \u001b[0mInference done 141/2756. Dataloading: 0.0032 s/iter. Inference: 0.2907 s/iter. Eval: 0.1036 s/iter. Total: 0.3976 s/iter. ETA=0:17:19\n",
            "\u001b[32m[02/20 08:43:22 d2.evaluation.evaluator]: \u001b[0mInference done 154/2756. Dataloading: 0.0032 s/iter. Inference: 0.2914 s/iter. Eval: 0.1027 s/iter. Total: 0.3974 s/iter. ETA=0:17:14\n",
            "\u001b[32m[02/20 08:43:27 d2.evaluation.evaluator]: \u001b[0mInference done 168/2756. Dataloading: 0.0032 s/iter. Inference: 0.2915 s/iter. Eval: 0.1014 s/iter. Total: 0.3962 s/iter. ETA=0:17:05\n",
            "\u001b[32m[02/20 08:43:32 d2.evaluation.evaluator]: \u001b[0mInference done 181/2756. Dataloading: 0.0032 s/iter. Inference: 0.2907 s/iter. Eval: 0.1025 s/iter. Total: 0.3965 s/iter. ETA=0:17:00\n",
            "\u001b[32m[02/20 08:43:38 d2.evaluation.evaluator]: \u001b[0mInference done 195/2756. Dataloading: 0.0032 s/iter. Inference: 0.2912 s/iter. Eval: 0.1009 s/iter. Total: 0.3954 s/iter. ETA=0:16:52\n",
            "\u001b[32m[02/20 08:43:43 d2.evaluation.evaluator]: \u001b[0mInference done 208/2756. Dataloading: 0.0032 s/iter. Inference: 0.2912 s/iter. Eval: 0.1017 s/iter. Total: 0.3963 s/iter. ETA=0:16:49\n",
            "\u001b[32m[02/20 08:43:48 d2.evaluation.evaluator]: \u001b[0mInference done 221/2756. Dataloading: 0.0032 s/iter. Inference: 0.2911 s/iter. Eval: 0.1017 s/iter. Total: 0.3961 s/iter. ETA=0:16:44\n",
            "\u001b[32m[02/20 08:43:53 d2.evaluation.evaluator]: \u001b[0mInference done 234/2756. Dataloading: 0.0032 s/iter. Inference: 0.2906 s/iter. Eval: 0.1020 s/iter. Total: 0.3959 s/iter. ETA=0:16:38\n",
            "\u001b[32m[02/20 08:43:58 d2.evaluation.evaluator]: \u001b[0mInference done 247/2756. Dataloading: 0.0032 s/iter. Inference: 0.2912 s/iter. Eval: 0.1011 s/iter. Total: 0.3956 s/iter. ETA=0:16:32\n",
            "\u001b[32m[02/20 08:44:03 d2.evaluation.evaluator]: \u001b[0mInference done 261/2756. Dataloading: 0.0032 s/iter. Inference: 0.2915 s/iter. Eval: 0.1000 s/iter. Total: 0.3948 s/iter. ETA=0:16:24\n",
            "\u001b[32m[02/20 08:44:09 d2.evaluation.evaluator]: \u001b[0mInference done 274/2756. Dataloading: 0.0032 s/iter. Inference: 0.2918 s/iter. Eval: 0.0997 s/iter. Total: 0.3948 s/iter. ETA=0:16:20\n",
            "\u001b[32m[02/20 08:44:14 d2.evaluation.evaluator]: \u001b[0mInference done 287/2756. Dataloading: 0.0032 s/iter. Inference: 0.2915 s/iter. Eval: 0.1004 s/iter. Total: 0.3952 s/iter. ETA=0:16:15\n",
            "\u001b[32m[02/20 08:44:19 d2.evaluation.evaluator]: \u001b[0mInference done 301/2756. Dataloading: 0.0032 s/iter. Inference: 0.2919 s/iter. Eval: 0.0995 s/iter. Total: 0.3947 s/iter. ETA=0:16:09\n",
            "\u001b[32m[02/20 08:44:24 d2.evaluation.evaluator]: \u001b[0mInference done 314/2756. Dataloading: 0.0032 s/iter. Inference: 0.2916 s/iter. Eval: 0.0997 s/iter. Total: 0.3946 s/iter. ETA=0:16:03\n",
            "\u001b[32m[02/20 08:44:30 d2.evaluation.evaluator]: \u001b[0mInference done 327/2756. Dataloading: 0.0032 s/iter. Inference: 0.2914 s/iter. Eval: 0.1000 s/iter. Total: 0.3948 s/iter. ETA=0:15:58\n",
            "\u001b[32m[02/20 08:44:35 d2.evaluation.evaluator]: \u001b[0mInference done 340/2756. Dataloading: 0.0033 s/iter. Inference: 0.2915 s/iter. Eval: 0.1000 s/iter. Total: 0.3948 s/iter. ETA=0:15:53\n",
            "\u001b[32m[02/20 08:44:40 d2.evaluation.evaluator]: \u001b[0mInference done 353/2756. Dataloading: 0.0033 s/iter. Inference: 0.2912 s/iter. Eval: 0.0999 s/iter. Total: 0.3945 s/iter. ETA=0:15:47\n",
            "\u001b[32m[02/20 08:44:45 d2.evaluation.evaluator]: \u001b[0mInference done 365/2756. Dataloading: 0.0044 s/iter. Inference: 0.2914 s/iter. Eval: 0.0998 s/iter. Total: 0.3957 s/iter. ETA=0:15:46\n",
            "\u001b[32m[02/20 08:44:50 d2.evaluation.evaluator]: \u001b[0mInference done 378/2756. Dataloading: 0.0044 s/iter. Inference: 0.2913 s/iter. Eval: 0.0998 s/iter. Total: 0.3956 s/iter. ETA=0:15:40\n",
            "\u001b[32m[02/20 08:44:55 d2.evaluation.evaluator]: \u001b[0mInference done 391/2756. Dataloading: 0.0044 s/iter. Inference: 0.2911 s/iter. Eval: 0.1004 s/iter. Total: 0.3959 s/iter. ETA=0:15:36\n",
            "\u001b[32m[02/20 08:45:00 d2.evaluation.evaluator]: \u001b[0mInference done 404/2756. Dataloading: 0.0044 s/iter. Inference: 0.2911 s/iter. Eval: 0.1002 s/iter. Total: 0.3958 s/iter. ETA=0:15:30\n",
            "\u001b[32m[02/20 08:45:05 d2.evaluation.evaluator]: \u001b[0mInference done 417/2756. Dataloading: 0.0043 s/iter. Inference: 0.2910 s/iter. Eval: 0.1004 s/iter. Total: 0.3958 s/iter. ETA=0:15:25\n",
            "\u001b[32m[02/20 08:45:11 d2.evaluation.evaluator]: \u001b[0mInference done 430/2756. Dataloading: 0.0043 s/iter. Inference: 0.2910 s/iter. Eval: 0.1004 s/iter. Total: 0.3958 s/iter. ETA=0:15:20\n",
            "\u001b[32m[02/20 08:45:16 d2.evaluation.evaluator]: \u001b[0mInference done 443/2756. Dataloading: 0.0042 s/iter. Inference: 0.2908 s/iter. Eval: 0.1010 s/iter. Total: 0.3962 s/iter. ETA=0:15:16\n",
            "\u001b[32m[02/20 08:45:21 d2.evaluation.evaluator]: \u001b[0mInference done 456/2756. Dataloading: 0.0042 s/iter. Inference: 0.2908 s/iter. Eval: 0.1009 s/iter. Total: 0.3959 s/iter. ETA=0:15:10\n",
            "\u001b[32m[02/20 08:45:26 d2.evaluation.evaluator]: \u001b[0mInference done 469/2756. Dataloading: 0.0042 s/iter. Inference: 0.2903 s/iter. Eval: 0.1012 s/iter. Total: 0.3958 s/iter. ETA=0:15:05\n",
            "\u001b[32m[02/20 08:45:31 d2.evaluation.evaluator]: \u001b[0mInference done 482/2756. Dataloading: 0.0042 s/iter. Inference: 0.2907 s/iter. Eval: 0.1008 s/iter. Total: 0.3957 s/iter. ETA=0:14:59\n",
            "\u001b[32m[02/20 08:45:36 d2.evaluation.evaluator]: \u001b[0mInference done 495/2756. Dataloading: 0.0041 s/iter. Inference: 0.2906 s/iter. Eval: 0.1011 s/iter. Total: 0.3959 s/iter. ETA=0:14:55\n",
            "\u001b[32m[02/20 08:45:42 d2.evaluation.evaluator]: \u001b[0mInference done 508/2756. Dataloading: 0.0041 s/iter. Inference: 0.2905 s/iter. Eval: 0.1013 s/iter. Total: 0.3960 s/iter. ETA=0:14:50\n",
            "\u001b[32m[02/20 08:45:47 d2.evaluation.evaluator]: \u001b[0mInference done 521/2756. Dataloading: 0.0041 s/iter. Inference: 0.2907 s/iter. Eval: 0.1012 s/iter. Total: 0.3961 s/iter. ETA=0:14:45\n",
            "\u001b[32m[02/20 08:45:52 d2.evaluation.evaluator]: \u001b[0mInference done 534/2756. Dataloading: 0.0041 s/iter. Inference: 0.2905 s/iter. Eval: 0.1015 s/iter. Total: 0.3962 s/iter. ETA=0:14:40\n",
            "\u001b[32m[02/20 08:45:57 d2.evaluation.evaluator]: \u001b[0mInference done 547/2756. Dataloading: 0.0041 s/iter. Inference: 0.2905 s/iter. Eval: 0.1018 s/iter. Total: 0.3964 s/iter. ETA=0:14:35\n",
            "\u001b[32m[02/20 08:46:02 d2.evaluation.evaluator]: \u001b[0mInference done 560/2756. Dataloading: 0.0040 s/iter. Inference: 0.2905 s/iter. Eval: 0.1019 s/iter. Total: 0.3965 s/iter. ETA=0:14:30\n",
            "\u001b[32m[02/20 08:46:08 d2.evaluation.evaluator]: \u001b[0mInference done 573/2756. Dataloading: 0.0040 s/iter. Inference: 0.2904 s/iter. Eval: 0.1023 s/iter. Total: 0.3968 s/iter. ETA=0:14:26\n",
            "\u001b[32m[02/20 08:46:13 d2.evaluation.evaluator]: \u001b[0mInference done 586/2756. Dataloading: 0.0040 s/iter. Inference: 0.2905 s/iter. Eval: 0.1023 s/iter. Total: 0.3969 s/iter. ETA=0:14:21\n",
            "\u001b[32m[02/20 08:46:18 d2.evaluation.evaluator]: \u001b[0mInference done 599/2756. Dataloading: 0.0040 s/iter. Inference: 0.2906 s/iter. Eval: 0.1022 s/iter. Total: 0.3969 s/iter. ETA=0:14:16\n",
            "\u001b[32m[02/20 08:46:23 d2.evaluation.evaluator]: \u001b[0mInference done 612/2756. Dataloading: 0.0040 s/iter. Inference: 0.2908 s/iter. Eval: 0.1022 s/iter. Total: 0.3970 s/iter. ETA=0:14:11\n",
            "\u001b[32m[02/20 08:46:29 d2.evaluation.evaluator]: \u001b[0mInference done 625/2756. Dataloading: 0.0039 s/iter. Inference: 0.2907 s/iter. Eval: 0.1023 s/iter. Total: 0.3970 s/iter. ETA=0:14:06\n",
            "\u001b[32m[02/20 08:46:34 d2.evaluation.evaluator]: \u001b[0mInference done 638/2756. Dataloading: 0.0039 s/iter. Inference: 0.2905 s/iter. Eval: 0.1028 s/iter. Total: 0.3973 s/iter. ETA=0:14:01\n",
            "\u001b[32m[02/20 08:46:39 d2.evaluation.evaluator]: \u001b[0mInference done 651/2756. Dataloading: 0.0039 s/iter. Inference: 0.2903 s/iter. Eval: 0.1030 s/iter. Total: 0.3972 s/iter. ETA=0:13:56\n",
            "\u001b[32m[02/20 08:46:44 d2.evaluation.evaluator]: \u001b[0mInference done 664/2756. Dataloading: 0.0039 s/iter. Inference: 0.2904 s/iter. Eval: 0.1031 s/iter. Total: 0.3975 s/iter. ETA=0:13:51\n",
            "\u001b[32m[02/20 08:46:50 d2.evaluation.evaluator]: \u001b[0mInference done 677/2756. Dataloading: 0.0039 s/iter. Inference: 0.2902 s/iter. Eval: 0.1034 s/iter. Total: 0.3975 s/iter. ETA=0:13:46\n",
            "\u001b[32m[02/20 08:46:55 d2.evaluation.evaluator]: \u001b[0mInference done 690/2756. Dataloading: 0.0039 s/iter. Inference: 0.2901 s/iter. Eval: 0.1034 s/iter. Total: 0.3975 s/iter. ETA=0:13:41\n",
            "\u001b[32m[02/20 08:47:00 d2.evaluation.evaluator]: \u001b[0mInference done 703/2756. Dataloading: 0.0039 s/iter. Inference: 0.2899 s/iter. Eval: 0.1039 s/iter. Total: 0.3977 s/iter. ETA=0:13:36\n",
            "\u001b[32m[02/20 08:47:05 d2.evaluation.evaluator]: \u001b[0mInference done 716/2756. Dataloading: 0.0038 s/iter. Inference: 0.2894 s/iter. Eval: 0.1047 s/iter. Total: 0.3980 s/iter. ETA=0:13:32\n",
            "\u001b[32m[02/20 08:47:11 d2.evaluation.evaluator]: \u001b[0mInference done 729/2756. Dataloading: 0.0038 s/iter. Inference: 0.2892 s/iter. Eval: 0.1051 s/iter. Total: 0.3982 s/iter. ETA=0:13:27\n",
            "\u001b[32m[02/20 08:47:16 d2.evaluation.evaluator]: \u001b[0mInference done 742/2756. Dataloading: 0.0038 s/iter. Inference: 0.2890 s/iter. Eval: 0.1054 s/iter. Total: 0.3983 s/iter. ETA=0:13:22\n",
            "\u001b[32m[02/20 08:47:21 d2.evaluation.evaluator]: \u001b[0mInference done 755/2756. Dataloading: 0.0038 s/iter. Inference: 0.2888 s/iter. Eval: 0.1057 s/iter. Total: 0.3984 s/iter. ETA=0:13:17\n",
            "\u001b[32m[02/20 08:47:26 d2.evaluation.evaluator]: \u001b[0mInference done 768/2756. Dataloading: 0.0038 s/iter. Inference: 0.2886 s/iter. Eval: 0.1059 s/iter. Total: 0.3984 s/iter. ETA=0:13:11\n",
            "\u001b[32m[02/20 08:47:32 d2.evaluation.evaluator]: \u001b[0mInference done 781/2756. Dataloading: 0.0038 s/iter. Inference: 0.2884 s/iter. Eval: 0.1064 s/iter. Total: 0.3987 s/iter. ETA=0:13:07\n",
            "\u001b[32m[02/20 08:47:37 d2.evaluation.evaluator]: \u001b[0mInference done 794/2756. Dataloading: 0.0038 s/iter. Inference: 0.2884 s/iter. Eval: 0.1065 s/iter. Total: 0.3988 s/iter. ETA=0:13:02\n",
            "\u001b[32m[02/20 08:47:42 d2.evaluation.evaluator]: \u001b[0mInference done 807/2756. Dataloading: 0.0038 s/iter. Inference: 0.2884 s/iter. Eval: 0.1066 s/iter. Total: 0.3989 s/iter. ETA=0:12:57\n",
            "\u001b[32m[02/20 08:47:48 d2.evaluation.evaluator]: \u001b[0mInference done 820/2756. Dataloading: 0.0038 s/iter. Inference: 0.2884 s/iter. Eval: 0.1068 s/iter. Total: 0.3990 s/iter. ETA=0:12:52\n",
            "\u001b[32m[02/20 08:47:53 d2.evaluation.evaluator]: \u001b[0mInference done 833/2756. Dataloading: 0.0038 s/iter. Inference: 0.2884 s/iter. Eval: 0.1066 s/iter. Total: 0.3988 s/iter. ETA=0:12:46\n",
            "\u001b[32m[02/20 08:47:58 d2.evaluation.evaluator]: \u001b[0mInference done 846/2756. Dataloading: 0.0038 s/iter. Inference: 0.2883 s/iter. Eval: 0.1066 s/iter. Total: 0.3987 s/iter. ETA=0:12:41\n",
            "\u001b[32m[02/20 08:48:03 d2.evaluation.evaluator]: \u001b[0mInference done 859/2756. Dataloading: 0.0037 s/iter. Inference: 0.2883 s/iter. Eval: 0.1068 s/iter. Total: 0.3990 s/iter. ETA=0:12:36\n",
            "\u001b[32m[02/20 08:48:08 d2.evaluation.evaluator]: \u001b[0mInference done 872/2756. Dataloading: 0.0037 s/iter. Inference: 0.2882 s/iter. Eval: 0.1069 s/iter. Total: 0.3989 s/iter. ETA=0:12:31\n",
            "\u001b[32m[02/20 08:48:13 d2.evaluation.evaluator]: \u001b[0mInference done 885/2756. Dataloading: 0.0037 s/iter. Inference: 0.2881 s/iter. Eval: 0.1069 s/iter. Total: 0.3988 s/iter. ETA=0:12:26\n",
            "\u001b[32m[02/20 08:48:19 d2.evaluation.evaluator]: \u001b[0mInference done 898/2756. Dataloading: 0.0037 s/iter. Inference: 0.2880 s/iter. Eval: 0.1071 s/iter. Total: 0.3989 s/iter. ETA=0:12:21\n",
            "\u001b[32m[02/20 08:48:24 d2.evaluation.evaluator]: \u001b[0mInference done 911/2756. Dataloading: 0.0037 s/iter. Inference: 0.2880 s/iter. Eval: 0.1071 s/iter. Total: 0.3989 s/iter. ETA=0:12:15\n",
            "\u001b[32m[02/20 08:48:29 d2.evaluation.evaluator]: \u001b[0mInference done 924/2756. Dataloading: 0.0037 s/iter. Inference: 0.2880 s/iter. Eval: 0.1070 s/iter. Total: 0.3988 s/iter. ETA=0:12:10\n",
            "\u001b[32m[02/20 08:48:34 d2.evaluation.evaluator]: \u001b[0mInference done 937/2756. Dataloading: 0.0037 s/iter. Inference: 0.2881 s/iter. Eval: 0.1068 s/iter. Total: 0.3987 s/iter. ETA=0:12:05\n",
            "\u001b[32m[02/20 08:48:39 d2.evaluation.evaluator]: \u001b[0mInference done 951/2756. Dataloading: 0.0037 s/iter. Inference: 0.2882 s/iter. Eval: 0.1064 s/iter. Total: 0.3985 s/iter. ETA=0:11:59\n",
            "\u001b[32m[02/20 08:48:44 d2.evaluation.evaluator]: \u001b[0mInference done 964/2756. Dataloading: 0.0037 s/iter. Inference: 0.2883 s/iter. Eval: 0.1062 s/iter. Total: 0.3983 s/iter. ETA=0:11:53\n",
            "\u001b[32m[02/20 08:48:50 d2.evaluation.evaluator]: \u001b[0mInference done 977/2756. Dataloading: 0.0037 s/iter. Inference: 0.2884 s/iter. Eval: 0.1062 s/iter. Total: 0.3983 s/iter. ETA=0:11:48\n",
            "\u001b[32m[02/20 08:48:55 d2.evaluation.evaluator]: \u001b[0mInference done 990/2756. Dataloading: 0.0037 s/iter. Inference: 0.2883 s/iter. Eval: 0.1061 s/iter. Total: 0.3982 s/iter. ETA=0:11:43\n",
            "\u001b[32m[02/20 08:49:00 d2.evaluation.evaluator]: \u001b[0mInference done 1003/2756. Dataloading: 0.0037 s/iter. Inference: 0.2884 s/iter. Eval: 0.1062 s/iter. Total: 0.3983 s/iter. ETA=0:11:38\n",
            "\u001b[32m[02/20 08:49:05 d2.evaluation.evaluator]: \u001b[0mInference done 1016/2756. Dataloading: 0.0037 s/iter. Inference: 0.2884 s/iter. Eval: 0.1063 s/iter. Total: 0.3984 s/iter. ETA=0:11:33\n",
            "\u001b[32m[02/20 08:49:10 d2.evaluation.evaluator]: \u001b[0mInference done 1029/2756. Dataloading: 0.0037 s/iter. Inference: 0.2886 s/iter. Eval: 0.1061 s/iter. Total: 0.3984 s/iter. ETA=0:11:28\n",
            "\u001b[32m[02/20 08:49:16 d2.evaluation.evaluator]: \u001b[0mInference done 1042/2756. Dataloading: 0.0037 s/iter. Inference: 0.2885 s/iter. Eval: 0.1063 s/iter. Total: 0.3986 s/iter. ETA=0:11:23\n",
            "\u001b[32m[02/20 08:49:21 d2.evaluation.evaluator]: \u001b[0mInference done 1054/2756. Dataloading: 0.0037 s/iter. Inference: 0.2884 s/iter. Eval: 0.1067 s/iter. Total: 0.3988 s/iter. ETA=0:11:18\n",
            "\u001b[32m[02/20 08:49:26 d2.evaluation.evaluator]: \u001b[0mInference done 1067/2756. Dataloading: 0.0037 s/iter. Inference: 0.2886 s/iter. Eval: 0.1065 s/iter. Total: 0.3988 s/iter. ETA=0:11:13\n",
            "\u001b[32m[02/20 08:49:31 d2.evaluation.evaluator]: \u001b[0mInference done 1080/2756. Dataloading: 0.0037 s/iter. Inference: 0.2885 s/iter. Eval: 0.1067 s/iter. Total: 0.3990 s/iter. ETA=0:11:08\n",
            "\u001b[32m[02/20 08:49:37 d2.evaluation.evaluator]: \u001b[0mInference done 1092/2756. Dataloading: 0.0036 s/iter. Inference: 0.2886 s/iter. Eval: 0.1071 s/iter. Total: 0.3994 s/iter. ETA=0:11:04\n",
            "\u001b[32m[02/20 08:49:42 d2.evaluation.evaluator]: \u001b[0mInference done 1106/2756. Dataloading: 0.0036 s/iter. Inference: 0.2887 s/iter. Eval: 0.1067 s/iter. Total: 0.3992 s/iter. ETA=0:10:58\n",
            "\u001b[32m[02/20 08:49:47 d2.evaluation.evaluator]: \u001b[0mInference done 1120/2756. Dataloading: 0.0036 s/iter. Inference: 0.2890 s/iter. Eval: 0.1060 s/iter. Total: 0.3987 s/iter. ETA=0:10:52\n",
            "\u001b[32m[02/20 08:49:52 d2.evaluation.evaluator]: \u001b[0mInference done 1133/2756. Dataloading: 0.0036 s/iter. Inference: 0.2889 s/iter. Eval: 0.1061 s/iter. Total: 0.3987 s/iter. ETA=0:10:47\n",
            "\u001b[32m[02/20 08:49:57 d2.evaluation.evaluator]: \u001b[0mInference done 1146/2756. Dataloading: 0.0036 s/iter. Inference: 0.2888 s/iter. Eval: 0.1063 s/iter. Total: 0.3988 s/iter. ETA=0:10:42\n",
            "\u001b[32m[02/20 08:50:03 d2.evaluation.evaluator]: \u001b[0mInference done 1159/2756. Dataloading: 0.0036 s/iter. Inference: 0.2888 s/iter. Eval: 0.1063 s/iter. Total: 0.3988 s/iter. ETA=0:10:36\n",
            "\u001b[32m[02/20 08:50:08 d2.evaluation.evaluator]: \u001b[0mInference done 1172/2756. Dataloading: 0.0036 s/iter. Inference: 0.2889 s/iter. Eval: 0.1061 s/iter. Total: 0.3988 s/iter. ETA=0:10:31\n",
            "\u001b[32m[02/20 08:50:13 d2.evaluation.evaluator]: \u001b[0mInference done 1185/2756. Dataloading: 0.0036 s/iter. Inference: 0.2889 s/iter. Eval: 0.1063 s/iter. Total: 0.3990 s/iter. ETA=0:10:26\n",
            "\u001b[32m[02/20 08:50:18 d2.evaluation.evaluator]: \u001b[0mInference done 1198/2756. Dataloading: 0.0036 s/iter. Inference: 0.2890 s/iter. Eval: 0.1064 s/iter. Total: 0.3991 s/iter. ETA=0:10:21\n",
            "\u001b[32m[02/20 08:50:23 d2.evaluation.evaluator]: \u001b[0mInference done 1210/2756. Dataloading: 0.0036 s/iter. Inference: 0.2888 s/iter. Eval: 0.1068 s/iter. Total: 0.3993 s/iter. ETA=0:10:17\n",
            "\u001b[32m[02/20 08:50:29 d2.evaluation.evaluator]: \u001b[0mInference done 1223/2756. Dataloading: 0.0036 s/iter. Inference: 0.2889 s/iter. Eval: 0.1069 s/iter. Total: 0.3994 s/iter. ETA=0:10:12\n",
            "\u001b[32m[02/20 08:50:34 d2.evaluation.evaluator]: \u001b[0mInference done 1236/2756. Dataloading: 0.0036 s/iter. Inference: 0.2889 s/iter. Eval: 0.1068 s/iter. Total: 0.3993 s/iter. ETA=0:10:07\n",
            "\u001b[32m[02/20 08:50:39 d2.evaluation.evaluator]: \u001b[0mInference done 1249/2756. Dataloading: 0.0036 s/iter. Inference: 0.2889 s/iter. Eval: 0.1068 s/iter. Total: 0.3994 s/iter. ETA=0:10:01\n",
            "\u001b[32m[02/20 08:50:44 d2.evaluation.evaluator]: \u001b[0mInference done 1262/2756. Dataloading: 0.0036 s/iter. Inference: 0.2888 s/iter. Eval: 0.1068 s/iter. Total: 0.3993 s/iter. ETA=0:09:56\n",
            "\u001b[32m[02/20 08:50:50 d2.evaluation.evaluator]: \u001b[0mInference done 1275/2756. Dataloading: 0.0036 s/iter. Inference: 0.2889 s/iter. Eval: 0.1068 s/iter. Total: 0.3994 s/iter. ETA=0:09:51\n",
            "\u001b[32m[02/20 08:50:55 d2.evaluation.evaluator]: \u001b[0mInference done 1288/2756. Dataloading: 0.0036 s/iter. Inference: 0.2890 s/iter. Eval: 0.1067 s/iter. Total: 0.3994 s/iter. ETA=0:09:46\n",
            "\u001b[32m[02/20 08:51:00 d2.evaluation.evaluator]: \u001b[0mInference done 1299/2756. Dataloading: 0.0036 s/iter. Inference: 0.2891 s/iter. Eval: 0.1070 s/iter. Total: 0.3998 s/iter. ETA=0:09:42\n",
            "\u001b[32m[02/20 08:51:05 d2.evaluation.evaluator]: \u001b[0mInference done 1312/2756. Dataloading: 0.0036 s/iter. Inference: 0.2892 s/iter. Eval: 0.1071 s/iter. Total: 0.4000 s/iter. ETA=0:09:37\n",
            "\u001b[32m[02/20 08:51:10 d2.evaluation.evaluator]: \u001b[0mInference done 1325/2756. Dataloading: 0.0036 s/iter. Inference: 0.2891 s/iter. Eval: 0.1072 s/iter. Total: 0.4000 s/iter. ETA=0:09:32\n",
            "\u001b[32m[02/20 08:51:16 d2.evaluation.evaluator]: \u001b[0mInference done 1338/2756. Dataloading: 0.0036 s/iter. Inference: 0.2891 s/iter. Eval: 0.1072 s/iter. Total: 0.4000 s/iter. ETA=0:09:27\n",
            "\u001b[32m[02/20 08:51:21 d2.evaluation.evaluator]: \u001b[0mInference done 1351/2756. Dataloading: 0.0036 s/iter. Inference: 0.2892 s/iter. Eval: 0.1070 s/iter. Total: 0.3999 s/iter. ETA=0:09:21\n",
            "\u001b[32m[02/20 08:51:26 d2.evaluation.evaluator]: \u001b[0mInference done 1364/2756. Dataloading: 0.0036 s/iter. Inference: 0.2892 s/iter. Eval: 0.1070 s/iter. Total: 0.3999 s/iter. ETA=0:09:16\n",
            "\u001b[32m[02/20 08:51:31 d2.evaluation.evaluator]: \u001b[0mInference done 1377/2756. Dataloading: 0.0036 s/iter. Inference: 0.2893 s/iter. Eval: 0.1069 s/iter. Total: 0.3998 s/iter. ETA=0:09:11\n",
            "\u001b[32m[02/20 08:51:36 d2.evaluation.evaluator]: \u001b[0mInference done 1390/2756. Dataloading: 0.0036 s/iter. Inference: 0.2893 s/iter. Eval: 0.1070 s/iter. Total: 0.3999 s/iter. ETA=0:09:06\n",
            "\u001b[32m[02/20 08:51:42 d2.evaluation.evaluator]: \u001b[0mInference done 1403/2756. Dataloading: 0.0036 s/iter. Inference: 0.2892 s/iter. Eval: 0.1072 s/iter. Total: 0.4001 s/iter. ETA=0:09:01\n",
            "\u001b[32m[02/20 08:51:47 d2.evaluation.evaluator]: \u001b[0mInference done 1416/2756. Dataloading: 0.0036 s/iter. Inference: 0.2892 s/iter. Eval: 0.1071 s/iter. Total: 0.4000 s/iter. ETA=0:08:55\n",
            "\u001b[32m[02/20 08:51:52 d2.evaluation.evaluator]: \u001b[0mInference done 1429/2756. Dataloading: 0.0036 s/iter. Inference: 0.2891 s/iter. Eval: 0.1072 s/iter. Total: 0.4000 s/iter. ETA=0:08:50\n",
            "\u001b[32m[02/20 08:51:57 d2.evaluation.evaluator]: \u001b[0mInference done 1442/2756. Dataloading: 0.0036 s/iter. Inference: 0.2891 s/iter. Eval: 0.1073 s/iter. Total: 0.4000 s/iter. ETA=0:08:45\n",
            "\u001b[32m[02/20 08:52:02 d2.evaluation.evaluator]: \u001b[0mInference done 1455/2756. Dataloading: 0.0036 s/iter. Inference: 0.2892 s/iter. Eval: 0.1072 s/iter. Total: 0.4000 s/iter. ETA=0:08:40\n",
            "\u001b[32m[02/20 08:52:07 d2.evaluation.evaluator]: \u001b[0mInference done 1468/2756. Dataloading: 0.0036 s/iter. Inference: 0.2893 s/iter. Eval: 0.1070 s/iter. Total: 0.3999 s/iter. ETA=0:08:35\n",
            "\u001b[32m[02/20 08:52:13 d2.evaluation.evaluator]: \u001b[0mInference done 1481/2756. Dataloading: 0.0036 s/iter. Inference: 0.2893 s/iter. Eval: 0.1070 s/iter. Total: 0.3999 s/iter. ETA=0:08:29\n",
            "\u001b[32m[02/20 08:52:18 d2.evaluation.evaluator]: \u001b[0mInference done 1494/2756. Dataloading: 0.0036 s/iter. Inference: 0.2893 s/iter. Eval: 0.1070 s/iter. Total: 0.4000 s/iter. ETA=0:08:24\n",
            "\u001b[32m[02/20 08:52:23 d2.evaluation.evaluator]: \u001b[0mInference done 1507/2756. Dataloading: 0.0036 s/iter. Inference: 0.2894 s/iter. Eval: 0.1069 s/iter. Total: 0.3999 s/iter. ETA=0:08:19\n",
            "\u001b[32m[02/20 08:52:28 d2.evaluation.evaluator]: \u001b[0mInference done 1520/2756. Dataloading: 0.0036 s/iter. Inference: 0.2894 s/iter. Eval: 0.1069 s/iter. Total: 0.3999 s/iter. ETA=0:08:14\n",
            "\u001b[32m[02/20 08:52:34 d2.evaluation.evaluator]: \u001b[0mInference done 1533/2756. Dataloading: 0.0035 s/iter. Inference: 0.2893 s/iter. Eval: 0.1070 s/iter. Total: 0.4000 s/iter. ETA=0:08:09\n",
            "\u001b[32m[02/20 08:52:39 d2.evaluation.evaluator]: \u001b[0mInference done 1546/2756. Dataloading: 0.0035 s/iter. Inference: 0.2894 s/iter. Eval: 0.1070 s/iter. Total: 0.4000 s/iter. ETA=0:08:03\n",
            "\u001b[32m[02/20 08:52:44 d2.evaluation.evaluator]: \u001b[0mInference done 1559/2756. Dataloading: 0.0035 s/iter. Inference: 0.2894 s/iter. Eval: 0.1070 s/iter. Total: 0.4000 s/iter. ETA=0:07:58\n",
            "\u001b[32m[02/20 08:52:49 d2.evaluation.evaluator]: \u001b[0mInference done 1572/2756. Dataloading: 0.0035 s/iter. Inference: 0.2893 s/iter. Eval: 0.1070 s/iter. Total: 0.4000 s/iter. ETA=0:07:53\n",
            "\u001b[32m[02/20 08:52:54 d2.evaluation.evaluator]: \u001b[0mInference done 1585/2756. Dataloading: 0.0035 s/iter. Inference: 0.2894 s/iter. Eval: 0.1071 s/iter. Total: 0.4001 s/iter. ETA=0:07:48\n",
            "\u001b[32m[02/20 08:53:00 d2.evaluation.evaluator]: \u001b[0mInference done 1598/2756. Dataloading: 0.0035 s/iter. Inference: 0.2894 s/iter. Eval: 0.1070 s/iter. Total: 0.4000 s/iter. ETA=0:07:43\n",
            "\u001b[32m[02/20 08:53:05 d2.evaluation.evaluator]: \u001b[0mInference done 1611/2756. Dataloading: 0.0035 s/iter. Inference: 0.2893 s/iter. Eval: 0.1071 s/iter. Total: 0.4001 s/iter. ETA=0:07:38\n",
            "\u001b[32m[02/20 08:53:10 d2.evaluation.evaluator]: \u001b[0mInference done 1624/2756. Dataloading: 0.0035 s/iter. Inference: 0.2893 s/iter. Eval: 0.1072 s/iter. Total: 0.4001 s/iter. ETA=0:07:32\n",
            "\u001b[32m[02/20 08:53:15 d2.evaluation.evaluator]: \u001b[0mInference done 1637/2756. Dataloading: 0.0035 s/iter. Inference: 0.2893 s/iter. Eval: 0.1072 s/iter. Total: 0.4001 s/iter. ETA=0:07:27\n",
            "\u001b[32m[02/20 08:53:20 d2.evaluation.evaluator]: \u001b[0mInference done 1650/2756. Dataloading: 0.0035 s/iter. Inference: 0.2893 s/iter. Eval: 0.1071 s/iter. Total: 0.4000 s/iter. ETA=0:07:22\n",
            "\u001b[32m[02/20 08:53:26 d2.evaluation.evaluator]: \u001b[0mInference done 1663/2756. Dataloading: 0.0035 s/iter. Inference: 0.2892 s/iter. Eval: 0.1074 s/iter. Total: 0.4002 s/iter. ETA=0:07:17\n",
            "\u001b[32m[02/20 08:53:31 d2.evaluation.evaluator]: \u001b[0mInference done 1676/2756. Dataloading: 0.0035 s/iter. Inference: 0.2891 s/iter. Eval: 0.1075 s/iter. Total: 0.4002 s/iter. ETA=0:07:12\n",
            "\u001b[32m[02/20 08:53:36 d2.evaluation.evaluator]: \u001b[0mInference done 1689/2756. Dataloading: 0.0035 s/iter. Inference: 0.2890 s/iter. Eval: 0.1076 s/iter. Total: 0.4002 s/iter. ETA=0:07:07\n",
            "\u001b[32m[02/20 08:53:41 d2.evaluation.evaluator]: \u001b[0mInference done 1702/2756. Dataloading: 0.0035 s/iter. Inference: 0.2890 s/iter. Eval: 0.1076 s/iter. Total: 0.4002 s/iter. ETA=0:07:01\n",
            "\u001b[32m[02/20 08:53:47 d2.evaluation.evaluator]: \u001b[0mInference done 1715/2756. Dataloading: 0.0035 s/iter. Inference: 0.2889 s/iter. Eval: 0.1077 s/iter. Total: 0.4002 s/iter. ETA=0:06:56\n",
            "\u001b[32m[02/20 08:53:52 d2.evaluation.evaluator]: \u001b[0mInference done 1728/2756. Dataloading: 0.0035 s/iter. Inference: 0.2887 s/iter. Eval: 0.1080 s/iter. Total: 0.4003 s/iter. ETA=0:06:51\n",
            "\u001b[32m[02/20 08:53:57 d2.evaluation.evaluator]: \u001b[0mInference done 1741/2756. Dataloading: 0.0035 s/iter. Inference: 0.2886 s/iter. Eval: 0.1082 s/iter. Total: 0.4004 s/iter. ETA=0:06:46\n",
            "\u001b[32m[02/20 08:54:03 d2.evaluation.evaluator]: \u001b[0mInference done 1754/2756. Dataloading: 0.0035 s/iter. Inference: 0.2885 s/iter. Eval: 0.1082 s/iter. Total: 0.4004 s/iter. ETA=0:06:41\n",
            "\u001b[32m[02/20 08:54:08 d2.evaluation.evaluator]: \u001b[0mInference done 1767/2756. Dataloading: 0.0035 s/iter. Inference: 0.2884 s/iter. Eval: 0.1084 s/iter. Total: 0.4004 s/iter. ETA=0:06:36\n",
            "\u001b[32m[02/20 08:54:13 d2.evaluation.evaluator]: \u001b[0mInference done 1780/2756. Dataloading: 0.0035 s/iter. Inference: 0.2884 s/iter. Eval: 0.1085 s/iter. Total: 0.4005 s/iter. ETA=0:06:30\n",
            "\u001b[32m[02/20 08:54:18 d2.evaluation.evaluator]: \u001b[0mInference done 1792/2756. Dataloading: 0.0035 s/iter. Inference: 0.2883 s/iter. Eval: 0.1087 s/iter. Total: 0.4006 s/iter. ETA=0:06:26\n",
            "\u001b[32m[02/20 08:54:24 d2.evaluation.evaluator]: \u001b[0mInference done 1805/2756. Dataloading: 0.0035 s/iter. Inference: 0.2883 s/iter. Eval: 0.1087 s/iter. Total: 0.4007 s/iter. ETA=0:06:21\n",
            "\u001b[32m[02/20 08:54:29 d2.evaluation.evaluator]: \u001b[0mInference done 1818/2756. Dataloading: 0.0035 s/iter. Inference: 0.2883 s/iter. Eval: 0.1088 s/iter. Total: 0.4007 s/iter. ETA=0:06:15\n",
            "\u001b[32m[02/20 08:54:34 d2.evaluation.evaluator]: \u001b[0mInference done 1831/2756. Dataloading: 0.0035 s/iter. Inference: 0.2883 s/iter. Eval: 0.1088 s/iter. Total: 0.4007 s/iter. ETA=0:06:10\n",
            "\u001b[32m[02/20 08:54:39 d2.evaluation.evaluator]: \u001b[0mInference done 1844/2756. Dataloading: 0.0035 s/iter. Inference: 0.2883 s/iter. Eval: 0.1087 s/iter. Total: 0.4006 s/iter. ETA=0:06:05\n",
            "\u001b[32m[02/20 08:54:44 d2.evaluation.evaluator]: \u001b[0mInference done 1856/2756. Dataloading: 0.0035 s/iter. Inference: 0.2883 s/iter. Eval: 0.1089 s/iter. Total: 0.4008 s/iter. ETA=0:06:00\n",
            "\u001b[32m[02/20 08:54:50 d2.evaluation.evaluator]: \u001b[0mInference done 1869/2756. Dataloading: 0.0035 s/iter. Inference: 0.2883 s/iter. Eval: 0.1090 s/iter. Total: 0.4008 s/iter. ETA=0:05:55\n",
            "\u001b[32m[02/20 08:54:55 d2.evaluation.evaluator]: \u001b[0mInference done 1882/2756. Dataloading: 0.0035 s/iter. Inference: 0.2882 s/iter. Eval: 0.1090 s/iter. Total: 0.4008 s/iter. ETA=0:05:50\n",
            "\u001b[32m[02/20 08:55:00 d2.evaluation.evaluator]: \u001b[0mInference done 1895/2756. Dataloading: 0.0035 s/iter. Inference: 0.2882 s/iter. Eval: 0.1091 s/iter. Total: 0.4009 s/iter. ETA=0:05:45\n",
            "\u001b[32m[02/20 08:55:05 d2.evaluation.evaluator]: \u001b[0mInference done 1908/2756. Dataloading: 0.0035 s/iter. Inference: 0.2882 s/iter. Eval: 0.1091 s/iter. Total: 0.4009 s/iter. ETA=0:05:39\n",
            "\u001b[32m[02/20 08:55:10 d2.evaluation.evaluator]: \u001b[0mInference done 1921/2756. Dataloading: 0.0035 s/iter. Inference: 0.2882 s/iter. Eval: 0.1090 s/iter. Total: 0.4008 s/iter. ETA=0:05:34\n",
            "\u001b[32m[02/20 08:55:16 d2.evaluation.evaluator]: \u001b[0mInference done 1934/2756. Dataloading: 0.0035 s/iter. Inference: 0.2883 s/iter. Eval: 0.1090 s/iter. Total: 0.4008 s/iter. ETA=0:05:29\n",
            "\u001b[32m[02/20 08:55:21 d2.evaluation.evaluator]: \u001b[0mInference done 1947/2756. Dataloading: 0.0035 s/iter. Inference: 0.2883 s/iter. Eval: 0.1089 s/iter. Total: 0.4008 s/iter. ETA=0:05:24\n",
            "\u001b[32m[02/20 08:55:26 d2.evaluation.evaluator]: \u001b[0mInference done 1960/2756. Dataloading: 0.0035 s/iter. Inference: 0.2883 s/iter. Eval: 0.1089 s/iter. Total: 0.4009 s/iter. ETA=0:05:19\n",
            "\u001b[32m[02/20 08:55:31 d2.evaluation.evaluator]: \u001b[0mInference done 1973/2756. Dataloading: 0.0035 s/iter. Inference: 0.2884 s/iter. Eval: 0.1088 s/iter. Total: 0.4008 s/iter. ETA=0:05:13\n",
            "\u001b[32m[02/20 08:55:36 d2.evaluation.evaluator]: \u001b[0mInference done 1985/2756. Dataloading: 0.0035 s/iter. Inference: 0.2884 s/iter. Eval: 0.1090 s/iter. Total: 0.4010 s/iter. ETA=0:05:09\n",
            "\u001b[32m[02/20 08:55:41 d2.evaluation.evaluator]: \u001b[0mInference done 1997/2756. Dataloading: 0.0035 s/iter. Inference: 0.2884 s/iter. Eval: 0.1092 s/iter. Total: 0.4011 s/iter. ETA=0:05:04\n",
            "\u001b[32m[02/20 08:55:46 d2.evaluation.evaluator]: \u001b[0mInference done 2009/2756. Dataloading: 0.0035 s/iter. Inference: 0.2884 s/iter. Eval: 0.1092 s/iter. Total: 0.4012 s/iter. ETA=0:04:59\n",
            "\u001b[32m[02/20 08:55:51 d2.evaluation.evaluator]: \u001b[0mInference done 2021/2756. Dataloading: 0.0035 s/iter. Inference: 0.2884 s/iter. Eval: 0.1094 s/iter. Total: 0.4013 s/iter. ETA=0:04:54\n",
            "\u001b[32m[02/20 08:55:57 d2.evaluation.evaluator]: \u001b[0mInference done 2033/2756. Dataloading: 0.0035 s/iter. Inference: 0.2884 s/iter. Eval: 0.1096 s/iter. Total: 0.4015 s/iter. ETA=0:04:50\n",
            "\u001b[32m[02/20 08:56:02 d2.evaluation.evaluator]: \u001b[0mInference done 2047/2756. Dataloading: 0.0035 s/iter. Inference: 0.2886 s/iter. Eval: 0.1092 s/iter. Total: 0.4013 s/iter. ETA=0:04:44\n",
            "\u001b[32m[02/20 08:56:07 d2.evaluation.evaluator]: \u001b[0mInference done 2061/2756. Dataloading: 0.0035 s/iter. Inference: 0.2887 s/iter. Eval: 0.1088 s/iter. Total: 0.4011 s/iter. ETA=0:04:38\n",
            "\u001b[32m[02/20 08:56:12 d2.evaluation.evaluator]: \u001b[0mInference done 2073/2756. Dataloading: 0.0035 s/iter. Inference: 0.2887 s/iter. Eval: 0.1089 s/iter. Total: 0.4012 s/iter. ETA=0:04:34\n",
            "\u001b[32m[02/20 08:56:17 d2.evaluation.evaluator]: \u001b[0mInference done 2086/2756. Dataloading: 0.0035 s/iter. Inference: 0.2886 s/iter. Eval: 0.1090 s/iter. Total: 0.4012 s/iter. ETA=0:04:28\n",
            "\u001b[32m[02/20 08:56:22 d2.evaluation.evaluator]: \u001b[0mInference done 2099/2756. Dataloading: 0.0035 s/iter. Inference: 0.2887 s/iter. Eval: 0.1090 s/iter. Total: 0.4012 s/iter. ETA=0:04:23\n",
            "\u001b[32m[02/20 08:56:28 d2.evaluation.evaluator]: \u001b[0mInference done 2112/2756. Dataloading: 0.0035 s/iter. Inference: 0.2888 s/iter. Eval: 0.1089 s/iter. Total: 0.4012 s/iter. ETA=0:04:18\n",
            "\u001b[32m[02/20 08:56:33 d2.evaluation.evaluator]: \u001b[0mInference done 2125/2756. Dataloading: 0.0035 s/iter. Inference: 0.2888 s/iter. Eval: 0.1087 s/iter. Total: 0.4011 s/iter. ETA=0:04:13\n",
            "\u001b[32m[02/20 08:56:38 d2.evaluation.evaluator]: \u001b[0mInference done 2137/2756. Dataloading: 0.0035 s/iter. Inference: 0.2888 s/iter. Eval: 0.1088 s/iter. Total: 0.4012 s/iter. ETA=0:04:08\n",
            "\u001b[32m[02/20 08:56:43 d2.evaluation.evaluator]: \u001b[0mInference done 2150/2756. Dataloading: 0.0035 s/iter. Inference: 0.2889 s/iter. Eval: 0.1088 s/iter. Total: 0.4012 s/iter. ETA=0:04:03\n",
            "\u001b[32m[02/20 08:56:48 d2.evaluation.evaluator]: \u001b[0mInference done 2163/2756. Dataloading: 0.0035 s/iter. Inference: 0.2889 s/iter. Eval: 0.1088 s/iter. Total: 0.4013 s/iter. ETA=0:03:57\n",
            "\u001b[32m[02/20 08:56:54 d2.evaluation.evaluator]: \u001b[0mInference done 2176/2756. Dataloading: 0.0035 s/iter. Inference: 0.2890 s/iter. Eval: 0.1088 s/iter. Total: 0.4013 s/iter. ETA=0:03:52\n",
            "\u001b[32m[02/20 08:56:59 d2.evaluation.evaluator]: \u001b[0mInference done 2189/2756. Dataloading: 0.0035 s/iter. Inference: 0.2890 s/iter. Eval: 0.1088 s/iter. Total: 0.4013 s/iter. ETA=0:03:47\n",
            "\u001b[32m[02/20 08:57:04 d2.evaluation.evaluator]: \u001b[0mInference done 2202/2756. Dataloading: 0.0035 s/iter. Inference: 0.2890 s/iter. Eval: 0.1088 s/iter. Total: 0.4013 s/iter. ETA=0:03:42\n",
            "\u001b[32m[02/20 08:57:09 d2.evaluation.evaluator]: \u001b[0mInference done 2215/2756. Dataloading: 0.0035 s/iter. Inference: 0.2890 s/iter. Eval: 0.1087 s/iter. Total: 0.4013 s/iter. ETA=0:03:37\n",
            "\u001b[32m[02/20 08:57:14 d2.evaluation.evaluator]: \u001b[0mInference done 2228/2756. Dataloading: 0.0035 s/iter. Inference: 0.2891 s/iter. Eval: 0.1087 s/iter. Total: 0.4013 s/iter. ETA=0:03:31\n",
            "\u001b[32m[02/20 08:57:20 d2.evaluation.evaluator]: \u001b[0mInference done 2241/2756. Dataloading: 0.0035 s/iter. Inference: 0.2891 s/iter. Eval: 0.1086 s/iter. Total: 0.4013 s/iter. ETA=0:03:26\n",
            "\u001b[32m[02/20 08:57:25 d2.evaluation.evaluator]: \u001b[0mInference done 2254/2756. Dataloading: 0.0035 s/iter. Inference: 0.2891 s/iter. Eval: 0.1087 s/iter. Total: 0.4013 s/iter. ETA=0:03:21\n",
            "\u001b[32m[02/20 08:57:30 d2.evaluation.evaluator]: \u001b[0mInference done 2267/2756. Dataloading: 0.0035 s/iter. Inference: 0.2891 s/iter. Eval: 0.1087 s/iter. Total: 0.4014 s/iter. ETA=0:03:16\n",
            "\u001b[32m[02/20 08:57:35 d2.evaluation.evaluator]: \u001b[0mInference done 2280/2756. Dataloading: 0.0035 s/iter. Inference: 0.2892 s/iter. Eval: 0.1086 s/iter. Total: 0.4013 s/iter. ETA=0:03:11\n",
            "\u001b[32m[02/20 08:57:41 d2.evaluation.evaluator]: \u001b[0mInference done 2293/2756. Dataloading: 0.0035 s/iter. Inference: 0.2892 s/iter. Eval: 0.1086 s/iter. Total: 0.4013 s/iter. ETA=0:03:05\n",
            "\u001b[32m[02/20 08:57:46 d2.evaluation.evaluator]: \u001b[0mInference done 2306/2756. Dataloading: 0.0035 s/iter. Inference: 0.2892 s/iter. Eval: 0.1086 s/iter. Total: 0.4013 s/iter. ETA=0:03:00\n",
            "\u001b[32m[02/20 08:57:51 d2.evaluation.evaluator]: \u001b[0mInference done 2319/2756. Dataloading: 0.0035 s/iter. Inference: 0.2893 s/iter. Eval: 0.1086 s/iter. Total: 0.4014 s/iter. ETA=0:02:55\n",
            "\u001b[32m[02/20 08:57:56 d2.evaluation.evaluator]: \u001b[0mInference done 2332/2756. Dataloading: 0.0035 s/iter. Inference: 0.2893 s/iter. Eval: 0.1086 s/iter. Total: 0.4014 s/iter. ETA=0:02:50\n",
            "\u001b[32m[02/20 08:58:02 d2.evaluation.evaluator]: \u001b[0mInference done 2345/2756. Dataloading: 0.0035 s/iter. Inference: 0.2893 s/iter. Eval: 0.1086 s/iter. Total: 0.4015 s/iter. ETA=0:02:44\n",
            "\u001b[32m[02/20 08:58:07 d2.evaluation.evaluator]: \u001b[0mInference done 2358/2756. Dataloading: 0.0035 s/iter. Inference: 0.2893 s/iter. Eval: 0.1086 s/iter. Total: 0.4015 s/iter. ETA=0:02:39\n",
            "\u001b[32m[02/20 08:58:12 d2.evaluation.evaluator]: \u001b[0mInference done 2371/2756. Dataloading: 0.0035 s/iter. Inference: 0.2893 s/iter. Eval: 0.1086 s/iter. Total: 0.4015 s/iter. ETA=0:02:34\n",
            "\u001b[32m[02/20 08:58:18 d2.evaluation.evaluator]: \u001b[0mInference done 2384/2756. Dataloading: 0.0035 s/iter. Inference: 0.2893 s/iter. Eval: 0.1087 s/iter. Total: 0.4015 s/iter. ETA=0:02:29\n",
            "\u001b[32m[02/20 08:58:23 d2.evaluation.evaluator]: \u001b[0mInference done 2397/2756. Dataloading: 0.0035 s/iter. Inference: 0.2892 s/iter. Eval: 0.1087 s/iter. Total: 0.4015 s/iter. ETA=0:02:24\n",
            "\u001b[32m[02/20 08:58:28 d2.evaluation.evaluator]: \u001b[0mInference done 2410/2756. Dataloading: 0.0035 s/iter. Inference: 0.2892 s/iter. Eval: 0.1088 s/iter. Total: 0.4016 s/iter. ETA=0:02:18\n",
            "\u001b[32m[02/20 08:58:33 d2.evaluation.evaluator]: \u001b[0mInference done 2423/2756. Dataloading: 0.0035 s/iter. Inference: 0.2892 s/iter. Eval: 0.1088 s/iter. Total: 0.4016 s/iter. ETA=0:02:13\n",
            "\u001b[32m[02/20 08:58:39 d2.evaluation.evaluator]: \u001b[0mInference done 2435/2756. Dataloading: 0.0035 s/iter. Inference: 0.2894 s/iter. Eval: 0.1089 s/iter. Total: 0.4018 s/iter. ETA=0:02:08\n",
            "\u001b[32m[02/20 08:58:44 d2.evaluation.evaluator]: \u001b[0mInference done 2448/2756. Dataloading: 0.0035 s/iter. Inference: 0.2894 s/iter. Eval: 0.1088 s/iter. Total: 0.4018 s/iter. ETA=0:02:03\n",
            "\u001b[32m[02/20 08:58:49 d2.evaluation.evaluator]: \u001b[0mInference done 2461/2756. Dataloading: 0.0035 s/iter. Inference: 0.2894 s/iter. Eval: 0.1088 s/iter. Total: 0.4018 s/iter. ETA=0:01:58\n",
            "\u001b[32m[02/20 08:58:54 d2.evaluation.evaluator]: \u001b[0mInference done 2474/2756. Dataloading: 0.0035 s/iter. Inference: 0.2895 s/iter. Eval: 0.1087 s/iter. Total: 0.4017 s/iter. ETA=0:01:53\n",
            "\u001b[32m[02/20 08:59:00 d2.evaluation.evaluator]: \u001b[0mInference done 2487/2756. Dataloading: 0.0035 s/iter. Inference: 0.2895 s/iter. Eval: 0.1087 s/iter. Total: 0.4018 s/iter. ETA=0:01:48\n",
            "\u001b[32m[02/20 08:59:05 d2.evaluation.evaluator]: \u001b[0mInference done 2499/2756. Dataloading: 0.0035 s/iter. Inference: 0.2896 s/iter. Eval: 0.1087 s/iter. Total: 0.4019 s/iter. ETA=0:01:43\n",
            "\u001b[32m[02/20 08:59:10 d2.evaluation.evaluator]: \u001b[0mInference done 2512/2756. Dataloading: 0.0035 s/iter. Inference: 0.2896 s/iter. Eval: 0.1088 s/iter. Total: 0.4019 s/iter. ETA=0:01:38\n",
            "\u001b[32m[02/20 08:59:15 d2.evaluation.evaluator]: \u001b[0mInference done 2525/2756. Dataloading: 0.0035 s/iter. Inference: 0.2896 s/iter. Eval: 0.1088 s/iter. Total: 0.4019 s/iter. ETA=0:01:32\n",
            "\u001b[32m[02/20 08:59:21 d2.evaluation.evaluator]: \u001b[0mInference done 2538/2756. Dataloading: 0.0035 s/iter. Inference: 0.2895 s/iter. Eval: 0.1089 s/iter. Total: 0.4020 s/iter. ETA=0:01:27\n",
            "\u001b[32m[02/20 08:59:26 d2.evaluation.evaluator]: \u001b[0mInference done 2551/2756. Dataloading: 0.0035 s/iter. Inference: 0.2896 s/iter. Eval: 0.1088 s/iter. Total: 0.4019 s/iter. ETA=0:01:22\n",
            "\u001b[32m[02/20 08:59:31 d2.evaluation.evaluator]: \u001b[0mInference done 2564/2756. Dataloading: 0.0034 s/iter. Inference: 0.2896 s/iter. Eval: 0.1089 s/iter. Total: 0.4020 s/iter. ETA=0:01:17\n",
            "\u001b[32m[02/20 08:59:36 d2.evaluation.evaluator]: \u001b[0mInference done 2577/2756. Dataloading: 0.0034 s/iter. Inference: 0.2896 s/iter. Eval: 0.1089 s/iter. Total: 0.4020 s/iter. ETA=0:01:11\n",
            "\u001b[32m[02/20 08:59:42 d2.evaluation.evaluator]: \u001b[0mInference done 2590/2756. Dataloading: 0.0034 s/iter. Inference: 0.2897 s/iter. Eval: 0.1089 s/iter. Total: 0.4021 s/iter. ETA=0:01:06\n",
            "\u001b[32m[02/20 08:59:47 d2.evaluation.evaluator]: \u001b[0mInference done 2603/2756. Dataloading: 0.0034 s/iter. Inference: 0.2896 s/iter. Eval: 0.1090 s/iter. Total: 0.4022 s/iter. ETA=0:01:01\n",
            "\u001b[32m[02/20 08:59:53 d2.evaluation.evaluator]: \u001b[0mInference done 2616/2756. Dataloading: 0.0034 s/iter. Inference: 0.2897 s/iter. Eval: 0.1090 s/iter. Total: 0.4022 s/iter. ETA=0:00:56\n",
            "\u001b[32m[02/20 08:59:58 d2.evaluation.evaluator]: \u001b[0mInference done 2629/2756. Dataloading: 0.0034 s/iter. Inference: 0.2897 s/iter. Eval: 0.1090 s/iter. Total: 0.4023 s/iter. ETA=0:00:51\n",
            "\u001b[32m[02/20 09:00:03 d2.evaluation.evaluator]: \u001b[0mInference done 2642/2756. Dataloading: 0.0034 s/iter. Inference: 0.2897 s/iter. Eval: 0.1091 s/iter. Total: 0.4023 s/iter. ETA=0:00:45\n",
            "\u001b[32m[02/20 09:00:09 d2.evaluation.evaluator]: \u001b[0mInference done 2655/2756. Dataloading: 0.0034 s/iter. Inference: 0.2898 s/iter. Eval: 0.1090 s/iter. Total: 0.4024 s/iter. ETA=0:00:40\n",
            "\u001b[32m[02/20 09:00:14 d2.evaluation.evaluator]: \u001b[0mInference done 2667/2756. Dataloading: 0.0034 s/iter. Inference: 0.2898 s/iter. Eval: 0.1092 s/iter. Total: 0.4025 s/iter. ETA=0:00:35\n",
            "\u001b[32m[02/20 09:00:19 d2.evaluation.evaluator]: \u001b[0mInference done 2680/2756. Dataloading: 0.0034 s/iter. Inference: 0.2898 s/iter. Eval: 0.1091 s/iter. Total: 0.4025 s/iter. ETA=0:00:30\n",
            "\u001b[32m[02/20 09:00:24 d2.evaluation.evaluator]: \u001b[0mInference done 2693/2756. Dataloading: 0.0034 s/iter. Inference: 0.2898 s/iter. Eval: 0.1091 s/iter. Total: 0.4025 s/iter. ETA=0:00:25\n",
            "\u001b[32m[02/20 09:00:30 d2.evaluation.evaluator]: \u001b[0mInference done 2706/2756. Dataloading: 0.0034 s/iter. Inference: 0.2898 s/iter. Eval: 0.1092 s/iter. Total: 0.4026 s/iter. ETA=0:00:20\n",
            "\u001b[32m[02/20 09:00:35 d2.evaluation.evaluator]: \u001b[0mInference done 2719/2756. Dataloading: 0.0034 s/iter. Inference: 0.2899 s/iter. Eval: 0.1091 s/iter. Total: 0.4025 s/iter. ETA=0:00:14\n",
            "\u001b[32m[02/20 09:00:40 d2.evaluation.evaluator]: \u001b[0mInference done 2732/2756. Dataloading: 0.0034 s/iter. Inference: 0.2899 s/iter. Eval: 0.1091 s/iter. Total: 0.4025 s/iter. ETA=0:00:09\n",
            "\u001b[32m[02/20 09:00:45 d2.evaluation.evaluator]: \u001b[0mInference done 2745/2756. Dataloading: 0.0034 s/iter. Inference: 0.2899 s/iter. Eval: 0.1091 s/iter. Total: 0.4025 s/iter. ETA=0:00:04\n",
            "\u001b[32m[02/20 09:00:50 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:18:27.354067 (0.402528 s / iter per device, on 1 devices)\n",
            "\u001b[32m[02/20 09:00:50 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:13:17 (0.289942 s / iter per device, on 1 devices)\n",
            "\u001b[32m[02/20 09:00:50 d2.evaluation.panoptic_evaluation]: \u001b[0mWriting all panoptic predictions to /tmp/panoptic_eval4za3o81k ...\n",
            "\u001b[32m[02/20 09:01:08 d2.evaluation.panoptic_evaluation]: \u001b[0mPanoptic Evaluation Results:\n",
            "|        |   PQ   |   SQ   |   RQ   |  #categories  |\n",
            "|:------:|:------:|:------:|:------:|:-------------:|\n",
            "|  All   | 48.677 | 80.128 | 57.050 |      72       |\n",
            "| Things | 51.590 | 84.676 | 59.195 |      42       |\n",
            "| Stuff  | 44.598 | 73.762 | 54.046 |      30       |\n",
            "\u001b[32m[02/20 09:01:09 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[4m\u001b[5m\u001b[31mERROR\u001b[0m \u001b[32m[02/20 09:01:09 d2.engine.train_loop]: \u001b[0mException during training:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/engine/train_loop.py\", line 156, in train\n",
            "    self.after_step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/engine/train_loop.py\", line 190, in after_step\n",
            "    h.after_step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/engine/hooks.py\", line 556, in after_step\n",
            "    self._do_eval()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/engine/hooks.py\", line 529, in _do_eval\n",
            "    results = self._func()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/engine/defaults.py\", line 455, in test_and_save_results\n",
            "    self._last_eval_results = self.test(self.cfg, self.model)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/engine/defaults.py\", line 619, in test\n",
            "    results_i = inference_on_dataset(model, data_loader, evaluator)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/evaluation/evaluator.py\", line 213, in inference_on_dataset\n",
            "    results = evaluator.evaluate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/evaluation/evaluator.py\", line 93, in evaluate\n",
            "    result = evaluator.evaluate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/evaluation/coco_evaluation.py\", line 206, in evaluate\n",
            "    self._eval_predictions(predictions, img_ids=img_ids)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/evaluation/coco_evaluation.py\", line 235, in _eval_predictions\n",
            "    assert min(all_contiguous_ids) == 0 and max(all_contiguous_ids) == num_classes - 1\n",
            "AssertionError\n",
            "\u001b[32m[02/20 09:01:09 d2.engine.hooks]: \u001b[0mOverall training speed: 4997 iterations in 1:12:48 (0.8743 s / it)\n",
            "\u001b[32m[02/20 09:01:09 d2.engine.hooks]: \u001b[0mTotal training time: 1:31:56 (0:19:07 on hooks)\n",
            "\u001b[32m[02/20 09:01:09 d2.utils.events]: \u001b[0m eta: 3 days, 16:07:25  iter: 4999  total_loss: 57.23  loss_ce: 0.7798  loss_mask: 0.2461  loss_dice: 0.837  loss_bbox: 0.2239  loss_giou: 0.6268  loss_ce_dn: 0.1156  loss_mask_dn: 0.2409  loss_dice_dn: 0.8039  loss_bbox_dn: 0.1925  loss_giou_dn: 0.5294  loss_ce_0: 1.424  loss_mask_0: 0.2178  loss_dice_0: 0.8388  loss_bbox_0: 0.3457  loss_giou_0: 0.8435  loss_ce_dn_0: 0.7274  loss_mask_dn_0: 0.6776  loss_dice_dn_0: 2.755  loss_bbox_dn_0: 0.5307  loss_giou_dn_0: 0.9421  loss_ce_1: 1.32  loss_mask_1: 0.2144  loss_dice_1: 0.8355  loss_bbox_1: 0.2812  loss_giou_1: 0.7179  loss_ce_dn_1: 0.2356  loss_mask_dn_1: 0.2661  loss_dice_dn_1: 0.8884  loss_bbox_dn_1: 0.2984  loss_giou_dn_1: 0.6864  loss_ce_2: 1.145  loss_mask_2: 0.2284  loss_dice_2: 0.8965  loss_bbox_2: 0.2571  loss_giou_2: 0.653  loss_ce_dn_2: 0.1841  loss_mask_dn_2: 0.2447  loss_dice_dn_2: 0.84  loss_bbox_dn_2: 0.2466  loss_giou_dn_2: 0.6201  loss_ce_3: 0.9758  loss_mask_3: 0.197  loss_dice_3: 0.8432  loss_bbox_3: 0.2502  loss_giou_3: 0.6322  loss_ce_dn_3: 0.1578  loss_mask_dn_3: 0.2586  loss_dice_dn_3: 0.8328  loss_bbox_dn_3: 0.2135  loss_giou_dn_3: 0.5841  loss_ce_4: 0.8947  loss_mask_4: 0.1963  loss_dice_4: 0.8319  loss_bbox_4: 0.2172  loss_giou_4: 0.6067  loss_ce_dn_4: 0.1377  loss_mask_dn_4: 0.2408  loss_dice_dn_4: 0.8211  loss_bbox_dn_4: 0.2021  loss_giou_dn_4: 0.5617  loss_ce_5: 0.8337  loss_mask_5: 0.2457  loss_dice_5: 0.825  loss_bbox_5: 0.2172  loss_giou_5: 0.6312  loss_ce_dn_5: 0.129  loss_mask_dn_5: 0.2219  loss_dice_dn_5: 0.815  loss_bbox_dn_5: 0.2003  loss_giou_dn_5: 0.5571  loss_ce_6: 0.8214  loss_mask_6: 0.2561  loss_dice_6: 0.8625  loss_bbox_6: 0.2178  loss_giou_6: 0.5988  loss_ce_dn_6: 0.1207  loss_mask_dn_6: 0.2348  loss_dice_dn_6: 0.8036  loss_bbox_dn_6: 0.1954  loss_giou_dn_6: 0.5419  loss_ce_7: 0.7974  loss_mask_7: 0.2502  loss_dice_7: 0.7868  loss_bbox_7: 0.2354  loss_giou_7: 0.6248  loss_ce_dn_7: 0.1163  loss_mask_dn_7: 0.2367  loss_dice_dn_7: 0.8187  loss_bbox_dn_7: 0.1923  loss_giou_dn_7: 0.5391  loss_ce_8: 0.7932  loss_mask_8: 0.2411  loss_dice_8: 0.7981  loss_bbox_8: 0.2187  loss_giou_8: 0.6192  loss_ce_dn_8: 0.1147  loss_mask_dn_8: 0.2373  loss_dice_dn_8: 0.8055  loss_bbox_dn_8: 0.1926  loss_giou_dn_8: 0.5292  loss_ce_interm: 1.407  loss_mask_interm: 0.2198  loss_dice_interm: 0.8232  loss_bbox_interm: 0.3368  loss_giou_interm: 0.8217    time: 0.8741  last_time: 0.9422  data_time: 0.0127  last_data_time: 0.0117   lr: 1.25e-05  max_mem: 12027M\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/MaskDINO/train_net.py\", line 377, in <module>\n",
            "    launch(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/engine/launch.py\", line 84, in launch\n",
            "    main_func(*args)\n",
            "  File \"/content/MaskDINO/train_net.py\", line 364, in main\n",
            "    return trainer.train()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/engine/defaults.py\", line 486, in train\n",
            "    super().train(self.start_iter, self.max_iter)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/engine/train_loop.py\", line 156, in train\n",
            "    self.after_step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/engine/train_loop.py\", line 190, in after_step\n",
            "    h.after_step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/engine/hooks.py\", line 556, in after_step\n",
            "    self._do_eval()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/engine/hooks.py\", line 529, in _do_eval\n",
            "    results = self._func()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/engine/defaults.py\", line 455, in test_and_save_results\n",
            "    self._last_eval_results = self.test(self.cfg, self.model)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/engine/defaults.py\", line 619, in test\n",
            "    results_i = inference_on_dataset(model, data_loader, evaluator)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/evaluation/evaluator.py\", line 213, in inference_on_dataset\n",
            "    results = evaluator.evaluate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/evaluation/evaluator.py\", line 93, in evaluate\n",
            "    result = evaluator.evaluate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/evaluation/coco_evaluation.py\", line 206, in evaluate\n",
            "    self._eval_predictions(predictions, img_ids=img_ids)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/detectron2/evaluation/coco_evaluation.py\", line 235, in _eval_predictions\n",
            "    assert min(all_contiguous_ids) == 0 and max(all_contiguous_ids) == num_classes - 1\n",
            "AssertionError\n"
          ]
        }
      ],
      "source": [
        "!python train_net.py --num-gpus 1 --config-file configs/coco/panoptic-segmentation/maskdino_R50_bs16_50ep_3s_dowsample1_2048.yaml \\\n",
        "  SOLVER.IMS_PER_BATCH 2 SOLVER.BASE_LR 0.0000125 MODEL.WEIGHTS ../drive/MyDrive/AAAAA/v3-67_model_0004999.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_Dl500Q0Faq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfcb678c-2372-48c6-b611-dd431cee4c89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MaskDINO\n"
          ]
        }
      ],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3JePYEc0HZu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b47e304-d0b1-483f-f37f-b38df19e0e9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'MaskDINO/output/log.txt' -> 'drive/MyDrive/CCCCC_bert/output/log.txt'\n",
            "'MaskDINO/output/config.yaml' -> 'drive/MyDrive/CCCCC_bert/output/config.yaml'\n",
            "'MaskDINO/output/metrics.json' -> 'drive/MyDrive/CCCCC_bert/output/metrics.json'\n",
            "'MaskDINO/output/events.out.tfevents.1708363851.01850670854b.17574.0' -> 'drive/MyDrive/CCCCC_bert/output/events.out.tfevents.1708363851.01850670854b.17574.0'\n",
            "'MaskDINO/output/model_0004999.pth' -> 'drive/MyDrive/CCCCC_bert/output/model_0004999.pth'\n",
            "'MaskDINO/output/last_checkpoint' -> 'drive/MyDrive/CCCCC_bert/output/last_checkpoint'\n",
            "'MaskDINO/output/inference/instances_predictions.pth' -> 'drive/MyDrive/CCCCC_bert/output/inference/instances_predictions.pth'\n",
            "'MaskDINO/output/inference/predictions.json' -> 'drive/MyDrive/CCCCC_bert/output/inference/predictions.json'\n"
          ]
        }
      ],
      "source": [
        "%cp -av MaskDINO/output/ drive/MyDrive/CCCCC_bert/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4923H1ck0d4A"
      },
      "outputs": [],
      "source": [
        "%cd MaskDINO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "js1ezUuynJs9"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePnU6avPQVZn"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbt6m-vKY3lw"
      },
      "outputs": [],
      "source": [
        "!python train_net.py --eval-only --num-gpus 1 --config-file configs/coco/panoptic-segmentation/maskdino_R50_bs16_50ep_3s_dowsample1_2048.yaml \\\n",
        "  MODEL.WEIGHTS  ../drive/MyDrive/AAAAA/v3-66_model_0004999.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfrzHfP6QlWy"
      },
      "source": [
        "# Model Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGvTCEhRjQh2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e38a9da5-9956-4fc7-a947-5d6ef6794461"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MaskDINO/demo\n"
          ]
        }
      ],
      "source": [
        "%cd demo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (OPTIONAL) Parts of program to be modified if you use custom dataset\n",
        "\n",
        "The modified parts are used to anticipate an unknown class"
      ],
      "metadata": {
        "id": "q_BWhYHJLJok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### visualizer.py in detectron/utils\n",
        "\n",
        "for image demo"
      ],
      "metadata": {
        "id": "c309K0Z0LUkR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "import colorsys\n",
        "import logging\n",
        "import math\n",
        "import numpy as np\n",
        "from enum import Enum, unique\n",
        "import cv2\n",
        "import matplotlib as mpl\n",
        "import matplotlib.colors as mplc\n",
        "import matplotlib.figure as mplfigure\n",
        "import pycocotools.mask as mask_util\n",
        "import torch\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
        "from PIL import Image\n",
        "\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.structures import BitMasks, Boxes, BoxMode, Keypoints, PolygonMasks, RotatedBoxes\n",
        "from detectron2.utils.file_io import PathManager\n",
        "\n",
        "from .colormap import random_color\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "__all__ = [\"ColorMode\", \"VisImage\", \"Visualizer\"]\n",
        "\n",
        "\n",
        "_SMALL_OBJECT_AREA_THRESH = 1000\n",
        "_LARGE_MASK_AREA_THRESH = 120000\n",
        "_OFF_WHITE = (1.0, 1.0, 240.0 / 255)\n",
        "_BLACK = (0, 0, 0)\n",
        "_RED = (1.0, 0, 0)\n",
        "\n",
        "_KEYPOINT_THRESHOLD = 0.05\n",
        "\n",
        "\n",
        "@unique\n",
        "class ColorMode(Enum):\n",
        "    \"\"\"\n",
        "    Enum of different color modes to use for instance visualizations.\n",
        "    \"\"\"\n",
        "\n",
        "    IMAGE = 0\n",
        "    \"\"\"\n",
        "    Picks a random color for every instance and overlay segmentations with low opacity.\n",
        "    \"\"\"\n",
        "    SEGMENTATION = 1\n",
        "    \"\"\"\n",
        "    Let instances of the same category have similar colors\n",
        "    (from metadata.thing_colors), and overlay them with\n",
        "    high opacity. This provides more attention on the quality of segmentation.\n",
        "    \"\"\"\n",
        "    IMAGE_BW = 2\n",
        "    \"\"\"\n",
        "    Same as IMAGE, but convert all areas without masks to gray-scale.\n",
        "    Only available for drawing per-instance mask predictions.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "class GenericMask:\n",
        "    \"\"\"\n",
        "    Attribute:\n",
        "        polygons (list[ndarray]): list[ndarray]: polygons for this mask.\n",
        "            Each ndarray has format [x, y, x, y, ...]\n",
        "        mask (ndarray): a binary mask\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mask_or_polygons, height, width):\n",
        "        self._mask = self._polygons = self._has_holes = None\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "\n",
        "        m = mask_or_polygons\n",
        "        if isinstance(m, dict):\n",
        "            # RLEs\n",
        "            assert \"counts\" in m and \"size\" in m\n",
        "            if isinstance(m[\"counts\"], list):  # uncompressed RLEs\n",
        "                h, w = m[\"size\"]\n",
        "                assert h == height and w == width\n",
        "                m = mask_util.frPyObjects(m, h, w)\n",
        "            self._mask = mask_util.decode(m)[:, :]\n",
        "            return\n",
        "\n",
        "        if isinstance(m, list):  # list[ndarray]\n",
        "            self._polygons = [np.asarray(x).reshape(-1) for x in m]\n",
        "            return\n",
        "\n",
        "        if isinstance(m, np.ndarray):  # assumed to be a binary mask\n",
        "            assert m.shape[1] != 2, m.shape\n",
        "            assert m.shape == (\n",
        "                height,\n",
        "                width,\n",
        "            ), f\"mask shape: {m.shape}, target dims: {height}, {width}\"\n",
        "            self._mask = m.astype(\"uint8\")\n",
        "            return\n",
        "\n",
        "        raise ValueError(\"GenericMask cannot handle object {} of type '{}'\".format(m, type(m)))\n",
        "\n",
        "    @property\n",
        "    def mask(self):\n",
        "        if self._mask is None:\n",
        "            self._mask = self.polygons_to_mask(self._polygons)\n",
        "        return self._mask\n",
        "\n",
        "    @property\n",
        "    def polygons(self):\n",
        "        if self._polygons is None:\n",
        "            self._polygons, self._has_holes = self.mask_to_polygons(self._mask)\n",
        "        return self._polygons\n",
        "\n",
        "    @property\n",
        "    def has_holes(self):\n",
        "        if self._has_holes is None:\n",
        "            if self._mask is not None:\n",
        "                self._polygons, self._has_holes = self.mask_to_polygons(self._mask)\n",
        "            else:\n",
        "                self._has_holes = False  # if original format is polygon, does not have holes\n",
        "        return self._has_holes\n",
        "\n",
        "    def mask_to_polygons(self, mask):\n",
        "        # cv2.RETR_CCOMP flag retrieves all the contours and arranges them to a 2-level\n",
        "        # hierarchy. External contours (boundary) of the object are placed in hierarchy-1.\n",
        "        # Internal contours (holes) are placed in hierarchy-2.\n",
        "        # cv2.CHAIN_APPROX_NONE flag gets vertices of polygons from contours.\n",
        "        mask = np.ascontiguousarray(mask)  # some versions of cv2 does not support incontiguous arr\n",
        "        res = cv2.findContours(mask.astype(\"uint8\"), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n",
        "        hierarchy = res[-1]\n",
        "        if hierarchy is None:  # empty mask\n",
        "            return [], False\n",
        "        has_holes = (hierarchy.reshape(-1, 4)[:, 3] >= 0).sum() > 0\n",
        "        res = res[-2]\n",
        "        res = [x.flatten() for x in res]\n",
        "        # These coordinates from OpenCV are integers in range [0, W-1 or H-1].\n",
        "        # We add 0.5 to turn them into real-value coordinate space. A better solution\n",
        "        # would be to first +0.5 and then dilate the returned polygon by 0.5.\n",
        "        res = [x + 0.5 for x in res if len(x) >= 6]\n",
        "        return res, has_holes\n",
        "\n",
        "    def polygons_to_mask(self, polygons):\n",
        "        rle = mask_util.frPyObjects(polygons, self.height, self.width)\n",
        "        rle = mask_util.merge(rle)\n",
        "        return mask_util.decode(rle)[:, :]\n",
        "\n",
        "    def area(self):\n",
        "        return self.mask.sum()\n",
        "\n",
        "    def bbox(self):\n",
        "        p = mask_util.frPyObjects(self.polygons, self.height, self.width)\n",
        "        p = mask_util.merge(p)\n",
        "        bbox = mask_util.toBbox(p)\n",
        "        bbox[2] += bbox[0]\n",
        "        bbox[3] += bbox[1]\n",
        "        return bbox\n",
        "\n",
        "\n",
        "class _PanopticPrediction:\n",
        "    \"\"\"\n",
        "    Unify different panoptic annotation/prediction formats\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, panoptic_seg, segments_info, metadata=None):\n",
        "        if segments_info is None:\n",
        "            assert metadata is not None\n",
        "            # If \"segments_info\" is None, we assume \"panoptic_img\" is a\n",
        "            # H*W int32 image storing the panoptic_id in the format of\n",
        "            # category_id * label_divisor + instance_id. We reserve -1 for\n",
        "            # VOID label.\n",
        "            label_divisor = metadata.label_divisor\n",
        "            segments_info = []\n",
        "            for panoptic_label in np.unique(panoptic_seg.numpy()):\n",
        "                if panoptic_label == -1:\n",
        "                    # VOID region.\n",
        "                    continue\n",
        "                pred_class = panoptic_label // label_divisor\n",
        "                isthing = pred_class in metadata.thing_dataset_id_to_contiguous_id.values()\n",
        "                segments_info.append(\n",
        "                    {\n",
        "                        \"id\": int(panoptic_label),\n",
        "                        \"category_id\": int(pred_class),\n",
        "                        \"isthing\": bool(isthing),\n",
        "                    }\n",
        "                )\n",
        "        del metadata\n",
        "\n",
        "        self._seg = panoptic_seg\n",
        "\n",
        "        self._sinfo = {s[\"id\"]: s for s in segments_info}  # seg id -> seg info\n",
        "        segment_ids, areas = torch.unique(panoptic_seg, sorted=True, return_counts=True)\n",
        "        areas = areas.numpy()\n",
        "        sorted_idxs = np.argsort(-areas)\n",
        "        self._seg_ids, self._seg_areas = segment_ids[sorted_idxs], areas[sorted_idxs]\n",
        "        self._seg_ids = self._seg_ids.tolist()\n",
        "        for sid, area in zip(self._seg_ids, self._seg_areas):\n",
        "            if sid in self._sinfo:\n",
        "                self._sinfo[sid][\"area\"] = float(area)\n",
        "\n",
        "    def non_empty_mask(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            (H, W) array, a mask for all pixels that have a prediction\n",
        "        \"\"\"\n",
        "        empty_ids = []\n",
        "        for id in self._seg_ids:\n",
        "            if id not in self._sinfo:\n",
        "                empty_ids.append(id)\n",
        "        if len(empty_ids) == 0:\n",
        "            return np.zeros(self._seg.shape, dtype=np.uint8)\n",
        "        assert (\n",
        "            len(empty_ids) == 1\n",
        "        ), \">1 ids corresponds to no labels. This is currently not supported\"\n",
        "        return (self._seg != empty_ids[0]).numpy().astype(bool)\n",
        "\n",
        "    def semantic_masks(self):\n",
        "        for sid in self._seg_ids:\n",
        "            sinfo = self._sinfo.get(sid)\n",
        "            if sinfo is None or sinfo[\"isthing\"]:\n",
        "                # Some pixels (e.g. id 0 in PanopticFPN) have no instance or semantic predictions.\n",
        "                continue\n",
        "            yield (self._seg == sid).numpy().astype(bool), sinfo\n",
        "\n",
        "    def instance_masks(self):\n",
        "        for sid in self._seg_ids:\n",
        "            sinfo = self._sinfo.get(sid)\n",
        "            if sinfo is None or not sinfo[\"isthing\"]:\n",
        "                continue\n",
        "            mask = (self._seg == sid).numpy().astype(bool)\n",
        "            if mask.sum() > 0:\n",
        "                yield mask, sinfo\n",
        "\n",
        "\n",
        "def _create_text_labels(classes, scores, class_names, is_crowd=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        classes (list[int] or None):\n",
        "        scores (list[float] or None):\n",
        "        class_names (list[str] or None):\n",
        "        is_crowd (list[bool] or None):\n",
        "\n",
        "    Returns:\n",
        "        list[str] or None\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    if classes is not None:\n",
        "        if class_names is not None and len(class_names) > 0:\n",
        "            # with open(\"cls_name.txt\", \"a\") as debug_file:\n",
        "            #     debug_file.write(f\"classses: {classes} \\n classes_name: {class_names} \\n\")\n",
        "            # labels = [class_names[i] for i in classes]\n",
        "            for i in classes:\n",
        "              if i < len(class_names):\n",
        "                labels.append(class_names[i])\n",
        "              else:\n",
        "                labels.append(\"Unknown Class\")\n",
        "        else:\n",
        "            labels = [str(i) for i in classes]\n",
        "    if scores is not None:\n",
        "        if labels is None:\n",
        "            labels = [\"{:.0f}%\".format(s * 100) for s in scores]\n",
        "        else:\n",
        "            labels = [\"{} {:.0f}%\".format(l, s * 100) for l, s in zip(labels, scores)]\n",
        "    if labels is not None and is_crowd is not None:\n",
        "        labels = [l + (\"|crowd\" if crowd else \"\") for l, crowd in zip(labels, is_crowd)]\n",
        "    return labels\n",
        "\n",
        "\n",
        "class VisImage:\n",
        "    def __init__(self, img, scale=1.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (ndarray): an RGB image of shape (H, W, 3) in range [0, 255].\n",
        "            scale (float): scale the input image\n",
        "        \"\"\"\n",
        "        self.img = img\n",
        "        self.scale = scale\n",
        "        self.width, self.height = img.shape[1], img.shape[0]\n",
        "        self._setup_figure(img)\n",
        "\n",
        "    def _setup_figure(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            Same as in :meth:`__init__()`.\n",
        "\n",
        "        Returns:\n",
        "            fig (matplotlib.pyplot.figure): top level container for all the image plot elements.\n",
        "            ax (matplotlib.pyplot.Axes): contains figure elements and sets the coordinate system.\n",
        "        \"\"\"\n",
        "        fig = mplfigure.Figure(frameon=False)\n",
        "        self.dpi = fig.get_dpi()\n",
        "        # add a small 1e-2 to avoid precision lost due to matplotlib's truncation\n",
        "        # (https://github.com/matplotlib/matplotlib/issues/15363)\n",
        "        fig.set_size_inches(\n",
        "            (self.width * self.scale + 1e-2) / self.dpi,\n",
        "            (self.height * self.scale + 1e-2) / self.dpi,\n",
        "        )\n",
        "        self.canvas = FigureCanvasAgg(fig)\n",
        "        # self.canvas = mpl.backends.backend_cairo.FigureCanvasCairo(fig)\n",
        "        ax = fig.add_axes([0.0, 0.0, 1.0, 1.0])\n",
        "        ax.axis(\"off\")\n",
        "        self.fig = fig\n",
        "        self.ax = ax\n",
        "        self.reset_image(img)\n",
        "\n",
        "    def reset_image(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img: same as in __init__\n",
        "        \"\"\"\n",
        "        img = img.astype(\"uint8\")\n",
        "        self.ax.imshow(img, extent=(0, self.width, self.height, 0), interpolation=\"nearest\")\n",
        "\n",
        "    def save(self, filepath):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            filepath (str): a string that contains the absolute path, including the file name, where\n",
        "                the visualized image will be saved.\n",
        "        \"\"\"\n",
        "        self.fig.savefig(filepath)\n",
        "\n",
        "    def get_image(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            ndarray:\n",
        "                the visualized image of shape (H, W, 3) (RGB) in uint8 type.\n",
        "                The shape is scaled w.r.t the input image using the given `scale` argument.\n",
        "        \"\"\"\n",
        "        canvas = self.canvas\n",
        "        s, (width, height) = canvas.print_to_buffer()\n",
        "        # buf = io.BytesIO()  # works for cairo backend\n",
        "        # canvas.print_rgba(buf)\n",
        "        # width, height = self.width, self.height\n",
        "        # s = buf.getvalue()\n",
        "\n",
        "        buffer = np.frombuffer(s, dtype=\"uint8\")\n",
        "\n",
        "        img_rgba = buffer.reshape(height, width, 4)\n",
        "        rgb, alpha = np.split(img_rgba, [3], axis=2)\n",
        "        return rgb.astype(\"uint8\")\n",
        "\n",
        "\n",
        "class Visualizer:\n",
        "    \"\"\"\n",
        "    Visualizer that draws data about detection/segmentation on images.\n",
        "\n",
        "    It contains methods like `draw_{text,box,circle,line,binary_mask,polygon}`\n",
        "    that draw primitive objects to images, as well as high-level wrappers like\n",
        "    `draw_{instance_predictions,sem_seg,panoptic_seg_predictions,dataset_dict}`\n",
        "    that draw composite data in some pre-defined style.\n",
        "\n",
        "    Note that the exact visualization style for the high-level wrappers are subject to change.\n",
        "    Style such as color, opacity, label contents, visibility of labels, or even the visibility\n",
        "    of objects themselves (e.g. when the object is too small) may change according\n",
        "    to different heuristics, as long as the results still look visually reasonable.\n",
        "\n",
        "    To obtain a consistent style, you can implement custom drawing functions with the\n",
        "    abovementioned primitive methods instead. If you need more customized visualization\n",
        "    styles, you can process the data yourself following their format documented in\n",
        "    tutorials (:doc:`/tutorials/models`, :doc:`/tutorials/datasets`). This class does not\n",
        "    intend to satisfy everyone's preference on drawing styles.\n",
        "\n",
        "    This visualizer focuses on high rendering quality rather than performance. It is not\n",
        "    designed to be used for real-time applications.\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO implement a fast, rasterized version using OpenCV\n",
        "\n",
        "    def __init__(self, img_rgb, metadata=None, scale=1.0, instance_mode=ColorMode.IMAGE):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_rgb: a numpy array of shape (H, W, C), where H and W correspond to\n",
        "                the height and width of the image respectively. C is the number of\n",
        "                color channels. The image is required to be in RGB format since that\n",
        "                is a requirement of the Matplotlib library. The image is also expected\n",
        "                to be in the range [0, 255].\n",
        "            metadata (Metadata): dataset metadata (e.g. class names and colors)\n",
        "            instance_mode (ColorMode): defines one of the pre-defined style for drawing\n",
        "                instances on an image.\n",
        "        \"\"\"\n",
        "        self.img = np.asarray(img_rgb).clip(0, 255).astype(np.uint8)\n",
        "        if metadata is None:\n",
        "            metadata = MetadataCatalog.get(\"__nonexist__\")\n",
        "        self.metadata = metadata\n",
        "        self.output = VisImage(self.img, scale=scale)\n",
        "        self.cpu_device = torch.device(\"cpu\")\n",
        "\n",
        "        # too small texts are useless, therefore clamp to 9\n",
        "        self._default_font_size = max(\n",
        "            np.sqrt(self.output.height * self.output.width) // 90, 10 // scale\n",
        "        )\n",
        "        self._instance_mode = instance_mode\n",
        "        self.keypoint_threshold = _KEYPOINT_THRESHOLD\n",
        "\n",
        "    def draw_instance_predictions(self, predictions):\n",
        "        \"\"\"\n",
        "        Draw instance-level prediction results on an image.\n",
        "\n",
        "        Args:\n",
        "            predictions (Instances): the output of an instance detection/segmentation\n",
        "                model. Following fields will be used to draw:\n",
        "                \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\" (or \"pred_masks_rle\").\n",
        "\n",
        "        Returns:\n",
        "            output (VisImage): image object with visualizations.\n",
        "        \"\"\"\n",
        "        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n",
        "        scores = predictions.scores if predictions.has(\"scores\") else None\n",
        "        classes = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\n",
        "        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n",
        "        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n",
        "\n",
        "        if predictions.has(\"pred_masks\"):\n",
        "            masks = np.asarray(predictions.pred_masks)\n",
        "            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n",
        "        else:\n",
        "            masks = None\n",
        "\n",
        "        if self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\n",
        "            colors = [\n",
        "                self._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in classes\n",
        "            ]\n",
        "            alpha = 0.8\n",
        "        else:\n",
        "            colors = None\n",
        "            alpha = 0.5\n",
        "\n",
        "        if self._instance_mode == ColorMode.IMAGE_BW:\n",
        "            self.output.reset_image(\n",
        "                self._create_grayscale_image(\n",
        "                    (predictions.pred_masks.any(dim=0) > 0).numpy()\n",
        "                    if predictions.has(\"pred_masks\")\n",
        "                    else None\n",
        "                )\n",
        "            )\n",
        "            alpha = 0.3\n",
        "\n",
        "        self.overlay_instances(\n",
        "            masks=masks,\n",
        "            boxes=boxes,\n",
        "            labels=labels,\n",
        "            keypoints=keypoints,\n",
        "            assigned_colors=colors,\n",
        "            alpha=alpha,\n",
        "        )\n",
        "        return self.output\n",
        "\n",
        "    def draw_sem_seg(self, sem_seg, area_threshold=None, alpha=0.8):\n",
        "        \"\"\"\n",
        "        Draw semantic segmentation predictions/labels.\n",
        "\n",
        "        Args:\n",
        "            sem_seg (Tensor or ndarray): the segmentation of shape (H, W).\n",
        "                Each value is the integer label of the pixel.\n",
        "            area_threshold (int): segments with less than `area_threshold` are not drawn.\n",
        "            alpha (float): the larger it is, the more opaque the segmentations are.\n",
        "\n",
        "        Returns:\n",
        "            output (VisImage): image object with visualizations.\n",
        "        \"\"\"\n",
        "        if isinstance(sem_seg, torch.Tensor):\n",
        "            sem_seg = sem_seg.numpy()\n",
        "        labels, areas = np.unique(sem_seg, return_counts=True)\n",
        "        sorted_idxs = np.argsort(-areas).tolist()\n",
        "        labels = labels[sorted_idxs]\n",
        "        for label in filter(lambda l: l < len(self.metadata.stuff_classes), labels):\n",
        "            try:\n",
        "                mask_color = [x / 255 for x in self.metadata.stuff_colors[label]]\n",
        "            except (AttributeError, IndexError):\n",
        "                mask_color = None\n",
        "\n",
        "            binary_mask = (sem_seg == label).astype(np.uint8)\n",
        "            text = self.metadata.stuff_classes[label]\n",
        "            self.draw_binary_mask(\n",
        "                binary_mask,\n",
        "                color=mask_color,\n",
        "                edge_color=_OFF_WHITE,\n",
        "                text=text,\n",
        "                alpha=alpha,\n",
        "                area_threshold=area_threshold,\n",
        "            )\n",
        "        return self.output\n",
        "\n",
        "    def draw_panoptic_seg(self, panoptic_seg, segments_info, area_threshold=None, alpha=0.7):\n",
        "        \"\"\"\n",
        "        Draw panoptic prediction annotations or results.\n",
        "\n",
        "        Args:\n",
        "            panoptic_seg (Tensor): of shape (height, width) where the values are ids for each\n",
        "                segment.\n",
        "            segments_info (list[dict] or None): Describe each segment in `panoptic_seg`.\n",
        "                If it is a ``list[dict]``, each dict contains keys \"id\", \"category_id\".\n",
        "                If None, category id of each pixel is computed by\n",
        "                ``pixel // metadata.label_divisor``.\n",
        "            area_threshold (int): stuff segments with less than `area_threshold` are not drawn.\n",
        "\n",
        "        Returns:\n",
        "            output (VisImage): image object with visualizations.\n",
        "        \"\"\"\n",
        "        pred = _PanopticPrediction(panoptic_seg, segments_info, self.metadata)\n",
        "\n",
        "        if self._instance_mode == ColorMode.IMAGE_BW:\n",
        "            self.output.reset_image(self._create_grayscale_image(pred.non_empty_mask()))\n",
        "\n",
        "        # draw mask for all semantic segments first i.e. \"stuff\"\n",
        "        for mask, sinfo in pred.semantic_masks():\n",
        "            category_idx = sinfo[\"category_id\"]\n",
        "            try:\n",
        "                mask_color = [x / 255 for x in self.metadata.stuff_colors[category_idx]]\n",
        "            except AttributeError:\n",
        "                mask_color = None\n",
        "\n",
        "            text = self.metadata.stuff_classes[category_idx]\n",
        "            self.draw_binary_mask(\n",
        "                mask,\n",
        "                color=mask_color,\n",
        "                edge_color=_OFF_WHITE,\n",
        "                text=text,\n",
        "                alpha=alpha,\n",
        "                area_threshold=area_threshold,\n",
        "            )\n",
        "\n",
        "        # draw mask for all instances second\n",
        "        all_instances = list(pred.instance_masks())\n",
        "        if len(all_instances) == 0:\n",
        "            return self.output\n",
        "        masks, sinfo = list(zip(*all_instances))\n",
        "        category_ids = [x[\"category_id\"] for x in sinfo]\n",
        "\n",
        "        try:\n",
        "            scores = [x[\"score\"] for x in sinfo]\n",
        "        except KeyError:\n",
        "            scores = None\n",
        "        labels = _create_text_labels(\n",
        "            category_ids, scores, self.metadata.thing_classes, [x.get(\"iscrowd\", 0) for x in sinfo]\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            colors = []\n",
        "            for c in category_ids:\n",
        "              if c < len(self.metadata.thing_colors):\n",
        "                colors.append(self._jitter([x / 255 for x in self.metadata.thing_colors[c]]))\n",
        "              else:\n",
        "                colors.append([0, 0, 0])\n",
        "            # colors = [\n",
        "            #     self._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in category_ids\n",
        "            # ]\n",
        "        except AttributeError:\n",
        "            colors = None\n",
        "        self.overlay_instances(masks=masks, labels=labels, assigned_colors=colors, alpha=alpha)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    draw_panoptic_seg_predictions = draw_panoptic_seg  # backward compatibility\n",
        "\n",
        "    def draw_dataset_dict(self, dic):\n",
        "        \"\"\"\n",
        "        Draw annotations/segmentations in Detectron2 Dataset format.\n",
        "\n",
        "        Args:\n",
        "            dic (dict): annotation/segmentation data of one image, in Detectron2 Dataset format.\n",
        "\n",
        "        Returns:\n",
        "            output (VisImage): image object with visualizations.\n",
        "        \"\"\"\n",
        "        annos = dic.get(\"annotations\", None)\n",
        "        if annos:\n",
        "            if \"segmentation\" in annos[0]:\n",
        "                masks = [x[\"segmentation\"] for x in annos]\n",
        "            else:\n",
        "                masks = None\n",
        "            if \"keypoints\" in annos[0]:\n",
        "                keypts = [x[\"keypoints\"] for x in annos]\n",
        "                keypts = np.array(keypts).reshape(len(annos), -1, 3)\n",
        "            else:\n",
        "                keypts = None\n",
        "\n",
        "            boxes = [\n",
        "                BoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS)\n",
        "                if len(x[\"bbox\"]) == 4\n",
        "                else x[\"bbox\"]\n",
        "                for x in annos\n",
        "            ]\n",
        "\n",
        "            colors = None\n",
        "            category_ids = [x[\"category_id\"] for x in annos]\n",
        "            if self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\n",
        "                colors = [\n",
        "                    self._jitter([x / 255 for x in self.metadata.thing_colors[c]])\n",
        "                    for c in category_ids\n",
        "                ]\n",
        "            names = self.metadata.get(\"thing_classes\", None)\n",
        "            labels = _create_text_labels(\n",
        "                category_ids,\n",
        "                scores=None,\n",
        "                class_names=names,\n",
        "                is_crowd=[x.get(\"iscrowd\", 0) for x in annos],\n",
        "            )\n",
        "            self.overlay_instances(\n",
        "                labels=labels, boxes=boxes, masks=masks, keypoints=keypts, assigned_colors=colors\n",
        "            )\n",
        "\n",
        "        sem_seg = dic.get(\"sem_seg\", None)\n",
        "        if sem_seg is None and \"sem_seg_file_name\" in dic:\n",
        "            with PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\n",
        "                sem_seg = Image.open(f)\n",
        "                sem_seg = np.asarray(sem_seg, dtype=\"uint8\")\n",
        "        if sem_seg is not None:\n",
        "            self.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\n",
        "\n",
        "        pan_seg = dic.get(\"pan_seg\", None)\n",
        "        if pan_seg is None and \"pan_seg_file_name\" in dic:\n",
        "            with PathManager.open(dic[\"pan_seg_file_name\"], \"rb\") as f:\n",
        "                pan_seg = Image.open(f)\n",
        "                pan_seg = np.asarray(pan_seg)\n",
        "                from panopticapi.utils import rgb2id\n",
        "\n",
        "                pan_seg = rgb2id(pan_seg)\n",
        "        if pan_seg is not None:\n",
        "            segments_info = dic[\"segments_info\"]\n",
        "            pan_seg = torch.tensor(pan_seg)\n",
        "            self.draw_panoptic_seg(pan_seg, segments_info, area_threshold=0, alpha=0.5)\n",
        "        return self.output\n",
        "\n",
        "    def overlay_instances(\n",
        "        self,\n",
        "        *,\n",
        "        boxes=None,\n",
        "        labels=None,\n",
        "        masks=None,\n",
        "        keypoints=None,\n",
        "        assigned_colors=None,\n",
        "        alpha=0.5,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            boxes (Boxes, RotatedBoxes or ndarray): either a :class:`Boxes`,\n",
        "                or an Nx4 numpy array of XYXY_ABS format for the N objects in a single image,\n",
        "                or a :class:`RotatedBoxes`,\n",
        "                or an Nx5 numpy array of (x_center, y_center, width, height, angle_degrees) format\n",
        "                for the N objects in a single image,\n",
        "            labels (list[str]): the text to be displayed for each instance.\n",
        "            masks (masks-like object): Supported types are:\n",
        "\n",
        "                * :class:`detectron2.structures.PolygonMasks`,\n",
        "                  :class:`detectron2.structures.BitMasks`.\n",
        "                * list[list[ndarray]]: contains the segmentation masks for all objects in one image.\n",
        "                  The first level of the list corresponds to individual instances. The second\n",
        "                  level to all the polygon that compose the instance, and the third level\n",
        "                  to the polygon coordinates. The third level should have the format of\n",
        "                  [x0, y0, x1, y1, ..., xn, yn] (n >= 3).\n",
        "                * list[ndarray]: each ndarray is a binary mask of shape (H, W).\n",
        "                * list[dict]: each dict is a COCO-style RLE.\n",
        "            keypoints (Keypoint or array like): an array-like object of shape (N, K, 3),\n",
        "                where the N is the number of instances and K is the number of keypoints.\n",
        "                The last dimension corresponds to (x, y, visibility or score).\n",
        "            assigned_colors (list[matplotlib.colors]): a list of colors, where each color\n",
        "                corresponds to each mask or box in the image. Refer to 'matplotlib.colors'\n",
        "                for full list of formats that the colors are accepted in.\n",
        "        Returns:\n",
        "            output (VisImage): image object with visualizations.\n",
        "        \"\"\"\n",
        "        num_instances = 0\n",
        "        if boxes is not None:\n",
        "            boxes = self._convert_boxes(boxes)\n",
        "            num_instances = len(boxes)\n",
        "        if masks is not None:\n",
        "            masks = self._convert_masks(masks)\n",
        "            if num_instances:\n",
        "                assert len(masks) == num_instances\n",
        "            else:\n",
        "                num_instances = len(masks)\n",
        "        if keypoints is not None:\n",
        "            if num_instances:\n",
        "                assert len(keypoints) == num_instances\n",
        "            else:\n",
        "                num_instances = len(keypoints)\n",
        "            keypoints = self._convert_keypoints(keypoints)\n",
        "        if labels is not None:\n",
        "            assert len(labels) == num_instances\n",
        "        if assigned_colors is None:\n",
        "            assigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]\n",
        "        if num_instances == 0:\n",
        "            return self.output\n",
        "        if boxes is not None and boxes.shape[1] == 5:\n",
        "            return self.overlay_rotated_instances(\n",
        "                boxes=boxes, labels=labels, assigned_colors=assigned_colors\n",
        "            )\n",
        "\n",
        "        # Display in largest to smallest order to reduce occlusion.\n",
        "        areas = None\n",
        "        if boxes is not None:\n",
        "            areas = np.prod(boxes[:, 2:] - boxes[:, :2], axis=1)\n",
        "        elif masks is not None:\n",
        "            areas = np.asarray([x.area() for x in masks])\n",
        "\n",
        "        if areas is not None:\n",
        "            sorted_idxs = np.argsort(-areas).tolist()\n",
        "            # Re-order overlapped instances in descending order.\n",
        "            boxes = boxes[sorted_idxs] if boxes is not None else None\n",
        "            labels = [labels[k] for k in sorted_idxs] if labels is not None else None\n",
        "            masks = [masks[idx] for idx in sorted_idxs] if masks is not None else None\n",
        "            assigned_colors = [assigned_colors[idx] for idx in sorted_idxs]\n",
        "            keypoints = keypoints[sorted_idxs] if keypoints is not None else None\n",
        "\n",
        "        for i in range(num_instances):\n",
        "            color = assigned_colors[i]\n",
        "            if boxes is not None:\n",
        "                self.draw_box(boxes[i], edge_color=color)\n",
        "\n",
        "            if masks is not None:\n",
        "                for segment in masks[i].polygons:\n",
        "                    self.draw_polygon(segment.reshape(-1, 2), color, alpha=alpha)\n",
        "\n",
        "            if labels is not None:\n",
        "                # first get a box\n",
        "                if boxes is not None:\n",
        "                    x0, y0, x1, y1 = boxes[i]\n",
        "                    text_pos = (x0, y0)  # if drawing boxes, put text on the box corner.\n",
        "                    horiz_align = \"left\"\n",
        "                elif masks is not None:\n",
        "                    # skip small mask without polygon\n",
        "                    if len(masks[i].polygons) == 0:\n",
        "                        continue\n",
        "\n",
        "                    x0, y0, x1, y1 = masks[i].bbox()\n",
        "\n",
        "                    # draw text in the center (defined by median) when box is not drawn\n",
        "                    # median is less sensitive to outliers.\n",
        "                    text_pos = np.median(masks[i].mask.nonzero(), axis=1)[::-1]\n",
        "                    horiz_align = \"center\"\n",
        "                else:\n",
        "                    continue  # drawing the box confidence for keypoints isn't very useful.\n",
        "                # for small objects, draw text at the side to avoid occlusion\n",
        "                instance_area = (y1 - y0) * (x1 - x0)\n",
        "                if (\n",
        "                    instance_area < _SMALL_OBJECT_AREA_THRESH * self.output.scale\n",
        "                    or y1 - y0 < 40 * self.output.scale\n",
        "                ):\n",
        "                    if y1 >= self.output.height - 5:\n",
        "                        text_pos = (x1, y0)\n",
        "                    else:\n",
        "                        text_pos = (x0, y1)\n",
        "\n",
        "                height_ratio = (y1 - y0) / np.sqrt(self.output.height * self.output.width)\n",
        "                lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n",
        "                font_size = (\n",
        "                    np.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2)\n",
        "                    * 0.5\n",
        "                    * self._default_font_size\n",
        "                )\n",
        "                self.draw_text(\n",
        "                    labels[i],\n",
        "                    text_pos,\n",
        "                    color=lighter_color,\n",
        "                    horizontal_alignment=horiz_align,\n",
        "                    font_size=font_size,\n",
        "                )\n",
        "\n",
        "        # draw keypoints\n",
        "        if keypoints is not None:\n",
        "            for keypoints_per_instance in keypoints:\n",
        "                self.draw_and_connect_keypoints(keypoints_per_instance)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def overlay_rotated_instances(self, boxes=None, labels=None, assigned_colors=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            boxes (ndarray): an Nx5 numpy array of\n",
        "                (x_center, y_center, width, height, angle_degrees) format\n",
        "                for the N objects in a single image.\n",
        "            labels (list[str]): the text to be displayed for each instance.\n",
        "            assigned_colors (list[matplotlib.colors]): a list of colors, where each color\n",
        "                corresponds to each mask or box in the image. Refer to 'matplotlib.colors'\n",
        "                for full list of formats that the colors are accepted in.\n",
        "\n",
        "        Returns:\n",
        "            output (VisImage): image object with visualizations.\n",
        "        \"\"\"\n",
        "        num_instances = len(boxes)\n",
        "\n",
        "        if assigned_colors is None:\n",
        "            assigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]\n",
        "        if num_instances == 0:\n",
        "            return self.output\n",
        "\n",
        "        # Display in largest to smallest order to reduce occlusion.\n",
        "        if boxes is not None:\n",
        "            areas = boxes[:, 2] * boxes[:, 3]\n",
        "\n",
        "        sorted_idxs = np.argsort(-areas).tolist()\n",
        "        # Re-order overlapped instances in descending order.\n",
        "        boxes = boxes[sorted_idxs]\n",
        "        labels = [labels[k] for k in sorted_idxs] if labels is not None else None\n",
        "        colors = [assigned_colors[idx] for idx in sorted_idxs]\n",
        "\n",
        "        for i in range(num_instances):\n",
        "            self.draw_rotated_box_with_label(\n",
        "                boxes[i], edge_color=colors[i], label=labels[i] if labels is not None else None\n",
        "            )\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def draw_and_connect_keypoints(self, keypoints):\n",
        "        \"\"\"\n",
        "        Draws keypoints of an instance and follows the rules for keypoint connections\n",
        "        to draw lines between appropriate keypoints. This follows color heuristics for\n",
        "        line color.\n",
        "\n",
        "        Args:\n",
        "            keypoints (Tensor): a tensor of shape (K, 3), where K is the number of keypoints\n",
        "                and the last dimension corresponds to (x, y, probability).\n",
        "\n",
        "        Returns:\n",
        "            output (VisImage): image object with visualizations.\n",
        "        \"\"\"\n",
        "        visible = {}\n",
        "        keypoint_names = self.metadata.get(\"keypoint_names\")\n",
        "        for idx, keypoint in enumerate(keypoints):\n",
        "\n",
        "            # draw keypoint\n",
        "            x, y, prob = keypoint\n",
        "            if prob > self.keypoint_threshold:\n",
        "                self.draw_circle((x, y), color=_RED)\n",
        "                if keypoint_names:\n",
        "                    keypoint_name = keypoint_names[idx]\n",
        "                    visible[keypoint_name] = (x, y)\n",
        "\n",
        "        if self.metadata.get(\"keypoint_connection_rules\"):\n",
        "            for kp0, kp1, color in self.metadata.keypoint_connection_rules:\n",
        "                if kp0 in visible and kp1 in visible:\n",
        "                    x0, y0 = visible[kp0]\n",
        "                    x1, y1 = visible[kp1]\n",
        "                    color = tuple(x / 255.0 for x in color)\n",
        "                    self.draw_line([x0, x1], [y0, y1], color=color)\n",
        "\n",
        "        # draw lines from nose to mid-shoulder and mid-shoulder to mid-hip\n",
        "        # Note that this strategy is specific to person keypoints.\n",
        "        # For other keypoints, it should just do nothing\n",
        "        try:\n",
        "            ls_x, ls_y = visible[\"left_shoulder\"]\n",
        "            rs_x, rs_y = visible[\"right_shoulder\"]\n",
        "            mid_shoulder_x, mid_shoulder_y = (ls_x + rs_x) / 2, (ls_y + rs_y) / 2\n",
        "        except KeyError:\n",
        "            pass\n",
        "        else:\n",
        "            # draw line from nose to mid-shoulder\n",
        "            nose_x, nose_y = visible.get(\"nose\", (None, None))\n",
        "            if nose_x is not None:\n",
        "                self.draw_line([nose_x, mid_shoulder_x], [nose_y, mid_shoulder_y], color=_RED)\n",
        "\n",
        "            try:\n",
        "                # draw line from mid-shoulder to mid-hip\n",
        "                lh_x, lh_y = visible[\"left_hip\"]\n",
        "                rh_x, rh_y = visible[\"right_hip\"]\n",
        "            except KeyError:\n",
        "                pass\n",
        "            else:\n",
        "                mid_hip_x, mid_hip_y = (lh_x + rh_x) / 2, (lh_y + rh_y) / 2\n",
        "                self.draw_line([mid_hip_x, mid_shoulder_x], [mid_hip_y, mid_shoulder_y], color=_RED)\n",
        "        return self.output\n",
        "\n",
        "    \"\"\"\n",
        "    Primitive drawing functions:\n",
        "    \"\"\"\n",
        "\n",
        "    def draw_text(\n",
        "        self,\n",
        "        text,\n",
        "        position,\n",
        "        *,\n",
        "        font_size=None,\n",
        "        color=\"g\",\n",
        "        horizontal_alignment=\"center\",\n",
        "        rotation=0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            text (str): class label\n",
        "            position (tuple): a tuple of the x and y coordinates to place text on image.\n",
        "            font_size (int, optional): font of the text. If not provided, a font size\n",
        "                proportional to the image width is calculated and used.\n",
        "            color: color of the text. Refer to `matplotlib.colors` for full list\n",
        "                of formats that are accepted.\n",
        "            horizontal_alignment (str): see `matplotlib.text.Text`\n",
        "            rotation: rotation angle in degrees CCW\n",
        "\n",
        "        Returns:\n",
        "            output (VisImage): image object with text drawn.\n",
        "        \"\"\"\n",
        "        if not font_size:\n",
        "            font_size = self._default_font_size\n",
        "\n",
        "        # since the text background is dark, we don't want the text to be dark\n",
        "        color = np.maximum(list(mplc.to_rgb(color)), 0.2)\n",
        "        color[np.argmax(color)] = max(0.8, np.max(color))\n",
        "\n",
        "        x, y = position\n",
        "        self.output.ax.text(\n",
        "            x,\n",
        "            y,\n",
        "            text,\n",
        "            size=font_size * self.output.scale,\n",
        "            family=\"sans-serif\",\n",
        "            bbox={\"facecolor\": \"black\", \"alpha\": 0.8, \"pad\": 0.7, \"edgecolor\": \"none\"},\n",
        "            verticalalignment=\"top\",\n",
        "            horizontalalignment=horizontal_alignment,\n",
        "            color=color,\n",
        "            zorder=10,\n",
        "            rotation=rotation,\n",
        "        )\n",
        "        return self.output\n",
        "\n",
        "    def draw_box(self, box_coord, alpha=0.5, edge_color=\"g\", line_style=\"-\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            box_coord (tuple): a tuple containing x0, y0, x1, y1 coordinates, where x0 and y0\n",
        "                are the coordinates of the image's top left corner. x1 and y1 are the\n",
        "                coordinates of the image's bottom right corner.\n",
        "            alpha (float): blending efficient. Smaller values lead to more transparent masks.\n",
        "            edge_color: color of the outline of the box. Refer to `matplotlib.colors`\n",
        "                for full list of formats that are accepted.\n",
        "            line_style (string): the string to use to create the outline of the boxes.\n",
        "\n",
        "        Returns:\n",
        "            output (VisImage): image object with box drawn.\n",
        "        \"\"\"\n",
        "        x0, y0, x1, y1 = box_coord\n",
        "        width = x1 - x0\n",
        "        height = y1 - y0\n",
        "\n",
        "        linewidth = max(self._default_font_size / 4, 1)\n",
        "\n",
        "        self.output.ax.add_patch(\n",
        "            mpl.patches.Rectangle(\n",
        "                (x0, y0),\n",
        "                width,\n",
        "                height,\n",
        "                fill=False,\n",
        "                edgecolor=edge_color,\n",
        "                linewidth=linewidth * self.output.scale,\n",
        "                alpha=alpha,\n",
        "                linestyle=line_style,\n",
        "            )\n",
        "        )\n",
        "        return self.output\n",
        "\n",
        "    def draw_rotated_box_with_label(\n",
        "        self, rotated_box, alpha=0.5, edge_color=\"g\", line_style=\"-\", label=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Draw a rotated box with label on its top-left corner.\n",
        "\n",
        "        Args:\n",
        "            rotated_box (tuple): a tuple containing (cnt_x, cnt_y, w, h, angle),\n",
        "                where cnt_x and cnt_y are the center coordinates of the box.\n",
        "                w and h are the width and height of the box. angle represents how\n",
        "                many degrees the box is rotated CCW with regard to the 0-degree box.\n",
        "            alpha (float): blending efficient. Smaller values lead to more transparent masks.\n",
        "            edge_color: color of the outline of the box. Refer to `matplotlib.colors`\n",
        "                for full list of formats that are accepted.\n",
        "            line_style (string): the string to use to create the outline of the boxes.\n",
        "            label (string): label for rotated box. It will not be rendered when set to None.\n",
        "\n",
        "        Returns:\n",
        "            output (VisImage): image object with box drawn.\n",
        "        \"\"\"\n",
        "        cnt_x, cnt_y, w, h, angle = rotated_box\n",
        "        area = w * h\n",
        "        # use thinner lines when the box is small\n",
        "        linewidth = self._default_font_size / (\n",
        "            6 if area < _SMALL_OBJECT_AREA_THRESH * self.output.scale else 3\n",
        "        )\n",
        "\n",
        "        theta = angle * math.pi / 180.0\n",
        "        c = math.cos(theta)\n",
        "        s = math.sin(theta)\n",
        "        rect = [(-w / 2, h / 2), (-w / 2, -h / 2), (w / 2, -h / 2), (w / 2, h / 2)]\n",
        "        # x: left->right ; y: top->down\n",
        "        rotated_rect = [(s * yy + c * xx + cnt_x, c * yy - s * xx + cnt_y) for (xx, yy) in rect]\n",
        "        for k in range(4):\n",
        "            j = (k + 1) % 4\n",
        "            self.draw_line(\n",
        "                [rotated_rect[k][0], rotated_rect[j][0]],\n",
        "                [rotated_rect[k][1], rotated_rect[j][1]],\n",
        "                color=edge_color,\n",
        "                linestyle=\"--\" if k == 1 else line_style,\n",
        "                linewidth=linewidth,\n",
        "            )\n",
        "\n",
        "        if label is not None:\n",
        "            text_pos = rotated_rect[1]  # topleft corner\n",
        "\n",
        "            height_ratio = h / np.sqrt(self.output.height * self.output.width)\n",
        "            label_color = self._change_color_brightness(edge_color, brightness_factor=0.7)\n",
        "            font_size = (\n",
        "                np.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2) * 0.5 * self._default_font_size\n",
        "            )\n",
        "            self.draw_text(label, text_pos, color=label_color, font_size=font_size, rotation=angle)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def draw_circle(self, circle_coord, color, radius=3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            circle_coord (list(int) or tuple(int)): contains the x and y coordinates\n",
        "                of the center of the circle.\n",
        "            color: color of the polygon. Refer to `matplotlib.colors` for a full list of\n",
        "                formats that are accepted.\n",
        "            radius (int): radius of the circle.\n",
        "\n",
        "        Returns:\n",
        "            output (VisImage): image object with box drawn.\n",
        "        \"\"\"\n",
        "        x, y = circle_coord\n",
        "        self.output.ax.add_patch(\n",
        "            mpl.patches.Circle(circle_coord, radius=radius, fill=True, color=color)\n",
        "        )\n",
        "        return self.output\n",
        "\n",
        "    def draw_line(self, x_data, y_data, color, linestyle=\"-\", linewidth=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x_data (list[int]): a list containing x values of all the points being drawn.\n",
        "                Length of list should match the length of y_data.\n",
        "            y_data (list[int]): a list containing y values of all the points being drawn.\n",
        "                Length of list should match the length of x_data.\n",
        "            color: color of the line. Refer to `matplotlib.colors` for a full list of\n",
        "                formats that are accepted.\n",
        "            linestyle: style of the line. Refer to `matplotlib.lines.Line2D`\n",
        "                for a full list of formats that are accepted.\n",
        "            linewidth (float or None): width of the line. When it's None,\n",
        "                a default value will be computed and used.\n",
        "\n",
        "        Returns:\n",
        "            output (VisImage): image object with line drawn.\n",
        "        \"\"\"\n",
        "        if linewidth is None:\n",
        "            linewidth = self._default_font_size / 3\n",
        "        linewidth = max(linewidth, 1)\n",
        "        self.output.ax.add_line(\n",
        "            mpl.lines.Line2D(\n",
        "                x_data,\n",
        "                y_data,\n",
        "                linewidth=linewidth * self.output.scale,\n",
        "                color=color,\n",
        "                linestyle=linestyle,\n",
        "            )\n",
        "        )\n",
        "        return self.output\n",
        "\n",
        "    def draw_binary_mask(\n",
        "        self, binary_mask, color=None, *, edge_color=None, text=None, alpha=0.5, area_threshold=10\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            binary_mask (ndarray): numpy array of shape (H, W), where H is the image height and\n",
        "                W is the image width. Each value in the array is either a 0 or 1 value of uint8\n",
        "                type.\n",
        "            color: color of the mask. Refer to `matplotlib.colors` for a full list of\n",
        "                formats that are accepted. If None, will pick a random color.\n",
        "            edge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\n",
        "                full list of formats that are accepted.\n",
        "            text (str): if None, will be drawn on the object\n",
        "            alpha (float): blending efficient. Smaller values lead to more transparent masks.\n",
        "            area_threshold (float): a connected component smaller than this area will not be shown.\n",
        "\n",
        "        Returns:\n",
        "            output (VisImage): image object with mask drawn.\n",
        "        \"\"\"\n",
        "        if color is None:\n",
        "            color = random_color(rgb=True, maximum=1)\n",
        "        color = mplc.to_rgb(color)\n",
        "\n",
        "        has_valid_segment = False\n",
        "        binary_mask = binary_mask.astype(\"uint8\")  # opencv needs uint8\n",
        "        mask = GenericMask(binary_mask, self.output.height, self.output.width)\n",
        "        shape2d = (binary_mask.shape[0], binary_mask.shape[1])\n",
        "\n",
        "        if not mask.has_holes:\n",
        "            # draw polygons for regular masks\n",
        "            for segment in mask.polygons:\n",
        "                area = mask_util.area(mask_util.frPyObjects([segment], shape2d[0], shape2d[1]))\n",
        "                if area < (area_threshold or 0):\n",
        "                    continue\n",
        "                has_valid_segment = True\n",
        "                segment = segment.reshape(-1, 2)\n",
        "                self.draw_polygon(segment, color=color, edge_color=edge_color, alpha=alpha)\n",
        "        else:\n",
        "            # TODO: Use Path/PathPatch to draw vector graphics:\n",
        "            # https://stackoverflow.com/questions/8919719/how-to-plot-a-complex-polygon\n",
        "            rgba = np.zeros(shape2d + (4,), dtype=\"float32\")\n",
        "            rgba[:, :, :3] = color\n",
        "            rgba[:, :, 3] = (mask.mask == 1).astype(\"float32\") * alpha\n",
        "            has_valid_segment = True\n",
        "            self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\n",
        "\n",
        "        if text is not None and has_valid_segment:\n",
        "            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n",
        "            self._draw_text_in_mask(binary_mask, text, lighter_color)\n",
        "        return self.output\n",
        "\n",
        "    def draw_soft_mask(self, soft_mask, color=None, *, text=None, alpha=0.5):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            soft_mask (ndarray): float array of shape (H, W), each value in [0, 1].\n",
        "            color: color of the mask. Refer to `matplotlib.colors` for a full list of\n",
        "                formats that are accepted. If None, will pick a random color.\n",
        "            text (str): if None, will be drawn on the object\n",
        "            alpha (float): blending efficient. Smaller values lead to more transparent masks.\n",
        "\n",
        "        Returns:\n",
        "            output (VisImage): image object with mask drawn.\n",
        "        \"\"\"\n",
        "        if color is None:\n",
        "            color = random_color(rgb=True, maximum=1)\n",
        "        color = mplc.to_rgb(color)\n",
        "\n",
        "        shape2d = (soft_mask.shape[0], soft_mask.shape[1])\n",
        "        rgba = np.zeros(shape2d + (4,), dtype=\"float32\")\n",
        "        rgba[:, :, :3] = color\n",
        "        rgba[:, :, 3] = soft_mask * alpha\n",
        "        self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\n",
        "\n",
        "        if text is not None:\n",
        "            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n",
        "            binary_mask = (soft_mask > 0.5).astype(\"uint8\")\n",
        "            self._draw_text_in_mask(binary_mask, text, lighter_color)\n",
        "        return self.output\n",
        "\n",
        "    def draw_polygon(self, segment, color, edge_color=None, alpha=0.5):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            segment: numpy array of shape Nx2, containing all the points in the polygon.\n",
        "            color: color of the polygon. Refer to `matplotlib.colors` for a full list of\n",
        "                formats that are accepted.\n",
        "            edge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\n",
        "                full list of formats that are accepted. If not provided, a darker shade\n",
        "                of the polygon color will be used instead.\n",
        "            alpha (float): blending efficient. Smaller values lead to more transparent masks.\n",
        "\n",
        "        Returns:\n",
        "            output (VisImage): image object with polygon drawn.\n",
        "        \"\"\"\n",
        "        if edge_color is None:\n",
        "            # make edge color darker than the polygon color\n",
        "            if alpha > 0.8:\n",
        "                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n",
        "            else:\n",
        "                edge_color = color\n",
        "        edge_color = mplc.to_rgb(edge_color) + (1,)\n",
        "\n",
        "        polygon = mpl.patches.Polygon(\n",
        "            segment,\n",
        "            fill=True,\n",
        "            facecolor=mplc.to_rgb(color) + (alpha,),\n",
        "            edgecolor=edge_color,\n",
        "            linewidth=max(self._default_font_size // 15 * self.output.scale, 1),\n",
        "        )\n",
        "        self.output.ax.add_patch(polygon)\n",
        "        return self.output\n",
        "\n",
        "    \"\"\"\n",
        "    Internal methods:\n",
        "    \"\"\"\n",
        "\n",
        "    def _jitter(self, color):\n",
        "        \"\"\"\n",
        "        Randomly modifies given color to produce a slightly different color than the color given.\n",
        "\n",
        "        Args:\n",
        "            color (tuple[double]): a tuple of 3 elements, containing the RGB values of the color\n",
        "                picked. The values in the list are in the [0.0, 1.0] range.\n",
        "\n",
        "        Returns:\n",
        "            jittered_color (tuple[double]): a tuple of 3 elements, containing the RGB values of the\n",
        "                color after being jittered. The values in the list are in the [0.0, 1.0] range.\n",
        "        \"\"\"\n",
        "        color = mplc.to_rgb(color)\n",
        "        vec = np.random.rand(3)\n",
        "        # better to do it in another color space\n",
        "        vec = vec / np.linalg.norm(vec) * 0.5\n",
        "        res = np.clip(vec + color, 0, 1)\n",
        "        return tuple(res)\n",
        "\n",
        "    def _create_grayscale_image(self, mask=None):\n",
        "        \"\"\"\n",
        "        Create a grayscale version of the original image.\n",
        "        The colors in masked area, if given, will be kept.\n",
        "        \"\"\"\n",
        "        img_bw = self.img.astype(\"f4\").mean(axis=2)\n",
        "        img_bw = np.stack([img_bw] * 3, axis=2)\n",
        "        if mask is not None:\n",
        "            img_bw[mask] = self.img[mask]\n",
        "        return img_bw\n",
        "\n",
        "    def _change_color_brightness(self, color, brightness_factor):\n",
        "        \"\"\"\n",
        "        Depending on the brightness_factor, gives a lighter or darker color i.e. a color with\n",
        "        less or more saturation than the original color.\n",
        "\n",
        "        Args:\n",
        "            color: color of the polygon. Refer to `matplotlib.colors` for a full list of\n",
        "                formats that are accepted.\n",
        "            brightness_factor (float): a value in [-1.0, 1.0] range. A lightness factor of\n",
        "                0 will correspond to no change, a factor in [-1.0, 0) range will result in\n",
        "                a darker color and a factor in (0, 1.0] range will result in a lighter color.\n",
        "\n",
        "        Returns:\n",
        "            modified_color (tuple[double]): a tuple containing the RGB values of the\n",
        "                modified color. Each value in the tuple is in the [0.0, 1.0] range.\n",
        "        \"\"\"\n",
        "        assert brightness_factor >= -1.0 and brightness_factor <= 1.0\n",
        "        color = mplc.to_rgb(color)\n",
        "        polygon_color = colorsys.rgb_to_hls(*mplc.to_rgb(color))\n",
        "        modified_lightness = polygon_color[1] + (brightness_factor * polygon_color[1])\n",
        "        modified_lightness = 0.0 if modified_lightness < 0.0 else modified_lightness\n",
        "        modified_lightness = 1.0 if modified_lightness > 1.0 else modified_lightness\n",
        "        modified_color = colorsys.hls_to_rgb(polygon_color[0], modified_lightness, polygon_color[2])\n",
        "        return tuple(np.clip(modified_color, 0.0, 1.0))\n",
        "\n",
        "    def _convert_boxes(self, boxes):\n",
        "        \"\"\"\n",
        "        Convert different format of boxes to an NxB array, where B = 4 or 5 is the box dimension.\n",
        "        \"\"\"\n",
        "        if isinstance(boxes, Boxes) or isinstance(boxes, RotatedBoxes):\n",
        "            return boxes.tensor.detach().numpy()\n",
        "        else:\n",
        "            return np.asarray(boxes)\n",
        "\n",
        "    def _convert_masks(self, masks_or_polygons):\n",
        "        \"\"\"\n",
        "        Convert different format of masks or polygons to a tuple of masks and polygons.\n",
        "\n",
        "        Returns:\n",
        "            list[GenericMask]:\n",
        "        \"\"\"\n",
        "\n",
        "        m = masks_or_polygons\n",
        "        if isinstance(m, PolygonMasks):\n",
        "            m = m.polygons\n",
        "        if isinstance(m, BitMasks):\n",
        "            m = m.tensor.numpy()\n",
        "        if isinstance(m, torch.Tensor):\n",
        "            m = m.numpy()\n",
        "        ret = []\n",
        "        for x in m:\n",
        "            if isinstance(x, GenericMask):\n",
        "                ret.append(x)\n",
        "            else:\n",
        "                ret.append(GenericMask(x, self.output.height, self.output.width))\n",
        "        return ret\n",
        "\n",
        "    def _draw_text_in_mask(self, binary_mask, text, color):\n",
        "        \"\"\"\n",
        "        Find proper places to draw text given a binary mask.\n",
        "        \"\"\"\n",
        "        # TODO sometimes drawn on wrong objects. the heuristics here can improve.\n",
        "        _num_cc, cc_labels, stats, centroids = cv2.connectedComponentsWithStats(binary_mask, 8)\n",
        "        if stats[1:, -1].size == 0:\n",
        "            return\n",
        "        largest_component_id = np.argmax(stats[1:, -1]) + 1\n",
        "\n",
        "        # draw text on the largest component, as well as other very large components.\n",
        "        for cid in range(1, _num_cc):\n",
        "            if cid == largest_component_id or stats[cid, -1] > _LARGE_MASK_AREA_THRESH:\n",
        "                # median is more stable than centroid\n",
        "                # center = centroids[largest_component_id]\n",
        "                center = np.median((cc_labels == cid).nonzero(), axis=1)[::-1]\n",
        "                self.draw_text(text, center, color=color)\n",
        "\n",
        "    def _convert_keypoints(self, keypoints):\n",
        "        if isinstance(keypoints, Keypoints):\n",
        "            keypoints = keypoints.tensor\n",
        "        keypoints = np.asarray(keypoints)\n",
        "        return keypoints\n",
        "\n",
        "    def get_output(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            output (VisImage): the image output containing the visualizations added\n",
        "            to the image.\n",
        "        \"\"\"\n",
        "        return self.output\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "gjMrkAx2Lr4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### video_visualizer.py in detectron2/utils"
      ],
      "metadata": {
        "id": "vDRY9m04Lxo7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "import numpy as np\n",
        "from typing import List\n",
        "import pycocotools.mask as mask_util\n",
        "\n",
        "from detectron2.structures import Instances\n",
        "from detectron2.utils.visualizer import (\n",
        "    ColorMode,\n",
        "    Visualizer,\n",
        "    _create_text_labels,\n",
        "    _PanopticPrediction,\n",
        ")\n",
        "\n",
        "from .colormap import random_color, random_colors\n",
        "\n",
        "\n",
        "class _DetectedInstance:\n",
        "    \"\"\"\n",
        "    Used to store data about detected objects in video frame,\n",
        "    in order to transfer color to objects in the future frames.\n",
        "\n",
        "    Attributes:\n",
        "        label (int):\n",
        "        bbox (tuple[float]):\n",
        "        mask_rle (dict):\n",
        "        color (tuple[float]): RGB colors in range (0, 1)\n",
        "        ttl (int): time-to-live for the instance. For example, if ttl=2,\n",
        "            the instance color can be transferred to objects in the next two frames.\n",
        "    \"\"\"\n",
        "\n",
        "    __slots__ = [\"label\", \"bbox\", \"mask_rle\", \"color\", \"ttl\"]\n",
        "\n",
        "    def __init__(self, label, bbox, mask_rle, color, ttl):\n",
        "        self.label = label\n",
        "        self.bbox = bbox\n",
        "        self.mask_rle = mask_rle\n",
        "        self.color = color\n",
        "        self.ttl = ttl\n",
        "\n",
        "\n",
        "class VideoVisualizer:\n",
        "    def __init__(self, metadata, instance_mode=ColorMode.IMAGE):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            metadata (MetadataCatalog): image metadata.\n",
        "        \"\"\"\n",
        "        self.metadata = metadata\n",
        "        self._old_instances = []\n",
        "        assert instance_mode in [\n",
        "            ColorMode.IMAGE,\n",
        "            ColorMode.IMAGE_BW,\n",
        "        ], \"Other mode not supported yet.\"\n",
        "        self._instance_mode = instance_mode\n",
        "        self._max_num_instances = self.metadata.get(\"max_num_instances\", 74)\n",
        "        self._assigned_colors = {}\n",
        "        self._color_pool = random_colors(self._max_num_instances, rgb=True, maximum=1)\n",
        "        self._color_idx_set = set(range(len(self._color_pool)))\n",
        "\n",
        "    def draw_instance_predictions(self, frame, predictions):\n",
        "        \"\"\"\n",
        "        Draw instance-level prediction results on an image.\n",
        "\n",
        "        Args:\n",
        "            frame (ndarray): an RGB image of shape (H, W, C), in the range [0, 255].\n",
        "            predictions (Instances): the output of an instance detection/segmentation\n",
        "                model. Following fields will be used to draw:\n",
        "                \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\" (or \"pred_masks_rle\").\n",
        "\n",
        "        Returns:\n",
        "            output (VisImage): image object with visualizations.\n",
        "        \"\"\"\n",
        "        frame_visualizer = Visualizer(frame, self.metadata)\n",
        "        num_instances = len(predictions)\n",
        "        if num_instances == 0:\n",
        "            return frame_visualizer.output\n",
        "\n",
        "        boxes = predictions.pred_boxes.tensor.numpy() if predictions.has(\"pred_boxes\") else None\n",
        "        scores = predictions.scores if predictions.has(\"scores\") else None\n",
        "        classes = predictions.pred_classes.numpy() if predictions.has(\"pred_classes\") else None\n",
        "        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n",
        "        colors = predictions.COLOR if predictions.has(\"COLOR\") else [None] * len(predictions)\n",
        "        periods = predictions.ID_period if predictions.has(\"ID_period\") else None\n",
        "        period_threshold = self.metadata.get(\"period_threshold\", 0)\n",
        "        visibilities = (\n",
        "            [True] * len(predictions)\n",
        "            if periods is None\n",
        "            else [x > period_threshold for x in periods]\n",
        "        )\n",
        "\n",
        "        if predictions.has(\"pred_masks\"):\n",
        "            masks = predictions.pred_masks\n",
        "            # mask IOU is not yet enabled\n",
        "            # masks_rles = mask_util.encode(np.asarray(masks.permute(1, 2, 0), order=\"F\"))\n",
        "            # assert len(masks_rles) == num_instances\n",
        "        else:\n",
        "            masks = None\n",
        "\n",
        "        if not predictions.has(\"COLOR\"):\n",
        "            if predictions.has(\"ID\"):\n",
        "                colors = self._assign_colors_by_id(predictions)\n",
        "            else:\n",
        "                # ToDo: clean old assign color method and use a default tracker to assign id\n",
        "                detected = [\n",
        "                    _DetectedInstance(classes[i], boxes[i], mask_rle=None, color=colors[i], ttl=8)\n",
        "                    for i in range(num_instances)\n",
        "                ]\n",
        "                colors = self._assign_colors(detected)\n",
        "\n",
        "        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n",
        "\n",
        "        if self._instance_mode == ColorMode.IMAGE_BW:\n",
        "            # any() returns uint8 tensor\n",
        "            frame_visualizer.output.reset_image(\n",
        "                frame_visualizer._create_grayscale_image(\n",
        "                    (masks.any(dim=0) > 0).numpy() if masks is not None else None\n",
        "                )\n",
        "            )\n",
        "            alpha = 0.3\n",
        "        else:\n",
        "            alpha = 0.5\n",
        "\n",
        "        labels = (\n",
        "            None\n",
        "            if labels is None\n",
        "            else [y[0] for y in filter(lambda x: x[1], zip(labels, visibilities))]\n",
        "        )  # noqa\n",
        "        assigned_colors = (\n",
        "            None\n",
        "            if colors is None\n",
        "            else [y[0] for y in filter(lambda x: x[1], zip(colors, visibilities))]\n",
        "        )  # noqa\n",
        "        frame_visualizer.overlay_instances(\n",
        "            boxes=None if masks is not None else boxes[visibilities],  # boxes are a bit distracting\n",
        "            masks=None if masks is None else masks[visibilities],\n",
        "            labels=labels,\n",
        "            keypoints=None if keypoints is None else keypoints[visibilities],\n",
        "            assigned_colors=assigned_colors,\n",
        "            alpha=alpha,\n",
        "        )\n",
        "\n",
        "        return frame_visualizer.output\n",
        "\n",
        "    def draw_sem_seg(self, frame, sem_seg, area_threshold=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sem_seg (ndarray or Tensor): semantic segmentation of shape (H, W),\n",
        "                each value is the integer label.\n",
        "            area_threshold (Optional[int]): only draw segmentations larger than the threshold\n",
        "        \"\"\"\n",
        "        # don't need to do anything special\n",
        "        frame_visualizer = Visualizer(frame, self.metadata)\n",
        "        frame_visualizer.draw_sem_seg(sem_seg, area_threshold=None)\n",
        "        return frame_visualizer.output\n",
        "\n",
        "    def draw_panoptic_seg_predictions(\n",
        "        self, frame, panoptic_seg, segments_info, area_threshold=None, alpha=0.5\n",
        "    ):\n",
        "        frame_visualizer = Visualizer(frame, self.metadata)\n",
        "        pred = _PanopticPrediction(panoptic_seg, segments_info, self.metadata)\n",
        "\n",
        "        if self._instance_mode == ColorMode.IMAGE_BW:\n",
        "            frame_visualizer.output.reset_image(\n",
        "                frame_visualizer._create_grayscale_image(pred.non_empty_mask())\n",
        "            )\n",
        "\n",
        "        # draw mask for all semantic segments first i.e. \"stuff\"\n",
        "        for mask, sinfo in pred.semantic_masks():\n",
        "            category_idx = sinfo[\"category_id\"]\n",
        "            try:\n",
        "                mask_color = [x / 255 for x in self.metadata.stuff_colors[category_idx]]\n",
        "            except AttributeError:\n",
        "                mask_color = None\n",
        "\n",
        "            frame_visualizer.draw_binary_mask(\n",
        "                mask,\n",
        "                color=mask_color,\n",
        "                text=self.metadata.stuff_classes[category_idx],\n",
        "                alpha=alpha,\n",
        "                area_threshold=area_threshold,\n",
        "            )\n",
        "\n",
        "        all_instances = list(pred.instance_masks())\n",
        "        if len(all_instances) == 0:\n",
        "            return frame_visualizer.output\n",
        "        # draw mask for all instances second\n",
        "        masks, sinfo = list(zip(*all_instances))\n",
        "        num_instances = len(masks)\n",
        "        masks_rles = mask_util.encode(\n",
        "            np.asarray(np.asarray(masks).transpose(1, 2, 0), dtype=np.uint8, order=\"F\")\n",
        "        )\n",
        "        assert len(masks_rles) == num_instances\n",
        "\n",
        "        category_ids = [x[\"category_id\"] for x in sinfo]\n",
        "        detected = [\n",
        "            _DetectedInstance(category_ids[i], bbox=None, mask_rle=masks_rles[i], color=None, ttl=8)\n",
        "            for i in range(num_instances)\n",
        "        ]\n",
        "        colors = self._assign_colors(detected)\n",
        "        # labels = [self.metadata.thing_classes[k] for k in category_ids]\n",
        "        labels = []\n",
        "        for k in category_ids:\n",
        "          if 0 <= k < len(self.metadata.thing_classes):\n",
        "            labels.append(self.metadata.thing_classes[k])\n",
        "          else:\n",
        "            labels.append(\"Unknown Label\")\n",
        "\n",
        "        frame_visualizer.overlay_instances(\n",
        "            boxes=None,\n",
        "            masks=masks,\n",
        "            labels=labels,\n",
        "            keypoints=None,\n",
        "            assigned_colors=colors,\n",
        "            alpha=alpha,\n",
        "        )\n",
        "        return frame_visualizer.output\n",
        "\n",
        "    def _assign_colors(self, instances):\n",
        "        \"\"\"\n",
        "        Naive tracking heuristics to assign same color to the same instance,\n",
        "        will update the internal state of tracked instances.\n",
        "\n",
        "        Returns:\n",
        "            list[tuple[float]]: list of colors.\n",
        "        \"\"\"\n",
        "\n",
        "        # Compute iou with either boxes or masks:\n",
        "        is_crowd = np.zeros((len(instances),), dtype=bool)\n",
        "        if instances[0].bbox is None:\n",
        "            assert instances[0].mask_rle is not None\n",
        "            # use mask iou only when box iou is None\n",
        "            # because box seems good enough\n",
        "            rles_old = [x.mask_rle for x in self._old_instances]\n",
        "            rles_new = [x.mask_rle for x in instances]\n",
        "            ious = mask_util.iou(rles_old, rles_new, is_crowd)\n",
        "            threshold = 0.5\n",
        "        else:\n",
        "            boxes_old = [x.bbox for x in self._old_instances]\n",
        "            boxes_new = [x.bbox for x in instances]\n",
        "            ious = mask_util.iou(boxes_old, boxes_new, is_crowd)\n",
        "            threshold = 0.6\n",
        "        if len(ious) == 0:\n",
        "            ious = np.zeros((len(self._old_instances), len(instances)), dtype=\"float32\")\n",
        "\n",
        "        # Only allow matching instances of the same label:\n",
        "        for old_idx, old in enumerate(self._old_instances):\n",
        "            for new_idx, new in enumerate(instances):\n",
        "                if old.label != new.label:\n",
        "                    ious[old_idx, new_idx] = 0\n",
        "\n",
        "        matched_new_per_old = np.asarray(ious).argmax(axis=1)\n",
        "        max_iou_per_old = np.asarray(ious).max(axis=1)\n",
        "\n",
        "        # Try to find match for each old instance:\n",
        "        extra_instances = []\n",
        "        for idx, inst in enumerate(self._old_instances):\n",
        "            if max_iou_per_old[idx] > threshold:\n",
        "                newidx = matched_new_per_old[idx]\n",
        "                if instances[newidx].color is None:\n",
        "                    instances[newidx].color = inst.color\n",
        "                    continue\n",
        "            # If an old instance does not match any new instances,\n",
        "            # keep it for the next frame in case it is just missed by the detector\n",
        "            inst.ttl -= 1\n",
        "            if inst.ttl > 0:\n",
        "                extra_instances.append(inst)\n",
        "\n",
        "        # Assign random color to newly-detected instances:\n",
        "        for inst in instances:\n",
        "            if inst.color is None:\n",
        "                inst.color = random_color(rgb=True, maximum=1)\n",
        "        self._old_instances = instances[:] + extra_instances\n",
        "        return [d.color for d in instances]\n",
        "\n",
        "    def _assign_colors_by_id(self, instances: Instances) -> List:\n",
        "        colors = []\n",
        "        untracked_ids = set(self._assigned_colors.keys())\n",
        "        for id in instances.ID:\n",
        "            if id in self._assigned_colors:\n",
        "                colors.append(self._color_pool[self._assigned_colors[id]])\n",
        "                untracked_ids.remove(id)\n",
        "            else:\n",
        "                assert (\n",
        "                    len(self._color_idx_set) >= 1\n",
        "                ), f\"Number of id exceeded maximum, \\\n",
        "                    max = {self._max_num_instances}\"\n",
        "                idx = self._color_idx_set.pop()\n",
        "                color = self._color_pool[idx]\n",
        "                self._assigned_colors[id] = idx\n",
        "                colors.append(color)\n",
        "        for id in untracked_ids:\n",
        "            self._color_idx_set.add(self._assigned_colors[id])\n",
        "            del self._assigned_colors[id]\n",
        "        return colors\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "7XNS0H1dL23M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmwaWqZq1GyQ"
      },
      "source": [
        "## Demo on Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlLlJxI5TgHF",
        "outputId": "bf6e3ec9-15cd-4161-a9d2-2269f8448982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/demo.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python demo.py --config-file ../configs/coco/panoptic-segmentation/maskdino_R50_bs16_50ep_3s_dowsample1_2048.yaml \\\n",
        "  --input ../../drive/MyDrive/panoptic_test/kitchen/*.jpeg --output pred_kitchen/ \\\n",
        "  --opts MODEL.WEIGHTS ../../drive/MyDrive/AAAAA/v3-66_model_0004999.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhP2dNBl1Rdq"
      },
      "source": [
        "## Demo on Videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4HYyls01LKv"
      },
      "outputs": [],
      "source": [
        "!python demo.py --config-file ../configs/coco/panoptic-segmentation/maskdino_R50_bs16_50ep_3s_dowsample1_2048.yaml \\\n",
        "  --video-input station.mp4 --output video_uji8.mov \\\n",
        "  --opts MODEL.WEIGHTS ../../drive/MyDrive/AAAAA/v3-66_model_0004999.pth"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "1MlxcYsLHBWe",
        "B6VgtE_8NhTr",
        "uWyM2PwUHMvX",
        "UL9L_W4WNYqy",
        "YtK5QMYY2ZIG",
        "8Vr-wAoh3Uxx",
        "iGDXz7cmHnpr",
        "-NH0C9KwNoqN",
        "K6OaMsJCJgQn",
        "y8I8tgAdvr4T",
        "pSyUMmCBPaTT",
        "cl1r2Ls9Pgmz",
        "ePnU6avPQVZn",
        "FfrzHfP6QlWy",
        "q_BWhYHJLJok",
        "c309K0Z0LUkR",
        "vDRY9m04Lxo7",
        "GmwaWqZq1GyQ",
        "RhP2dNBl1Rdq"
      ],
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}